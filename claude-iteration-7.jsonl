{"type":"system","subtype":"hook_response","session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"7ae6373d-acca-47d3-86b8-d0c70c859237","hook_name":"SessionStart:startup","hook_event":"SessionStart","stdout":"# Beads Workflow Context\n\n> **Context Recovery**: Run `bd prime` after compaction, clear, or new session\n> Hooks auto-call this in Claude Code when .beads/ detected\n\n# ðŸš¨ SESSION CLOSE PROTOCOL ðŸš¨\n\n**CRITICAL**: Before saying \"done\" or \"complete\", you MUST run this checklist:\n\n```\n[ ] 1. git status              (check what changed)\n[ ] 2. git add <files>         (stage code changes)\n[ ] 3. bd sync                 (commit beads changes)\n[ ] 4. git commit -m \"...\"     (commit code)\n[ ] 5. bd sync                 (commit any new beads changes)\n[ ] 6. git push                (push to remote)\n```\n\n**NEVER skip this.** Work is not done until pushed.\n\n## Core Rules\n- Track strategic work in beads (multi-session, dependencies, discovered work)\n- Use `bd create` for issues, TodoWrite for simple single-session execution\n- When in doubt, prefer bdâ€”persistence you don't need beats lost context\n- Git workflow: hooks auto-sync, run `bd sync` at session end\n- Session management: check `bd ready` for available work\n\n## Essential Commands\n\n### Finding Work\n- `bd ready` - Show issues ready to work (no blockers)\n- `bd list --status=open` - All open issues\n- `bd list --status=in_progress` - Your active work\n- `bd show <id>` - Detailed issue view with dependencies\n\n### Creating & Updating\n- `bd create --title=\"...\" --type=task|bug|feature --priority=2` - New issue\n  - Priority: 0-4 or P0-P4 (0=critical, 2=medium, 4=backlog). NOT \"high\"/\"medium\"/\"low\"\n- `bd update <id> --status=in_progress` - Claim work\n- `bd update <id> --assignee=username` - Assign to someone\n- `bd close <id>` - Mark complete\n- `bd close <id1> <id2> ...` - Close multiple issues at once (more efficient)\n- `bd close <id> --reason=\"explanation\"` - Close with reason\n- **Tip**: When creating multiple issues/tasks/epics, use parallel subagents for efficiency\n\n### Dependencies & Blocking\n- `bd dep add <issue> <depends-on>` - Add dependency (issue depends on depends-on)\n- `bd blocked` - Show all blocked issues\n- `bd show <id>` - See what's blocking/blocked by this issue\n\n### Sync & Collaboration\n- `bd sync` - Sync with git remote (run at session end)\n- `bd sync --status` - Check sync status without syncing\n\n### Project Health\n- `bd stats` - Project statistics (open/closed/blocked counts)\n- `bd doctor` - Check for issues (sync problems, missing hooks)\n\n## Common Workflows\n\n**Starting work:**\n```bash\nbd ready           # Find available work\nbd show <id>       # Review issue details\nbd update <id> --status=in_progress  # Claim it\n```\n\n**Completing work:**\n```bash\nbd close <id1> <id2> ...    # Close all completed issues at once\nbd sync                     # Push to remote\n```\n\n**Creating dependent work:**\n```bash\n# Run bd create commands in parallel (use subagents for many items)\nbd create --title=\"Implement feature X\" --type=feature\nbd create --title=\"Write tests for X\" --type=task\nbd dep add beads-yyy beads-xxx  # Tests depend on Feature (Feature blocks tests)\n```\n","stderr":"","exit_code":0}
{"type":"system","subtype":"init","cwd":"/Users/admin/MechInt","session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","tools":["Task","TaskOutput","Bash","Glob","Grep","ExitPlanMode","Read","Edit","Write","NotebookEdit","WebFetch","TodoWrite","WebSearch","KillShell","AskUserQuestion","Skill","EnterPlanMode"],"mcp_servers":[],"model":"claude-sonnet-4-5-20250929","permissionMode":"bypassPermissions","slash_commands":["compact","context","cost","init","pr-comments","release-notes","review","security-review"],"apiKeySource":"none","claude_code_version":"2.1.1","output_style":"default","agents":["Bash","general-purpose","statusline-setup","Explore","Plan"],"skills":[],"plugins":[{"name":"rust-analyzer-lsp","path":"/Users/admin/.claude/plugins/cache/claude-plugins-official/rust-analyzer-lsp/1.0.0"}],"uuid":"da4eae86-5025-4ee4-acd9-7ddcd1642dc3"}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01BW4m4ZAQyRytB5WxERQYhC","type":"message","role":"assistant","content":[{"type":"text","text":"I'll help you work on the next task from the BEADS queue. Let me start by checking what work is available."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":8502,"cache_read_input_tokens":14470,"cache_creation":{"ephemeral_5m_input_tokens":8502,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"d1d4bc91-302e-4c9c-bab1-da5392b4964b"}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01BW4m4ZAQyRytB5WxERQYhC","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01TzGA12Hoc4FW9z79r54Byd","name":"Bash","input":{"command":"bd ready --json","description":"Check available tasks in BEADS"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":8502,"cache_read_input_tokens":14470,"cache_creation":{"ephemeral_5m_input_tokens":8502,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"fc80f148-9e01-4ff6-a6c9-49e6a869a12d"}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01BW4m4ZAQyRytB5WxERQYhC","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01ArFSQFvb3xG3dERCRa3kKN","name":"Read","input":{"file_path":"/Users/admin/MechInt/PROGRESS.txt"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":8502,"cache_read_input_tokens":14470,"cache_creation":{"ephemeral_5m_input_tokens":8502,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"e1026d35-5bd4-494b-89b7-5c796b197bd7"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01TzGA12Hoc4FW9z79r54Byd","type":"tool_result","content":"[\n  {\n    \"id\": \"MechInt-rj9\",\n    \"title\": \"Validate grokking robustness with weight decay sweep\",\n    \"description\": \"NOTES.md Section 3.3 states 'If grokking fails, increase this value [weight decay]'. Need empirical validation of sensitivity.\\n\\nExperiment design:\\n- Baseline: weight_decay = 1.0 (current)\\n- Lower: weight_decay = 0.5 (should delay or prevent grokking)\\n- Higher: weight_decay = 2.0 (should accelerate grokking)\\n- Control: different random seeds (42, 123, 456)\\n\\nSuccess criteria:\\n- All runs with WD=1.0 should grok by epoch 10k\\n- WD=0.5 may fail to grok by epoch 20k\\n- WD=2.0 should grok earlier (~epoch 5k)\\n\\nImplementation:\\n- Create sweep_weight_decay.sh script\\n- Run 3 seeds Ã— 3 WD values = 9 total runs\\n- Log grok epoch for each run\\n- Generate comparison plots\\n- Document findings\\n\\nThis is LOW PRIORITY - only run after confirming baseline groks.\\nEstimated compute: 9 runs Ã— 10-15 hours = 90-135 GPU hours\\n\\nLocation: new scripts/sweep_weight_decay.sh\\nReference: NOTES.md Section 3.3 (weight decay sensitivity)\",\n    \"status\": \"open\",\n    \"priority\": 2,\n    \"issue_type\": \"task\",\n    \"created_at\": \"2026-01-07T23:43:07.618786Z\",\n    \"created_by\": \"admin\",\n    \"updated_at\": \"2026-01-07T23:43:07.618786Z\",\n    \"labels\": [\n      \"verify-2\"\n    ]\n  },\n  {\n    \"id\": \"MechInt-7ab\",\n    \"title\": \"Clean up dead code warnings in analysis modules\",\n    \"description\": \"Compilation generates 72 dead code warnings from unused analysis/verification functions. After integrating core functionality (other verify-2 issues), audit and clean up.\\n\\nCurrent warnings:\\n- src/analysis.rs: 28 warnings (FFT functions, embedding analysis)\\n- src/verify.rs: 27 warnings (verification methods)\\n- src/plotting.rs: 16 warnings (visualization functions)  \\n- src/checkpoint.rs: 3 warnings (checkpoint utilities)\\n\\nAction after P0/P1 issues complete:\\n1. Identify which functions are now used\\n2. Remove genuinely unused code OR\\n3. Document why keeping for future use\\n4. Add #[allow(dead_code)] with justification if keeping\\n\\nDo NOT do this now - many 'unused' functions will be integrated by P0 issues.\\n\\nLocation: src/analysis.rs, src/verify.rs, src/plotting.rs, src/checkpoint.rs\\nReference: Cargo build output (72 warnings)\",\n    \"status\": \"open\",\n    \"priority\": 2,\n    \"issue_type\": \"task\",\n    \"created_at\": \"2026-01-07T23:43:08.348768Z\",\n    \"created_by\": \"admin\",\n    \"updated_at\": \"2026-01-07T23:43:08.348768Z\",\n    \"labels\": [\n      \"verify-2\"\n    ]\n  }\n]","is_error":false}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"464c39dc-3372-40e3-be82-2d4cb06842db","tool_use_result":{"stdout":"[\n  {\n    \"id\": \"MechInt-rj9\",\n    \"title\": \"Validate grokking robustness with weight decay sweep\",\n    \"description\": \"NOTES.md Section 3.3 states 'If grokking fails, increase this value [weight decay]'. Need empirical validation of sensitivity.\\n\\nExperiment design:\\n- Baseline: weight_decay = 1.0 (current)\\n- Lower: weight_decay = 0.5 (should delay or prevent grokking)\\n- Higher: weight_decay = 2.0 (should accelerate grokking)\\n- Control: different random seeds (42, 123, 456)\\n\\nSuccess criteria:\\n- All runs with WD=1.0 should grok by epoch 10k\\n- WD=0.5 may fail to grok by epoch 20k\\n- WD=2.0 should grok earlier (~epoch 5k)\\n\\nImplementation:\\n- Create sweep_weight_decay.sh script\\n- Run 3 seeds Ã— 3 WD values = 9 total runs\\n- Log grok epoch for each run\\n- Generate comparison plots\\n- Document findings\\n\\nThis is LOW PRIORITY - only run after confirming baseline groks.\\nEstimated compute: 9 runs Ã— 10-15 hours = 90-135 GPU hours\\n\\nLocation: new scripts/sweep_weight_decay.sh\\nReference: NOTES.md Section 3.3 (weight decay sensitivity)\",\n    \"status\": \"open\",\n    \"priority\": 2,\n    \"issue_type\": \"task\",\n    \"created_at\": \"2026-01-07T23:43:07.618786Z\",\n    \"created_by\": \"admin\",\n    \"updated_at\": \"2026-01-07T23:43:07.618786Z\",\n    \"labels\": [\n      \"verify-2\"\n    ]\n  },\n  {\n    \"id\": \"MechInt-7ab\",\n    \"title\": \"Clean up dead code warnings in analysis modules\",\n    \"description\": \"Compilation generates 72 dead code warnings from unused analysis/verification functions. After integrating core functionality (other verify-2 issues), audit and clean up.\\n\\nCurrent warnings:\\n- src/analysis.rs: 28 warnings (FFT functions, embedding analysis)\\n- src/verify.rs: 27 warnings (verification methods)\\n- src/plotting.rs: 16 warnings (visualization functions)  \\n- src/checkpoint.rs: 3 warnings (checkpoint utilities)\\n\\nAction after P0/P1 issues complete:\\n1. Identify which functions are now used\\n2. Remove genuinely unused code OR\\n3. Document why keeping for future use\\n4. Add #[allow(dead_code)] with justification if keeping\\n\\nDo NOT do this now - many 'unused' functions will be integrated by P0 issues.\\n\\nLocation: src/analysis.rs, src/verify.rs, src/plotting.rs, src/checkpoint.rs\\nReference: Cargo build output (72 warnings)\",\n    \"status\": \"open\",\n    \"priority\": 2,\n    \"issue_type\": \"task\",\n    \"created_at\": \"2026-01-07T23:43:08.348768Z\",\n    \"created_by\": \"admin\",\n    \"updated_at\": \"2026-01-07T23:43:08.348768Z\",\n    \"labels\": [\n      \"verify-2\"\n    ]\n  }\n]","stderr":"","interrupted":false,"isImage":false}}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01ArFSQFvb3xG3dERCRa3kKN","type":"tool_result","content":"     1â†’## [2026-01-07 21:21:38] Completed MechInt-mhp\n     2â†’- Added restricted-loss FFT filtering helpers and dataset loss evaluation\n     3â†’- Added restricted-loss verification logic with synthetic tests\n     4â†’- Added token-weight forward path for filtered embeddings and ignored git scratch dir\n     5â†’- Suggestions: integrate restricted/excluded loss into training metrics when ready\n     6â†’\n     7â†’## [2026-01-07 21:40:20] Completed MechInt-7ak\n     8â†’- Added excluded-loss computation (top-k frequencies removed) and saved excluded loss history\n     9â†’- Added excluded-loss spike verification and synthetic tests\n    10â†’- Logged excluded loss for initial/grokking/final checkpoints when available\n    11â†’- Suggestions: tune excluded-loss spike thresholds once real training metrics are available\n    12â†’\n    13â†’## [2026-01-07 21:47:45] Completed MechInt-fej\n    14â†’- Added Fourier-plane projection and clock-geometry verification for token embeddings\n    15â†’- Added unit tests for clean-circle pass and wrong-frequency failure cases\n    16â†’- Suggestions: wire embedding clock verification into any future checkpoint analysis pipeline\n    17â†’\n    18â†’## [2026-01-07 22:10:05] Completed MechInt-4uj\n    19â†’- Matched transformer defaults to the 1-layer spec and removed LayerNorm usage\n    20â†’- Added causal-mask wiring plus unit tests for config, embeddings, biases, activation, and masking\n    21â†’- Suggestions: update training/spec docs if any remaining references assume 2 layers\n    22â†’\n    23â†’## [2026-01-07 22:16:47] Completed MechInt-abt\n    24â†’- Added MLP post-ReLU activation capture and rectified-sine verification via FFT/correlation metrics\n    25â†’- Added synthetic verification tests for rectified sine pass and noise failure cases\n    26â†’- Suggestions: wire MLP activation verification into training analysis runs post-grok\n    27â†’\n    28â†’## [2026-01-07 22:19:58] Completed MechInt-e78\n    29â†’- Added grokking-spec validation for training hyperparameters and wired it into startup\n    30â†’- Expanded training config tests to flag deviations from required optimizer and schedule values\n    31â†’- Suggestions: extend validation to any CLI/config overrides if added later\n    32â†’\n    33â†’## [2026-01-07 22:25:27] Completed MechInt-6z5\n    34â†’- Added accuracy jump validation around the grokking transition using validation accuracy windows\n    35â†’- Updated grokking verification config with an accuracy window and expanded unit tests\n    36â†’- Suggestions: tune accuracy window/tolerance if real runs show gradual pre-transition creep\n    37â†’\n    38â†’## [2026-01-07 22:32:47] Completed MechInt-bq4\n    39â†’- Added pairwise manifold circularity verification using post-ReLU MLP activations\n    40â†’- Loaded pre/post checkpoints to compare ring emergence and logged verification output\n    41â†’- Added synthetic unit tests for pass/fail circularity transitions\n    42â†’## [2026-01-07 22:47:30] Completed MechInt-pnn\n    43â†’- Added diagonal ridge verification tests for constructive interference surfaces\n    44â†’- Implemented 5 comprehensive test cases covering diagonal, anti-diagonal, axis-aligned, flat, and noise patterns\n    45â†’- Tests confirm metric correctly detects diagonal periodicity stronger than axis-aligned periodicity\n    46â†’- Suggestions: apply this verification to post-grokking MLP neuron activations to validate interference hypothesis\n    47â†’\n    48â†’## [2026-01-07 23:05:00] Completed MechInt-4qy\n    49â†’- Added snake curve shape verification to validate grokking phenomenon on log-scale plots\n    50â†’- Implemented verify_snake_curve_shape() with checks for early train convergence, val plateau, delayed val jump\n    51â†’- Added 5 comprehensive test cases covering valid grokking curves and common failure modes\n    52â†’- Verified log-scale plotting functions (plot_loss_history_dual_logscale, plot_accuracy_history_logscale, plot_grokking_combined_logscale) use .log_scale() API correctly\n    53â†’- All 44 library tests pass including new snake curve verification tests\n    54â†’- Suggestions: apply snake curve verification to actual training runs to confirm characteristic grokking shape\n    55â†’\n    56â†’## [2026-01-07 23:30:00] Completed MechInt-coo\n    57â†’- Implemented weight norm decay verification for grokking phenomenon\n    58â†’- Added compute_model_weight_norm() to compute L2 norm of token embeddings\n    59â†’- Added verify_weight_norm_decay() with monotonic decrease and relative threshold checks\n    60â†’- Added 5 comprehensive unit tests covering pass/fail scenarios and edge cases\n    61â†’- All 49 library tests pass\n    62â†’- Suggestions: wire weight norm tracking into training loop to capture norms at each checkpoint\n    63â†’\n    64â†’## [2026-01-07 23:45:00] Completed MechInt-b1p\n    65â†’- Added FFT dominant frequency verification for embedding analysis\n    66â†’- Implemented verify_fft_dominant_frequencies_from_embeddings() with 3 key metrics:\n    67â†’  1. Modulus frequency fraction (checks if p=113 appears significantly)\n    68â†’  2. Normalized spectrum entropy (detects white noise vs structured peaks)\n    69â†’  3. Peak-to-median ratio (ensures strong dominant frequencies)\n    70â†’- Added FFTDominantFrequencyConfig and FFTDominantFrequencyReport structs\n    71â†’- Added 5 comprehensive unit tests covering pass/fail scenarios\n    72â†’- All 54 library tests pass\n    73â†’- Suggestions: wire this verification into post-grokking checkpoint analysis to validate Fourier structure hypothesis\n    74â†’\n    75â†’## [2026-01-08 00:05:00] Completed MechInt-ybn\n    76â†’- Determined Section 5.5 (6D manifold visualization) is OUT OF SCOPE for this repository\n    77â†’- Section 5.5 describes Claude 3.5 Haiku internal representations for character/line counting\n    78â†’- This repo focuses on modular arithmetic grokking (Power et al. 2022)\n    79â†’- Requires proprietary model access and different task domain (NLP vs arithmetic)\n    80â†’- All other spec visualizations (5.1-5.4) are implemented and in scope\n    81â†’- Decision documented in docs/section_5.5_scope_decision.md for future reference\n    82â†’- Suggestions: 6D manifold analysis would require separate repo with Claude API/model access\n    83â†’\n    84â†’## [2026-01-08 00:15:00] Completed MechInt-6cn\n    85â†’- Verified burn 0.20.0-pre.6 is the latest available version\n    86â†’- Updated all compatible dependencies (35 packages) via cargo update\n    87â†’- Confirmed all 54 library tests pass after updates\n    88â†’- Code already uses Burn 0.20 API correctly (TrainStep/ValidStep, ClassificationOutput, Module traits)\n    89â†’- No API migration needed - implementation is current and correct\n    90â†’- Suggestions: next task is MechInt-jzl (transformer alignment), but it may already be complete\n    91â†’\n    92â†’## [2026-01-08 00:30:00] Completed MechInt-jzl\n    93â†’- Verified transformer model is fully aligned with Burn 0.20 Module/nn API\n    94â†’- Confirmed all model components use correct patterns:\n    95â†’  - Module derive macro on Transformer, TransformerLayer, and MLP structs\n    96â†’  - Proper nn imports: Embedding, Linear, MultiHeadAttention, Relu from burn::nn\n    97â†’  - Config pattern with .init() method for model creation\n    98â†’  - TrainStep/ValidStep traits implemented correctly in training.rs\n    99â†’  - ClassificationOutput used for forward_classification\n   100â†’- All 54 library tests pass, no deprecated API warnings from clippy\n   101â†’- Library and binaries compile successfully with Burn 0.20.0-pre.6\n   102â†’- Suggestions: check bd ready for next available task\n   103â†’## [2026-01-08 00:45:00] Completed MechInt-3is\n   104â†’- Integrated restricted/excluded loss metrics into training loop to detect internal restructuring\n   105â†’- Added RestrictedLossHistory struct and save/load methods (mirrors ExcludedLossHistory)\n   106â†’- Added RestrictedLossDropConfig and verify_restricted_loss_drop function in verify.rs\n   107â†’- Metrics computed at key checkpoints: epoch 100 (~step 1k), epoch 500 (~step 5k), grokking, and final\n   108â†’- Both restricted and excluded loss saved to JSON files in artifacts/\n   109â†’- Added verification logic: restricted loss should drop BEFORE validation accuracy improves\n   110â†’- Added verification logic: excluded loss should spike as model switches to Fourier algorithm\n   111â†’- Added 2 comprehensive unit tests for restricted loss drop verification\n   112â†’- All 56 library tests pass (added 2 new tests)\n   113â†’- Suggestions: next tasks are visualization pipeline (MechInt-c0w) and automatic grokking detection (MechInt-391)\n   114â†’\n   115â†’## [2026-01-08 01:15:00] Completed MechInt-391\n   116â†’- Implemented automatic grokking phase transition detection with rolling window analysis\n   117â†’- Added detect_grokking_transition() function in main.rs with configurable thresholds:\n   118â†’  - MIN_PLATEAU_STEP=1000: ensures detection only after memorization phase\n   119â†’  - MIN_ACCURACY_JUMP=20%: requires significant spike (not gradual improvement)\n   120â†’  - WINDOW_SIZE=500: lookback window for baseline calculation\n   121â†’  - TARGET_VAL_ACC=90%: target validation accuracy threshold\n   122â†’- Detection identifies characteristic sudden spike from plateau (~1%) to high accuracy (>90%)\n   123â†’- Integrated into post-training analysis with detailed logging showing:\n   124â†’  - Exact epoch and step when grokking occurs\n   125â†’  - Baseline validation accuracy (plateau level)\n   126â†’  - Post-grok validation accuracy\n   127â†’  - Magnitude of accuracy jump\n   128â†’- Automatic checkpoint saving at detected grokking point\n   129â†’- Fallback to simple threshold detection if no clear spike found\n   130â†’- Added 3 comprehensive unit tests (total 59 tests, all passing):\n   131â†’  - test_detect_grokking_transition_finds_spike: validates detection works\n   132â†’  - test_detect_grokking_transition_no_spike: prevents false positives on gradual improvement\n   133â†’  - test_detect_grokking_transition_early_spike_ignored: enforces plateau requirement\n   134â†’- Added tempfile dev dependency for test isolation\n   135â†’- Expected to detect grokking around step ~7,000 (epoch ~700) in real training runs\n   136â†’- Suggestions: next tasks are visualization pipeline (MechInt-c0w) and weight norm tracking (MechInt-fi4)\n   137â†’## [2026-01-08 01:30:00] Completed MechInt-fi4\n   138â†’- Integrated weight norm tracking at key checkpoints to monitor weight decay mechanism\n   139â†’- Compute model L2 norm (token embeddings) at: step 0, epochs 100/500, grokking transition, final\n   140â†’- Save weight norm history to artifacts/weight_norm_history.json\n   141â†’- Added weight norm decay verification with default config (10% relative decrease threshold)\n   142â†’- Verification confirms weight decay drives transition: high-norm memorization â†’ gradual decay â†’ sharp drop at grok â†’ low-norm generalization\n   143â†’- Expected timeline: \n   144â†’  * Step 0-1k (epoch 0-100): High norms during memorization phase\n   145â†’  * Step 1k-7k (epoch 100-700): Gradual norm decrease during plateau\n   146â†’  * Step ~7k (epoch ~700): Sharp norm drop at grokking transition\n   147â†’  * Step 7k+ (epoch 700+): Low norm stabilization in generalizing circuit\n   148â†’- All 56 library tests pass, code compiles cleanly\n   149â†’- Suggestions: next task is visualization pipeline (MechInt-c0w) to plot weight norm evolution alongside loss/accuracy curves\n   150â†’## [2026-01-08 01:45:00] Completed MechInt-c0w\n   151â†’- Integrated visualization pipeline to automatically generate plots at key checkpoints\n   152â†’- Added embedding grid visualizations (3x3) for: initial, memorization (e100), plateau (e500), grokking, and final states\n   153â†’- Organized all visualizations into artifacts/visualizations/ directory with sectioned logging\n   154â†’- Snake curves (log-scale) automatically generated to show characteristic grokking shape\n   155â†’- FFT analysis plots (frequency distribution, spectra) generated for post-training analysis\n   156â†’- Embedding visualizations reveal circular geometry emergence at grokking transition\n   157â†’- Added generate_checkpoint_embeddings_visualization() helper function\n   158â†’- Uses extract_all_embeddings() and select_interesting_dimensions() for automatic dimension selection\n   159â†’- Comprehensive error handling with graceful fallback when checkpoints unavailable\n   160â†’- All 59 tests pass, code compiles cleanly\n   161â†’- Suggestions: next task is likely MechInt-l1a (core verification) or MechInt-t75 (checkpoint management)\n   162â†’## [2026-01-08 02:00:00] Completed MechInt-t75\n   163â†’- Implemented labeled checkpoint management at key training phases for comprehensive grokking analysis\n   164â†’- Added checkpoint saving at epoch 100 (~step 1,000): memorization_e100\n   165â†’  - Captures end of memorization phase when train accuracy reaches 100%\n   166â†’- Added checkpoint saving at epoch 500 (~step 5,000): plateau_e500\n   167â†’  - Captures deep plateau phase when validation accuracy frozen at ~1%\n   168â†’- Added checkpoint saving at epoch 1500 (~step 15,000): postgrok_e1500\n   169â†’  - Captures post-grokking solidification phase after generalization occurs\n   170â†’- Existing grokking checkpoint (auto-detected at ~epoch 700) and final checkpoint preserved\n   171â†’- All checkpoints saved to artifacts/checkpoint_labeled/ with descriptive labels\n   172â†’- Conditional logic prevents errors on short training runs (checks num_epochs threshold)\n   173â†’- Infrastructure uses existing copy_checkpoint_set() to save model, optimizer, and scheduler state\n   174â†’- Enables post-hoc analysis: restricted/excluded loss, weight norm evolution, embedding geometry, MLP activations\n   175â†’- All 59 tests pass, code compiles cleanly\n   176â†’- Suggestions: next tasks are MechInt-l1a (verification documentation) or MechInt-at3 (3D visualization)\n   177â†’\n   178â†’## [2026-01-08 00:30:00] Completed MechInt-l1a\n   179â†’- Verified core implementation is 100% specification-compliant through comprehensive code review and testing\n   180â†’- Confirmed all 56 library tests pass (grokking detection, snake curve, restricted/excluded loss, weight norm decay, FFT analysis, embedding geometry, MLP activations)\n   181â†’- Validated implementation against NOTES.md specification:\n   182â†’  * Dataset: modulus=113, train_fraction=0.4, full 12,769 sample space\n   183â†’  * Model: 1-layer transformer, d_model=128, n_heads=4, d_ff=512, ReLU activation, no LayerNorm\n   184â†’  * Optimizer: AdamW (Î²1=0.9, Î²2=0.98, lr=1e-3, weight_decay=1.0)\n   185â†’  * Training: batch_size=512, num_epochs=3000 (~30k steps), seed=42\n   186â†’- Confirmed all advanced features integrated and tested:\n   187â†’  * Automatic grokking detection at expected step ~7k (epoch ~700)\n   188â†’  * Checkpoint management at key phases (memorization, plateau, grokking, post-grok, final)\n   189â†’  * Visualization pipeline (snake curves, embeddings, FFT analysis)\n   190â†’  * Restricted/excluded loss tracking and verification\n   191â†’  * Weight norm decay monitoring and verification\n   192â†’- Implementation is production-ready; empirical validation requires multi-hour training run\n   193â†’- Suggestions: Next tasks are MechInt-at3 (3D surface visualization) or run full training experiment for empirical validation\n   194â†’\n   195â†’## [2026-01-08 00:38:00] Completed MechInt-at3\n   196â†’- Implemented 3D activation surface visualization with JSON export and Python Plotly visualization script\n   197â†’- Added src/export.rs module with ActivationSurface struct for JSON serialization and deserialization\n   198â†’- Created scripts/visualize_activation_surface.py for interactive 3D surface plots and diagonal FFT analysis\n   199â†’- Integrated activation surface export into main analysis pipeline (exports 5 neuron surfaces from post-grokking checkpoint)\n   200â†’- Added comprehensive documentation in scripts/README.md with setup instructions and theoretical background\n   201â†’- Added scripts/requirements.txt for Python dependencies (numpy, plotly)\n   202â†’- Python script generates:\n   203â†’  * Interactive 3D surface plots showing activation values across all (x,y) input pairs\n   204â†’  * 2D heatmap view for pattern analysis\n   205â†’  * Diagonal periodicity analysis via FFT to quantify Fourier structure\n   206â†’  * Summary statistics: diagonal mean/std/range, dominant frequency, periodicity strength\n   207â†’- Exports activation surfaces at epoch 1500 (post-grokking) to capture mature Fourier algorithm structure\n   208â†’- All 58 library tests pass (added 2 tests for ActivationSurface save/load functionality)\n   209â†’- Reveals characteristic \"corrugated iron\" diagonal wave structure proving Fourier algorithm discovery\n   210â†’- Usage: cargo run --release (exports JSON), then python scripts/visualize_activation_surface.py artifacts/activation_surface_neuron_0.json\n   211â†’- Suggestions: Next tasks are MechInt-rj9 (weight decay sweep) or MechInt-7ab (dead code cleanup)\n   212â†’\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"7b7cb8f4-640f-4089-8c11-4c224fbf6b37","tool_use_result":{"type":"text","file":{"filePath":"/Users/admin/MechInt/PROGRESS.txt","content":"## [2026-01-07 21:21:38] Completed MechInt-mhp\n- Added restricted-loss FFT filtering helpers and dataset loss evaluation\n- Added restricted-loss verification logic with synthetic tests\n- Added token-weight forward path for filtered embeddings and ignored git scratch dir\n- Suggestions: integrate restricted/excluded loss into training metrics when ready\n\n## [2026-01-07 21:40:20] Completed MechInt-7ak\n- Added excluded-loss computation (top-k frequencies removed) and saved excluded loss history\n- Added excluded-loss spike verification and synthetic tests\n- Logged excluded loss for initial/grokking/final checkpoints when available\n- Suggestions: tune excluded-loss spike thresholds once real training metrics are available\n\n## [2026-01-07 21:47:45] Completed MechInt-fej\n- Added Fourier-plane projection and clock-geometry verification for token embeddings\n- Added unit tests for clean-circle pass and wrong-frequency failure cases\n- Suggestions: wire embedding clock verification into any future checkpoint analysis pipeline\n\n## [2026-01-07 22:10:05] Completed MechInt-4uj\n- Matched transformer defaults to the 1-layer spec and removed LayerNorm usage\n- Added causal-mask wiring plus unit tests for config, embeddings, biases, activation, and masking\n- Suggestions: update training/spec docs if any remaining references assume 2 layers\n\n## [2026-01-07 22:16:47] Completed MechInt-abt\n- Added MLP post-ReLU activation capture and rectified-sine verification via FFT/correlation metrics\n- Added synthetic verification tests for rectified sine pass and noise failure cases\n- Suggestions: wire MLP activation verification into training analysis runs post-grok\n\n## [2026-01-07 22:19:58] Completed MechInt-e78\n- Added grokking-spec validation for training hyperparameters and wired it into startup\n- Expanded training config tests to flag deviations from required optimizer and schedule values\n- Suggestions: extend validation to any CLI/config overrides if added later\n\n## [2026-01-07 22:25:27] Completed MechInt-6z5\n- Added accuracy jump validation around the grokking transition using validation accuracy windows\n- Updated grokking verification config with an accuracy window and expanded unit tests\n- Suggestions: tune accuracy window/tolerance if real runs show gradual pre-transition creep\n\n## [2026-01-07 22:32:47] Completed MechInt-bq4\n- Added pairwise manifold circularity verification using post-ReLU MLP activations\n- Loaded pre/post checkpoints to compare ring emergence and logged verification output\n- Added synthetic unit tests for pass/fail circularity transitions\n## [2026-01-07 22:47:30] Completed MechInt-pnn\n- Added diagonal ridge verification tests for constructive interference surfaces\n- Implemented 5 comprehensive test cases covering diagonal, anti-diagonal, axis-aligned, flat, and noise patterns\n- Tests confirm metric correctly detects diagonal periodicity stronger than axis-aligned periodicity\n- Suggestions: apply this verification to post-grokking MLP neuron activations to validate interference hypothesis\n\n## [2026-01-07 23:05:00] Completed MechInt-4qy\n- Added snake curve shape verification to validate grokking phenomenon on log-scale plots\n- Implemented verify_snake_curve_shape() with checks for early train convergence, val plateau, delayed val jump\n- Added 5 comprehensive test cases covering valid grokking curves and common failure modes\n- Verified log-scale plotting functions (plot_loss_history_dual_logscale, plot_accuracy_history_logscale, plot_grokking_combined_logscale) use .log_scale() API correctly\n- All 44 library tests pass including new snake curve verification tests\n- Suggestions: apply snake curve verification to actual training runs to confirm characteristic grokking shape\n\n## [2026-01-07 23:30:00] Completed MechInt-coo\n- Implemented weight norm decay verification for grokking phenomenon\n- Added compute_model_weight_norm() to compute L2 norm of token embeddings\n- Added verify_weight_norm_decay() with monotonic decrease and relative threshold checks\n- Added 5 comprehensive unit tests covering pass/fail scenarios and edge cases\n- All 49 library tests pass\n- Suggestions: wire weight norm tracking into training loop to capture norms at each checkpoint\n\n## [2026-01-07 23:45:00] Completed MechInt-b1p\n- Added FFT dominant frequency verification for embedding analysis\n- Implemented verify_fft_dominant_frequencies_from_embeddings() with 3 key metrics:\n  1. Modulus frequency fraction (checks if p=113 appears significantly)\n  2. Normalized spectrum entropy (detects white noise vs structured peaks)\n  3. Peak-to-median ratio (ensures strong dominant frequencies)\n- Added FFTDominantFrequencyConfig and FFTDominantFrequencyReport structs\n- Added 5 comprehensive unit tests covering pass/fail scenarios\n- All 54 library tests pass\n- Suggestions: wire this verification into post-grokking checkpoint analysis to validate Fourier structure hypothesis\n\n## [2026-01-08 00:05:00] Completed MechInt-ybn\n- Determined Section 5.5 (6D manifold visualization) is OUT OF SCOPE for this repository\n- Section 5.5 describes Claude 3.5 Haiku internal representations for character/line counting\n- This repo focuses on modular arithmetic grokking (Power et al. 2022)\n- Requires proprietary model access and different task domain (NLP vs arithmetic)\n- All other spec visualizations (5.1-5.4) are implemented and in scope\n- Decision documented in docs/section_5.5_scope_decision.md for future reference\n- Suggestions: 6D manifold analysis would require separate repo with Claude API/model access\n\n## [2026-01-08 00:15:00] Completed MechInt-6cn\n- Verified burn 0.20.0-pre.6 is the latest available version\n- Updated all compatible dependencies (35 packages) via cargo update\n- Confirmed all 54 library tests pass after updates\n- Code already uses Burn 0.20 API correctly (TrainStep/ValidStep, ClassificationOutput, Module traits)\n- No API migration needed - implementation is current and correct\n- Suggestions: next task is MechInt-jzl (transformer alignment), but it may already be complete\n\n## [2026-01-08 00:30:00] Completed MechInt-jzl\n- Verified transformer model is fully aligned with Burn 0.20 Module/nn API\n- Confirmed all model components use correct patterns:\n  - Module derive macro on Transformer, TransformerLayer, and MLP structs\n  - Proper nn imports: Embedding, Linear, MultiHeadAttention, Relu from burn::nn\n  - Config pattern with .init() method for model creation\n  - TrainStep/ValidStep traits implemented correctly in training.rs\n  - ClassificationOutput used for forward_classification\n- All 54 library tests pass, no deprecated API warnings from clippy\n- Library and binaries compile successfully with Burn 0.20.0-pre.6\n- Suggestions: check bd ready for next available task\n## [2026-01-08 00:45:00] Completed MechInt-3is\n- Integrated restricted/excluded loss metrics into training loop to detect internal restructuring\n- Added RestrictedLossHistory struct and save/load methods (mirrors ExcludedLossHistory)\n- Added RestrictedLossDropConfig and verify_restricted_loss_drop function in verify.rs\n- Metrics computed at key checkpoints: epoch 100 (~step 1k), epoch 500 (~step 5k), grokking, and final\n- Both restricted and excluded loss saved to JSON files in artifacts/\n- Added verification logic: restricted loss should drop BEFORE validation accuracy improves\n- Added verification logic: excluded loss should spike as model switches to Fourier algorithm\n- Added 2 comprehensive unit tests for restricted loss drop verification\n- All 56 library tests pass (added 2 new tests)\n- Suggestions: next tasks are visualization pipeline (MechInt-c0w) and automatic grokking detection (MechInt-391)\n\n## [2026-01-08 01:15:00] Completed MechInt-391\n- Implemented automatic grokking phase transition detection with rolling window analysis\n- Added detect_grokking_transition() function in main.rs with configurable thresholds:\n  - MIN_PLATEAU_STEP=1000: ensures detection only after memorization phase\n  - MIN_ACCURACY_JUMP=20%: requires significant spike (not gradual improvement)\n  - WINDOW_SIZE=500: lookback window for baseline calculation\n  - TARGET_VAL_ACC=90%: target validation accuracy threshold\n- Detection identifies characteristic sudden spike from plateau (~1%) to high accuracy (>90%)\n- Integrated into post-training analysis with detailed logging showing:\n  - Exact epoch and step when grokking occurs\n  - Baseline validation accuracy (plateau level)\n  - Post-grok validation accuracy\n  - Magnitude of accuracy jump\n- Automatic checkpoint saving at detected grokking point\n- Fallback to simple threshold detection if no clear spike found\n- Added 3 comprehensive unit tests (total 59 tests, all passing):\n  - test_detect_grokking_transition_finds_spike: validates detection works\n  - test_detect_grokking_transition_no_spike: prevents false positives on gradual improvement\n  - test_detect_grokking_transition_early_spike_ignored: enforces plateau requirement\n- Added tempfile dev dependency for test isolation\n- Expected to detect grokking around step ~7,000 (epoch ~700) in real training runs\n- Suggestions: next tasks are visualization pipeline (MechInt-c0w) and weight norm tracking (MechInt-fi4)\n## [2026-01-08 01:30:00] Completed MechInt-fi4\n- Integrated weight norm tracking at key checkpoints to monitor weight decay mechanism\n- Compute model L2 norm (token embeddings) at: step 0, epochs 100/500, grokking transition, final\n- Save weight norm history to artifacts/weight_norm_history.json\n- Added weight norm decay verification with default config (10% relative decrease threshold)\n- Verification confirms weight decay drives transition: high-norm memorization â†’ gradual decay â†’ sharp drop at grok â†’ low-norm generalization\n- Expected timeline: \n  * Step 0-1k (epoch 0-100): High norms during memorization phase\n  * Step 1k-7k (epoch 100-700): Gradual norm decrease during plateau\n  * Step ~7k (epoch ~700): Sharp norm drop at grokking transition\n  * Step 7k+ (epoch 700+): Low norm stabilization in generalizing circuit\n- All 56 library tests pass, code compiles cleanly\n- Suggestions: next task is visualization pipeline (MechInt-c0w) to plot weight norm evolution alongside loss/accuracy curves\n## [2026-01-08 01:45:00] Completed MechInt-c0w\n- Integrated visualization pipeline to automatically generate plots at key checkpoints\n- Added embedding grid visualizations (3x3) for: initial, memorization (e100), plateau (e500), grokking, and final states\n- Organized all visualizations into artifacts/visualizations/ directory with sectioned logging\n- Snake curves (log-scale) automatically generated to show characteristic grokking shape\n- FFT analysis plots (frequency distribution, spectra) generated for post-training analysis\n- Embedding visualizations reveal circular geometry emergence at grokking transition\n- Added generate_checkpoint_embeddings_visualization() helper function\n- Uses extract_all_embeddings() and select_interesting_dimensions() for automatic dimension selection\n- Comprehensive error handling with graceful fallback when checkpoints unavailable\n- All 59 tests pass, code compiles cleanly\n- Suggestions: next task is likely MechInt-l1a (core verification) or MechInt-t75 (checkpoint management)\n## [2026-01-08 02:00:00] Completed MechInt-t75\n- Implemented labeled checkpoint management at key training phases for comprehensive grokking analysis\n- Added checkpoint saving at epoch 100 (~step 1,000): memorization_e100\n  - Captures end of memorization phase when train accuracy reaches 100%\n- Added checkpoint saving at epoch 500 (~step 5,000): plateau_e500\n  - Captures deep plateau phase when validation accuracy frozen at ~1%\n- Added checkpoint saving at epoch 1500 (~step 15,000): postgrok_e1500\n  - Captures post-grokking solidification phase after generalization occurs\n- Existing grokking checkpoint (auto-detected at ~epoch 700) and final checkpoint preserved\n- All checkpoints saved to artifacts/checkpoint_labeled/ with descriptive labels\n- Conditional logic prevents errors on short training runs (checks num_epochs threshold)\n- Infrastructure uses existing copy_checkpoint_set() to save model, optimizer, and scheduler state\n- Enables post-hoc analysis: restricted/excluded loss, weight norm evolution, embedding geometry, MLP activations\n- All 59 tests pass, code compiles cleanly\n- Suggestions: next tasks are MechInt-l1a (verification documentation) or MechInt-at3 (3D visualization)\n\n## [2026-01-08 00:30:00] Completed MechInt-l1a\n- Verified core implementation is 100% specification-compliant through comprehensive code review and testing\n- Confirmed all 56 library tests pass (grokking detection, snake curve, restricted/excluded loss, weight norm decay, FFT analysis, embedding geometry, MLP activations)\n- Validated implementation against NOTES.md specification:\n  * Dataset: modulus=113, train_fraction=0.4, full 12,769 sample space\n  * Model: 1-layer transformer, d_model=128, n_heads=4, d_ff=512, ReLU activation, no LayerNorm\n  * Optimizer: AdamW (Î²1=0.9, Î²2=0.98, lr=1e-3, weight_decay=1.0)\n  * Training: batch_size=512, num_epochs=3000 (~30k steps), seed=42\n- Confirmed all advanced features integrated and tested:\n  * Automatic grokking detection at expected step ~7k (epoch ~700)\n  * Checkpoint management at key phases (memorization, plateau, grokking, post-grok, final)\n  * Visualization pipeline (snake curves, embeddings, FFT analysis)\n  * Restricted/excluded loss tracking and verification\n  * Weight norm decay monitoring and verification\n- Implementation is production-ready; empirical validation requires multi-hour training run\n- Suggestions: Next tasks are MechInt-at3 (3D surface visualization) or run full training experiment for empirical validation\n\n## [2026-01-08 00:38:00] Completed MechInt-at3\n- Implemented 3D activation surface visualization with JSON export and Python Plotly visualization script\n- Added src/export.rs module with ActivationSurface struct for JSON serialization and deserialization\n- Created scripts/visualize_activation_surface.py for interactive 3D surface plots and diagonal FFT analysis\n- Integrated activation surface export into main analysis pipeline (exports 5 neuron surfaces from post-grokking checkpoint)\n- Added comprehensive documentation in scripts/README.md with setup instructions and theoretical background\n- Added scripts/requirements.txt for Python dependencies (numpy, plotly)\n- Python script generates:\n  * Interactive 3D surface plots showing activation values across all (x,y) input pairs\n  * 2D heatmap view for pattern analysis\n  * Diagonal periodicity analysis via FFT to quantify Fourier structure\n  * Summary statistics: diagonal mean/std/range, dominant frequency, periodicity strength\n- Exports activation surfaces at epoch 1500 (post-grokking) to capture mature Fourier algorithm structure\n- All 58 library tests pass (added 2 tests for ActivationSurface save/load functionality)\n- Reveals characteristic \"corrugated iron\" diagonal wave structure proving Fourier algorithm discovery\n- Usage: cargo run --release (exports JSON), then python scripts/visualize_activation_surface.py artifacts/activation_surface_neuron_0.json\n- Suggestions: Next tasks are MechInt-rj9 (weight decay sweep) or MechInt-7ab (dead code cleanup)\n","numLines":212,"startLine":1,"totalLines":212}}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01PXAt9Es5tvRUHm68Tdwpoe","type":"message","role":"assistant","content":[{"type":"text","text":"Based on the BEADS queue and PROGRESS.txt, I can see that the primary implementation tasks are complete. The two remaining tasks in the ready queue are both `verify-2` priority tasks:\n\n1. **MechInt-rj9**: Weight decay sweep experiment (low priority, compute-intensive)\n2. **MechInt-7ab**: Clean up dead code warnings (maintenance task)\n\nGiven that:\n- All core functionality is implemented and verified\n- The project is waiting for long training runs for empirical validation\n- There are 72 dead code warnings cluttering the compilation output\n- The dead code cleanup task explicitly says to do it \"after P0/P1 issues complete\"\n\nI'll work on **MechInt-7ab** to clean up the dead code warnings. This is a good maintenance task that will improve code quality and make the compilation output cleaner."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":7156,"cache_read_input_tokens":22972,"cache_creation":{"ephemeral_5m_input_tokens":7156,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"c4f8e3f4-4470-42bd-a107-86f9a86a8eeb"}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01PXAt9Es5tvRUHm68Tdwpoe","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01Dk4jtZkNsxTah4vzeEscBk","name":"Bash","input":{"command":"bd update MechInt-7ab --status=in_progress","description":"Claim the dead code cleanup task"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":7156,"cache_read_input_tokens":22972,"cache_creation":{"ephemeral_5m_input_tokens":7156,"ephemeral_1h_input_tokens":0},"output_tokens":280,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"36f21594-7b9c-4f3b-bd67-278e44a761d4"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01Dk4jtZkNsxTah4vzeEscBk","type":"tool_result","content":"âœ“ Updated issue: MechInt-7ab","is_error":false}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"cd1fae6c-44f6-4f01-8ede-d15d4b5756c5","tool_use_result":{"stdout":"âœ“ Updated issue: MechInt-7ab","stderr":"","interrupted":false,"isImage":false}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01W9C7o6npcJ6oPBmB4T39Lf","type":"message","role":"assistant","content":[{"type":"text","text":"Now let me assess the dead code warnings by building the project."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":6,"cache_creation_input_tokens":305,"cache_read_input_tokens":30128,"cache_creation":{"ephemeral_5m_input_tokens":305,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"539e9415-a4c7-4fc4-afcb-88c4fb1d2d6a"}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01W9C7o6npcJ6oPBmB4T39Lf","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01PsDya9qeJcm6zkuLRqggwK","name":"Bash","input":{"command":"cargo build 2>&1 | grep \"warning:\" | wc -l","description":"Count total dead code warnings","timeout":120000}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":6,"cache_creation_input_tokens":305,"cache_read_input_tokens":30128,"cache_creation":{"ephemeral_5m_input_tokens":305,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"557f6997-ced3-4b65-bef7-05aa12fc4ce7"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01PsDya9qeJcm6zkuLRqggwK","type":"tool_result","content":"      59","is_error":false}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"e628fab3-2067-445d-abc0-8f5e5c4aed45","tool_use_result":{"stdout":"      59","stderr":"","interrupted":false,"isImage":false}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01PeruSQ1aUituR2SvnovK9L","type":"message","role":"assistant","content":[{"type":"text","text":"Good, we're down from the original 72 warnings. Let me get a detailed breakdown to understand what's actually unused."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":6,"cache_creation_input_tokens":135,"cache_read_input_tokens":30433,"cache_creation":{"ephemeral_5m_input_tokens":135,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"4b42cae0-4571-4164-805d-321fa43c0df6"}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01PeruSQ1aUituR2SvnovK9L","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01VBu1hrcAx7JweiqexMKtK1","name":"Bash","input":{"command":"cargo build 2>&1 | grep \"warning:\" | sort | uniq -c | sort -rn | head -40","description":"Get breakdown of warning types","timeout":120000}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":6,"cache_creation_input_tokens":135,"cache_read_input_tokens":30433,"cache_creation":{"ephemeral_5m_input_tokens":135,"ephemeral_1h_input_tokens":0},"output_tokens":145,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"fc6e61ab-0392-4b4a-bd28-dbd8e44f0891"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01VBu1hrcAx7JweiqexMKtK1","type":"tool_result","content":"   4 warning: associated function `default_for_modulus` is never used\n   2 warning: method `add_snapshot` is never used\n   1 warning: struct `SnakeCurveReport` is never constructed\n   1 warning: struct `SnakeCurveConfig` is never constructed\n   1 warning: struct `RestrictedLossVerificationConfig` is never constructed\n   1 warning: struct `RestrictedLossReport` is never constructed\n   1 warning: struct `MlpWaveVerificationConfig` is never constructed\n   1 warning: struct `MlpWaveReport` is never constructed\n   1 warning: struct `MlpWaveNeuronReport` is never constructed\n   1 warning: struct `FFTDominantFrequencyReport` is never constructed\n   1 warning: struct `FFTDominantFrequencyConfig` is never constructed\n   1 warning: struct `EmbeddingClockReport` is never constructed\n   1 warning: struct `EmbeddingClockConfig` is never constructed\n   1 warning: struct `DiagonalRidgeReport` is never constructed\n   1 warning: struct `DiagonalRidgeConfig` is never constructed\n   1 warning: function `wrap_angle` is never used\n   1 warning: function `wrap_angle_signed` is never used\n   1 warning: function `viridis_to_rgb` is never used\n   1 warning: function `verify_snake_curve_shape` is never used\n   1 warning: function `verify_restricted_loss_early_drop` is never used\n   1 warning: function `verify_mlp_activation_waves` is never used\n   1 warning: function `verify_mlp_activation_waves_from_signals` is never used\n   1 warning: function `verify_fft_dominant_frequencies` is never used\n   1 warning: function `verify_fft_dominant_frequencies_from_embeddings` is never used\n   1 warning: function `verify_embedding_clock_geometry` is never used\n   1 warning: function `verify_constructive_interference_surface_from_grid` is never used\n   1 warning: function `token_to_rgb` is never used\n   1 warning: function `surface_mean_variance` is never used\n   1 warning: function `surface_dimensions` is never used\n   1 warning: function `surface_autocorrelation_scores` is never used\n   1 warning: function `save_labeled_checkpoint` is never used\n   1 warning: function `save_checkpoint` is never used\n   1 warning: function `render_scatter_plot` is never used\n   1 warning: function `render_dimension_plot` is never used\n   1 warning: function `project_embeddings_to_fourier_plane` is never used\n   1 warning: function `plot_loss_history` is never used\n   1 warning: function `plot_embedding_grid_fast` is never used\n   1 warning: function `pearson_correlation` is never used\n   1 warning: function `order_fraction` is never used\n   1 warning: function `normalize_vector` is never used","is_error":false}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"d4656f5c-9e8c-4b0c-8cbf-a3d1710133f8","tool_use_result":{"stdout":"   4 warning: associated function `default_for_modulus` is never used\n   2 warning: method `add_snapshot` is never used\n   1 warning: struct `SnakeCurveReport` is never constructed\n   1 warning: struct `SnakeCurveConfig` is never constructed\n   1 warning: struct `RestrictedLossVerificationConfig` is never constructed\n   1 warning: struct `RestrictedLossReport` is never constructed\n   1 warning: struct `MlpWaveVerificationConfig` is never constructed\n   1 warning: struct `MlpWaveReport` is never constructed\n   1 warning: struct `MlpWaveNeuronReport` is never constructed\n   1 warning: struct `FFTDominantFrequencyReport` is never constructed\n   1 warning: struct `FFTDominantFrequencyConfig` is never constructed\n   1 warning: struct `EmbeddingClockReport` is never constructed\n   1 warning: struct `EmbeddingClockConfig` is never constructed\n   1 warning: struct `DiagonalRidgeReport` is never constructed\n   1 warning: struct `DiagonalRidgeConfig` is never constructed\n   1 warning: function `wrap_angle` is never used\n   1 warning: function `wrap_angle_signed` is never used\n   1 warning: function `viridis_to_rgb` is never used\n   1 warning: function `verify_snake_curve_shape` is never used\n   1 warning: function `verify_restricted_loss_early_drop` is never used\n   1 warning: function `verify_mlp_activation_waves` is never used\n   1 warning: function `verify_mlp_activation_waves_from_signals` is never used\n   1 warning: function `verify_fft_dominant_frequencies` is never used\n   1 warning: function `verify_fft_dominant_frequencies_from_embeddings` is never used\n   1 warning: function `verify_embedding_clock_geometry` is never used\n   1 warning: function `verify_constructive_interference_surface_from_grid` is never used\n   1 warning: function `token_to_rgb` is never used\n   1 warning: function `surface_mean_variance` is never used\n   1 warning: function `surface_dimensions` is never used\n   1 warning: function `surface_autocorrelation_scores` is never used\n   1 warning: function `save_labeled_checkpoint` is never used\n   1 warning: function `save_checkpoint` is never used\n   1 warning: function `render_scatter_plot` is never used\n   1 warning: function `render_dimension_plot` is never used\n   1 warning: function `project_embeddings_to_fourier_plane` is never used\n   1 warning: function `plot_loss_history` is never used\n   1 warning: function `plot_embedding_grid_fast` is never used\n   1 warning: function `pearson_correlation` is never used\n   1 warning: function `order_fraction` is never used\n   1 warning: function `normalize_vector` is never used","stderr":"","interrupted":false,"isImage":false}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_014UUqf4uajxVLeugEd8rvD9","type":"message","role":"assistant","content":[{"type":"text","text":"Now let me understand the strategy. Many of these functions are actually **used in tests** but not in the main binary. Let me check if they're actually dead code or just used in tests:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":6,"cache_creation_input_tokens":906,"cache_read_input_tokens":30568,"cache_creation":{"ephemeral_5m_input_tokens":906,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"5cb60ae9-0c51-4a5e-b126-e8e0706d074c"}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_014UUqf4uajxVLeugEd8rvD9","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01K1xvMWPGkC8VaPdnJhi1Wg","name":"Bash","input":{"command":"cargo test 2>&1 | tail -5","description":"Run tests to confirm functionality","timeout":120000}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":6,"cache_creation_input_tokens":906,"cache_read_input_tokens":30568,"cache_creation":{"ephemeral_5m_input_tokens":906,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"a16c470a-ff0c-4995-a25f-d80fcb27bef0"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01K1xvMWPGkC8VaPdnJhi1Wg","type":"tool_result","content":"running 0 tests\n\ntest result: ok. 0 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out; finished in 0.00s","is_error":false}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"51b29604-f518-4717-a533-ed4a7e914f6f","tool_use_result":{"stdout":"running 0 tests\n\ntest result: ok. 0 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out; finished in 0.00s","stderr":"","interrupted":false,"isImage":false}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01AiVrTvCJnnLo4VTRqxtAuz","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_0179BHrUEQqKttFJ8EcSTFpn","name":"Bash","input":{"command":"cargo test --lib 2>&1 | tail -5","description":"Run library tests to check usage","timeout":120000}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":5,"cache_creation_input_tokens":302,"cache_read_input_tokens":31474,"cache_creation":{"ephemeral_5m_input_tokens":302,"ephemeral_1h_input_tokens":0},"output_tokens":25,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"096612c9-99c8-4c46-a8b2-f87fab59c0a8"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_0179BHrUEQqKttFJ8EcSTFpn","type":"tool_result","content":"test model::tests::test_no_layer_norm_modules ... ok\ntest model::tests::test_forward_pass ... ok\n\ntest result: ok. 58 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out; finished in 0.20s","is_error":false}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"7846f952-c982-46d0-9f65-7dafe3b61427","tool_use_result":{"stdout":"test model::tests::test_no_layer_norm_modules ... ok\ntest model::tests::test_forward_pass ... ok\n\ntest result: ok. 58 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out; finished in 0.20s","stderr":"","interrupted":false,"isImage":false}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_019JM39CnkoQ3LRYGtm2Q9HS","type":"message","role":"assistant","content":[{"type":"text","text":"Good, tests pass. Now let me analyze the situation more carefully. The warnings are for code that's defined but not called from the main binary or from other library code. However, many of these functions are valuable verification functions that **should** be integrated into the analysis pipeline.\n\nLet me read the main.rs file to see what's actually being used:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":5,"cache_creation_input_tokens":184,"cache_read_input_tokens":31776,"cache_creation":{"ephemeral_5m_input_tokens":184,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"743f9bc5-7e2b-4aed-b35e-3e208e1067f2"}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_019JM39CnkoQ3LRYGtm2Q9HS","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01Ur7gNthwA7yiaENzhnKKwK","name":"Read","input":{"file_path":"/Users/admin/MechInt/src/main.rs","limit":100}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":5,"cache_creation_input_tokens":184,"cache_read_input_tokens":31776,"cache_creation":{"ephemeral_5m_input_tokens":184,"ephemeral_1h_input_tokens":0},"output_tokens":157,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"41f7aee5-6c22-4bad-b01c-00b09d7d204f"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01Ur7gNthwA7yiaENzhnKKwK","type":"tool_result","content":"     1â†’#![recursion_limit = \"256\"]\n     2â†’\n     3â†’mod analysis;\n     4â†’mod checkpoint;\n     5â†’mod data;\n     6â†’mod export;\n     7â†’mod model;\n     8â†’mod plotting;\n     9â†’mod training;\n    10â†’mod training_config;\n    11â†’mod verify;\n    12â†’\n    13â†’use burn::{\n    14â†’    backend::{wgpu::WgpuDevice, Autodiff, Wgpu},\n    15â†’    data::dataset::Dataset,\n    16â†’    optim::lr_scheduler::linear::LinearLrSchedulerConfig,\n    17â†’    record::CompactRecorder,\n    18â†’    train::{\n    19â†’        metric::{AccuracyMetric, LossMetric, NumericEntry},\n    20â†’        Learner, LearningParadigm, SupervisedTraining, TrainingStrategy,\n    21â†’    },\n    22â†’};\n    23â†’use data::{build_dataloaders, ModularAdditionDataset};\n    24â†’use model::{Transformer, TransformerConfig};\n    25â†’use training_config::TrainingConfig;\n    26â†’use std::{fs, path::Path};\n    27â†’\n    28â†’type WgpuBackend = Wgpu;\n    29â†’type MyAutodiffBackend = Autodiff<WgpuBackend>;\n    30â†’\n    31â†’fn main() {\n    32â†’    let training_config = TrainingConfig::default();\n    33â†’    training_config\n    34â†’        .validate_grokking_spec()\n    35â†’        .expect(\"Training config must match grokking spec\");\n    36â†’\n    37â†’    println!(\"ðŸš€ Grokking experiment starting...\");\n    38â†’    println!(\"Configuration:\");\n    39â†’    println!(\"  - Model: 1-layer transformer (4 heads, dim=128, MLP=512)\");\n    40â†’    println!(\"  - Optimizer: AdamW (Î²1=0.9, Î²2=0.98, weight decay=1.0)\");\n    41â†’    println!(\n    42â†’        \"  - Learning rate: {} with {}-step linear warmup\",\n    43â†’        training_config.base_learning_rate, training_config.warmup_steps\n    44â†’    );\n    45â†’    println!(\"  - Batch size: {}\", training_config.batch_size);\n    46â†’    println!(\"  - Training epochs: {}\", training_config.num_epochs);\n    47â†’    println!(\n    48â†’        \"  - Target: Modular addition (a + b) mod {}\",\n    49â†’        ModularAdditionDataset::modulus()\n    50â†’    );\n    51â†’    println!(\"  - Tracking: Train/Val Loss AND Accuracy (Burn metrics)\");\n    52â†’    println!();\n    53â†’\n    54â†’    // Setup device\n    55â†’    let device = WgpuDevice::default();\n    56â†’    println!(\"ðŸ“± Using device: {:?}\", device);\n    57â†’    println!();\n    58â†’\n    59â†’    // Create datasets\n    60â†’    let train_dataset = ModularAdditionDataset::new(true, training_config.seed);\n    61â†’    let val_dataset = ModularAdditionDataset::new(false, training_config.seed);\n    62â†’    println!(\"ðŸ“Š Dataset sizes:\");\n    63â†’    println!(\"  - Training: {} examples\", train_dataset.len());\n    64â†’    println!(\"  - Validation: {} examples\", val_dataset.len());\n    65â†’    println!();\n    66â†’\n    67â†’    // Create model\n    68â†’    let config = TransformerConfig::default();\n    69â†’    let model: Transformer<MyAutodiffBackend> = config.init(&device);\n    70â†’    println!(\"âœ… Model initialized\");\n    71â†’\n    72â†’    // Setup optimizer to match paper (AdamW with high weight decay, Î²2=0.98)\n    73â†’    let optim_config = training_config.optimizer_config();\n    74â†’    let optim = optim_config.init();\n    75â†’    println!(\"âœ… AdamW optimizer initialized (Î²1=0.9, Î²2=0.98, weight decay=1.0)\");\n    76â†’    println!();\n    77â†’\n    78â†’    // Training configuration (matching paper)\n    79â†’    let batch_size = training_config.batch_size;\n    80â†’    let num_workers = training_config.num_workers;\n    81â†’    let seed = training_config.seed;\n    82â†’    let num_epochs = training_config.num_epochs;\n    83â†’    let base_learning_rate = training_config.base_learning_rate;\n    84â†’    let warmup_steps = training_config.warmup_steps;\n    85â†’\n    86â†’    let steps_per_epoch = training_config.steps_per_epoch(train_dataset.len());\n    87â†’    let total_steps = training_config.total_steps(train_dataset.len());\n    88â†’\n    89â†’    let artifact_dir = \"artifacts\";\n    90â†’    std::fs::create_dir_all(artifact_dir).ok();\n    91â†’\n    92â†’    // Build Burn dataloaders\n    93â†’    let (dataloader_train, dataloader_val) =\n    94â†’        build_dataloaders::<MyAutodiffBackend>(batch_size, num_workers, seed, device.clone());\n    95â†’\n    96â†’    let warmup_start = (base_learning_rate / warmup_steps as f64).max(1.0e-9);\n    97â†’    let lr_scheduler = LinearLrSchedulerConfig::new(\n    98â†’        warmup_start,\n    99â†’        base_learning_rate,\n   100â†’        warmup_steps,\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"4dffaa02-5d61-4391-b2bc-82173e8ec828","tool_use_result":{"type":"text","file":{"filePath":"/Users/admin/MechInt/src/main.rs","content":"#![recursion_limit = \"256\"]\n\nmod analysis;\nmod checkpoint;\nmod data;\nmod export;\nmod model;\nmod plotting;\nmod training;\nmod training_config;\nmod verify;\n\nuse burn::{\n    backend::{wgpu::WgpuDevice, Autodiff, Wgpu},\n    data::dataset::Dataset,\n    optim::lr_scheduler::linear::LinearLrSchedulerConfig,\n    record::CompactRecorder,\n    train::{\n        metric::{AccuracyMetric, LossMetric, NumericEntry},\n        Learner, LearningParadigm, SupervisedTraining, TrainingStrategy,\n    },\n};\nuse data::{build_dataloaders, ModularAdditionDataset};\nuse model::{Transformer, TransformerConfig};\nuse training_config::TrainingConfig;\nuse std::{fs, path::Path};\n\ntype WgpuBackend = Wgpu;\ntype MyAutodiffBackend = Autodiff<WgpuBackend>;\n\nfn main() {\n    let training_config = TrainingConfig::default();\n    training_config\n        .validate_grokking_spec()\n        .expect(\"Training config must match grokking spec\");\n\n    println!(\"ðŸš€ Grokking experiment starting...\");\n    println!(\"Configuration:\");\n    println!(\"  - Model: 1-layer transformer (4 heads, dim=128, MLP=512)\");\n    println!(\"  - Optimizer: AdamW (Î²1=0.9, Î²2=0.98, weight decay=1.0)\");\n    println!(\n        \"  - Learning rate: {} with {}-step linear warmup\",\n        training_config.base_learning_rate, training_config.warmup_steps\n    );\n    println!(\"  - Batch size: {}\", training_config.batch_size);\n    println!(\"  - Training epochs: {}\", training_config.num_epochs);\n    println!(\n        \"  - Target: Modular addition (a + b) mod {}\",\n        ModularAdditionDataset::modulus()\n    );\n    println!(\"  - Tracking: Train/Val Loss AND Accuracy (Burn metrics)\");\n    println!();\n\n    // Setup device\n    let device = WgpuDevice::default();\n    println!(\"ðŸ“± Using device: {:?}\", device);\n    println!();\n\n    // Create datasets\n    let train_dataset = ModularAdditionDataset::new(true, training_config.seed);\n    let val_dataset = ModularAdditionDataset::new(false, training_config.seed);\n    println!(\"ðŸ“Š Dataset sizes:\");\n    println!(\"  - Training: {} examples\", train_dataset.len());\n    println!(\"  - Validation: {} examples\", val_dataset.len());\n    println!();\n\n    // Create model\n    let config = TransformerConfig::default();\n    let model: Transformer<MyAutodiffBackend> = config.init(&device);\n    println!(\"âœ… Model initialized\");\n\n    // Setup optimizer to match paper (AdamW with high weight decay, Î²2=0.98)\n    let optim_config = training_config.optimizer_config();\n    let optim = optim_config.init();\n    println!(\"âœ… AdamW optimizer initialized (Î²1=0.9, Î²2=0.98, weight decay=1.0)\");\n    println!();\n\n    // Training configuration (matching paper)\n    let batch_size = training_config.batch_size;\n    let num_workers = training_config.num_workers;\n    let seed = training_config.seed;\n    let num_epochs = training_config.num_epochs;\n    let base_learning_rate = training_config.base_learning_rate;\n    let warmup_steps = training_config.warmup_steps;\n\n    let steps_per_epoch = training_config.steps_per_epoch(train_dataset.len());\n    let total_steps = training_config.total_steps(train_dataset.len());\n\n    let artifact_dir = \"artifacts\";\n    std::fs::create_dir_all(artifact_dir).ok();\n\n    // Build Burn dataloaders\n    let (dataloader_train, dataloader_val) =\n        build_dataloaders::<MyAutodiffBackend>(batch_size, num_workers, seed, device.clone());\n\n    let warmup_start = (base_learning_rate / warmup_steps as f64).max(1.0e-9);\n    let lr_scheduler = LinearLrSchedulerConfig::new(\n        warmup_start,\n        base_learning_rate,\n        warmup_steps,","numLines":100,"startLine":1,"totalLines":1404}}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01QBD2yFwRoYT1Mj65J8fRrk","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01PA6q9S9vGsre6ioK6iL7LF","name":"Read","input":{"file_path":"/Users/admin/MechInt/src/main.rs","offset":200,"limit":200}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":5,"cache_creation_input_tokens":1692,"cache_read_input_tokens":31960,"cache_creation":{"ephemeral_5m_input_tokens":1692,"ephemeral_1h_input_tokens":0},"output_tokens":18,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"8c9b49c8-88da-46b9-95c5-82a95ac1fcdf"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01PA6q9S9vGsre6ioK6iL7LF","type":"tool_result","content":"   200â†’        println!(\"   This sudden jump from ~random ({:.1}%) to ~perfect ({:.1}%)\",\n   201â†’                 info.baseline_val_acc, info.grok_val_acc);\n   202â†’        println!(\"   is the hallmark of the grokking phenomenon!\");\n   203â†’        println!();\n   204â†’\n   205â†’        if let Err(e) = copy_checkpoint_set(artifact_dir, *epoch, \"grokking\") {\n   206â†’            eprintln!(\"âš ï¸  Warning: Could not save grokking checkpoint: {}\", e);\n   207â†’        } else {\n   208â†’            println!(\"ðŸ’¾ Saved grokking checkpoint at epoch {} (step {})\", epoch, step);\n   209â†’        }\n   210â†’\n   211â†’        Some(*epoch)\n   212â†’    } else {\n   213â†’        println!(\"âš ï¸  No clear grokking transition detected.\");\n   214â†’        println!(\"   Possible reasons:\");\n   215â†’        println!(\"     - Training hasn't run long enough (expected ~step 7,000)\");\n   216â†’        println!(\"     - Gradual improvement instead of sudden spike\");\n   217â†’        println!(\"     - Grokking may occur beyond current epoch count\");\n   218â†’        println!();\n   219â†’        println!(\"   Falling back to simple threshold detection...\");\n   220â†’\n   221â†’        let fallback = find_grokking_epoch(artifact_dir, num_epochs, steps_per_epoch, 90.0);\n   222â†’        if let Some(epoch) = fallback {\n   223â†’            println!(\"   Found validation accuracy >90% at epoch {}\", epoch);\n   224â†’            if let Err(e) = copy_checkpoint_set(artifact_dir, epoch, \"grokking\") {\n   225â†’                eprintln!(\"âš ï¸  Warning: Could not save grokking checkpoint: {}\", e);\n   226â†’            }\n   227â†’        } else {\n   228â†’            println!(\"   No epoch reached 90% validation accuracy yet.\");\n   229â†’        }\n   230â†’\n   231â†’        fallback\n   232â†’    };\n   233â†’\n   234â†’    if let Err(e) = copy_checkpoint_set(artifact_dir, num_epochs, \"final\") {\n   235â†’        eprintln!(\"âš ï¸  Warning: Could not save final checkpoint: {}\", e);\n   236â†’    }\n   237â†’\n   238â†’    println!();\n   239â†’    println!(\"{}\", \"=\".repeat(80));\n   240â†’    println!(\"âœ… Training complete!\");\n   241â†’    println!();\n   242â†’\n   243â†’    // CRITICAL: Verify on complete dataset (no sampling!)\n   244â†’    println!(\"{}\", \"=\".repeat(80));\n   245â†’    println!(\"ðŸ” VERIFICATION (Full Dataset - No Sampling)\");\n   246â†’    println!(\"{}\", \"=\".repeat(80));\n   247â†’    println!();\n   248â†’\n   249â†’    let (true_train_acc, true_val_acc) = verify::verify_full_accuracy(&model, &device);\n   250â†’\n   251â†’    println!();\n   252â†’    verify::test_specific_examples(&model, &device);\n   253â†’\n   254â†’    println!();\n   255â†’    if true_train_acc < 0.99 || true_val_acc < 0.99 {\n   256â†’        println!(\"âš ï¸  WARNING: Model did NOT actually learn the algorithm!\");\n   257â†’        println!(\"   This confirms we were fooled by random sampling variance.\");\n   258â†’    } else {\n   259â†’        println!(\"âœ… CONFIRMED: Model actually learned modular addition!\");\n   260â†’    }\n   261â†’\n   262â†’    // Perform FFT analysis on learned embeddings\n   263â†’    println!();\n   264â†’    println!(\"{}\", \"=\".repeat(80));\n   265â†’    println!(\"ðŸ“Š Post-Training Analysis\");\n   266â†’    println!(\"{}\", \"=\".repeat(80));\n   267â†’    println!();\n   268â†’\n   269â†’    let fft_analysis = analysis::analyze_embeddings_fft(&model);\n   270â†’    let fft_analysis_path = format!(\"{artifact_dir}/fft_analysis.json\");\n   271â†’    if let Err(e) = analysis::save_fft_analysis(&fft_analysis, &fft_analysis_path) {\n   272â†’        eprintln!(\"âš ï¸  Warning: Could not save FFT analysis: {}\", e);\n   273â†’    }\n   274â†’\n   275â†’    println!();\n   276â†’    println!(\"ðŸ“‰ Loss Evolution Analysis:\");\n   277â†’    if let (Some(&(_, train_init)), Some(&(_, val_init))) =\n   278â†’        (loss_history.train_snapshots.first(), loss_history.val_snapshots.first())\n   279â†’    {\n   280â†’        if let (Some(&(_, train_final)), Some(&(_, val_final))) =\n   281â†’            (loss_history.train_snapshots.last(), loss_history.val_snapshots.last())\n   282â†’        {\n   283â†’            println!(\"   Initial - Train: {:.4}, Val: {:.4}\", train_init, val_init);\n   284â†’            println!(\"   Final   - Train: {:.4}, Val: {:.4}\", train_final, val_final);\n   285â†’            println!(\"   Train decrease: {:.4}\", train_init - train_final);\n   286â†’            println!(\"   Val decrease: {:.4}\", val_init - val_final);\n   287â†’        }\n   288â†’    }\n   289â†’\n   290â†’    let loss_history_path = format!(\"{artifact_dir}/loss_history.json\");\n   291â†’    if let Err(e) = loss_history.save(&loss_history_path) {\n   292â†’        eprintln!(\"âš ï¸  Warning: Could not save loss history: {}\", e);\n   293â†’    }\n   294â†’\n   295â†’    let accuracy_history_path = format!(\"{artifact_dir}/accuracy_history.json\");\n   296â†’    if let Err(e) = accuracy_history.save(&accuracy_history_path) {\n   297â†’        eprintln!(\"âš ï¸  Warning: Could not save accuracy history: {}\", e);\n   298â†’    }\n   299â†’\n   300â†’    println!();\n   301â†’    println!(\"{}\", \"=\".repeat(80));\n   302â†’    println!(\"ðŸ”¬ Advanced Metrics (Restricted/Excluded Loss)\");\n   303â†’    println!(\"{}\", \"=\".repeat(80));\n   304â†’    println!();\n   305â†’\n   306â†’    let restricted_top_k = 5usize;\n   307â†’    let excluded_top_k = 5usize;\n   308â†’    let metrics_batch_size = training_config.batch_size;\n   309â†’    let mut restricted_history = analysis::RestrictedLossHistory::new();\n   310â†’    let mut excluded_history = analysis::ExcludedLossHistory::new();\n   311â†’\n   312â†’    // Key checkpoints based on expected timeline:\n   313â†’    // - Epoch 100 (~step 1,000): End of memorization\n   314â†’    // - Epoch 500 (~step 5,000): Deep in plateau\n   315â†’    // - Grokking epoch (~step 7,000): Phase transition\n   316â†’    // - Final epoch: Post-grok solidification\n   317â†’    let checkpoint_epochs = vec![100, 500];\n   318â†’\n   319â†’    println!(\"ðŸ” Computing restricted/excluded loss at key checkpoints...\");\n   320â†’    println!(\"   (These metrics detect internal restructuring before grokking)\");\n   321â†’    println!();\n   322â†’\n   323â†’    // Initial model (step 0)\n   324â†’    let initial_model: Transformer<MyAutodiffBackend> = TransformerConfig::default().init(&device);\n   325â†’    match analysis::compute_restricted_loss(\n   326â†’        &initial_model,\n   327â†’        &train_dataset,\n   328â†’        &device,\n   329â†’        restricted_top_k,\n   330â†’        metrics_batch_size,\n   331â†’    ) {\n   332â†’        Ok(loss) => {\n   333â†’            restricted_history.add_snapshot(0, loss);\n   334â†’            println!(\"   Step 0 - Restricted loss (top_k={}): {:.4}\", restricted_top_k, loss);\n   335â†’        }\n   336â†’        Err(err) => {\n   337â†’            eprintln!(\"âš ï¸  Warning: Could not compute restricted loss (initial): {}\", err);\n   338â†’        }\n   339â†’    }\n   340â†’\n   341â†’    match analysis::compute_excluded_loss(\n   342â†’        &initial_model,\n   343â†’        &train_dataset,\n   344â†’        &device,\n   345â†’        excluded_top_k,\n   346â†’        metrics_batch_size,\n   347â†’    ) {\n   348â†’        Ok(loss) => {\n   349â†’            excluded_history.add_snapshot(0, loss);\n   350â†’            println!(\"   Step 0 - Excluded loss (top_k={}): {:.4}\", excluded_top_k, loss);\n   351â†’        }\n   352â†’        Err(err) => {\n   353â†’            eprintln!(\"âš ï¸  Warning: Could not compute excluded loss (initial): {}\", err);\n   354â†’        }\n   355â†’    }\n   356â†’\n   357â†’    // Checkpoint epochs (100, 500)\n   358â†’    for &epoch in &checkpoint_epochs {\n   359â†’        if epoch > num_epochs {\n   360â†’            continue;\n   361â†’        }\n   362â†’\n   363â†’        let checkpoint_step = (epoch - 1) * steps_per_epoch;\n   364â†’        let checkpoint_path = format!(\"{artifact_dir}/checkpoint/model-{epoch}.mpk\");\n   365â†’\n   366â†’        if !Path::new(&checkpoint_path).exists() {\n   367â†’            continue;\n   368â†’        }\n   369â†’\n   370â†’        match checkpoint::load_checkpoint::<MyAutodiffBackend>(&checkpoint_path, &device) {\n   371â†’            Ok(checkpoint_model) => {\n   372â†’                match analysis::compute_restricted_loss(\n   373â†’                    &checkpoint_model,\n   374â†’                    &train_dataset,\n   375â†’                    &device,\n   376â†’                    restricted_top_k,\n   377â†’                    metrics_batch_size,\n   378â†’                ) {\n   379â†’                    Ok(loss) => {\n   380â†’                        restricted_history.add_snapshot(checkpoint_step, loss);\n   381â†’                        println!(\"   Step {} (epoch {}) - Restricted loss: {:.4}\",\n   382â†’                                 checkpoint_step, epoch, loss);\n   383â†’                    }\n   384â†’                    Err(err) => {\n   385â†’                        eprintln!(\"âš ï¸  Warning: Could not compute restricted loss (epoch {}): {}\",\n   386â†’                                  epoch, err);\n   387â†’                    }\n   388â†’                }\n   389â†’\n   390â†’                match analysis::compute_excluded_loss(\n   391â†’                    &checkpoint_model,\n   392â†’                    &train_dataset,\n   393â†’                    &device,\n   394â†’                    excluded_top_k,\n   395â†’                    metrics_batch_size,\n   396â†’                ) {\n   397â†’                    Ok(loss) => {\n   398â†’                        excluded_history.add_snapshot(checkpoint_step, loss);\n   399â†’                        println!(\"   Step {} (epoch {}) - Excluded loss: {:.4}\",\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"2d00641e-2215-499e-8614-8d85cf9df173","tool_use_result":{"type":"text","file":{"filePath":"/Users/admin/MechInt/src/main.rs","content":"        println!(\"   This sudden jump from ~random ({:.1}%) to ~perfect ({:.1}%)\",\n                 info.baseline_val_acc, info.grok_val_acc);\n        println!(\"   is the hallmark of the grokking phenomenon!\");\n        println!();\n\n        if let Err(e) = copy_checkpoint_set(artifact_dir, *epoch, \"grokking\") {\n            eprintln!(\"âš ï¸  Warning: Could not save grokking checkpoint: {}\", e);\n        } else {\n            println!(\"ðŸ’¾ Saved grokking checkpoint at epoch {} (step {})\", epoch, step);\n        }\n\n        Some(*epoch)\n    } else {\n        println!(\"âš ï¸  No clear grokking transition detected.\");\n        println!(\"   Possible reasons:\");\n        println!(\"     - Training hasn't run long enough (expected ~step 7,000)\");\n        println!(\"     - Gradual improvement instead of sudden spike\");\n        println!(\"     - Grokking may occur beyond current epoch count\");\n        println!();\n        println!(\"   Falling back to simple threshold detection...\");\n\n        let fallback = find_grokking_epoch(artifact_dir, num_epochs, steps_per_epoch, 90.0);\n        if let Some(epoch) = fallback {\n            println!(\"   Found validation accuracy >90% at epoch {}\", epoch);\n            if let Err(e) = copy_checkpoint_set(artifact_dir, epoch, \"grokking\") {\n                eprintln!(\"âš ï¸  Warning: Could not save grokking checkpoint: {}\", e);\n            }\n        } else {\n            println!(\"   No epoch reached 90% validation accuracy yet.\");\n        }\n\n        fallback\n    };\n\n    if let Err(e) = copy_checkpoint_set(artifact_dir, num_epochs, \"final\") {\n        eprintln!(\"âš ï¸  Warning: Could not save final checkpoint: {}\", e);\n    }\n\n    println!();\n    println!(\"{}\", \"=\".repeat(80));\n    println!(\"âœ… Training complete!\");\n    println!();\n\n    // CRITICAL: Verify on complete dataset (no sampling!)\n    println!(\"{}\", \"=\".repeat(80));\n    println!(\"ðŸ” VERIFICATION (Full Dataset - No Sampling)\");\n    println!(\"{}\", \"=\".repeat(80));\n    println!();\n\n    let (true_train_acc, true_val_acc) = verify::verify_full_accuracy(&model, &device);\n\n    println!();\n    verify::test_specific_examples(&model, &device);\n\n    println!();\n    if true_train_acc < 0.99 || true_val_acc < 0.99 {\n        println!(\"âš ï¸  WARNING: Model did NOT actually learn the algorithm!\");\n        println!(\"   This confirms we were fooled by random sampling variance.\");\n    } else {\n        println!(\"âœ… CONFIRMED: Model actually learned modular addition!\");\n    }\n\n    // Perform FFT analysis on learned embeddings\n    println!();\n    println!(\"{}\", \"=\".repeat(80));\n    println!(\"ðŸ“Š Post-Training Analysis\");\n    println!(\"{}\", \"=\".repeat(80));\n    println!();\n\n    let fft_analysis = analysis::analyze_embeddings_fft(&model);\n    let fft_analysis_path = format!(\"{artifact_dir}/fft_analysis.json\");\n    if let Err(e) = analysis::save_fft_analysis(&fft_analysis, &fft_analysis_path) {\n        eprintln!(\"âš ï¸  Warning: Could not save FFT analysis: {}\", e);\n    }\n\n    println!();\n    println!(\"ðŸ“‰ Loss Evolution Analysis:\");\n    if let (Some(&(_, train_init)), Some(&(_, val_init))) =\n        (loss_history.train_snapshots.first(), loss_history.val_snapshots.first())\n    {\n        if let (Some(&(_, train_final)), Some(&(_, val_final))) =\n            (loss_history.train_snapshots.last(), loss_history.val_snapshots.last())\n        {\n            println!(\"   Initial - Train: {:.4}, Val: {:.4}\", train_init, val_init);\n            println!(\"   Final   - Train: {:.4}, Val: {:.4}\", train_final, val_final);\n            println!(\"   Train decrease: {:.4}\", train_init - train_final);\n            println!(\"   Val decrease: {:.4}\", val_init - val_final);\n        }\n    }\n\n    let loss_history_path = format!(\"{artifact_dir}/loss_history.json\");\n    if let Err(e) = loss_history.save(&loss_history_path) {\n        eprintln!(\"âš ï¸  Warning: Could not save loss history: {}\", e);\n    }\n\n    let accuracy_history_path = format!(\"{artifact_dir}/accuracy_history.json\");\n    if let Err(e) = accuracy_history.save(&accuracy_history_path) {\n        eprintln!(\"âš ï¸  Warning: Could not save accuracy history: {}\", e);\n    }\n\n    println!();\n    println!(\"{}\", \"=\".repeat(80));\n    println!(\"ðŸ”¬ Advanced Metrics (Restricted/Excluded Loss)\");\n    println!(\"{}\", \"=\".repeat(80));\n    println!();\n\n    let restricted_top_k = 5usize;\n    let excluded_top_k = 5usize;\n    let metrics_batch_size = training_config.batch_size;\n    let mut restricted_history = analysis::RestrictedLossHistory::new();\n    let mut excluded_history = analysis::ExcludedLossHistory::new();\n\n    // Key checkpoints based on expected timeline:\n    // - Epoch 100 (~step 1,000): End of memorization\n    // - Epoch 500 (~step 5,000): Deep in plateau\n    // - Grokking epoch (~step 7,000): Phase transition\n    // - Final epoch: Post-grok solidification\n    let checkpoint_epochs = vec![100, 500];\n\n    println!(\"ðŸ” Computing restricted/excluded loss at key checkpoints...\");\n    println!(\"   (These metrics detect internal restructuring before grokking)\");\n    println!();\n\n    // Initial model (step 0)\n    let initial_model: Transformer<MyAutodiffBackend> = TransformerConfig::default().init(&device);\n    match analysis::compute_restricted_loss(\n        &initial_model,\n        &train_dataset,\n        &device,\n        restricted_top_k,\n        metrics_batch_size,\n    ) {\n        Ok(loss) => {\n            restricted_history.add_snapshot(0, loss);\n            println!(\"   Step 0 - Restricted loss (top_k={}): {:.4}\", restricted_top_k, loss);\n        }\n        Err(err) => {\n            eprintln!(\"âš ï¸  Warning: Could not compute restricted loss (initial): {}\", err);\n        }\n    }\n\n    match analysis::compute_excluded_loss(\n        &initial_model,\n        &train_dataset,\n        &device,\n        excluded_top_k,\n        metrics_batch_size,\n    ) {\n        Ok(loss) => {\n            excluded_history.add_snapshot(0, loss);\n            println!(\"   Step 0 - Excluded loss (top_k={}): {:.4}\", excluded_top_k, loss);\n        }\n        Err(err) => {\n            eprintln!(\"âš ï¸  Warning: Could not compute excluded loss (initial): {}\", err);\n        }\n    }\n\n    // Checkpoint epochs (100, 500)\n    for &epoch in &checkpoint_epochs {\n        if epoch > num_epochs {\n            continue;\n        }\n\n        let checkpoint_step = (epoch - 1) * steps_per_epoch;\n        let checkpoint_path = format!(\"{artifact_dir}/checkpoint/model-{epoch}.mpk\");\n\n        if !Path::new(&checkpoint_path).exists() {\n            continue;\n        }\n\n        match checkpoint::load_checkpoint::<MyAutodiffBackend>(&checkpoint_path, &device) {\n            Ok(checkpoint_model) => {\n                match analysis::compute_restricted_loss(\n                    &checkpoint_model,\n                    &train_dataset,\n                    &device,\n                    restricted_top_k,\n                    metrics_batch_size,\n                ) {\n                    Ok(loss) => {\n                        restricted_history.add_snapshot(checkpoint_step, loss);\n                        println!(\"   Step {} (epoch {}) - Restricted loss: {:.4}\",\n                                 checkpoint_step, epoch, loss);\n                    }\n                    Err(err) => {\n                        eprintln!(\"âš ï¸  Warning: Could not compute restricted loss (epoch {}): {}\",\n                                  epoch, err);\n                    }\n                }\n\n                match analysis::compute_excluded_loss(\n                    &checkpoint_model,\n                    &train_dataset,\n                    &device,\n                    excluded_top_k,\n                    metrics_batch_size,\n                ) {\n                    Ok(loss) => {\n                        excluded_history.add_snapshot(checkpoint_step, loss);\n                        println!(\"   Step {} (epoch {}) - Excluded loss: {:.4}\",","numLines":200,"startLine":200,"totalLines":1404}}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01F84cRnhnpd2AAjJB71knsx","type":"message","role":"assistant","content":[{"type":"text","text":"Let me continue reading to see what verification functions are actually being called:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":5,"cache_creation_input_tokens":3105,"cache_read_input_tokens":33652,"cache_creation":{"ephemeral_5m_input_tokens":3105,"ephemeral_1h_input_tokens":0},"output_tokens":3,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"0e94b74a-ca53-4055-b4c7-c6d4a22e21ba"}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01F84cRnhnpd2AAjJB71knsx","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01FbC5JECWjdgCr7mpJAykQi","name":"Bash","input":{"command":"grep -n \"verify::\" src/main.rs | head -20","description":"Check which verify functions are used"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":5,"cache_creation_input_tokens":3105,"cache_read_input_tokens":33652,"cache_creation":{"ephemeral_5m_input_tokens":3105,"ephemeral_1h_input_tokens":0},"output_tokens":104,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"e91d78f2-6fc8-43b3-a884-f6dfe02650e9"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01FbC5JECWjdgCr7mpJAykQi","type":"tool_result","content":"164:        verify::GrokkingVerificationConfig::default_for_modulus(ModularAdditionDataset::modulus());\n165:    match verify::verify_grokking_phase_transition(\n249:    let (true_train_acc, true_val_acc) = verify::verify_full_accuracy(&model, &device);\n252:    verify::test_specific_examples(&model, &device);\n520:            let drop_config = verify::RestrictedLossDropConfig {\n523:            match verify::verify_restricted_loss_drop(&restricted_history.snapshots, &drop_config) {\n549:            let spike_config = verify::ExcludedLossSpikeConfig {\n552:            match verify::verify_excluded_loss_spike(&excluded_history.snapshots, &spike_config) {\n645:            let decay_config = verify::WeightNormDecayConfig::default_for_grokking();\n657:            match verify::verify_weight_norm_decay(&norm_tuples, grok_step, &decay_config) {\n701:                let manifold_config = verify::PairwiseManifoldConfig::default_for_modulus(\n704:                match verify::verify_mlp_pairwise_manifold_transition(\n1220:                match verify::collect_mlp_activation_surface(&model, device, neuron_index) {","is_error":false}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"f1cc8fa9-1d54-4153-861e-eb3c804fb0b2","tool_use_result":{"stdout":"164:        verify::GrokkingVerificationConfig::default_for_modulus(ModularAdditionDataset::modulus());\n165:    match verify::verify_grokking_phase_transition(\n249:    let (true_train_acc, true_val_acc) = verify::verify_full_accuracy(&model, &device);\n252:    verify::test_specific_examples(&model, &device);\n520:            let drop_config = verify::RestrictedLossDropConfig {\n523:            match verify::verify_restricted_loss_drop(&restricted_history.snapshots, &drop_config) {\n549:            let spike_config = verify::ExcludedLossSpikeConfig {\n552:            match verify::verify_excluded_loss_spike(&excluded_history.snapshots, &spike_config) {\n645:            let decay_config = verify::WeightNormDecayConfig::default_for_grokking();\n657:            match verify::verify_weight_norm_decay(&norm_tuples, grok_step, &decay_config) {\n701:                let manifold_config = verify::PairwiseManifoldConfig::default_for_modulus(\n704:                match verify::verify_mlp_pairwise_manifold_transition(\n1220:                match verify::collect_mlp_activation_surface(&model, device, neuron_index) {","stderr":"","interrupted":false,"isImage":false}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_016sbwayPUdynN5oo9qGooex","type":"message","role":"assistant","content":[{"type":"text","text":"Now let me check which analysis functions are used:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":5,"cache_creation_input_tokens":443,"cache_read_input_tokens":36757,"cache_creation":{"ephemeral_5m_input_tokens":443,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"2c0da072-63b4-4ed2-af61-447635c4e5b5"}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_016sbwayPUdynN5oo9qGooex","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01UqaKddp5fFdedNssxX7fg8","name":"Bash","input":{"command":"grep -n \"analysis::\" src/main.rs | head -30","description":"Check which analysis functions are used"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":5,"cache_creation_input_tokens":443,"cache_read_input_tokens":36757,"cache_creation":{"ephemeral_5m_input_tokens":443,"ephemeral_1h_input_tokens":0},"output_tokens":100,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"94da8962-9787-4c9c-8e2a-399ba75c5a6e"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01UqaKddp5fFdedNssxX7fg8","type":"tool_result","content":"269:    let fft_analysis = analysis::analyze_embeddings_fft(&model);\n271:    if let Err(e) = analysis::save_fft_analysis(&fft_analysis, &fft_analysis_path) {\n309:    let mut restricted_history = analysis::RestrictedLossHistory::new();\n310:    let mut excluded_history = analysis::ExcludedLossHistory::new();\n325:    match analysis::compute_restricted_loss(\n341:    match analysis::compute_excluded_loss(\n372:                match analysis::compute_restricted_loss(\n390:                match analysis::compute_excluded_loss(\n421:                    match analysis::compute_restricted_loss(\n441:                    match analysis::compute_excluded_loss(\n477:    match analysis::compute_restricted_loss(\n493:    match analysis::compute_excluded_loss(\n573:    let mut weight_norm_history = analysis::WeightNormHistory::new();\n580:    let initial_norm = analysis::compute_model_weight_norm(&initial_model);\n599:                let norm = analysis::compute_model_weight_norm(&checkpoint_model);\n616:                    let norm = analysis::compute_model_weight_norm(&grokking_model);\n630:    let final_norm = analysis::compute_model_weight_norm(&model);\n896:) -> (analysis::LossHistory, analysis::AccuracyHistory) {\n897:    let mut loss_history = analysis::LossHistory::new();\n898:    let mut accuracy_history = analysis::AccuracyHistory::new();\n1114:                let embeddings = analysis::extract_all_embeddings(&model);","is_error":false}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"019fca67-cf0c-4773-a670-a901f6992e4b","tool_use_result":{"stdout":"269:    let fft_analysis = analysis::analyze_embeddings_fft(&model);\n271:    if let Err(e) = analysis::save_fft_analysis(&fft_analysis, &fft_analysis_path) {\n309:    let mut restricted_history = analysis::RestrictedLossHistory::new();\n310:    let mut excluded_history = analysis::ExcludedLossHistory::new();\n325:    match analysis::compute_restricted_loss(\n341:    match analysis::compute_excluded_loss(\n372:                match analysis::compute_restricted_loss(\n390:                match analysis::compute_excluded_loss(\n421:                    match analysis::compute_restricted_loss(\n441:                    match analysis::compute_excluded_loss(\n477:    match analysis::compute_restricted_loss(\n493:    match analysis::compute_excluded_loss(\n573:    let mut weight_norm_history = analysis::WeightNormHistory::new();\n580:    let initial_norm = analysis::compute_model_weight_norm(&initial_model);\n599:                let norm = analysis::compute_model_weight_norm(&checkpoint_model);\n616:                    let norm = analysis::compute_model_weight_norm(&grokking_model);\n630:    let final_norm = analysis::compute_model_weight_norm(&model);\n896:) -> (analysis::LossHistory, analysis::AccuracyHistory) {\n897:    let mut loss_history = analysis::LossHistory::new();\n898:    let mut accuracy_history = analysis::AccuracyHistory::new();\n1114:                let embeddings = analysis::extract_all_embeddings(&model);","stderr":"","interrupted":false,"isImage":false}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01TBM7bBuTtWEpfs7u8nfpDt","type":"message","role":"assistant","content":[{"type":"text","text":"Now let me check the plotting functions:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":5,"cache_creation_input_tokens":507,"cache_read_input_tokens":37200,"cache_creation":{"ephemeral_5m_input_tokens":507,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"8363fca2-917e-4d30-9849-f8b6fd5ae8d1"}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01TBM7bBuTtWEpfs7u8nfpDt","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01D28b5W5oM2oJpWUZBwVbsF","name":"Bash","input":{"command":"grep -n \"plotting::\" src/main.rs | head -30","description":"Check which plotting functions are used"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":5,"cache_creation_input_tokens":507,"cache_read_input_tokens":37200,"cache_creation":{"ephemeral_5m_input_tokens":507,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"652b0233-fada-47f1-a94a-4830f8b35992"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01D28b5W5oM2oJpWUZBwVbsF","type":"tool_result","content":"763:    if let Err(e) = plotting::plot_loss_history_dual(\n772:    if let Err(e) = plotting::plot_accuracy_history(\n781:    if let Err(e) = plotting::plot_grokking_combined(\n796:    if let Err(e) = plotting::plot_loss_history_dual_logscale(\n805:    if let Err(e) = plotting::plot_accuracy_history_logscale(\n814:    if let Err(e) = plotting::plot_grokking_combined_logscale(\n836:        plotting::plot_fft_frequency_distribution(&fft_freq_data, &format!(\"{}/fft_frequencies.png\", viz_dir))\n853:    if let Err(e) = plotting::plot_fft_spectra(&token_spectra, &format!(\"{}/fft_spectra.png\", viz_dir)) {\n1122:                let interesting_dims = plotting::select_interesting_dimensions(&embeddings, 3);\n1139:                match plotting::plot_embedding_grid_3x3(&embeddings, &dims, &output_path, &title) {","is_error":false}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"5b4a0225-fa80-4353-99fa-f466002d4fb5","tool_use_result":{"stdout":"763:    if let Err(e) = plotting::plot_loss_history_dual(\n772:    if let Err(e) = plotting::plot_accuracy_history(\n781:    if let Err(e) = plotting::plot_grokking_combined(\n796:    if let Err(e) = plotting::plot_loss_history_dual_logscale(\n805:    if let Err(e) = plotting::plot_accuracy_history_logscale(\n814:    if let Err(e) = plotting::plot_grokking_combined_logscale(\n836:        plotting::plot_fft_frequency_distribution(&fft_freq_data, &format!(\"{}/fft_frequencies.png\", viz_dir))\n853:    if let Err(e) = plotting::plot_fft_spectra(&token_spectra, &format!(\"{}/fft_spectra.png\", viz_dir)) {\n1122:                let interesting_dims = plotting::select_interesting_dimensions(&embeddings, 3);\n1139:                match plotting::plot_embedding_grid_3x3(&embeddings, &dims, &output_path, &title) {","stderr":"","interrupted":false,"isImage":false}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01TA8N7mAzYCpGos9JtSxdoT","type":"message","role":"assistant","content":[{"type":"text","text":"Now I have a clear picture. Let me analyze the actual warnings and categorize them:\n\n**Strategy:**\n1. Functions that ARE used in tests â†’ mark with `#[cfg(test)]` or `#[allow(dead_code)]` with justification\n2. Functions that SHOULD be integrated â†’ document why keeping them\n3. Functions that are genuinely unused â†’ remove them\n\nLet me check what's used in tests:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":4,"cache_creation_input_tokens":493,"cache_read_input_tokens":37707,"cache_creation":{"ephemeral_5m_input_tokens":493,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"7fa4e207-bb35-40b5-a03f-4d50f46f7167"}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01TA8N7mAzYCpGos9JtSxdoT","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01B3BVx3h1oZJS4ocdCh5Q1n","name":"Bash","input":{"command":"grep -r \"verify_snake_curve_shape\\|verify_restricted_loss_early_drop\\|verify_mlp_activation_waves\\|verify_fft_dominant_frequencies\\|verify_embedding_clock_geometry\\|verify_constructive_interference_surface\" src/ --include=\"*.rs\" | grep -v \"^src/verify.rs:\" | head -20","description":"Check test usage of verification functions"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":4,"cache_creation_input_tokens":493,"cache_read_input_tokens":37707,"cache_creation":{"ephemeral_5m_input_tokens":493,"ephemeral_1h_input_tokens":0},"output_tokens":252,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"1278b448-9488-4781-a785-ee6e72bcef51"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01B3BVx3h1oZJS4ocdCh5Q1n","type":"tool_result","content":"src/plotting.rs:        use crate::verify::{verify_snake_curve_shape, SnakeCurveConfig};\nsrc/plotting.rs:        let report = verify_snake_curve_shape(&train_acc, &val_acc, &config)\nsrc/analysis.rs:pub fn verify_fft_dominant_frequencies<B: Backend>(\nsrc/analysis.rs:    verify_fft_dominant_frequencies_from_embeddings(&embeddings, config)\nsrc/analysis.rs:pub fn verify_fft_dominant_frequencies_from_embeddings(\nsrc/analysis.rs:pub fn verify_embedding_clock_geometry(\nsrc/analysis.rs:            verify_embedding_clock_geometry(&embeddings, modulus, &config).expect(\"expected pass\");\nsrc/analysis.rs:        let err = verify_embedding_clock_geometry(&embeddings, modulus, &config)\nsrc/analysis.rs:        let report = verify_fft_dominant_frequencies_from_embeddings(&embeddings, &config)\nsrc/analysis.rs:        let err = verify_fft_dominant_frequencies_from_embeddings(&embeddings, &config)\nsrc/analysis.rs:        let err = verify_fft_dominant_frequencies_from_embeddings(&embeddings, &config)\nsrc/analysis.rs:        let err = verify_fft_dominant_frequencies_from_embeddings(&embeddings, &config)\nsrc/analysis.rs:        let err = verify_fft_dominant_frequencies_from_embeddings(&embeddings, &config)","is_error":false}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"c8b1235d-b837-488d-8505-987d3490ff0a","tool_use_result":{"stdout":"src/plotting.rs:        use crate::verify::{verify_snake_curve_shape, SnakeCurveConfig};\nsrc/plotting.rs:        let report = verify_snake_curve_shape(&train_acc, &val_acc, &config)\nsrc/analysis.rs:pub fn verify_fft_dominant_frequencies<B: Backend>(\nsrc/analysis.rs:    verify_fft_dominant_frequencies_from_embeddings(&embeddings, config)\nsrc/analysis.rs:pub fn verify_fft_dominant_frequencies_from_embeddings(\nsrc/analysis.rs:pub fn verify_embedding_clock_geometry(\nsrc/analysis.rs:            verify_embedding_clock_geometry(&embeddings, modulus, &config).expect(\"expected pass\");\nsrc/analysis.rs:        let err = verify_embedding_clock_geometry(&embeddings, modulus, &config)\nsrc/analysis.rs:        let report = verify_fft_dominant_frequencies_from_embeddings(&embeddings, &config)\nsrc/analysis.rs:        let err = verify_fft_dominant_frequencies_from_embeddings(&embeddings, &config)\nsrc/analysis.rs:        let err = verify_fft_dominant_frequencies_from_embeddings(&embeddings, &config)\nsrc/analysis.rs:        let err = verify_fft_dominant_frequencies_from_embeddings(&embeddings, &config)\nsrc/analysis.rs:        let err = verify_fft_dominant_frequencies_from_embeddings(&embeddings, &config)","stderr":"","interrupted":false,"isImage":false}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01HwREAHyecCsuVigTJecSMh","type":"message","role":"assistant","content":[{"type":"text","text":"These verification functions are used in **tests**, not in main code. The warnings are expected because they're library functions designed for verification but not yet integrated into the main analysis pipeline. According to the task description, we should either:\n1. Keep them with `#[allow(dead_code)]` and justification\n2. Remove them if genuinely unused\n\nGiven that these are **verification functions** that were intentionally implemented for future use (as evidenced by the PROGRESS.txt entries), the right approach is to add `#[allow(dead_code)]` with clear documentation explaining they're reserved for future integration or manual verification.\n\nLet me create a comprehensive solution:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":627,"cache_read_input_tokens":38200,"cache_creation":{"ephemeral_5m_input_tokens":627,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"c717289f-0332-4917-9514-db4557ae59e4"}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01HwREAHyecCsuVigTJecSMh","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01MJUjRCAGPYv5MgaNhofyvp","name":"Read","input":{"file_path":"/Users/admin/MechInt/src/verify.rs","offset":1,"limit":100}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":627,"cache_read_input_tokens":38200,"cache_creation":{"ephemeral_5m_input_tokens":627,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"a5128ba7-c9a9-4465-9b93-2325e90190e8"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01MJUjRCAGPYv5MgaNhofyvp","type":"tool_result","content":"     1â†’use burn::{\n     2â†’    data::dataset::Dataset,\n     3â†’    tensor::{backend::Backend, Int, Tensor},\n     4â†’};\n     5â†’use rustfft::{num_complex::Complex, FftPlanner};\n     6â†’\n     7â†’use crate::analysis::{AccuracyHistory, LossHistory};\n     8â†’use crate::data::ModularAdditionDataset;\n     9â†’use crate::model::Transformer;\n    10â†’\n    11â†’#[derive(Debug, Clone)]\n    12â†’pub struct GrokkingVerificationConfig {\n    13â†’    pub early_train_acc_threshold: f32,\n    14â†’    pub early_step_max: usize,\n    15â†’    pub plateau_min_step: usize,\n    16â†’    pub generalization_window: usize,\n    17â†’    pub chance_accuracy: f32,\n    18â†’    pub chance_tolerance: f32,\n    19â†’    pub target_val_acc_threshold: f32,\n    20â†’    pub accuracy_window: usize,\n    21â†’    pub loss_window: usize,\n    22â†’    pub loss_drop_fraction: f64,\n    23â†’}\n    24â†’\n    25â†’impl GrokkingVerificationConfig {\n    26â†’    pub fn default_for_modulus(modulus: usize) -> Self {\n    27â†’        Self {\n    28â†’            early_train_acc_threshold: 0.99,\n    29â†’            early_step_max: 1000,\n    30â†’            plateau_min_step: 1000,\n    31â†’            generalization_window: 10_000,\n    32â†’            chance_accuracy: 1.0 / modulus as f32,\n    33â†’            chance_tolerance: 0.02,\n    34â†’            target_val_acc_threshold: 0.99,\n    35â†’            accuracy_window: 5,\n    36â†’            loss_window: 5,\n    37â†’            loss_drop_fraction: 0.5,\n    38â†’        }\n    39â†’    }\n    40â†’}\n    41â†’\n    42â†’#[derive(Debug, Clone)]\n    43â†’pub struct GrokkingPhaseReport {\n    44â†’    pub early_train_step: usize,\n    45â†’    pub transition_step: usize,\n    46â†’    pub plateau_max_val_acc: f32,\n    47â†’    pub loss_drop_ratio: f64,\n    48â†’}\n    49â†’\n    50â†’#[derive(Debug, Clone)]\n    51â†’pub struct RestrictedLossVerificationConfig {\n    52â†’    pub plateau_min_step: usize,\n    53â†’    pub drop_window: usize,\n    54â†’    pub drop_fraction: f64,\n    55â†’    pub min_step_lead: usize,\n    56â†’}\n    57â†’\n    58â†’#[derive(Debug, Clone)]\n    59â†’pub struct RestrictedLossReport {\n    60â†’    pub restricted_drop_step: usize,\n    61â†’    pub full_drop_step: usize,\n    62â†’}\n    63â†’\n    64â†’#[derive(Debug, Clone)]\n    65â†’pub struct RestrictedLossDropConfig {\n    66â†’    pub min_relative_decrease: f64,\n    67â†’}\n    68â†’\n    69â†’#[derive(Debug, Clone)]\n    70â†’pub struct RestrictedLossDropReport {\n    71â†’    pub drop_step: usize,\n    72â†’    pub drop_loss: f64,\n    73â†’    pub baseline_loss: f64,\n    74â†’}\n    75â†’\n    76â†’#[derive(Debug, Clone)]\n    77â†’pub struct ExcludedLossSpikeConfig {\n    78â†’    pub min_relative_increase: f64,\n    79â†’}\n    80â†’\n    81â†’#[derive(Debug, Clone)]\n    82â†’pub struct ExcludedLossSpikeReport {\n    83â†’    pub spike_step: usize,\n    84â†’    pub spike_loss: f64,\n    85â†’    pub baseline_loss: f64,\n    86â†’}\n    87â†’\n    88â†’#[derive(Debug, Clone)]\n    89â†’pub struct MlpWaveVerificationConfig {\n    90â†’    pub max_negative_fraction: f64,\n    91â†’    pub negative_tolerance: f64,\n    92â†’    pub min_dominant_ratio: f64,\n    93â†’    pub min_correlation: f64,\n    94â†’    pub phase_steps: usize,\n    95â†’}\n    96â†’\n    97â†’#[derive(Debug, Clone)]\n    98â†’pub struct MlpWaveNeuronReport {\n    99â†’    pub neuron_index: usize,\n   100â†’    pub dominant_frequency: usize,\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"4c6f51ca-7e15-4ebb-888a-65e6dec7ab08","tool_use_result":{"type":"text","file":{"filePath":"/Users/admin/MechInt/src/verify.rs","content":"use burn::{\n    data::dataset::Dataset,\n    tensor::{backend::Backend, Int, Tensor},\n};\nuse rustfft::{num_complex::Complex, FftPlanner};\n\nuse crate::analysis::{AccuracyHistory, LossHistory};\nuse crate::data::ModularAdditionDataset;\nuse crate::model::Transformer;\n\n#[derive(Debug, Clone)]\npub struct GrokkingVerificationConfig {\n    pub early_train_acc_threshold: f32,\n    pub early_step_max: usize,\n    pub plateau_min_step: usize,\n    pub generalization_window: usize,\n    pub chance_accuracy: f32,\n    pub chance_tolerance: f32,\n    pub target_val_acc_threshold: f32,\n    pub accuracy_window: usize,\n    pub loss_window: usize,\n    pub loss_drop_fraction: f64,\n}\n\nimpl GrokkingVerificationConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        Self {\n            early_train_acc_threshold: 0.99,\n            early_step_max: 1000,\n            plateau_min_step: 1000,\n            generalization_window: 10_000,\n            chance_accuracy: 1.0 / modulus as f32,\n            chance_tolerance: 0.02,\n            target_val_acc_threshold: 0.99,\n            accuracy_window: 5,\n            loss_window: 5,\n            loss_drop_fraction: 0.5,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct GrokkingPhaseReport {\n    pub early_train_step: usize,\n    pub transition_step: usize,\n    pub plateau_max_val_acc: f32,\n    pub loss_drop_ratio: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossVerificationConfig {\n    pub plateau_min_step: usize,\n    pub drop_window: usize,\n    pub drop_fraction: f64,\n    pub min_step_lead: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossReport {\n    pub restricted_drop_step: usize,\n    pub full_drop_step: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossDropConfig {\n    pub min_relative_decrease: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossDropReport {\n    pub drop_step: usize,\n    pub drop_loss: f64,\n    pub baseline_loss: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct ExcludedLossSpikeConfig {\n    pub min_relative_increase: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct ExcludedLossSpikeReport {\n    pub spike_step: usize,\n    pub spike_loss: f64,\n    pub baseline_loss: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct MlpWaveVerificationConfig {\n    pub max_negative_fraction: f64,\n    pub negative_tolerance: f64,\n    pub min_dominant_ratio: f64,\n    pub min_correlation: f64,\n    pub phase_steps: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct MlpWaveNeuronReport {\n    pub neuron_index: usize,\n    pub dominant_frequency: usize,","numLines":100,"startLine":1,"totalLines":2190}}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01PQDC4pF8sJu84pv4XTVwjg","type":"message","role":"assistant","content":[{"type":"text","text":"Now I'll implement a systematic cleanup approach. Based on my analysis:\n\n1. **Keep with `#[allow(dead_code)]`**: Verification functions that are implemented but not yet integrated into main pipeline\n2. **Keep**: Structs used as return types or configs  \n3. **Remove**: Truly unused utility functions\n\nLet me start by identifying what to keep vs remove:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":1519,"cache_read_input_tokens":38827,"cache_creation":{"ephemeral_5m_input_tokens":1519,"ephemeral_1h_input_tokens":0},"output_tokens":4,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"1e37c418-808c-46bd-bad7-d12e1a73533f"}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01PQDC4pF8sJu84pv4XTVwjg","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01XEPbihyNxVP4xcQzTnkdzb","name":"Bash","input":{"command":"cargo build 2>&1 | grep \"warning:\" | grep \"is never\" | awk '{for(i=1;i<=NF;i++) if($i==\"function\" || $i==\"struct\" || $i==\"method\" || $i==\"associated\") print $(i+1), $i}' | sort -u","description":"Extract all dead code items","timeout":120000}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":1519,"cache_read_input_tokens":38827,"cache_creation":{"ephemeral_5m_input_tokens":1519,"ephemeral_1h_input_tokens":0},"output_tokens":4,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"709a953c-33dd-4f4c-b5da-e43d6ec6f3e2"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01XEPbihyNxVP4xcQzTnkdzb","type":"tool_result","content":"`add_snapshot` method\n`analyze_mlp_activation_wave` function\n`angular_coverage` function\n`average_autocorrelation` function\n`baseline_loss` function\n`best_rectified_sine_correlation` function\n`default_for_modulus` function\n`DiagonalRidgeConfig` struct\n`DiagonalRidgeReport` struct\n`dominant_frequency_ratio` function\n`dot` function\n`EmbeddingClockConfig` struct\n`EmbeddingClockReport` struct\n`FFTDominantFrequencyConfig` struct\n`FFTDominantFrequencyReport` struct\n`find_drop_step` function\n`list_checkpoints` function\n`load_json` function\n`MlpWaveNeuronReport` struct\n`MlpWaveReport` struct\n`MlpWaveVerificationConfig` struct\n`normalize_vector` function\n`order_fraction` function\n`pearson_correlation` function\n`plot_embedding_grid_fast` function\n`plot_loss_history` function\n`project_embeddings_to_fourier_plane` function\n`render_dimension_plot` function\n`render_scatter_plot` function\n`RestrictedLossReport` struct\n`RestrictedLossVerificationConfig` struct\n`save_checkpoint` function\n`save_labeled_checkpoint` function\n`SnakeCurveConfig` struct\n`SnakeCurveReport` struct\n`surface_autocorrelation_scores` function\n`surface_dimensions` function\n`surface_mean_variance` function\n`token_to_rgb` function\n`verify_constructive_interference_surface_from_grid` function\n`verify_embedding_clock_geometry` function\n`verify_fft_dominant_frequencies_from_embeddings` function\n`verify_fft_dominant_frequencies` function\n`verify_mlp_activation_waves_from_signals` function\n`verify_mlp_activation_waves` function\n`verify_restricted_loss_early_drop` function\n`verify_snake_curve_shape` function\n`viridis_to_rgb` function\n`wrap_angle_signed` function\n`wrap_angle` function\nfunction associated","is_error":false}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"89a919e6-e175-44ed-8f06-449c02c451a8","tool_use_result":{"stdout":"`add_snapshot` method\n`analyze_mlp_activation_wave` function\n`angular_coverage` function\n`average_autocorrelation` function\n`baseline_loss` function\n`best_rectified_sine_correlation` function\n`default_for_modulus` function\n`DiagonalRidgeConfig` struct\n`DiagonalRidgeReport` struct\n`dominant_frequency_ratio` function\n`dot` function\n`EmbeddingClockConfig` struct\n`EmbeddingClockReport` struct\n`FFTDominantFrequencyConfig` struct\n`FFTDominantFrequencyReport` struct\n`find_drop_step` function\n`list_checkpoints` function\n`load_json` function\n`MlpWaveNeuronReport` struct\n`MlpWaveReport` struct\n`MlpWaveVerificationConfig` struct\n`normalize_vector` function\n`order_fraction` function\n`pearson_correlation` function\n`plot_embedding_grid_fast` function\n`plot_loss_history` function\n`project_embeddings_to_fourier_plane` function\n`render_dimension_plot` function\n`render_scatter_plot` function\n`RestrictedLossReport` struct\n`RestrictedLossVerificationConfig` struct\n`save_checkpoint` function\n`save_labeled_checkpoint` function\n`SnakeCurveConfig` struct\n`SnakeCurveReport` struct\n`surface_autocorrelation_scores` function\n`surface_dimensions` function\n`surface_mean_variance` function\n`token_to_rgb` function\n`verify_constructive_interference_surface_from_grid` function\n`verify_embedding_clock_geometry` function\n`verify_fft_dominant_frequencies_from_embeddings` function\n`verify_fft_dominant_frequencies` function\n`verify_mlp_activation_waves_from_signals` function\n`verify_mlp_activation_waves` function\n`verify_restricted_loss_early_drop` function\n`verify_snake_curve_shape` function\n`viridis_to_rgb` function\n`wrap_angle_signed` function\n`wrap_angle` function\nfunction associated","stderr":"","interrupted":false,"isImage":false}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01V6hgAFyz2bkswPWo7sCoNi","type":"message","role":"assistant","content":[{"type":"text","text":"Now let me create a comprehensive cleanup plan. I'll add `#[allow(dead_code)]` to functions/structs that are part of the verification API but not yet integrated, and remove truly useless helper functions.\n\nLet me start with the analysis module:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":775,"cache_read_input_tokens":40346,"cache_creation":{"ephemeral_5m_input_tokens":775,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"b82c58ca-2f93-4596-96b2-6e04ea020157"}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01V6hgAFyz2bkswPWo7sCoNi","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01MjyEBafrom3b412X6Q7iEt","name":"Read","input":{"file_path":"/Users/admin/MechInt/src/analysis.rs","offset":530,"limit":50}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":775,"cache_read_input_tokens":40346,"cache_creation":{"ephemeral_5m_input_tokens":775,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"910eba23-213e-4849-b25f-32c696a9a163"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01MjyEBafrom3b412X6Q7iEt","type":"tool_result","content":"   530â†’    let vocab_size = ModularAdditionDataset::vocab_size();\n   531â†’    let mut embeddings = Vec::with_capacity(vocab_size);\n   532â†’\n   533â†’    for token_id in 0..vocab_size {\n   534â†’        embeddings.push(get_token_embedding(model, token_id));\n   535â†’    }\n   536â†’\n   537â†’    embeddings\n   538â†’}\n   539â†’\n   540â†’/// Verify that FFT analysis of embeddings shows dominant frequencies\n   541â†’/// that align with the modulus structure and nontrivial spectral patterns\n   542â†’pub fn verify_fft_dominant_frequencies<B: Backend>(\n   543â†’    model: &Transformer<B>,\n   544â†’    config: &FFTDominantFrequencyConfig,\n   545â†’) -> Result<FFTDominantFrequencyReport, String> {\n   546â†’    let embeddings = extract_all_embeddings(model);\n   547â†’    verify_fft_dominant_frequencies_from_embeddings(&embeddings, config)\n   548â†’}\n   549â†’\n   550â†’/// Verify FFT dominant frequencies from pre-extracted embeddings\n   551â†’pub fn verify_fft_dominant_frequencies_from_embeddings(\n   552â†’    embeddings: &[Vec<f64>],\n   553â†’    config: &FFTDominantFrequencyConfig,\n   554â†’) -> Result<FFTDominantFrequencyReport, String> {\n   555â†’    if embeddings.is_empty() {\n   556â†’        return Err(\"no embeddings provided\".to_string());\n   557â†’    }\n   558â†’\n   559â†’    let vocab_size = embeddings.len();\n   560â†’    if vocab_size < config.modulus {\n   561â†’        return Err(format!(\n   562â†’            \"embedding count {} less than modulus {}\",\n   563â†’            vocab_size, config.modulus\n   564â†’        ));\n   565â†’    }\n   566â†’\n   567â†’    // Analyze each embedding dimension and collect dominant frequencies\n   568â†’    let mut frequency_histogram: HashMap<usize, usize> = HashMap::new();\n   569â†’    let embedding_dim = embeddings[0].len();\n   570â†’\n   571â†’    // For each embedding dimension, compute FFT across all tokens and find dominant frequency\n   572â†’    for dim in 0..embedding_dim {\n   573â†’        let mut signal = Vec::with_capacity(vocab_size);\n   574â†’        for embedding in embeddings.iter().take(vocab_size) {\n   575â†’            if dim >= embedding.len() {\n   576â†’                return Err(format!(\n   577â†’                    \"embedding dimension mismatch at dim {}\",\n   578â†’                    dim\n   579â†’                ));\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"f4d3bd7c-f524-4d8f-92a2-21c12b38c176","tool_use_result":{"type":"text","file":{"filePath":"/Users/admin/MechInt/src/analysis.rs","content":"    let vocab_size = ModularAdditionDataset::vocab_size();\n    let mut embeddings = Vec::with_capacity(vocab_size);\n\n    for token_id in 0..vocab_size {\n        embeddings.push(get_token_embedding(model, token_id));\n    }\n\n    embeddings\n}\n\n/// Verify that FFT analysis of embeddings shows dominant frequencies\n/// that align with the modulus structure and nontrivial spectral patterns\npub fn verify_fft_dominant_frequencies<B: Backend>(\n    model: &Transformer<B>,\n    config: &FFTDominantFrequencyConfig,\n) -> Result<FFTDominantFrequencyReport, String> {\n    let embeddings = extract_all_embeddings(model);\n    verify_fft_dominant_frequencies_from_embeddings(&embeddings, config)\n}\n\n/// Verify FFT dominant frequencies from pre-extracted embeddings\npub fn verify_fft_dominant_frequencies_from_embeddings(\n    embeddings: &[Vec<f64>],\n    config: &FFTDominantFrequencyConfig,\n) -> Result<FFTDominantFrequencyReport, String> {\n    if embeddings.is_empty() {\n        return Err(\"no embeddings provided\".to_string());\n    }\n\n    let vocab_size = embeddings.len();\n    if vocab_size < config.modulus {\n        return Err(format!(\n            \"embedding count {} less than modulus {}\",\n            vocab_size, config.modulus\n        ));\n    }\n\n    // Analyze each embedding dimension and collect dominant frequencies\n    let mut frequency_histogram: HashMap<usize, usize> = HashMap::new();\n    let embedding_dim = embeddings[0].len();\n\n    // For each embedding dimension, compute FFT across all tokens and find dominant frequency\n    for dim in 0..embedding_dim {\n        let mut signal = Vec::with_capacity(vocab_size);\n        for embedding in embeddings.iter().take(vocab_size) {\n            if dim >= embedding.len() {\n                return Err(format!(\n                    \"embedding dimension mismatch at dim {}\",\n                    dim\n                ));","numLines":50,"startLine":530,"totalLines":1185}}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01AhCCnuYdQ8zwPcXmbjYdJQ","type":"message","role":"assistant","content":[{"type":"text","text":"Now I'll implement a systematic cleanup. The approach:\n\n1. **Verification functions used in tests** â†’ Add `#[allow(dead_code)]` with documentation\n2. **Config/Report structs** â†’ Keep (they're part of public API even if not constructed in main)\n3. **Utility functions never used** â†’ Remove completely\n4. **Methods on structs** â†’ Add `#[allow(dead_code)]` if intentional API\n\nLet me start:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":926,"cache_read_input_tokens":41121,"cache_creation":{"ephemeral_5m_input_tokens":926,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"b8fd421b-eecd-43dd-b03e-93d219e4eb57"}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01AhCCnuYdQ8zwPcXmbjYdJQ","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01UjoEZUYawgAkr7XfaCehzp","name":"Edit","input":{"replace_all":false,"file_path":"/Users/admin/MechInt/src/analysis.rs","old_string":"/// Verify that FFT analysis of embeddings shows dominant frequencies\n/// that align with the modulus structure and nontrivial spectral patterns\npub fn verify_fft_dominant_frequencies<B: Backend>(\n    model: &Transformer<B>,\n    config: &FFTDominantFrequencyConfig,\n) -> Result<FFTDominantFrequencyReport, String> {\n    let embeddings = extract_all_embeddings(model);\n    verify_fft_dominant_frequencies_from_embeddings(&embeddings, config)\n}\n\n/// Verify FFT dominant frequencies from pre-extracted embeddings\npub fn verify_fft_dominant_frequencies_from_embeddings(","new_string":"/// Verify that FFT analysis of embeddings shows dominant frequencies\n/// that align with the modulus structure and nontrivial spectral patterns\n///\n/// NOTE: Reserved for manual verification or future integration into analysis pipeline\n#[allow(dead_code)]\npub fn verify_fft_dominant_frequencies<B: Backend>(\n    model: &Transformer<B>,\n    config: &FFTDominantFrequencyConfig,\n) -> Result<FFTDominantFrequencyReport, String> {\n    let embeddings = extract_all_embeddings(model);\n    verify_fft_dominant_frequencies_from_embeddings(&embeddings, config)\n}\n\n/// Verify FFT dominant frequencies from pre-extracted embeddings\n///\n/// NOTE: Reserved for manual verification or future integration into analysis pipeline\n#[allow(dead_code)]\npub fn verify_fft_dominant_frequencies_from_embeddings("}}],"stop_reason":"tool_use","stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":926,"cache_read_input_tokens":41121,"cache_creation":{"ephemeral_5m_input_tokens":926,"ephemeral_1h_input_tokens":0},"output_tokens":574,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"d6a5389a-6fa5-4a9e-9aa4-35e6000bb333"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01UjoEZUYawgAkr7XfaCehzp","type":"tool_result","content":"The file /Users/admin/MechInt/src/analysis.rs has been updated. Here's the result of running `cat -n` on a snippet of the edited file:\n   536â†’\n   537â†’    embeddings\n   538â†’}\n   539â†’\n   540â†’/// Verify that FFT analysis of embeddings shows dominant frequencies\n   541â†’/// that align with the modulus structure and nontrivial spectral patterns\n   542â†’///\n   543â†’/// NOTE: Reserved for manual verification or future integration into analysis pipeline\n   544â†’#[allow(dead_code)]\n   545â†’pub fn verify_fft_dominant_frequencies<B: Backend>(\n   546â†’    model: &Transformer<B>,\n   547â†’    config: &FFTDominantFrequencyConfig,\n   548â†’) -> Result<FFTDominantFrequencyReport, String> {\n   549â†’    let embeddings = extract_all_embeddings(model);\n   550â†’    verify_fft_dominant_frequencies_from_embeddings(&embeddings, config)\n   551â†’}\n   552â†’\n   553â†’/// Verify FFT dominant frequencies from pre-extracted embeddings\n   554â†’///\n   555â†’/// NOTE: Reserved for manual verification or future integration into analysis pipeline\n   556â†’#[allow(dead_code)]\n   557â†’pub fn verify_fft_dominant_frequencies_from_embeddings(\n   558â†’    embeddings: &[Vec<f64>],\n   559â†’    config: &FFTDominantFrequencyConfig,\n   560â†’) -> Result<FFTDominantFrequencyReport, String> {\n   561â†’    if embeddings.is_empty() {"}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"f6b27c68-5ca8-4846-bb79-fe5efc7be229","tool_use_result":{"filePath":"/Users/admin/MechInt/src/analysis.rs","oldString":"/// Verify that FFT analysis of embeddings shows dominant frequencies\n/// that align with the modulus structure and nontrivial spectral patterns\npub fn verify_fft_dominant_frequencies<B: Backend>(\n    model: &Transformer<B>,\n    config: &FFTDominantFrequencyConfig,\n) -> Result<FFTDominantFrequencyReport, String> {\n    let embeddings = extract_all_embeddings(model);\n    verify_fft_dominant_frequencies_from_embeddings(&embeddings, config)\n}\n\n/// Verify FFT dominant frequencies from pre-extracted embeddings\npub fn verify_fft_dominant_frequencies_from_embeddings(","newString":"/// Verify that FFT analysis of embeddings shows dominant frequencies\n/// that align with the modulus structure and nontrivial spectral patterns\n///\n/// NOTE: Reserved for manual verification or future integration into analysis pipeline\n#[allow(dead_code)]\npub fn verify_fft_dominant_frequencies<B: Backend>(\n    model: &Transformer<B>,\n    config: &FFTDominantFrequencyConfig,\n) -> Result<FFTDominantFrequencyReport, String> {\n    let embeddings = extract_all_embeddings(model);\n    verify_fft_dominant_frequencies_from_embeddings(&embeddings, config)\n}\n\n/// Verify FFT dominant frequencies from pre-extracted embeddings\n///\n/// NOTE: Reserved for manual verification or future integration into analysis pipeline\n#[allow(dead_code)]\npub fn verify_fft_dominant_frequencies_from_embeddings(","originalFile":"use burn::{\n    data::dataset::Dataset,\n    nn::loss::CrossEntropyLossConfig,\n    tensor::{backend::Backend, Int, Tensor, TensorData},\n};\nuse rustfft::{num_complex::Complex, FftPlanner};\nuse serde::{Deserialize, Serialize};\nuse std::collections::{HashMap, HashSet};\n\nuse crate::{data::ModularAdditionDataset, model::Transformer};\n\n/// FFT analysis results for embeddings\n#[derive(Debug, Serialize, Deserialize)]\npub struct FFTAnalysis {\n    pub vocab_size: usize,\n    pub embedding_dim: usize,\n    pub dominant_frequencies: Vec<(usize, f64)>, // (token_id, dominant_frequency_index)\n    pub frequency_magnitudes: HashMap<usize, Vec<f64>>, // token_id -> magnitude spectrum\n}\n\n/// Analyze the learned token embeddings using FFT\npub fn analyze_embeddings_fft<B: Backend>(model: &Transformer<B>) -> FFTAnalysis {\n    println!(\"ðŸ” Performing FFT analysis on learned embeddings...\");\n\n    let vocab_size = ModularAdditionDataset::vocab_size();\n    let embedding_dim = 128;\n\n    // Extract token embeddings by forward passing through the embedding layer\n    let mut frequency_magnitudes = HashMap::new();\n    let mut dominant_frequencies = Vec::new();\n\n    for token_id in 0..vocab_size {\n        // Get the embedding for this token\n        let embedding = get_token_embedding(model, token_id);\n\n        // Compute FFT on the embedding vector\n        let fft_result = compute_fft(&embedding);\n\n        // Find dominant frequency\n        let dominant_freq_idx = find_dominant_frequency(&fft_result);\n        dominant_frequencies.push((token_id, dominant_freq_idx as f64));\n\n        // Store magnitude spectrum\n        frequency_magnitudes.insert(token_id, fft_result);\n\n        if token_id % 10 == 0 {\n            println!(\"  Token {:2}: dominant frequency = {}\", token_id, dominant_freq_idx);\n        }\n    }\n\n    println!(\"âœ… FFT analysis complete!\");\n    println!();\n    println!(\"ðŸ“Š Dominant Frequency Analysis:\");\n\n    // Analyze patterns\n    let mut freq_histogram: HashMap<usize, usize> = HashMap::new();\n    for (_, freq) in &dominant_frequencies {\n        *freq_histogram.entry(*freq as usize).or_insert(0) += 1;\n    }\n\n    let mut freq_counts: Vec<_> = freq_histogram.iter().collect();\n    freq_counts.sort_by(|a, b| b.1.cmp(a.1));\n\n    println!(\"  Most common dominant frequencies:\");\n    for (freq, count) in freq_counts.iter().take(5) {\n        println!(\"    Frequency {}: {} tokens\", freq, count);\n    }\n\n    // Check for modular structure (expected for mod p)\n    println!();\n    println!(\"ðŸ”¬ Checking for modular structure:\");\n    let modulus = ModularAdditionDataset::modulus();\n    if freq_histogram.contains_key(&modulus) {\n        println!(\n            \"  âœ“ Frequency {} (matching modulus) appears in embeddings\",\n            modulus\n        );\n    }\n\n    FFTAnalysis {\n        vocab_size,\n        embedding_dim,\n        dominant_frequencies,\n        frequency_magnitudes,\n    }\n}\n\n/// Extract the token embedding for a single token\nfn get_token_embedding<B: Backend>(model: &Transformer<B>, token_id: usize) -> Vec<f64> {\n    // Get the token embedding directly from the model\n    let embedding_tensor = model.get_token_embedding(token_id);\n\n    // Convert to vector - shape is [1, 1, embedding_dim], we want [embedding_dim]\n    let embedding_data: Vec<f32> = embedding_tensor.into_data().to_vec().unwrap();\n\n    // Convert to f64\n    embedding_data.iter().map(|&x| x as f64).collect()\n}\n\n/// Compute FFT on a vector of real values\nfn compute_fft(data: &[f64]) -> Vec<f64> {\n    let mut planner = FftPlanner::new();\n    let fft = planner.plan_fft_forward(data.len());\n\n    // Convert to complex numbers\n    let mut buffer: Vec<Complex<f64>> = data\n        .iter()\n        .map(|&x| Complex::new(x, 0.0))\n        .collect();\n\n    // Perform FFT\n    fft.process(&mut buffer);\n\n    // Compute magnitudes\n    buffer.iter().map(|c| c.norm()).collect()\n}\n\nfn inverse_fft(data: &mut [Complex<f64>]) {\n    let mut planner = FftPlanner::new();\n    let fft = planner.plan_fft_inverse(data.len());\n    fft.process(data);\n}\n\nfn top_k_frequency_indices(magnitudes: &[f64], k: usize) -> Vec<usize> {\n    let mut indexed: Vec<(usize, f64)> = magnitudes.iter().copied().enumerate().collect();\n    indexed.sort_by(|(idx_a, mag_a), (idx_b, mag_b)| {\n        mag_b\n            .partial_cmp(mag_a)\n            .unwrap_or(std::cmp::Ordering::Equal)\n            .then_with(|| idx_a.cmp(idx_b))\n    });\n\n    let keep = k.min(indexed.len());\n    indexed.iter().take(keep).map(|(idx, _)| *idx).collect()\n}\n\nfn restrict_signal_to_top_k_frequencies(signal: &[f64], top_k: usize) -> Vec<f64> {\n    if signal.is_empty() {\n        return Vec::new();\n    }\n\n    if top_k == 0 {\n        return vec![0.0; signal.len()];\n    }\n\n    let mut planner = FftPlanner::new();\n    let fft = planner.plan_fft_forward(signal.len());\n\n    let mut buffer: Vec<Complex<f64>> = signal\n        .iter()\n        .map(|&x| Complex::new(x, 0.0))\n        .collect();\n    fft.process(&mut buffer);\n\n    let magnitudes: Vec<f64> = buffer.iter().map(|c| c.norm()).collect();\n    let keep = top_k_frequency_indices(&magnitudes, top_k);\n    let keep_set: HashSet<usize> = keep.into_iter().collect();\n\n    for (idx, value) in buffer.iter_mut().enumerate() {\n        if !keep_set.contains(&idx) {\n            *value = Complex::new(0.0, 0.0);\n        }\n    }\n\n    inverse_fft(&mut buffer);\n\n    let scale = 1.0 / signal.len() as f64;\n    buffer.iter().map(|c| c.re * scale).collect()\n}\n\nfn exclude_top_k_frequencies(signal: &[f64], top_k: usize) -> Vec<f64> {\n    if signal.is_empty() {\n        return Vec::new();\n    }\n\n    if top_k == 0 {\n        return signal.to_vec();\n    }\n\n    if top_k >= signal.len() {\n        return vec![0.0; signal.len()];\n    }\n\n    let mut planner = FftPlanner::new();\n    let fft = planner.plan_fft_forward(signal.len());\n\n    let mut buffer: Vec<Complex<f64>> = signal\n        .iter()\n        .map(|&x| Complex::new(x, 0.0))\n        .collect();\n    fft.process(&mut buffer);\n\n    let magnitudes: Vec<f64> = buffer.iter().map(|c| c.norm()).collect();\n    let remove = top_k_frequency_indices(&magnitudes, top_k);\n    let remove_set: HashSet<usize> = remove.into_iter().collect();\n\n    for (idx, value) in buffer.iter_mut().enumerate() {\n        if remove_set.contains(&idx) {\n            *value = Complex::new(0.0, 0.0);\n        }\n    }\n\n    inverse_fft(&mut buffer);\n\n    let scale = 1.0 / signal.len() as f64;\n    buffer.iter().map(|c| c.re * scale).collect()\n}\n\nfn restricted_token_embedding_weights<B: Backend>(\n    model: &Transformer<B>,\n    top_k: usize,\n) -> Vec<f32> {\n    let weight = model.token_embedding_weights();\n    let [vocab_size, d_model] = weight.dims();\n    let weight_data: Vec<f32> = weight.into_data().to_vec().unwrap();\n    let mut restricted = vec![0.0f32; weight_data.len()];\n\n    for dim in 0..d_model {\n        let mut column = Vec::with_capacity(vocab_size);\n        for token in 0..vocab_size {\n            column.push(weight_data[token * d_model + dim] as f64);\n        }\n        let filtered = restrict_signal_to_top_k_frequencies(&column, top_k);\n        for token in 0..vocab_size {\n            restricted[token * d_model + dim] = filtered[token] as f32;\n        }\n    }\n\n    restricted\n}\n\nfn excluded_token_embedding_weights<B: Backend>(\n    model: &Transformer<B>,\n    top_k: usize,\n) -> Vec<f32> {\n    let weight = model.token_embedding_weights();\n    let [vocab_size, d_model] = weight.dims();\n    let weight_data: Vec<f32> = weight.into_data().to_vec().unwrap();\n    let mut excluded = vec![0.0f32; weight_data.len()];\n\n    for dim in 0..d_model {\n        let mut column = Vec::with_capacity(vocab_size);\n        for token in 0..vocab_size {\n            column.push(weight_data[token * d_model + dim] as f64);\n        }\n        let filtered = exclude_top_k_frequencies(&column, top_k);\n        for token in 0..vocab_size {\n            excluded[token * d_model + dim] = filtered[token] as f32;\n        }\n    }\n\n    excluded\n}\n\npub fn compute_restricted_loss<B: Backend>(\n    model: &Transformer<B>,\n    dataset: &ModularAdditionDataset,\n    device: &B::Device,\n    top_k: usize,\n    batch_size: usize,\n) -> Result<f64, String> {\n    if batch_size == 0 {\n        return Err(\"batch_size must be greater than 0\".to_string());\n    }\n\n    let restricted_weights = restricted_token_embedding_weights(model, top_k);\n    let [vocab_size, d_model] = model.token_embedding_weights().dims();\n    let token_weights = Tensor::<B, 2>::from_data(\n        TensorData::new(restricted_weights, [vocab_size, d_model]),\n        device,\n    );\n\n    let loss_fn = CrossEntropyLossConfig::new().init(device);\n    let mut total_loss = 0.0f64;\n    let mut total_batches = 0usize;\n\n    for batch_start in (0..dataset.len()).step_by(batch_size) {\n        let batch_end = (batch_start + batch_size).min(dataset.len());\n        let batch_len = batch_end - batch_start;\n        if batch_len == 0 {\n            break;\n        }\n\n        let mut inputs_vec = Vec::with_capacity(batch_len * 3);\n        let mut targets_vec = Vec::with_capacity(batch_len);\n\n        for idx in batch_start..batch_end {\n            let (input, target) = dataset.get(idx).ok_or_else(|| {\n                format!(\"missing dataset sample at index {}\", idx)\n            })?;\n            inputs_vec.extend(input.iter().map(|value| *value as i32));\n            targets_vec.push(target as i32);\n        }\n\n        let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n            .reshape([batch_len, 3]);\n        let targets = Tensor::<B, 1, Int>::from_ints(targets_vec.as_slice(), device);\n\n        let logits = model.forward_with_token_weights(inputs, token_weights.clone());\n        let loss = loss_fn.forward(logits, targets);\n        let loss_value: f64 = loss.into_data().to_vec().unwrap().get(0).copied().unwrap_or(0.0);\n        total_loss += loss_value;\n        total_batches += 1;\n    }\n\n    if total_batches == 0 {\n        return Err(\"no batches computed for restricted loss\".to_string());\n    }\n\n    Ok(total_loss / total_batches as f64)\n}\n\npub fn compute_excluded_loss<B: Backend>(\n    model: &Transformer<B>,\n    dataset: &ModularAdditionDataset,\n    device: &B::Device,\n    top_k: usize,\n    batch_size: usize,\n) -> Result<f64, String> {\n    if batch_size == 0 {\n        return Err(\"batch_size must be greater than 0\".to_string());\n    }\n\n    let excluded_weights = excluded_token_embedding_weights(model, top_k);\n    let [vocab_size, d_model] = model.token_embedding_weights().dims();\n    let token_weights = Tensor::<B, 2>::from_data(\n        TensorData::new(excluded_weights, [vocab_size, d_model]),\n        device,\n    );\n\n    let loss_fn = CrossEntropyLossConfig::new().init(device);\n    let mut total_loss = 0.0f64;\n    let mut total_batches = 0usize;\n\n    for batch_start in (0..dataset.len()).step_by(batch_size) {\n        let batch_end = (batch_start + batch_size).min(dataset.len());\n        let batch_len = batch_end - batch_start;\n        if batch_len == 0 {\n            break;\n        }\n\n        let mut inputs_vec = Vec::with_capacity(batch_len * 3);\n        let mut targets_vec = Vec::with_capacity(batch_len);\n\n        for idx in batch_start..batch_end {\n            let (input, target) = dataset.get(idx).ok_or_else(|| {\n                format!(\"missing dataset sample at index {}\", idx)\n            })?;\n            inputs_vec.extend(input.iter().map(|value| *value as i32));\n            targets_vec.push(target as i32);\n        }\n\n        let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n            .reshape([batch_len, 3]);\n        let targets = Tensor::<B, 1, Int>::from_ints(targets_vec.as_slice(), device);\n\n        let logits = model.forward_with_token_weights(inputs, token_weights.clone());\n        let loss = loss_fn.forward(logits, targets);\n        let loss_value: f64 = loss.into_data().to_vec().unwrap().get(0).copied().unwrap_or(0.0);\n        total_loss += loss_value;\n        total_batches += 1;\n    }\n\n    if total_batches == 0 {\n        return Err(\"no batches computed for excluded loss\".to_string());\n    }\n\n    Ok(total_loss / total_batches as f64)\n}\n\n/// Find the index of the dominant frequency (excluding DC component at index 0)\nfn find_dominant_frequency(magnitudes: &[f64]) -> usize {\n    magnitudes\n        .iter()\n        .enumerate()\n        .skip(1) // Skip DC component\n        .max_by(|(_, a), (_, b)| {\n            a.partial_cmp(b).unwrap_or(std::cmp::Ordering::Equal)\n        })\n        .map(|(idx, _)| idx)\n        .unwrap_or(0)\n}\n\n/// Save FFT analysis to JSON file\npub fn save_fft_analysis(analysis: &FFTAnalysis, filename: &str) -> std::io::Result<()> {\n    let json = serde_json::to_string_pretty(analysis)?;\n    std::fs::write(filename, json)?;\n    println!(\"ðŸ’¾ FFT analysis saved to: {}\", filename);\n    Ok(())\n}\n\n/// Track training and validation loss over time\n#[derive(Debug, Serialize, Deserialize)]\npub struct LossHistory {\n    pub train_snapshots: Vec<(usize, f64)>, // (step, loss)\n    pub val_snapshots: Vec<(usize, f64)>,   // (step, loss)\n}\n\n/// Track restricted loss over time (only top-k frequencies kept)\n#[derive(Debug, Serialize, Deserialize)]\npub struct RestrictedLossHistory {\n    pub snapshots: Vec<(usize, f64)>, // (step, loss)\n}\n\nimpl RestrictedLossHistory {\n    pub fn new() -> Self {\n        Self {\n            snapshots: Vec::new(),\n        }\n    }\n\n    pub fn add_snapshot(&mut self, step: usize, loss: f64) {\n        self.snapshots.push((step, loss));\n    }\n\n    pub fn save(&self, filename: &str) -> std::io::Result<()> {\n        let json = serde_json::to_string_pretty(self)?;\n        std::fs::write(filename, json)?;\n        println!(\"ðŸ’¾ Restricted loss history saved to: {}\", filename);\n        Ok(())\n    }\n}\n\n/// Track excluded loss over time (top-k frequencies removed)\n#[derive(Debug, Serialize, Deserialize)]\npub struct ExcludedLossHistory {\n    pub snapshots: Vec<(usize, f64)>, // (step, loss)\n}\n\nimpl ExcludedLossHistory {\n    pub fn new() -> Self {\n        Self {\n            snapshots: Vec::new(),\n        }\n    }\n\n    pub fn add_snapshot(&mut self, step: usize, loss: f64) {\n        self.snapshots.push((step, loss));\n    }\n\n    pub fn save(&self, filename: &str) -> std::io::Result<()> {\n        let json = serde_json::to_string_pretty(self)?;\n        std::fs::write(filename, json)?;\n        println!(\"ðŸ’¾ Excluded loss history saved to: {}\", filename);\n        Ok(())\n    }\n}\n\nimpl LossHistory {\n    pub fn new() -> Self {\n        Self {\n            train_snapshots: Vec::new(),\n            val_snapshots: Vec::new(),\n        }\n    }\n\n    pub fn add_snapshot(&mut self, step: usize, train_loss: f64, val_loss: f64) {\n        self.train_snapshots.push((step, train_loss));\n        self.val_snapshots.push((step, val_loss));\n    }\n\n    pub fn save(&self, filename: &str) -> std::io::Result<()> {\n        let json = serde_json::to_string_pretty(self)?;\n        std::fs::write(filename, json)?;\n        println!(\"ðŸ’¾ Loss history saved to: {}\", filename);\n        Ok(())\n    }\n}\n\n/// Legacy: Single loss tracking (kept for backward compatibility)\n#[derive(Debug, Serialize, Deserialize, Clone)]\npub struct WeightNormSnapshot {\n    pub step: usize,\n    pub total_norm: f64,\n}\n\n/// Legacy: Track weight norms over training (actually stores loss)\n#[derive(Debug, Serialize, Deserialize)]\npub struct WeightNormHistory {\n    pub snapshots: Vec<WeightNormSnapshot>,\n}\n\nimpl WeightNormHistory {\n    pub fn new() -> Self {\n        Self {\n            snapshots: Vec::new(),\n        }\n    }\n\n    pub fn add_snapshot(&mut self, step: usize, norm: f64) {\n        self.snapshots.push(WeightNormSnapshot {\n            step,\n            total_norm: norm,\n        });\n    }\n\n    pub fn save(&self, filename: &str) -> std::io::Result<()> {\n        let json = serde_json::to_string_pretty(self)?;\n        std::fs::write(filename, json)?;\n        println!(\"ðŸ’¾ Weight norm history saved to: {}\", filename);\n        Ok(())\n    }\n}\n\n/// Compute the L2 norm of all model parameters\n/// This returns the Frobenius norm (sum of squared weights) across all parameters\n/// Note: This is a simplified implementation that computes the norm of token embeddings,\n/// which contain the majority of the model parameters. For a complete implementation,\n/// we would need to access all Linear layer weights via the Record trait.\npub fn compute_model_weight_norm<B: Backend>(model: &Transformer<B>) -> f64 {\n    let mut total_norm_squared = 0.0f64;\n\n    // Token embedding weights: [vocab_size, d_model]\n    let token_emb_weights = model.token_embedding_weights();\n    let token_emb_data: Vec<f32> = token_emb_weights.into_data().to_vec().unwrap();\n    for &weight in &token_emb_data {\n        total_norm_squared += (weight as f64) * (weight as f64);\n    }\n\n    // Note: Position embeddings, attention weights, MLP weights, and LM head weights\n    // are not included in this simplified implementation. For weight decay verification,\n    // tracking the token embeddings (which are the largest weight matrices) is sufficient\n    // to observe the monotonic decrease under weight decay.\n\n    total_norm_squared.sqrt()\n}\n\n/// Extract all token embeddings as a matrix [vocab_size, embedding_dim]\npub fn extract_all_embeddings<B: Backend>(model: &Transformer<B>) -> Vec<Vec<f64>> {\n    let vocab_size = ModularAdditionDataset::vocab_size();\n    let mut embeddings = Vec::with_capacity(vocab_size);\n\n    for token_id in 0..vocab_size {\n        embeddings.push(get_token_embedding(model, token_id));\n    }\n\n    embeddings\n}\n\n/// Verify that FFT analysis of embeddings shows dominant frequencies\n/// that align with the modulus structure and nontrivial spectral patterns\npub fn verify_fft_dominant_frequencies<B: Backend>(\n    model: &Transformer<B>,\n    config: &FFTDominantFrequencyConfig,\n) -> Result<FFTDominantFrequencyReport, String> {\n    let embeddings = extract_all_embeddings(model);\n    verify_fft_dominant_frequencies_from_embeddings(&embeddings, config)\n}\n\n/// Verify FFT dominant frequencies from pre-extracted embeddings\npub fn verify_fft_dominant_frequencies_from_embeddings(\n    embeddings: &[Vec<f64>],\n    config: &FFTDominantFrequencyConfig,\n) -> Result<FFTDominantFrequencyReport, String> {\n    if embeddings.is_empty() {\n        return Err(\"no embeddings provided\".to_string());\n    }\n\n    let vocab_size = embeddings.len();\n    if vocab_size < config.modulus {\n        return Err(format!(\n            \"embedding count {} less than modulus {}\",\n            vocab_size, config.modulus\n        ));\n    }\n\n    // Analyze each embedding dimension and collect dominant frequencies\n    let mut frequency_histogram: HashMap<usize, usize> = HashMap::new();\n    let embedding_dim = embeddings[0].len();\n\n    // For each embedding dimension, compute FFT across all tokens and find dominant frequency\n    for dim in 0..embedding_dim {\n        let mut signal = Vec::with_capacity(vocab_size);\n        for embedding in embeddings.iter().take(vocab_size) {\n            if dim >= embedding.len() {\n                return Err(format!(\n                    \"embedding dimension mismatch at dim {}\",\n                    dim\n                ));\n            }\n            signal.push(embedding[dim]);\n        }\n\n        // Compute FFT and find dominant frequency\n        let magnitudes = compute_fft(&signal);\n        let dominant_freq = find_dominant_frequency(&magnitudes);\n        *frequency_histogram.entry(dominant_freq).or_insert(0) += 1;\n    }\n\n    // Check 1: Modulus frequency should appear significantly\n    let modulus_count = frequency_histogram.get(&config.modulus).copied().unwrap_or(0);\n    let modulus_frequency_fraction = modulus_count as f64 / embedding_dim as f64;\n\n    if modulus_frequency_fraction < config.min_modulus_frequency_fraction {\n        return Err(format!(\n            \"modulus frequency {} appears in only {:.3} of dimensions (threshold {:.3})\",\n            config.modulus, modulus_frequency_fraction, config.min_modulus_frequency_fraction\n        ));\n    }\n\n    // Check 2: Spectrum should NOT resemble white noise (low entropy)\n    // High entropy = uniform distribution (white noise), low entropy = structured peaks\n    let total_dims = embedding_dim as f64;\n    let mut entropy = 0.0;\n    for &count in frequency_histogram.values() {\n        if count > 0 {\n            let p = count as f64 / total_dims;\n            entropy -= p * p.log2();\n        }\n    }\n    // Normalize entropy by max possible (log2 of unique frequency count)\n    let max_entropy = (frequency_histogram.len() as f64).log2();\n    let normalized_entropy = if max_entropy > 0.0 {\n        entropy / max_entropy\n    } else {\n        0.0\n    };\n\n    if normalized_entropy > config.max_spectrum_entropy {\n        return Err(format!(\n            \"spectrum entropy {:.3} too high (white noise threshold {:.3})\",\n            normalized_entropy, config.max_spectrum_entropy\n        ));\n    }\n\n    // Check 3: Peak frequencies should be significantly stronger than median\n    let mut counts: Vec<usize> = frequency_histogram.values().copied().collect();\n    counts.sort_unstable();\n    let median_count = if counts.is_empty() {\n        0\n    } else {\n        counts[counts.len() / 2]\n    };\n    let peak_count = counts.last().copied().unwrap_or(0);\n    let peak_to_median_ratio = if median_count > 0 {\n        peak_count as f64 / median_count as f64\n    } else {\n        f64::INFINITY\n    };\n\n    if peak_to_median_ratio < config.min_peak_to_median_ratio {\n        return Err(format!(\n            \"peak to median ratio {:.3} below threshold {:.3}\",\n            peak_to_median_ratio, config.min_peak_to_median_ratio\n        ));\n    }\n\n    // Collect top frequencies for the report\n    let mut freq_counts: Vec<(usize, usize)> = frequency_histogram.into_iter().collect();\n    freq_counts.sort_by(|a, b| b.1.cmp(&a.1));\n    let top_frequencies: Vec<(usize, usize)> = freq_counts.into_iter().take(5).collect();\n\n    Ok(FFTDominantFrequencyReport {\n        modulus_frequency_fraction,\n        spectrum_entropy: normalized_entropy,\n        peak_to_median_ratio,\n        top_frequencies,\n    })\n}\n\n#[derive(Debug, Clone)]\npub struct FFTDominantFrequencyConfig {\n    pub modulus: usize,\n    pub min_modulus_frequency_fraction: f64,\n    pub max_spectrum_entropy: f64,\n    pub min_peak_to_median_ratio: f64,\n}\n\nimpl FFTDominantFrequencyConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        Self {\n            modulus,\n            min_modulus_frequency_fraction: 0.15,\n            max_spectrum_entropy: 0.85,\n            min_peak_to_median_ratio: 2.0,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct FFTDominantFrequencyReport {\n    pub modulus_frequency_fraction: f64,\n    pub spectrum_entropy: f64,\n    pub peak_to_median_ratio: f64,\n    pub top_frequencies: Vec<(usize, usize)>,\n}\n\n#[derive(Debug, Clone)]\npub struct EmbeddingClockConfig {\n    pub max_radius_std_fraction: f64,\n    pub min_angular_coverage: f64,\n    pub max_mean_angle_error: f64,\n    pub min_order_fraction: f64,\n}\n\nimpl EmbeddingClockConfig {\n    pub fn default_for_modulus(_modulus: usize) -> Self {\n        Self {\n            max_radius_std_fraction: 0.1,\n            min_angular_coverage: 0.9,\n            max_mean_angle_error: 0.35,\n            min_order_fraction: 0.9,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct EmbeddingClockReport {\n    pub mean_radius: f64,\n    pub radius_std: f64,\n    pub angular_coverage: f64,\n    pub mean_angle_error: f64,\n    pub order_fraction: f64,\n}\n\npub fn project_embeddings_to_fourier_plane(\n    embeddings: &[Vec<f64>],\n    modulus: usize,\n) -> Result<Vec<(f64, f64)>, String> {\n    let p = modulus;\n    if embeddings.len() < p {\n        return Err(\"embedding matrix smaller than modulus\".to_string());\n    }\n    if p == 0 {\n        return Err(\"modulus must be positive\".to_string());\n    }\n    let d = embeddings[0].len();\n    if d == 0 {\n        return Err(\"embedding dimension must be positive\".to_string());\n    }\n    if !embeddings[..p].iter().all(|row| row.len() == d) {\n        return Err(\"embedding rows have inconsistent dimension\".to_string());\n    }\n\n    let tau = std::f64::consts::PI * 2.0;\n    let mut basis_cos = vec![0.0f64; d];\n    let mut basis_sin = vec![0.0f64; d];\n    let scale = 2.0 / p as f64;\n\n    for (token, embedding) in embeddings[..p].iter().enumerate() {\n        let angle = tau * token as f64 / p as f64;\n        let cos_val = angle.cos();\n        let sin_val = angle.sin();\n        for dim in 0..d {\n            basis_cos[dim] += cos_val * embedding[dim] * scale;\n            basis_sin[dim] += sin_val * embedding[dim] * scale;\n        }\n    }\n\n    let basis_cos = normalize_vector(&basis_cos)?;\n    let basis_sin = normalize_vector(&basis_sin)?;\n\n    let mut coords = Vec::with_capacity(p);\n    for embedding in embeddings[..p].iter() {\n        let x = dot(embedding, &basis_cos);\n        let y = dot(embedding, &basis_sin);\n        coords.push((x, y));\n    }\n\n    Ok(coords)\n}\n\npub fn verify_embedding_clock_geometry(\n    embeddings: &[Vec<f64>],\n    modulus: usize,\n    config: &EmbeddingClockConfig,\n) -> Result<EmbeddingClockReport, String> {\n    let coords = project_embeddings_to_fourier_plane(embeddings, modulus)?;\n    let p = coords.len();\n    if p == 0 {\n        return Err(\"no embedding coordinates computed\".to_string());\n    }\n\n    let mut radii = Vec::with_capacity(p);\n    let mut angles = Vec::with_capacity(p);\n    for (x, y) in &coords {\n        let radius = (x * x + y * y).sqrt();\n        radii.push(radius);\n        angles.push(y.atan2(*x));\n    }\n\n    let mean_radius = radii.iter().sum::<f64>() / p as f64;\n    if mean_radius == 0.0 {\n        return Err(\"mean radius is zero\".to_string());\n    }\n    let radius_var = radii\n        .iter()\n        .map(|r| (r - mean_radius) * (r - mean_radius))\n        .sum::<f64>()\n        / p as f64;\n    let radius_std = radius_var.sqrt();\n\n    if radius_std / mean_radius > config.max_radius_std_fraction {\n        return Err(format!(\n            \"radius variance too large (std {:.4}, mean {:.4})\",\n            radius_std, mean_radius\n        ));\n    }\n\n    let tau = std::f64::consts::PI * 2.0;\n    let (mut sum_cos, mut sum_sin) = (0.0f64, 0.0f64);\n    for (token, angle) in angles.iter().enumerate() {\n        let expected = tau * token as f64 / p as f64;\n        let delta = angle - expected;\n        sum_cos += delta.cos();\n        sum_sin += delta.sin();\n    }\n    let offset = sum_sin.atan2(sum_cos);\n\n    let mut adjusted_angles = Vec::with_capacity(p);\n    let mut angle_errors = Vec::with_capacity(p);\n    for (token, angle) in angles.iter().enumerate() {\n        let expected = tau * token as f64 / p as f64;\n        let mut adjusted = angle - offset;\n        adjusted = wrap_angle(adjusted);\n        adjusted_angles.push(adjusted);\n        let diff = wrap_angle_signed(adjusted - expected);\n        angle_errors.push(diff.abs());\n    }\n\n    let mean_angle_error = angle_errors.iter().sum::<f64>() / p as f64;\n    if mean_angle_error > config.max_mean_angle_error {\n        return Err(format!(\n            \"mean angular error too large ({:.4} rad)\",\n            mean_angle_error\n        ));\n    }\n\n    let angular_coverage = angular_coverage(&adjusted_angles);\n    if angular_coverage < config.min_angular_coverage {\n        return Err(format!(\n            \"angular coverage too small ({:.4})\",\n            angular_coverage\n        ));\n    }\n\n    let order_fraction = order_fraction(&adjusted_angles, p);\n    if order_fraction < config.min_order_fraction {\n        return Err(format!(\n            \"token ordering too weak ({:.4})\",\n            order_fraction\n        ));\n    }\n\n    Ok(EmbeddingClockReport {\n        mean_radius,\n        radius_std,\n        angular_coverage,\n        mean_angle_error,\n        order_fraction,\n    })\n}\n\nfn normalize_vector(vec: &[f64]) -> Result<Vec<f64>, String> {\n    let norm = vec.iter().map(|v| v * v).sum::<f64>().sqrt();\n    if norm == 0.0 {\n        return Err(\"basis vector norm is zero\".to_string());\n    }\n    Ok(vec.iter().map(|v| v / norm).collect())\n}\n\nfn dot(a: &[f64], b: &[f64]) -> f64 {\n    a.iter().zip(b.iter()).map(|(x, y)| x * y).sum()\n}\n\nfn wrap_angle(angle: f64) -> f64 {\n    let tau = std::f64::consts::PI * 2.0;\n    let mut wrapped = angle % tau;\n    if wrapped < 0.0 {\n        wrapped += tau;\n    }\n    wrapped\n}\n\nfn wrap_angle_signed(angle: f64) -> f64 {\n    let tau = std::f64::consts::PI * 2.0;\n    let mut wrapped = (angle + std::f64::consts::PI) % tau;\n    if wrapped < 0.0 {\n        wrapped += tau;\n    }\n    wrapped - std::f64::consts::PI\n}\n\nfn angular_coverage(angles: &[f64]) -> f64 {\n    if angles.is_empty() {\n        return 0.0;\n    }\n    let tau = std::f64::consts::PI * 2.0;\n    let mut sorted = angles.to_vec();\n    sorted.sort_by(|a, b| a.partial_cmp(b).unwrap_or(std::cmp::Ordering::Equal));\n\n    let mut max_gap = 0.0;\n    for pair in sorted.windows(2) {\n        let gap = pair[1] - pair[0];\n        if gap > max_gap {\n            max_gap = gap;\n        }\n    }\n    let wrap_gap = tau - (sorted[sorted.len() - 1] - sorted[0]);\n    if wrap_gap > max_gap {\n        max_gap = wrap_gap;\n    }\n\n    (tau - max_gap) / tau\n}\n\nfn order_fraction(angles: &[f64], modulus: usize) -> f64 {\n    if angles.len() < 2 || modulus == 0 {\n        return 0.0;\n    }\n    let mut indexed: Vec<(usize, f64)> = angles.iter().copied().enumerate().collect();\n    indexed.sort_by(|a, b| a.1.partial_cmp(&b.1).unwrap_or(std::cmp::Ordering::Equal));\n\n    let mut good = 0usize;\n    for window in indexed.windows(2) {\n        let current = window[0].0;\n        let next = window[1].0;\n        if (next + modulus - current) % modulus == 1 {\n            good += 1;\n        }\n    }\n    let last = indexed[indexed.len() - 1].0;\n    let first = indexed[0].0;\n    if (first + modulus - last) % modulus == 1 {\n        good += 1;\n    }\n\n    good as f64 / modulus as f64\n}\n\n/// Track training and validation accuracy over time\n#[derive(Debug, Serialize, Deserialize)]\npub struct AccuracyHistory {\n    pub train_snapshots: Vec<(usize, f32)>, // (step, accuracy)\n    pub val_snapshots: Vec<(usize, f32)>,   // (step, accuracy)\n}\n\nimpl AccuracyHistory {\n    pub fn new() -> Self {\n        Self {\n            train_snapshots: Vec::new(),\n            val_snapshots: Vec::new(),\n        }\n    }\n\n    pub fn add_snapshot(&mut self, step: usize, train_acc: f32, val_acc: f32) {\n        self.train_snapshots.push((step, train_acc));\n        self.val_snapshots.push((step, val_acc));\n    }\n\n    pub fn save(&self, filename: &str) -> std::io::Result<()> {\n        let json = serde_json::to_string_pretty(self)?;\n        std::fs::write(filename, json)?;\n        println!(\"ðŸ’¾ Accuracy history saved to: {}\", filename);\n        Ok(())\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    fn make_circle_embeddings(modulus: usize) -> Vec<Vec<f64>> {\n        let tau = std::f64::consts::PI * 2.0;\n        (0..modulus)\n            .map(|token| {\n                let angle = tau * token as f64 / modulus as f64;\n                vec![angle.cos(), angle.sin(), 0.0, 0.0]\n            })\n            .collect()\n    }\n\n    #[test]\n    fn top_k_frequency_indices_stable_with_ties() {\n        let magnitudes = vec![0.1, 1.0, 1.0, 0.5];\n        let first = top_k_frequency_indices(&magnitudes, 2);\n        let second = top_k_frequency_indices(&magnitudes, 2);\n        assert_eq!(first, vec![1, 2]);\n        assert_eq!(first, second);\n    }\n\n    #[test]\n    fn restrict_signal_zeroes_when_top_k_zero() {\n        let signal = vec![1.0, -1.0, 0.5, -0.5];\n        let filtered = restrict_signal_to_top_k_frequencies(&signal, 0);\n        assert_eq!(filtered, vec![0.0, 0.0, 0.0, 0.0]);\n    }\n\n    #[test]\n    fn exclude_signal_no_change_when_top_k_zero() {\n        let signal = vec![1.0, -1.0, 0.5, -0.5];\n        let filtered = exclude_top_k_frequencies(&signal, 0);\n        assert_eq!(filtered, signal);\n    }\n\n    #[test]\n    fn exclude_signal_zeroes_when_top_k_all() {\n        let signal = vec![1.0, -1.0, 0.5, -0.5];\n        let filtered = exclude_top_k_frequencies(&signal, signal.len());\n        assert_eq!(filtered, vec![0.0, 0.0, 0.0, 0.0]);\n    }\n\n    #[test]\n    fn embedding_clock_geometry_passes_on_clean_circle() {\n        let modulus = 13;\n        let embeddings = make_circle_embeddings(modulus);\n        let config = EmbeddingClockConfig::default_for_modulus(modulus);\n        let report =\n            verify_embedding_clock_geometry(&embeddings, modulus, &config).expect(\"expected pass\");\n        assert!(report.angular_coverage > 0.9);\n        assert!(report.order_fraction > 0.9);\n        assert!(report.mean_angle_error < 0.1);\n    }\n\n    #[test]\n    fn embedding_clock_geometry_flags_wrong_frequency() {\n        let modulus = 11;\n        let tau = std::f64::consts::PI * 2.0;\n        let embeddings: Vec<Vec<f64>> = (0..modulus)\n            .map(|token| {\n                let angle = tau * token as f64 / modulus as f64;\n                vec![(2.0 * angle).cos(), (2.0 * angle).sin(), 0.0, 0.0]\n            })\n            .collect();\n        let config = EmbeddingClockConfig {\n            max_radius_std_fraction: 0.5,\n            min_angular_coverage: 0.1,\n            max_mean_angle_error: 0.2,\n            min_order_fraction: 0.1,\n        };\n        let err = verify_embedding_clock_geometry(&embeddings, modulus, &config)\n            .expect_err(\"expected ordering verification to fail\");\n        assert!(err.contains(\"mean angular error\"));\n    }\n\n    #[test]\n    fn fft_dominant_frequency_passes_on_modulus_structure() {\n        // Create embeddings with strong modulus-113 frequency component\n        // Mix in some other frequencies to get a realistic peak-to-median ratio\n        let modulus = 113;\n        let vocab_size = 114; // 0-112 + equals token\n        let embedding_dim = 128;\n        let tau = std::f64::consts::PI * 2.0;\n\n        let mut embeddings = Vec::with_capacity(vocab_size);\n        for token in 0..vocab_size {\n            let mut embedding = Vec::with_capacity(embedding_dim);\n            for dim in 0..embedding_dim {\n                // 60% of dimensions have modulus frequency, 40% have other frequencies\n                let freq = if dim % 5 < 3 {\n                    modulus\n                } else {\n                    // Use various other frequencies for the remaining dims\n                    20 + (dim % 20)\n                };\n                let angle = tau * freq as f64 * token as f64 / vocab_size as f64\n                    + dim as f64 * 0.1;\n                embedding.push(angle.sin());\n            }\n            embeddings.push(embedding);\n        }\n\n        let config = FFTDominantFrequencyConfig::default_for_modulus(modulus);\n        let report = verify_fft_dominant_frequencies_from_embeddings(&embeddings, &config)\n            .expect(\"expected modulus structure to pass\");\n\n        assert!(report.modulus_frequency_fraction >= config.min_modulus_frequency_fraction);\n        assert!(report.spectrum_entropy <= config.max_spectrum_entropy);\n        assert!(report.peak_to_median_ratio >= config.min_peak_to_median_ratio);\n        assert!(report.top_frequencies.iter().any(|(freq, _)| *freq == modulus));\n    }\n\n    #[test]\n    fn fft_dominant_frequency_fails_on_white_noise() {\n        // Create random embeddings (white noise spectrum)\n        let modulus = 113;\n        let vocab_size = 114;\n        let embedding_dim = 128;\n\n        let mut embeddings = Vec::with_capacity(vocab_size);\n        for token in 0..vocab_size {\n            let mut embedding = Vec::with_capacity(embedding_dim);\n            for dim in 0..embedding_dim {\n                // Pseudo-random values\n                let value = ((token * 17 + dim * 31) % 100) as f64 / 50.0 - 1.0;\n                embedding.push(value);\n            }\n            embeddings.push(embedding);\n        }\n\n        let config = FFTDominantFrequencyConfig::default_for_modulus(modulus);\n        let err = verify_fft_dominant_frequencies_from_embeddings(&embeddings, &config)\n            .expect_err(\"expected white noise to fail\");\n\n        // Should fail on either modulus frequency fraction or spectrum entropy\n        assert!(\n            err.contains(\"modulus frequency\") || err.contains(\"spectrum entropy\")\n                || err.contains(\"peak to median\")\n        );\n    }\n\n    #[test]\n    fn fft_dominant_frequency_fails_on_wrong_frequency() {\n        // Create embeddings with dominant frequency at 57 (not 113)\n        let modulus = 113;\n        let vocab_size = 114;\n        let embedding_dim = 128;\n        let tau = std::f64::consts::PI * 2.0;\n        let wrong_frequency = 57;\n\n        let mut embeddings = Vec::with_capacity(vocab_size);\n        for token in 0..vocab_size {\n            let mut embedding = Vec::with_capacity(embedding_dim);\n            for dim in 0..embedding_dim {\n                // Each dimension has a sine wave with wrong frequency\n                let angle = tau * wrong_frequency as f64 * token as f64 / vocab_size as f64\n                    + dim as f64 * 0.1;\n                embedding.push(angle.sin());\n            }\n            embeddings.push(embedding);\n        }\n\n        let config = FFTDominantFrequencyConfig::default_for_modulus(modulus);\n        let err = verify_fft_dominant_frequencies_from_embeddings(&embeddings, &config)\n            .expect_err(\"expected wrong frequency to fail\");\n\n        assert!(err.contains(\"modulus frequency\"));\n    }\n\n    #[test]\n    fn fft_dominant_frequency_fails_on_flat_embeddings() {\n        // Create constant embeddings (DC component only, no structure)\n        let modulus = 113;\n        let vocab_size = 114;\n        let embedding_dim = 128;\n\n        let embeddings = vec![vec![1.5; embedding_dim]; vocab_size];\n\n        let config = FFTDominantFrequencyConfig::default_for_modulus(modulus);\n        let err = verify_fft_dominant_frequencies_from_embeddings(&embeddings, &config)\n            .expect_err(\"expected flat embeddings to fail\");\n\n        assert!(err.contains(\"modulus frequency\") || err.contains(\"peak to median\"));\n    }\n\n    #[test]\n    fn fft_dominant_frequency_fails_on_weak_peaks() {\n        // Create embeddings with many frequencies at similar magnitudes (no clear peaks)\n        let modulus = 113;\n        let vocab_size = 114;\n        let embedding_dim = 128;\n        let tau = std::f64::consts::PI * 2.0;\n\n        let mut embeddings = Vec::with_capacity(vocab_size);\n        for token in 0..vocab_size {\n            let mut embedding = Vec::with_capacity(embedding_dim);\n            for dim in 0..embedding_dim {\n                // Mix many frequencies with similar amplitudes\n                let mut value = 0.0;\n                for freq in 1..10 {\n                    let angle =\n                        tau * freq as f64 * token as f64 / vocab_size as f64 + dim as f64 * 0.01;\n                    value += angle.sin() / 10.0;\n                }\n                embedding.push(value);\n            }\n            embeddings.push(embedding);\n        }\n\n        let config = FFTDominantFrequencyConfig {\n            modulus,\n            min_modulus_frequency_fraction: 0.15,\n            max_spectrum_entropy: 0.85,\n            min_peak_to_median_ratio: 2.0,\n        };\n        let err = verify_fft_dominant_frequencies_from_embeddings(&embeddings, &config)\n            .expect_err(\"expected weak peaks to fail\");\n\n        assert!(\n            err.contains(\"modulus frequency\") || err.contains(\"spectrum entropy\")\n                || err.contains(\"peak to median\")\n        );\n    }\n}\n","structuredPatch":[{"oldStart":539,"oldLines":6,"newStart":539,"newLines":9,"lines":[" "," /// Verify that FFT analysis of embeddings shows dominant frequencies"," /// that align with the modulus structure and nontrivial spectral patterns","+///","+/// NOTE: Reserved for manual verification or future integration into analysis pipeline","+#[allow(dead_code)]"," pub fn verify_fft_dominant_frequencies<B: Backend>(","     model: &Transformer<B>,","     config: &FFTDominantFrequencyConfig,"]},{"oldStart":548,"oldLines":6,"newStart":551,"newLines":9,"lines":[" }"," "," /// Verify FFT dominant frequencies from pre-extracted embeddings","+///","+/// NOTE: Reserved for manual verification or future integration into analysis pipeline","+#[allow(dead_code)]"," pub fn verify_fft_dominant_frequencies_from_embeddings(","     embeddings: &[Vec<f64>],","     config: &FFTDominantFrequencyConfig,"]}],"userModified":false,"replaceAll":false}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01NBmR3GW3Laq61mzX4jtLWZ","type":"message","role":"assistant","content":[{"type":"text","text":"Good. Now let me find and mark the other verification functions and config structures. Let me continue systematically:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":2,"cache_creation_input_tokens":1101,"cache_read_input_tokens":42047,"cache_creation":{"ephemeral_5m_input_tokens":1101,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"b5d48b46-7a0e-43dc-860a-7dfabe961f70"}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01NBmR3GW3Laq61mzX4jtLWZ","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01GAvVyD5TjyBSX7DiFQGs8f","name":"Bash","input":{"command":"grep -n \"pub fn verify_embedding_clock_geometry\" src/analysis.rs","description":"Find embedding clock verification function"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":2,"cache_creation_input_tokens":1101,"cache_read_input_tokens":42047,"cache_creation":{"ephemeral_5m_input_tokens":1101,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"08a06b49-7b45-454d-889c-a956c67783d6"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01GAvVyD5TjyBSX7DiFQGs8f","type":"tool_result","content":"769:pub fn verify_embedding_clock_geometry(","is_error":false}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"e6915e59-0f7b-4d49-bb09-43b619a143d9","tool_use_result":{"stdout":"769:pub fn verify_embedding_clock_geometry(","stderr":"","interrupted":false,"isImage":false}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01ADUnQwCrHRJXwougHenyTv","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01AHiqBT4qFy1SmbaDQwTWZp","name":"Read","input":{"file_path":"/Users/admin/MechInt/src/analysis.rs","offset":660,"limit":120}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":913,"cache_read_input_tokens":43148,"cache_creation":{"ephemeral_5m_input_tokens":913,"ephemeral_1h_input_tokens":0},"output_tokens":25,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"ca5528c8-b2df-43b5-bec2-ba0a6113ec1b"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01AHiqBT4qFy1SmbaDQwTWZp","type":"tool_result","content":"   660â†’        modulus_frequency_fraction,\n   661â†’        spectrum_entropy: normalized_entropy,\n   662â†’        peak_to_median_ratio,\n   663â†’        top_frequencies,\n   664â†’    })\n   665â†’}\n   666â†’\n   667â†’#[derive(Debug, Clone)]\n   668â†’pub struct FFTDominantFrequencyConfig {\n   669â†’    pub modulus: usize,\n   670â†’    pub min_modulus_frequency_fraction: f64,\n   671â†’    pub max_spectrum_entropy: f64,\n   672â†’    pub min_peak_to_median_ratio: f64,\n   673â†’}\n   674â†’\n   675â†’impl FFTDominantFrequencyConfig {\n   676â†’    pub fn default_for_modulus(modulus: usize) -> Self {\n   677â†’        Self {\n   678â†’            modulus,\n   679â†’            min_modulus_frequency_fraction: 0.15,\n   680â†’            max_spectrum_entropy: 0.85,\n   681â†’            min_peak_to_median_ratio: 2.0,\n   682â†’        }\n   683â†’    }\n   684â†’}\n   685â†’\n   686â†’#[derive(Debug, Clone)]\n   687â†’pub struct FFTDominantFrequencyReport {\n   688â†’    pub modulus_frequency_fraction: f64,\n   689â†’    pub spectrum_entropy: f64,\n   690â†’    pub peak_to_median_ratio: f64,\n   691â†’    pub top_frequencies: Vec<(usize, usize)>,\n   692â†’}\n   693â†’\n   694â†’#[derive(Debug, Clone)]\n   695â†’pub struct EmbeddingClockConfig {\n   696â†’    pub max_radius_std_fraction: f64,\n   697â†’    pub min_angular_coverage: f64,\n   698â†’    pub max_mean_angle_error: f64,\n   699â†’    pub min_order_fraction: f64,\n   700â†’}\n   701â†’\n   702â†’impl EmbeddingClockConfig {\n   703â†’    pub fn default_for_modulus(_modulus: usize) -> Self {\n   704â†’        Self {\n   705â†’            max_radius_std_fraction: 0.1,\n   706â†’            min_angular_coverage: 0.9,\n   707â†’            max_mean_angle_error: 0.35,\n   708â†’            min_order_fraction: 0.9,\n   709â†’        }\n   710â†’    }\n   711â†’}\n   712â†’\n   713â†’#[derive(Debug, Clone)]\n   714â†’pub struct EmbeddingClockReport {\n   715â†’    pub mean_radius: f64,\n   716â†’    pub radius_std: f64,\n   717â†’    pub angular_coverage: f64,\n   718â†’    pub mean_angle_error: f64,\n   719â†’    pub order_fraction: f64,\n   720â†’}\n   721â†’\n   722â†’pub fn project_embeddings_to_fourier_plane(\n   723â†’    embeddings: &[Vec<f64>],\n   724â†’    modulus: usize,\n   725â†’) -> Result<Vec<(f64, f64)>, String> {\n   726â†’    let p = modulus;\n   727â†’    if embeddings.len() < p {\n   728â†’        return Err(\"embedding matrix smaller than modulus\".to_string());\n   729â†’    }\n   730â†’    if p == 0 {\n   731â†’        return Err(\"modulus must be positive\".to_string());\n   732â†’    }\n   733â†’    let d = embeddings[0].len();\n   734â†’    if d == 0 {\n   735â†’        return Err(\"embedding dimension must be positive\".to_string());\n   736â†’    }\n   737â†’    if !embeddings[..p].iter().all(|row| row.len() == d) {\n   738â†’        return Err(\"embedding rows have inconsistent dimension\".to_string());\n   739â†’    }\n   740â†’\n   741â†’    let tau = std::f64::consts::PI * 2.0;\n   742â†’    let mut basis_cos = vec![0.0f64; d];\n   743â†’    let mut basis_sin = vec![0.0f64; d];\n   744â†’    let scale = 2.0 / p as f64;\n   745â†’\n   746â†’    for (token, embedding) in embeddings[..p].iter().enumerate() {\n   747â†’        let angle = tau * token as f64 / p as f64;\n   748â†’        let cos_val = angle.cos();\n   749â†’        let sin_val = angle.sin();\n   750â†’        for dim in 0..d {\n   751â†’            basis_cos[dim] += cos_val * embedding[dim] * scale;\n   752â†’            basis_sin[dim] += sin_val * embedding[dim] * scale;\n   753â†’        }\n   754â†’    }\n   755â†’\n   756â†’    let basis_cos = normalize_vector(&basis_cos)?;\n   757â†’    let basis_sin = normalize_vector(&basis_sin)?;\n   758â†’\n   759â†’    let mut coords = Vec::with_capacity(p);\n   760â†’    for embedding in embeddings[..p].iter() {\n   761â†’        let x = dot(embedding, &basis_cos);\n   762â†’        let y = dot(embedding, &basis_sin);\n   763â†’        coords.push((x, y));\n   764â†’    }\n   765â†’\n   766â†’    Ok(coords)\n   767â†’}\n   768â†’\n   769â†’pub fn verify_embedding_clock_geometry(\n   770â†’    embeddings: &[Vec<f64>],\n   771â†’    modulus: usize,\n   772â†’    config: &EmbeddingClockConfig,\n   773â†’) -> Result<EmbeddingClockReport, String> {\n   774â†’    let coords = project_embeddings_to_fourier_plane(embeddings, modulus)?;\n   775â†’    let p = coords.len();\n   776â†’    if p == 0 {\n   777â†’        return Err(\"no embedding coordinates computed\".to_string());\n   778â†’    }\n   779â†’\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"1f53c0c1-f00c-4bf9-9042-0d25ad54820c","tool_use_result":{"type":"text","file":{"filePath":"/Users/admin/MechInt/src/analysis.rs","content":"        modulus_frequency_fraction,\n        spectrum_entropy: normalized_entropy,\n        peak_to_median_ratio,\n        top_frequencies,\n    })\n}\n\n#[derive(Debug, Clone)]\npub struct FFTDominantFrequencyConfig {\n    pub modulus: usize,\n    pub min_modulus_frequency_fraction: f64,\n    pub max_spectrum_entropy: f64,\n    pub min_peak_to_median_ratio: f64,\n}\n\nimpl FFTDominantFrequencyConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        Self {\n            modulus,\n            min_modulus_frequency_fraction: 0.15,\n            max_spectrum_entropy: 0.85,\n            min_peak_to_median_ratio: 2.0,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct FFTDominantFrequencyReport {\n    pub modulus_frequency_fraction: f64,\n    pub spectrum_entropy: f64,\n    pub peak_to_median_ratio: f64,\n    pub top_frequencies: Vec<(usize, usize)>,\n}\n\n#[derive(Debug, Clone)]\npub struct EmbeddingClockConfig {\n    pub max_radius_std_fraction: f64,\n    pub min_angular_coverage: f64,\n    pub max_mean_angle_error: f64,\n    pub min_order_fraction: f64,\n}\n\nimpl EmbeddingClockConfig {\n    pub fn default_for_modulus(_modulus: usize) -> Self {\n        Self {\n            max_radius_std_fraction: 0.1,\n            min_angular_coverage: 0.9,\n            max_mean_angle_error: 0.35,\n            min_order_fraction: 0.9,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct EmbeddingClockReport {\n    pub mean_radius: f64,\n    pub radius_std: f64,\n    pub angular_coverage: f64,\n    pub mean_angle_error: f64,\n    pub order_fraction: f64,\n}\n\npub fn project_embeddings_to_fourier_plane(\n    embeddings: &[Vec<f64>],\n    modulus: usize,\n) -> Result<Vec<(f64, f64)>, String> {\n    let p = modulus;\n    if embeddings.len() < p {\n        return Err(\"embedding matrix smaller than modulus\".to_string());\n    }\n    if p == 0 {\n        return Err(\"modulus must be positive\".to_string());\n    }\n    let d = embeddings[0].len();\n    if d == 0 {\n        return Err(\"embedding dimension must be positive\".to_string());\n    }\n    if !embeddings[..p].iter().all(|row| row.len() == d) {\n        return Err(\"embedding rows have inconsistent dimension\".to_string());\n    }\n\n    let tau = std::f64::consts::PI * 2.0;\n    let mut basis_cos = vec![0.0f64; d];\n    let mut basis_sin = vec![0.0f64; d];\n    let scale = 2.0 / p as f64;\n\n    for (token, embedding) in embeddings[..p].iter().enumerate() {\n        let angle = tau * token as f64 / p as f64;\n        let cos_val = angle.cos();\n        let sin_val = angle.sin();\n        for dim in 0..d {\n            basis_cos[dim] += cos_val * embedding[dim] * scale;\n            basis_sin[dim] += sin_val * embedding[dim] * scale;\n        }\n    }\n\n    let basis_cos = normalize_vector(&basis_cos)?;\n    let basis_sin = normalize_vector(&basis_sin)?;\n\n    let mut coords = Vec::with_capacity(p);\n    for embedding in embeddings[..p].iter() {\n        let x = dot(embedding, &basis_cos);\n        let y = dot(embedding, &basis_sin);\n        coords.push((x, y));\n    }\n\n    Ok(coords)\n}\n\npub fn verify_embedding_clock_geometry(\n    embeddings: &[Vec<f64>],\n    modulus: usize,\n    config: &EmbeddingClockConfig,\n) -> Result<EmbeddingClockReport, String> {\n    let coords = project_embeddings_to_fourier_plane(embeddings, modulus)?;\n    let p = coords.len();\n    if p == 0 {\n        return Err(\"no embedding coordinates computed\".to_string());\n    }\n","numLines":120,"startLine":660,"totalLines":1191}}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01NAkMMNxkiAmRi7fRDpByRw","type":"message","role":"assistant","content":[{"type":"text","text":"I'll continue marking these functions. Let me apply a batch of annotations:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":2343,"cache_read_input_tokens":44061,"cache_creation":{"ephemeral_5m_input_tokens":2343,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"8959effe-019a-4b9d-a16b-20f141aebede"}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01NAkMMNxkiAmRi7fRDpByRw","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01M4KYAJtoRjHq8okhkedFc8","name":"Edit","input":{"replace_all":false,"file_path":"/Users/admin/MechInt/src/analysis.rs","old_string":"impl FFTDominantFrequencyConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        Self {\n            modulus,\n            min_modulus_frequency_fraction: 0.15,\n            max_spectrum_entropy: 0.85,\n            min_peak_to_median_ratio: 2.0,\n        }\n    }\n}","new_string":"impl FFTDominantFrequencyConfig {\n    /// NOTE: Reserved for future integration into analysis pipeline\n    #[allow(dead_code)]\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        Self {\n            modulus,\n            min_modulus_frequency_fraction: 0.15,\n            max_spectrum_entropy: 0.85,\n            min_peak_to_median_ratio: 2.0,\n        }\n    }\n}"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":2343,"cache_read_input_tokens":44061,"cache_creation":{"ephemeral_5m_input_tokens":2343,"ephemeral_1h_input_tokens":0},"output_tokens":326,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"2b59be60-f7f5-45de-976b-40f57826f1a2"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01M4KYAJtoRjHq8okhkedFc8","type":"tool_result","content":"The file /Users/admin/MechInt/src/analysis.rs has been updated. Here's the result of running `cat -n` on a snippet of the edited file:\n   671â†’    pub max_spectrum_entropy: f64,\n   672â†’    pub min_peak_to_median_ratio: f64,\n   673â†’}\n   674â†’\n   675â†’impl FFTDominantFrequencyConfig {\n   676â†’    /// NOTE: Reserved for future integration into analysis pipeline\n   677â†’    #[allow(dead_code)]\n   678â†’    pub fn default_for_modulus(modulus: usize) -> Self {\n   679â†’        Self {\n   680â†’            modulus,\n   681â†’            min_modulus_frequency_fraction: 0.15,\n   682â†’            max_spectrum_entropy: 0.85,\n   683â†’            min_peak_to_median_ratio: 2.0,\n   684â†’        }\n   685â†’    }\n   686â†’}\n   687â†’\n   688â†’#[derive(Debug, Clone)]\n   689â†’pub struct FFTDominantFrequencyReport {\n   690â†’    pub modulus_frequency_fraction: f64,"}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"d8f61490-92ee-4644-a6a4-83c8ebbcc1d9","tool_use_result":{"filePath":"/Users/admin/MechInt/src/analysis.rs","oldString":"impl FFTDominantFrequencyConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        Self {\n            modulus,\n            min_modulus_frequency_fraction: 0.15,\n            max_spectrum_entropy: 0.85,\n            min_peak_to_median_ratio: 2.0,\n        }\n    }\n}","newString":"impl FFTDominantFrequencyConfig {\n    /// NOTE: Reserved for future integration into analysis pipeline\n    #[allow(dead_code)]\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        Self {\n            modulus,\n            min_modulus_frequency_fraction: 0.15,\n            max_spectrum_entropy: 0.85,\n            min_peak_to_median_ratio: 2.0,\n        }\n    }\n}","originalFile":"use burn::{\n    data::dataset::Dataset,\n    nn::loss::CrossEntropyLossConfig,\n    tensor::{backend::Backend, Int, Tensor, TensorData},\n};\nuse rustfft::{num_complex::Complex, FftPlanner};\nuse serde::{Deserialize, Serialize};\nuse std::collections::{HashMap, HashSet};\n\nuse crate::{data::ModularAdditionDataset, model::Transformer};\n\n/// FFT analysis results for embeddings\n#[derive(Debug, Serialize, Deserialize)]\npub struct FFTAnalysis {\n    pub vocab_size: usize,\n    pub embedding_dim: usize,\n    pub dominant_frequencies: Vec<(usize, f64)>, // (token_id, dominant_frequency_index)\n    pub frequency_magnitudes: HashMap<usize, Vec<f64>>, // token_id -> magnitude spectrum\n}\n\n/// Analyze the learned token embeddings using FFT\npub fn analyze_embeddings_fft<B: Backend>(model: &Transformer<B>) -> FFTAnalysis {\n    println!(\"ðŸ” Performing FFT analysis on learned embeddings...\");\n\n    let vocab_size = ModularAdditionDataset::vocab_size();\n    let embedding_dim = 128;\n\n    // Extract token embeddings by forward passing through the embedding layer\n    let mut frequency_magnitudes = HashMap::new();\n    let mut dominant_frequencies = Vec::new();\n\n    for token_id in 0..vocab_size {\n        // Get the embedding for this token\n        let embedding = get_token_embedding(model, token_id);\n\n        // Compute FFT on the embedding vector\n        let fft_result = compute_fft(&embedding);\n\n        // Find dominant frequency\n        let dominant_freq_idx = find_dominant_frequency(&fft_result);\n        dominant_frequencies.push((token_id, dominant_freq_idx as f64));\n\n        // Store magnitude spectrum\n        frequency_magnitudes.insert(token_id, fft_result);\n\n        if token_id % 10 == 0 {\n            println!(\"  Token {:2}: dominant frequency = {}\", token_id, dominant_freq_idx);\n        }\n    }\n\n    println!(\"âœ… FFT analysis complete!\");\n    println!();\n    println!(\"ðŸ“Š Dominant Frequency Analysis:\");\n\n    // Analyze patterns\n    let mut freq_histogram: HashMap<usize, usize> = HashMap::new();\n    for (_, freq) in &dominant_frequencies {\n        *freq_histogram.entry(*freq as usize).or_insert(0) += 1;\n    }\n\n    let mut freq_counts: Vec<_> = freq_histogram.iter().collect();\n    freq_counts.sort_by(|a, b| b.1.cmp(a.1));\n\n    println!(\"  Most common dominant frequencies:\");\n    for (freq, count) in freq_counts.iter().take(5) {\n        println!(\"    Frequency {}: {} tokens\", freq, count);\n    }\n\n    // Check for modular structure (expected for mod p)\n    println!();\n    println!(\"ðŸ”¬ Checking for modular structure:\");\n    let modulus = ModularAdditionDataset::modulus();\n    if freq_histogram.contains_key(&modulus) {\n        println!(\n            \"  âœ“ Frequency {} (matching modulus) appears in embeddings\",\n            modulus\n        );\n    }\n\n    FFTAnalysis {\n        vocab_size,\n        embedding_dim,\n        dominant_frequencies,\n        frequency_magnitudes,\n    }\n}\n\n/// Extract the token embedding for a single token\nfn get_token_embedding<B: Backend>(model: &Transformer<B>, token_id: usize) -> Vec<f64> {\n    // Get the token embedding directly from the model\n    let embedding_tensor = model.get_token_embedding(token_id);\n\n    // Convert to vector - shape is [1, 1, embedding_dim], we want [embedding_dim]\n    let embedding_data: Vec<f32> = embedding_tensor.into_data().to_vec().unwrap();\n\n    // Convert to f64\n    embedding_data.iter().map(|&x| x as f64).collect()\n}\n\n/// Compute FFT on a vector of real values\nfn compute_fft(data: &[f64]) -> Vec<f64> {\n    let mut planner = FftPlanner::new();\n    let fft = planner.plan_fft_forward(data.len());\n\n    // Convert to complex numbers\n    let mut buffer: Vec<Complex<f64>> = data\n        .iter()\n        .map(|&x| Complex::new(x, 0.0))\n        .collect();\n\n    // Perform FFT\n    fft.process(&mut buffer);\n\n    // Compute magnitudes\n    buffer.iter().map(|c| c.norm()).collect()\n}\n\nfn inverse_fft(data: &mut [Complex<f64>]) {\n    let mut planner = FftPlanner::new();\n    let fft = planner.plan_fft_inverse(data.len());\n    fft.process(data);\n}\n\nfn top_k_frequency_indices(magnitudes: &[f64], k: usize) -> Vec<usize> {\n    let mut indexed: Vec<(usize, f64)> = magnitudes.iter().copied().enumerate().collect();\n    indexed.sort_by(|(idx_a, mag_a), (idx_b, mag_b)| {\n        mag_b\n            .partial_cmp(mag_a)\n            .unwrap_or(std::cmp::Ordering::Equal)\n            .then_with(|| idx_a.cmp(idx_b))\n    });\n\n    let keep = k.min(indexed.len());\n    indexed.iter().take(keep).map(|(idx, _)| *idx).collect()\n}\n\nfn restrict_signal_to_top_k_frequencies(signal: &[f64], top_k: usize) -> Vec<f64> {\n    if signal.is_empty() {\n        return Vec::new();\n    }\n\n    if top_k == 0 {\n        return vec![0.0; signal.len()];\n    }\n\n    let mut planner = FftPlanner::new();\n    let fft = planner.plan_fft_forward(signal.len());\n\n    let mut buffer: Vec<Complex<f64>> = signal\n        .iter()\n        .map(|&x| Complex::new(x, 0.0))\n        .collect();\n    fft.process(&mut buffer);\n\n    let magnitudes: Vec<f64> = buffer.iter().map(|c| c.norm()).collect();\n    let keep = top_k_frequency_indices(&magnitudes, top_k);\n    let keep_set: HashSet<usize> = keep.into_iter().collect();\n\n    for (idx, value) in buffer.iter_mut().enumerate() {\n        if !keep_set.contains(&idx) {\n            *value = Complex::new(0.0, 0.0);\n        }\n    }\n\n    inverse_fft(&mut buffer);\n\n    let scale = 1.0 / signal.len() as f64;\n    buffer.iter().map(|c| c.re * scale).collect()\n}\n\nfn exclude_top_k_frequencies(signal: &[f64], top_k: usize) -> Vec<f64> {\n    if signal.is_empty() {\n        return Vec::new();\n    }\n\n    if top_k == 0 {\n        return signal.to_vec();\n    }\n\n    if top_k >= signal.len() {\n        return vec![0.0; signal.len()];\n    }\n\n    let mut planner = FftPlanner::new();\n    let fft = planner.plan_fft_forward(signal.len());\n\n    let mut buffer: Vec<Complex<f64>> = signal\n        .iter()\n        .map(|&x| Complex::new(x, 0.0))\n        .collect();\n    fft.process(&mut buffer);\n\n    let magnitudes: Vec<f64> = buffer.iter().map(|c| c.norm()).collect();\n    let remove = top_k_frequency_indices(&magnitudes, top_k);\n    let remove_set: HashSet<usize> = remove.into_iter().collect();\n\n    for (idx, value) in buffer.iter_mut().enumerate() {\n        if remove_set.contains(&idx) {\n            *value = Complex::new(0.0, 0.0);\n        }\n    }\n\n    inverse_fft(&mut buffer);\n\n    let scale = 1.0 / signal.len() as f64;\n    buffer.iter().map(|c| c.re * scale).collect()\n}\n\nfn restricted_token_embedding_weights<B: Backend>(\n    model: &Transformer<B>,\n    top_k: usize,\n) -> Vec<f32> {\n    let weight = model.token_embedding_weights();\n    let [vocab_size, d_model] = weight.dims();\n    let weight_data: Vec<f32> = weight.into_data().to_vec().unwrap();\n    let mut restricted = vec![0.0f32; weight_data.len()];\n\n    for dim in 0..d_model {\n        let mut column = Vec::with_capacity(vocab_size);\n        for token in 0..vocab_size {\n            column.push(weight_data[token * d_model + dim] as f64);\n        }\n        let filtered = restrict_signal_to_top_k_frequencies(&column, top_k);\n        for token in 0..vocab_size {\n            restricted[token * d_model + dim] = filtered[token] as f32;\n        }\n    }\n\n    restricted\n}\n\nfn excluded_token_embedding_weights<B: Backend>(\n    model: &Transformer<B>,\n    top_k: usize,\n) -> Vec<f32> {\n    let weight = model.token_embedding_weights();\n    let [vocab_size, d_model] = weight.dims();\n    let weight_data: Vec<f32> = weight.into_data().to_vec().unwrap();\n    let mut excluded = vec![0.0f32; weight_data.len()];\n\n    for dim in 0..d_model {\n        let mut column = Vec::with_capacity(vocab_size);\n        for token in 0..vocab_size {\n            column.push(weight_data[token * d_model + dim] as f64);\n        }\n        let filtered = exclude_top_k_frequencies(&column, top_k);\n        for token in 0..vocab_size {\n            excluded[token * d_model + dim] = filtered[token] as f32;\n        }\n    }\n\n    excluded\n}\n\npub fn compute_restricted_loss<B: Backend>(\n    model: &Transformer<B>,\n    dataset: &ModularAdditionDataset,\n    device: &B::Device,\n    top_k: usize,\n    batch_size: usize,\n) -> Result<f64, String> {\n    if batch_size == 0 {\n        return Err(\"batch_size must be greater than 0\".to_string());\n    }\n\n    let restricted_weights = restricted_token_embedding_weights(model, top_k);\n    let [vocab_size, d_model] = model.token_embedding_weights().dims();\n    let token_weights = Tensor::<B, 2>::from_data(\n        TensorData::new(restricted_weights, [vocab_size, d_model]),\n        device,\n    );\n\n    let loss_fn = CrossEntropyLossConfig::new().init(device);\n    let mut total_loss = 0.0f64;\n    let mut total_batches = 0usize;\n\n    for batch_start in (0..dataset.len()).step_by(batch_size) {\n        let batch_end = (batch_start + batch_size).min(dataset.len());\n        let batch_len = batch_end - batch_start;\n        if batch_len == 0 {\n            break;\n        }\n\n        let mut inputs_vec = Vec::with_capacity(batch_len * 3);\n        let mut targets_vec = Vec::with_capacity(batch_len);\n\n        for idx in batch_start..batch_end {\n            let (input, target) = dataset.get(idx).ok_or_else(|| {\n                format!(\"missing dataset sample at index {}\", idx)\n            })?;\n            inputs_vec.extend(input.iter().map(|value| *value as i32));\n            targets_vec.push(target as i32);\n        }\n\n        let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n            .reshape([batch_len, 3]);\n        let targets = Tensor::<B, 1, Int>::from_ints(targets_vec.as_slice(), device);\n\n        let logits = model.forward_with_token_weights(inputs, token_weights.clone());\n        let loss = loss_fn.forward(logits, targets);\n        let loss_value: f64 = loss.into_data().to_vec().unwrap().get(0).copied().unwrap_or(0.0);\n        total_loss += loss_value;\n        total_batches += 1;\n    }\n\n    if total_batches == 0 {\n        return Err(\"no batches computed for restricted loss\".to_string());\n    }\n\n    Ok(total_loss / total_batches as f64)\n}\n\npub fn compute_excluded_loss<B: Backend>(\n    model: &Transformer<B>,\n    dataset: &ModularAdditionDataset,\n    device: &B::Device,\n    top_k: usize,\n    batch_size: usize,\n) -> Result<f64, String> {\n    if batch_size == 0 {\n        return Err(\"batch_size must be greater than 0\".to_string());\n    }\n\n    let excluded_weights = excluded_token_embedding_weights(model, top_k);\n    let [vocab_size, d_model] = model.token_embedding_weights().dims();\n    let token_weights = Tensor::<B, 2>::from_data(\n        TensorData::new(excluded_weights, [vocab_size, d_model]),\n        device,\n    );\n\n    let loss_fn = CrossEntropyLossConfig::new().init(device);\n    let mut total_loss = 0.0f64;\n    let mut total_batches = 0usize;\n\n    for batch_start in (0..dataset.len()).step_by(batch_size) {\n        let batch_end = (batch_start + batch_size).min(dataset.len());\n        let batch_len = batch_end - batch_start;\n        if batch_len == 0 {\n            break;\n        }\n\n        let mut inputs_vec = Vec::with_capacity(batch_len * 3);\n        let mut targets_vec = Vec::with_capacity(batch_len);\n\n        for idx in batch_start..batch_end {\n            let (input, target) = dataset.get(idx).ok_or_else(|| {\n                format!(\"missing dataset sample at index {}\", idx)\n            })?;\n            inputs_vec.extend(input.iter().map(|value| *value as i32));\n            targets_vec.push(target as i32);\n        }\n\n        let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n            .reshape([batch_len, 3]);\n        let targets = Tensor::<B, 1, Int>::from_ints(targets_vec.as_slice(), device);\n\n        let logits = model.forward_with_token_weights(inputs, token_weights.clone());\n        let loss = loss_fn.forward(logits, targets);\n        let loss_value: f64 = loss.into_data().to_vec().unwrap().get(0).copied().unwrap_or(0.0);\n        total_loss += loss_value;\n        total_batches += 1;\n    }\n\n    if total_batches == 0 {\n        return Err(\"no batches computed for excluded loss\".to_string());\n    }\n\n    Ok(total_loss / total_batches as f64)\n}\n\n/// Find the index of the dominant frequency (excluding DC component at index 0)\nfn find_dominant_frequency(magnitudes: &[f64]) -> usize {\n    magnitudes\n        .iter()\n        .enumerate()\n        .skip(1) // Skip DC component\n        .max_by(|(_, a), (_, b)| {\n            a.partial_cmp(b).unwrap_or(std::cmp::Ordering::Equal)\n        })\n        .map(|(idx, _)| idx)\n        .unwrap_or(0)\n}\n\n/// Save FFT analysis to JSON file\npub fn save_fft_analysis(analysis: &FFTAnalysis, filename: &str) -> std::io::Result<()> {\n    let json = serde_json::to_string_pretty(analysis)?;\n    std::fs::write(filename, json)?;\n    println!(\"ðŸ’¾ FFT analysis saved to: {}\", filename);\n    Ok(())\n}\n\n/// Track training and validation loss over time\n#[derive(Debug, Serialize, Deserialize)]\npub struct LossHistory {\n    pub train_snapshots: Vec<(usize, f64)>, // (step, loss)\n    pub val_snapshots: Vec<(usize, f64)>,   // (step, loss)\n}\n\n/// Track restricted loss over time (only top-k frequencies kept)\n#[derive(Debug, Serialize, Deserialize)]\npub struct RestrictedLossHistory {\n    pub snapshots: Vec<(usize, f64)>, // (step, loss)\n}\n\nimpl RestrictedLossHistory {\n    pub fn new() -> Self {\n        Self {\n            snapshots: Vec::new(),\n        }\n    }\n\n    pub fn add_snapshot(&mut self, step: usize, loss: f64) {\n        self.snapshots.push((step, loss));\n    }\n\n    pub fn save(&self, filename: &str) -> std::io::Result<()> {\n        let json = serde_json::to_string_pretty(self)?;\n        std::fs::write(filename, json)?;\n        println!(\"ðŸ’¾ Restricted loss history saved to: {}\", filename);\n        Ok(())\n    }\n}\n\n/// Track excluded loss over time (top-k frequencies removed)\n#[derive(Debug, Serialize, Deserialize)]\npub struct ExcludedLossHistory {\n    pub snapshots: Vec<(usize, f64)>, // (step, loss)\n}\n\nimpl ExcludedLossHistory {\n    pub fn new() -> Self {\n        Self {\n            snapshots: Vec::new(),\n        }\n    }\n\n    pub fn add_snapshot(&mut self, step: usize, loss: f64) {\n        self.snapshots.push((step, loss));\n    }\n\n    pub fn save(&self, filename: &str) -> std::io::Result<()> {\n        let json = serde_json::to_string_pretty(self)?;\n        std::fs::write(filename, json)?;\n        println!(\"ðŸ’¾ Excluded loss history saved to: {}\", filename);\n        Ok(())\n    }\n}\n\nimpl LossHistory {\n    pub fn new() -> Self {\n        Self {\n            train_snapshots: Vec::new(),\n            val_snapshots: Vec::new(),\n        }\n    }\n\n    pub fn add_snapshot(&mut self, step: usize, train_loss: f64, val_loss: f64) {\n        self.train_snapshots.push((step, train_loss));\n        self.val_snapshots.push((step, val_loss));\n    }\n\n    pub fn save(&self, filename: &str) -> std::io::Result<()> {\n        let json = serde_json::to_string_pretty(self)?;\n        std::fs::write(filename, json)?;\n        println!(\"ðŸ’¾ Loss history saved to: {}\", filename);\n        Ok(())\n    }\n}\n\n/// Legacy: Single loss tracking (kept for backward compatibility)\n#[derive(Debug, Serialize, Deserialize, Clone)]\npub struct WeightNormSnapshot {\n    pub step: usize,\n    pub total_norm: f64,\n}\n\n/// Legacy: Track weight norms over training (actually stores loss)\n#[derive(Debug, Serialize, Deserialize)]\npub struct WeightNormHistory {\n    pub snapshots: Vec<WeightNormSnapshot>,\n}\n\nimpl WeightNormHistory {\n    pub fn new() -> Self {\n        Self {\n            snapshots: Vec::new(),\n        }\n    }\n\n    pub fn add_snapshot(&mut self, step: usize, norm: f64) {\n        self.snapshots.push(WeightNormSnapshot {\n            step,\n            total_norm: norm,\n        });\n    }\n\n    pub fn save(&self, filename: &str) -> std::io::Result<()> {\n        let json = serde_json::to_string_pretty(self)?;\n        std::fs::write(filename, json)?;\n        println!(\"ðŸ’¾ Weight norm history saved to: {}\", filename);\n        Ok(())\n    }\n}\n\n/// Compute the L2 norm of all model parameters\n/// This returns the Frobenius norm (sum of squared weights) across all parameters\n/// Note: This is a simplified implementation that computes the norm of token embeddings,\n/// which contain the majority of the model parameters. For a complete implementation,\n/// we would need to access all Linear layer weights via the Record trait.\npub fn compute_model_weight_norm<B: Backend>(model: &Transformer<B>) -> f64 {\n    let mut total_norm_squared = 0.0f64;\n\n    // Token embedding weights: [vocab_size, d_model]\n    let token_emb_weights = model.token_embedding_weights();\n    let token_emb_data: Vec<f32> = token_emb_weights.into_data().to_vec().unwrap();\n    for &weight in &token_emb_data {\n        total_norm_squared += (weight as f64) * (weight as f64);\n    }\n\n    // Note: Position embeddings, attention weights, MLP weights, and LM head weights\n    // are not included in this simplified implementation. For weight decay verification,\n    // tracking the token embeddings (which are the largest weight matrices) is sufficient\n    // to observe the monotonic decrease under weight decay.\n\n    total_norm_squared.sqrt()\n}\n\n/// Extract all token embeddings as a matrix [vocab_size, embedding_dim]\npub fn extract_all_embeddings<B: Backend>(model: &Transformer<B>) -> Vec<Vec<f64>> {\n    let vocab_size = ModularAdditionDataset::vocab_size();\n    let mut embeddings = Vec::with_capacity(vocab_size);\n\n    for token_id in 0..vocab_size {\n        embeddings.push(get_token_embedding(model, token_id));\n    }\n\n    embeddings\n}\n\n/// Verify that FFT analysis of embeddings shows dominant frequencies\n/// that align with the modulus structure and nontrivial spectral patterns\n///\n/// NOTE: Reserved for manual verification or future integration into analysis pipeline\n#[allow(dead_code)]\npub fn verify_fft_dominant_frequencies<B: Backend>(\n    model: &Transformer<B>,\n    config: &FFTDominantFrequencyConfig,\n) -> Result<FFTDominantFrequencyReport, String> {\n    let embeddings = extract_all_embeddings(model);\n    verify_fft_dominant_frequencies_from_embeddings(&embeddings, config)\n}\n\n/// Verify FFT dominant frequencies from pre-extracted embeddings\n///\n/// NOTE: Reserved for manual verification or future integration into analysis pipeline\n#[allow(dead_code)]\npub fn verify_fft_dominant_frequencies_from_embeddings(\n    embeddings: &[Vec<f64>],\n    config: &FFTDominantFrequencyConfig,\n) -> Result<FFTDominantFrequencyReport, String> {\n    if embeddings.is_empty() {\n        return Err(\"no embeddings provided\".to_string());\n    }\n\n    let vocab_size = embeddings.len();\n    if vocab_size < config.modulus {\n        return Err(format!(\n            \"embedding count {} less than modulus {}\",\n            vocab_size, config.modulus\n        ));\n    }\n\n    // Analyze each embedding dimension and collect dominant frequencies\n    let mut frequency_histogram: HashMap<usize, usize> = HashMap::new();\n    let embedding_dim = embeddings[0].len();\n\n    // For each embedding dimension, compute FFT across all tokens and find dominant frequency\n    for dim in 0..embedding_dim {\n        let mut signal = Vec::with_capacity(vocab_size);\n        for embedding in embeddings.iter().take(vocab_size) {\n            if dim >= embedding.len() {\n                return Err(format!(\n                    \"embedding dimension mismatch at dim {}\",\n                    dim\n                ));\n            }\n            signal.push(embedding[dim]);\n        }\n\n        // Compute FFT and find dominant frequency\n        let magnitudes = compute_fft(&signal);\n        let dominant_freq = find_dominant_frequency(&magnitudes);\n        *frequency_histogram.entry(dominant_freq).or_insert(0) += 1;\n    }\n\n    // Check 1: Modulus frequency should appear significantly\n    let modulus_count = frequency_histogram.get(&config.modulus).copied().unwrap_or(0);\n    let modulus_frequency_fraction = modulus_count as f64 / embedding_dim as f64;\n\n    if modulus_frequency_fraction < config.min_modulus_frequency_fraction {\n        return Err(format!(\n            \"modulus frequency {} appears in only {:.3} of dimensions (threshold {:.3})\",\n            config.modulus, modulus_frequency_fraction, config.min_modulus_frequency_fraction\n        ));\n    }\n\n    // Check 2: Spectrum should NOT resemble white noise (low entropy)\n    // High entropy = uniform distribution (white noise), low entropy = structured peaks\n    let total_dims = embedding_dim as f64;\n    let mut entropy = 0.0;\n    for &count in frequency_histogram.values() {\n        if count > 0 {\n            let p = count as f64 / total_dims;\n            entropy -= p * p.log2();\n        }\n    }\n    // Normalize entropy by max possible (log2 of unique frequency count)\n    let max_entropy = (frequency_histogram.len() as f64).log2();\n    let normalized_entropy = if max_entropy > 0.0 {\n        entropy / max_entropy\n    } else {\n        0.0\n    };\n\n    if normalized_entropy > config.max_spectrum_entropy {\n        return Err(format!(\n            \"spectrum entropy {:.3} too high (white noise threshold {:.3})\",\n            normalized_entropy, config.max_spectrum_entropy\n        ));\n    }\n\n    // Check 3: Peak frequencies should be significantly stronger than median\n    let mut counts: Vec<usize> = frequency_histogram.values().copied().collect();\n    counts.sort_unstable();\n    let median_count = if counts.is_empty() {\n        0\n    } else {\n        counts[counts.len() / 2]\n    };\n    let peak_count = counts.last().copied().unwrap_or(0);\n    let peak_to_median_ratio = if median_count > 0 {\n        peak_count as f64 / median_count as f64\n    } else {\n        f64::INFINITY\n    };\n\n    if peak_to_median_ratio < config.min_peak_to_median_ratio {\n        return Err(format!(\n            \"peak to median ratio {:.3} below threshold {:.3}\",\n            peak_to_median_ratio, config.min_peak_to_median_ratio\n        ));\n    }\n\n    // Collect top frequencies for the report\n    let mut freq_counts: Vec<(usize, usize)> = frequency_histogram.into_iter().collect();\n    freq_counts.sort_by(|a, b| b.1.cmp(&a.1));\n    let top_frequencies: Vec<(usize, usize)> = freq_counts.into_iter().take(5).collect();\n\n    Ok(FFTDominantFrequencyReport {\n        modulus_frequency_fraction,\n        spectrum_entropy: normalized_entropy,\n        peak_to_median_ratio,\n        top_frequencies,\n    })\n}\n\n#[derive(Debug, Clone)]\npub struct FFTDominantFrequencyConfig {\n    pub modulus: usize,\n    pub min_modulus_frequency_fraction: f64,\n    pub max_spectrum_entropy: f64,\n    pub min_peak_to_median_ratio: f64,\n}\n\nimpl FFTDominantFrequencyConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        Self {\n            modulus,\n            min_modulus_frequency_fraction: 0.15,\n            max_spectrum_entropy: 0.85,\n            min_peak_to_median_ratio: 2.0,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct FFTDominantFrequencyReport {\n    pub modulus_frequency_fraction: f64,\n    pub spectrum_entropy: f64,\n    pub peak_to_median_ratio: f64,\n    pub top_frequencies: Vec<(usize, usize)>,\n}\n\n#[derive(Debug, Clone)]\npub struct EmbeddingClockConfig {\n    pub max_radius_std_fraction: f64,\n    pub min_angular_coverage: f64,\n    pub max_mean_angle_error: f64,\n    pub min_order_fraction: f64,\n}\n\nimpl EmbeddingClockConfig {\n    pub fn default_for_modulus(_modulus: usize) -> Self {\n        Self {\n            max_radius_std_fraction: 0.1,\n            min_angular_coverage: 0.9,\n            max_mean_angle_error: 0.35,\n            min_order_fraction: 0.9,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct EmbeddingClockReport {\n    pub mean_radius: f64,\n    pub radius_std: f64,\n    pub angular_coverage: f64,\n    pub mean_angle_error: f64,\n    pub order_fraction: f64,\n}\n\npub fn project_embeddings_to_fourier_plane(\n    embeddings: &[Vec<f64>],\n    modulus: usize,\n) -> Result<Vec<(f64, f64)>, String> {\n    let p = modulus;\n    if embeddings.len() < p {\n        return Err(\"embedding matrix smaller than modulus\".to_string());\n    }\n    if p == 0 {\n        return Err(\"modulus must be positive\".to_string());\n    }\n    let d = embeddings[0].len();\n    if d == 0 {\n        return Err(\"embedding dimension must be positive\".to_string());\n    }\n    if !embeddings[..p].iter().all(|row| row.len() == d) {\n        return Err(\"embedding rows have inconsistent dimension\".to_string());\n    }\n\n    let tau = std::f64::consts::PI * 2.0;\n    let mut basis_cos = vec![0.0f64; d];\n    let mut basis_sin = vec![0.0f64; d];\n    let scale = 2.0 / p as f64;\n\n    for (token, embedding) in embeddings[..p].iter().enumerate() {\n        let angle = tau * token as f64 / p as f64;\n        let cos_val = angle.cos();\n        let sin_val = angle.sin();\n        for dim in 0..d {\n            basis_cos[dim] += cos_val * embedding[dim] * scale;\n            basis_sin[dim] += sin_val * embedding[dim] * scale;\n        }\n    }\n\n    let basis_cos = normalize_vector(&basis_cos)?;\n    let basis_sin = normalize_vector(&basis_sin)?;\n\n    let mut coords = Vec::with_capacity(p);\n    for embedding in embeddings[..p].iter() {\n        let x = dot(embedding, &basis_cos);\n        let y = dot(embedding, &basis_sin);\n        coords.push((x, y));\n    }\n\n    Ok(coords)\n}\n\npub fn verify_embedding_clock_geometry(\n    embeddings: &[Vec<f64>],\n    modulus: usize,\n    config: &EmbeddingClockConfig,\n) -> Result<EmbeddingClockReport, String> {\n    let coords = project_embeddings_to_fourier_plane(embeddings, modulus)?;\n    let p = coords.len();\n    if p == 0 {\n        return Err(\"no embedding coordinates computed\".to_string());\n    }\n\n    let mut radii = Vec::with_capacity(p);\n    let mut angles = Vec::with_capacity(p);\n    for (x, y) in &coords {\n        let radius = (x * x + y * y).sqrt();\n        radii.push(radius);\n        angles.push(y.atan2(*x));\n    }\n\n    let mean_radius = radii.iter().sum::<f64>() / p as f64;\n    if mean_radius == 0.0 {\n        return Err(\"mean radius is zero\".to_string());\n    }\n    let radius_var = radii\n        .iter()\n        .map(|r| (r - mean_radius) * (r - mean_radius))\n        .sum::<f64>()\n        / p as f64;\n    let radius_std = radius_var.sqrt();\n\n    if radius_std / mean_radius > config.max_radius_std_fraction {\n        return Err(format!(\n            \"radius variance too large (std {:.4}, mean {:.4})\",\n            radius_std, mean_radius\n        ));\n    }\n\n    let tau = std::f64::consts::PI * 2.0;\n    let (mut sum_cos, mut sum_sin) = (0.0f64, 0.0f64);\n    for (token, angle) in angles.iter().enumerate() {\n        let expected = tau * token as f64 / p as f64;\n        let delta = angle - expected;\n        sum_cos += delta.cos();\n        sum_sin += delta.sin();\n    }\n    let offset = sum_sin.atan2(sum_cos);\n\n    let mut adjusted_angles = Vec::with_capacity(p);\n    let mut angle_errors = Vec::with_capacity(p);\n    for (token, angle) in angles.iter().enumerate() {\n        let expected = tau * token as f64 / p as f64;\n        let mut adjusted = angle - offset;\n        adjusted = wrap_angle(adjusted);\n        adjusted_angles.push(adjusted);\n        let diff = wrap_angle_signed(adjusted - expected);\n        angle_errors.push(diff.abs());\n    }\n\n    let mean_angle_error = angle_errors.iter().sum::<f64>() / p as f64;\n    if mean_angle_error > config.max_mean_angle_error {\n        return Err(format!(\n            \"mean angular error too large ({:.4} rad)\",\n            mean_angle_error\n        ));\n    }\n\n    let angular_coverage = angular_coverage(&adjusted_angles);\n    if angular_coverage < config.min_angular_coverage {\n        return Err(format!(\n            \"angular coverage too small ({:.4})\",\n            angular_coverage\n        ));\n    }\n\n    let order_fraction = order_fraction(&adjusted_angles, p);\n    if order_fraction < config.min_order_fraction {\n        return Err(format!(\n            \"token ordering too weak ({:.4})\",\n            order_fraction\n        ));\n    }\n\n    Ok(EmbeddingClockReport {\n        mean_radius,\n        radius_std,\n        angular_coverage,\n        mean_angle_error,\n        order_fraction,\n    })\n}\n\nfn normalize_vector(vec: &[f64]) -> Result<Vec<f64>, String> {\n    let norm = vec.iter().map(|v| v * v).sum::<f64>().sqrt();\n    if norm == 0.0 {\n        return Err(\"basis vector norm is zero\".to_string());\n    }\n    Ok(vec.iter().map(|v| v / norm).collect())\n}\n\nfn dot(a: &[f64], b: &[f64]) -> f64 {\n    a.iter().zip(b.iter()).map(|(x, y)| x * y).sum()\n}\n\nfn wrap_angle(angle: f64) -> f64 {\n    let tau = std::f64::consts::PI * 2.0;\n    let mut wrapped = angle % tau;\n    if wrapped < 0.0 {\n        wrapped += tau;\n    }\n    wrapped\n}\n\nfn wrap_angle_signed(angle: f64) -> f64 {\n    let tau = std::f64::consts::PI * 2.0;\n    let mut wrapped = (angle + std::f64::consts::PI) % tau;\n    if wrapped < 0.0 {\n        wrapped += tau;\n    }\n    wrapped - std::f64::consts::PI\n}\n\nfn angular_coverage(angles: &[f64]) -> f64 {\n    if angles.is_empty() {\n        return 0.0;\n    }\n    let tau = std::f64::consts::PI * 2.0;\n    let mut sorted = angles.to_vec();\n    sorted.sort_by(|a, b| a.partial_cmp(b).unwrap_or(std::cmp::Ordering::Equal));\n\n    let mut max_gap = 0.0;\n    for pair in sorted.windows(2) {\n        let gap = pair[1] - pair[0];\n        if gap > max_gap {\n            max_gap = gap;\n        }\n    }\n    let wrap_gap = tau - (sorted[sorted.len() - 1] - sorted[0]);\n    if wrap_gap > max_gap {\n        max_gap = wrap_gap;\n    }\n\n    (tau - max_gap) / tau\n}\n\nfn order_fraction(angles: &[f64], modulus: usize) -> f64 {\n    if angles.len() < 2 || modulus == 0 {\n        return 0.0;\n    }\n    let mut indexed: Vec<(usize, f64)> = angles.iter().copied().enumerate().collect();\n    indexed.sort_by(|a, b| a.1.partial_cmp(&b.1).unwrap_or(std::cmp::Ordering::Equal));\n\n    let mut good = 0usize;\n    for window in indexed.windows(2) {\n        let current = window[0].0;\n        let next = window[1].0;\n        if (next + modulus - current) % modulus == 1 {\n            good += 1;\n        }\n    }\n    let last = indexed[indexed.len() - 1].0;\n    let first = indexed[0].0;\n    if (first + modulus - last) % modulus == 1 {\n        good += 1;\n    }\n\n    good as f64 / modulus as f64\n}\n\n/// Track training and validation accuracy over time\n#[derive(Debug, Serialize, Deserialize)]\npub struct AccuracyHistory {\n    pub train_snapshots: Vec<(usize, f32)>, // (step, accuracy)\n    pub val_snapshots: Vec<(usize, f32)>,   // (step, accuracy)\n}\n\nimpl AccuracyHistory {\n    pub fn new() -> Self {\n        Self {\n            train_snapshots: Vec::new(),\n            val_snapshots: Vec::new(),\n        }\n    }\n\n    pub fn add_snapshot(&mut self, step: usize, train_acc: f32, val_acc: f32) {\n        self.train_snapshots.push((step, train_acc));\n        self.val_snapshots.push((step, val_acc));\n    }\n\n    pub fn save(&self, filename: &str) -> std::io::Result<()> {\n        let json = serde_json::to_string_pretty(self)?;\n        std::fs::write(filename, json)?;\n        println!(\"ðŸ’¾ Accuracy history saved to: {}\", filename);\n        Ok(())\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    fn make_circle_embeddings(modulus: usize) -> Vec<Vec<f64>> {\n        let tau = std::f64::consts::PI * 2.0;\n        (0..modulus)\n            .map(|token| {\n                let angle = tau * token as f64 / modulus as f64;\n                vec![angle.cos(), angle.sin(), 0.0, 0.0]\n            })\n            .collect()\n    }\n\n    #[test]\n    fn top_k_frequency_indices_stable_with_ties() {\n        let magnitudes = vec![0.1, 1.0, 1.0, 0.5];\n        let first = top_k_frequency_indices(&magnitudes, 2);\n        let second = top_k_frequency_indices(&magnitudes, 2);\n        assert_eq!(first, vec![1, 2]);\n        assert_eq!(first, second);\n    }\n\n    #[test]\n    fn restrict_signal_zeroes_when_top_k_zero() {\n        let signal = vec![1.0, -1.0, 0.5, -0.5];\n        let filtered = restrict_signal_to_top_k_frequencies(&signal, 0);\n        assert_eq!(filtered, vec![0.0, 0.0, 0.0, 0.0]);\n    }\n\n    #[test]\n    fn exclude_signal_no_change_when_top_k_zero() {\n        let signal = vec![1.0, -1.0, 0.5, -0.5];\n        let filtered = exclude_top_k_frequencies(&signal, 0);\n        assert_eq!(filtered, signal);\n    }\n\n    #[test]\n    fn exclude_signal_zeroes_when_top_k_all() {\n        let signal = vec![1.0, -1.0, 0.5, -0.5];\n        let filtered = exclude_top_k_frequencies(&signal, signal.len());\n        assert_eq!(filtered, vec![0.0, 0.0, 0.0, 0.0]);\n    }\n\n    #[test]\n    fn embedding_clock_geometry_passes_on_clean_circle() {\n        let modulus = 13;\n        let embeddings = make_circle_embeddings(modulus);\n        let config = EmbeddingClockConfig::default_for_modulus(modulus);\n        let report =\n            verify_embedding_clock_geometry(&embeddings, modulus, &config).expect(\"expected pass\");\n        assert!(report.angular_coverage > 0.9);\n        assert!(report.order_fraction > 0.9);\n        assert!(report.mean_angle_error < 0.1);\n    }\n\n    #[test]\n    fn embedding_clock_geometry_flags_wrong_frequency() {\n        let modulus = 11;\n        let tau = std::f64::consts::PI * 2.0;\n        let embeddings: Vec<Vec<f64>> = (0..modulus)\n            .map(|token| {\n                let angle = tau * token as f64 / modulus as f64;\n                vec![(2.0 * angle).cos(), (2.0 * angle).sin(), 0.0, 0.0]\n            })\n            .collect();\n        let config = EmbeddingClockConfig {\n            max_radius_std_fraction: 0.5,\n            min_angular_coverage: 0.1,\n            max_mean_angle_error: 0.2,\n            min_order_fraction: 0.1,\n        };\n        let err = verify_embedding_clock_geometry(&embeddings, modulus, &config)\n            .expect_err(\"expected ordering verification to fail\");\n        assert!(err.contains(\"mean angular error\"));\n    }\n\n    #[test]\n    fn fft_dominant_frequency_passes_on_modulus_structure() {\n        // Create embeddings with strong modulus-113 frequency component\n        // Mix in some other frequencies to get a realistic peak-to-median ratio\n        let modulus = 113;\n        let vocab_size = 114; // 0-112 + equals token\n        let embedding_dim = 128;\n        let tau = std::f64::consts::PI * 2.0;\n\n        let mut embeddings = Vec::with_capacity(vocab_size);\n        for token in 0..vocab_size {\n            let mut embedding = Vec::with_capacity(embedding_dim);\n            for dim in 0..embedding_dim {\n                // 60% of dimensions have modulus frequency, 40% have other frequencies\n                let freq = if dim % 5 < 3 {\n                    modulus\n                } else {\n                    // Use various other frequencies for the remaining dims\n                    20 + (dim % 20)\n                };\n                let angle = tau * freq as f64 * token as f64 / vocab_size as f64\n                    + dim as f64 * 0.1;\n                embedding.push(angle.sin());\n            }\n            embeddings.push(embedding);\n        }\n\n        let config = FFTDominantFrequencyConfig::default_for_modulus(modulus);\n        let report = verify_fft_dominant_frequencies_from_embeddings(&embeddings, &config)\n            .expect(\"expected modulus structure to pass\");\n\n        assert!(report.modulus_frequency_fraction >= config.min_modulus_frequency_fraction);\n        assert!(report.spectrum_entropy <= config.max_spectrum_entropy);\n        assert!(report.peak_to_median_ratio >= config.min_peak_to_median_ratio);\n        assert!(report.top_frequencies.iter().any(|(freq, _)| *freq == modulus));\n    }\n\n    #[test]\n    fn fft_dominant_frequency_fails_on_white_noise() {\n        // Create random embeddings (white noise spectrum)\n        let modulus = 113;\n        let vocab_size = 114;\n        let embedding_dim = 128;\n\n        let mut embeddings = Vec::with_capacity(vocab_size);\n        for token in 0..vocab_size {\n            let mut embedding = Vec::with_capacity(embedding_dim);\n            for dim in 0..embedding_dim {\n                // Pseudo-random values\n                let value = ((token * 17 + dim * 31) % 100) as f64 / 50.0 - 1.0;\n                embedding.push(value);\n            }\n            embeddings.push(embedding);\n        }\n\n        let config = FFTDominantFrequencyConfig::default_for_modulus(modulus);\n        let err = verify_fft_dominant_frequencies_from_embeddings(&embeddings, &config)\n            .expect_err(\"expected white noise to fail\");\n\n        // Should fail on either modulus frequency fraction or spectrum entropy\n        assert!(\n            err.contains(\"modulus frequency\") || err.contains(\"spectrum entropy\")\n                || err.contains(\"peak to median\")\n        );\n    }\n\n    #[test]\n    fn fft_dominant_frequency_fails_on_wrong_frequency() {\n        // Create embeddings with dominant frequency at 57 (not 113)\n        let modulus = 113;\n        let vocab_size = 114;\n        let embedding_dim = 128;\n        let tau = std::f64::consts::PI * 2.0;\n        let wrong_frequency = 57;\n\n        let mut embeddings = Vec::with_capacity(vocab_size);\n        for token in 0..vocab_size {\n            let mut embedding = Vec::with_capacity(embedding_dim);\n            for dim in 0..embedding_dim {\n                // Each dimension has a sine wave with wrong frequency\n                let angle = tau * wrong_frequency as f64 * token as f64 / vocab_size as f64\n                    + dim as f64 * 0.1;\n                embedding.push(angle.sin());\n            }\n            embeddings.push(embedding);\n        }\n\n        let config = FFTDominantFrequencyConfig::default_for_modulus(modulus);\n        let err = verify_fft_dominant_frequencies_from_embeddings(&embeddings, &config)\n            .expect_err(\"expected wrong frequency to fail\");\n\n        assert!(err.contains(\"modulus frequency\"));\n    }\n\n    #[test]\n    fn fft_dominant_frequency_fails_on_flat_embeddings() {\n        // Create constant embeddings (DC component only, no structure)\n        let modulus = 113;\n        let vocab_size = 114;\n        let embedding_dim = 128;\n\n        let embeddings = vec![vec![1.5; embedding_dim]; vocab_size];\n\n        let config = FFTDominantFrequencyConfig::default_for_modulus(modulus);\n        let err = verify_fft_dominant_frequencies_from_embeddings(&embeddings, &config)\n            .expect_err(\"expected flat embeddings to fail\");\n\n        assert!(err.contains(\"modulus frequency\") || err.contains(\"peak to median\"));\n    }\n\n    #[test]\n    fn fft_dominant_frequency_fails_on_weak_peaks() {\n        // Create embeddings with many frequencies at similar magnitudes (no clear peaks)\n        let modulus = 113;\n        let vocab_size = 114;\n        let embedding_dim = 128;\n        let tau = std::f64::consts::PI * 2.0;\n\n        let mut embeddings = Vec::with_capacity(vocab_size);\n        for token in 0..vocab_size {\n            let mut embedding = Vec::with_capacity(embedding_dim);\n            for dim in 0..embedding_dim {\n                // Mix many frequencies with similar amplitudes\n                let mut value = 0.0;\n                for freq in 1..10 {\n                    let angle =\n                        tau * freq as f64 * token as f64 / vocab_size as f64 + dim as f64 * 0.01;\n                    value += angle.sin() / 10.0;\n                }\n                embedding.push(value);\n            }\n            embeddings.push(embedding);\n        }\n\n        let config = FFTDominantFrequencyConfig {\n            modulus,\n            min_modulus_frequency_fraction: 0.15,\n            max_spectrum_entropy: 0.85,\n            min_peak_to_median_ratio: 2.0,\n        };\n        let err = verify_fft_dominant_frequencies_from_embeddings(&embeddings, &config)\n            .expect_err(\"expected weak peaks to fail\");\n\n        assert!(\n            err.contains(\"modulus frequency\") || err.contains(\"spectrum entropy\")\n                || err.contains(\"peak to median\")\n        );\n    }\n}\n","structuredPatch":[{"oldStart":673,"oldLines":6,"newStart":673,"newLines":8,"lines":[" }"," "," impl FFTDominantFrequencyConfig {","+    /// NOTE: Reserved for future integration into analysis pipeline","+    #[allow(dead_code)]","     pub fn default_for_modulus(modulus: usize) -> Self {","         Self {","             modulus,"]}],"userModified":false,"replaceAll":false}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01Bi2jJWeUx5a2NTT3RnMdQH","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01Hsr9SVZdVJM27vGnfhrfGJ","name":"Edit","input":{"replace_all":false,"file_path":"/Users/admin/MechInt/src/analysis.rs","old_string":"impl EmbeddingClockConfig {\n    pub fn default_for_modulus(_modulus: usize) -> Self {\n        Self {\n            max_radius_std_fraction: 0.1,\n            min_angular_coverage: 0.9,\n            max_mean_angle_error: 0.35,\n            min_order_fraction: 0.9,\n        }\n    }\n}","new_string":"impl EmbeddingClockConfig {\n    /// NOTE: Reserved for future integration into analysis pipeline\n    #[allow(dead_code)]\n    pub fn default_for_modulus(_modulus: usize) -> Self {\n        Self {\n            max_radius_std_fraction: 0.1,\n            min_angular_coverage: 0.9,\n            max_mean_angle_error: 0.35,\n            min_order_fraction: 0.9,\n        }\n    }\n}"}}],"stop_reason":"tool_use","stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":640,"cache_read_input_tokens":46404,"cache_creation":{"ephemeral_5m_input_tokens":640,"ephemeral_1h_input_tokens":0},"output_tokens":335,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"b368bd7f-a3b2-46e0-9f81-215d8d7eb636"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01Hsr9SVZdVJM27vGnfhrfGJ","type":"tool_result","content":"The file /Users/admin/MechInt/src/analysis.rs has been updated. Here's the result of running `cat -n` on a snippet of the edited file:\n   700â†’    pub max_mean_angle_error: f64,\n   701â†’    pub min_order_fraction: f64,\n   702â†’}\n   703â†’\n   704â†’impl EmbeddingClockConfig {\n   705â†’    /// NOTE: Reserved for future integration into analysis pipeline\n   706â†’    #[allow(dead_code)]\n   707â†’    pub fn default_for_modulus(_modulus: usize) -> Self {\n   708â†’        Self {\n   709â†’            max_radius_std_fraction: 0.1,\n   710â†’            min_angular_coverage: 0.9,\n   711â†’            max_mean_angle_error: 0.35,\n   712â†’            min_order_fraction: 0.9,\n   713â†’        }\n   714â†’    }\n   715â†’}\n   716â†’\n   717â†’#[derive(Debug, Clone)]\n   718â†’pub struct EmbeddingClockReport {\n   719â†’    pub mean_radius: f64,"}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"7573d3ae-c4f3-4692-8dce-d1848afc73fd","tool_use_result":{"filePath":"/Users/admin/MechInt/src/analysis.rs","oldString":"impl EmbeddingClockConfig {\n    pub fn default_for_modulus(_modulus: usize) -> Self {\n        Self {\n            max_radius_std_fraction: 0.1,\n            min_angular_coverage: 0.9,\n            max_mean_angle_error: 0.35,\n            min_order_fraction: 0.9,\n        }\n    }\n}","newString":"impl EmbeddingClockConfig {\n    /// NOTE: Reserved for future integration into analysis pipeline\n    #[allow(dead_code)]\n    pub fn default_for_modulus(_modulus: usize) -> Self {\n        Self {\n            max_radius_std_fraction: 0.1,\n            min_angular_coverage: 0.9,\n            max_mean_angle_error: 0.35,\n            min_order_fraction: 0.9,\n        }\n    }\n}","originalFile":"use burn::{\n    data::dataset::Dataset,\n    nn::loss::CrossEntropyLossConfig,\n    tensor::{backend::Backend, Int, Tensor, TensorData},\n};\nuse rustfft::{num_complex::Complex, FftPlanner};\nuse serde::{Deserialize, Serialize};\nuse std::collections::{HashMap, HashSet};\n\nuse crate::{data::ModularAdditionDataset, model::Transformer};\n\n/// FFT analysis results for embeddings\n#[derive(Debug, Serialize, Deserialize)]\npub struct FFTAnalysis {\n    pub vocab_size: usize,\n    pub embedding_dim: usize,\n    pub dominant_frequencies: Vec<(usize, f64)>, // (token_id, dominant_frequency_index)\n    pub frequency_magnitudes: HashMap<usize, Vec<f64>>, // token_id -> magnitude spectrum\n}\n\n/// Analyze the learned token embeddings using FFT\npub fn analyze_embeddings_fft<B: Backend>(model: &Transformer<B>) -> FFTAnalysis {\n    println!(\"ðŸ” Performing FFT analysis on learned embeddings...\");\n\n    let vocab_size = ModularAdditionDataset::vocab_size();\n    let embedding_dim = 128;\n\n    // Extract token embeddings by forward passing through the embedding layer\n    let mut frequency_magnitudes = HashMap::new();\n    let mut dominant_frequencies = Vec::new();\n\n    for token_id in 0..vocab_size {\n        // Get the embedding for this token\n        let embedding = get_token_embedding(model, token_id);\n\n        // Compute FFT on the embedding vector\n        let fft_result = compute_fft(&embedding);\n\n        // Find dominant frequency\n        let dominant_freq_idx = find_dominant_frequency(&fft_result);\n        dominant_frequencies.push((token_id, dominant_freq_idx as f64));\n\n        // Store magnitude spectrum\n        frequency_magnitudes.insert(token_id, fft_result);\n\n        if token_id % 10 == 0 {\n            println!(\"  Token {:2}: dominant frequency = {}\", token_id, dominant_freq_idx);\n        }\n    }\n\n    println!(\"âœ… FFT analysis complete!\");\n    println!();\n    println!(\"ðŸ“Š Dominant Frequency Analysis:\");\n\n    // Analyze patterns\n    let mut freq_histogram: HashMap<usize, usize> = HashMap::new();\n    for (_, freq) in &dominant_frequencies {\n        *freq_histogram.entry(*freq as usize).or_insert(0) += 1;\n    }\n\n    let mut freq_counts: Vec<_> = freq_histogram.iter().collect();\n    freq_counts.sort_by(|a, b| b.1.cmp(a.1));\n\n    println!(\"  Most common dominant frequencies:\");\n    for (freq, count) in freq_counts.iter().take(5) {\n        println!(\"    Frequency {}: {} tokens\", freq, count);\n    }\n\n    // Check for modular structure (expected for mod p)\n    println!();\n    println!(\"ðŸ”¬ Checking for modular structure:\");\n    let modulus = ModularAdditionDataset::modulus();\n    if freq_histogram.contains_key(&modulus) {\n        println!(\n            \"  âœ“ Frequency {} (matching modulus) appears in embeddings\",\n            modulus\n        );\n    }\n\n    FFTAnalysis {\n        vocab_size,\n        embedding_dim,\n        dominant_frequencies,\n        frequency_magnitudes,\n    }\n}\n\n/// Extract the token embedding for a single token\nfn get_token_embedding<B: Backend>(model: &Transformer<B>, token_id: usize) -> Vec<f64> {\n    // Get the token embedding directly from the model\n    let embedding_tensor = model.get_token_embedding(token_id);\n\n    // Convert to vector - shape is [1, 1, embedding_dim], we want [embedding_dim]\n    let embedding_data: Vec<f32> = embedding_tensor.into_data().to_vec().unwrap();\n\n    // Convert to f64\n    embedding_data.iter().map(|&x| x as f64).collect()\n}\n\n/// Compute FFT on a vector of real values\nfn compute_fft(data: &[f64]) -> Vec<f64> {\n    let mut planner = FftPlanner::new();\n    let fft = planner.plan_fft_forward(data.len());\n\n    // Convert to complex numbers\n    let mut buffer: Vec<Complex<f64>> = data\n        .iter()\n        .map(|&x| Complex::new(x, 0.0))\n        .collect();\n\n    // Perform FFT\n    fft.process(&mut buffer);\n\n    // Compute magnitudes\n    buffer.iter().map(|c| c.norm()).collect()\n}\n\nfn inverse_fft(data: &mut [Complex<f64>]) {\n    let mut planner = FftPlanner::new();\n    let fft = planner.plan_fft_inverse(data.len());\n    fft.process(data);\n}\n\nfn top_k_frequency_indices(magnitudes: &[f64], k: usize) -> Vec<usize> {\n    let mut indexed: Vec<(usize, f64)> = magnitudes.iter().copied().enumerate().collect();\n    indexed.sort_by(|(idx_a, mag_a), (idx_b, mag_b)| {\n        mag_b\n            .partial_cmp(mag_a)\n            .unwrap_or(std::cmp::Ordering::Equal)\n            .then_with(|| idx_a.cmp(idx_b))\n    });\n\n    let keep = k.min(indexed.len());\n    indexed.iter().take(keep).map(|(idx, _)| *idx).collect()\n}\n\nfn restrict_signal_to_top_k_frequencies(signal: &[f64], top_k: usize) -> Vec<f64> {\n    if signal.is_empty() {\n        return Vec::new();\n    }\n\n    if top_k == 0 {\n        return vec![0.0; signal.len()];\n    }\n\n    let mut planner = FftPlanner::new();\n    let fft = planner.plan_fft_forward(signal.len());\n\n    let mut buffer: Vec<Complex<f64>> = signal\n        .iter()\n        .map(|&x| Complex::new(x, 0.0))\n        .collect();\n    fft.process(&mut buffer);\n\n    let magnitudes: Vec<f64> = buffer.iter().map(|c| c.norm()).collect();\n    let keep = top_k_frequency_indices(&magnitudes, top_k);\n    let keep_set: HashSet<usize> = keep.into_iter().collect();\n\n    for (idx, value) in buffer.iter_mut().enumerate() {\n        if !keep_set.contains(&idx) {\n            *value = Complex::new(0.0, 0.0);\n        }\n    }\n\n    inverse_fft(&mut buffer);\n\n    let scale = 1.0 / signal.len() as f64;\n    buffer.iter().map(|c| c.re * scale).collect()\n}\n\nfn exclude_top_k_frequencies(signal: &[f64], top_k: usize) -> Vec<f64> {\n    if signal.is_empty() {\n        return Vec::new();\n    }\n\n    if top_k == 0 {\n        return signal.to_vec();\n    }\n\n    if top_k >= signal.len() {\n        return vec![0.0; signal.len()];\n    }\n\n    let mut planner = FftPlanner::new();\n    let fft = planner.plan_fft_forward(signal.len());\n\n    let mut buffer: Vec<Complex<f64>> = signal\n        .iter()\n        .map(|&x| Complex::new(x, 0.0))\n        .collect();\n    fft.process(&mut buffer);\n\n    let magnitudes: Vec<f64> = buffer.iter().map(|c| c.norm()).collect();\n    let remove = top_k_frequency_indices(&magnitudes, top_k);\n    let remove_set: HashSet<usize> = remove.into_iter().collect();\n\n    for (idx, value) in buffer.iter_mut().enumerate() {\n        if remove_set.contains(&idx) {\n            *value = Complex::new(0.0, 0.0);\n        }\n    }\n\n    inverse_fft(&mut buffer);\n\n    let scale = 1.0 / signal.len() as f64;\n    buffer.iter().map(|c| c.re * scale).collect()\n}\n\nfn restricted_token_embedding_weights<B: Backend>(\n    model: &Transformer<B>,\n    top_k: usize,\n) -> Vec<f32> {\n    let weight = model.token_embedding_weights();\n    let [vocab_size, d_model] = weight.dims();\n    let weight_data: Vec<f32> = weight.into_data().to_vec().unwrap();\n    let mut restricted = vec![0.0f32; weight_data.len()];\n\n    for dim in 0..d_model {\n        let mut column = Vec::with_capacity(vocab_size);\n        for token in 0..vocab_size {\n            column.push(weight_data[token * d_model + dim] as f64);\n        }\n        let filtered = restrict_signal_to_top_k_frequencies(&column, top_k);\n        for token in 0..vocab_size {\n            restricted[token * d_model + dim] = filtered[token] as f32;\n        }\n    }\n\n    restricted\n}\n\nfn excluded_token_embedding_weights<B: Backend>(\n    model: &Transformer<B>,\n    top_k: usize,\n) -> Vec<f32> {\n    let weight = model.token_embedding_weights();\n    let [vocab_size, d_model] = weight.dims();\n    let weight_data: Vec<f32> = weight.into_data().to_vec().unwrap();\n    let mut excluded = vec![0.0f32; weight_data.len()];\n\n    for dim in 0..d_model {\n        let mut column = Vec::with_capacity(vocab_size);\n        for token in 0..vocab_size {\n            column.push(weight_data[token * d_model + dim] as f64);\n        }\n        let filtered = exclude_top_k_frequencies(&column, top_k);\n        for token in 0..vocab_size {\n            excluded[token * d_model + dim] = filtered[token] as f32;\n        }\n    }\n\n    excluded\n}\n\npub fn compute_restricted_loss<B: Backend>(\n    model: &Transformer<B>,\n    dataset: &ModularAdditionDataset,\n    device: &B::Device,\n    top_k: usize,\n    batch_size: usize,\n) -> Result<f64, String> {\n    if batch_size == 0 {\n        return Err(\"batch_size must be greater than 0\".to_string());\n    }\n\n    let restricted_weights = restricted_token_embedding_weights(model, top_k);\n    let [vocab_size, d_model] = model.token_embedding_weights().dims();\n    let token_weights = Tensor::<B, 2>::from_data(\n        TensorData::new(restricted_weights, [vocab_size, d_model]),\n        device,\n    );\n\n    let loss_fn = CrossEntropyLossConfig::new().init(device);\n    let mut total_loss = 0.0f64;\n    let mut total_batches = 0usize;\n\n    for batch_start in (0..dataset.len()).step_by(batch_size) {\n        let batch_end = (batch_start + batch_size).min(dataset.len());\n        let batch_len = batch_end - batch_start;\n        if batch_len == 0 {\n            break;\n        }\n\n        let mut inputs_vec = Vec::with_capacity(batch_len * 3);\n        let mut targets_vec = Vec::with_capacity(batch_len);\n\n        for idx in batch_start..batch_end {\n            let (input, target) = dataset.get(idx).ok_or_else(|| {\n                format!(\"missing dataset sample at index {}\", idx)\n            })?;\n            inputs_vec.extend(input.iter().map(|value| *value as i32));\n            targets_vec.push(target as i32);\n        }\n\n        let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n            .reshape([batch_len, 3]);\n        let targets = Tensor::<B, 1, Int>::from_ints(targets_vec.as_slice(), device);\n\n        let logits = model.forward_with_token_weights(inputs, token_weights.clone());\n        let loss = loss_fn.forward(logits, targets);\n        let loss_value: f64 = loss.into_data().to_vec().unwrap().get(0).copied().unwrap_or(0.0);\n        total_loss += loss_value;\n        total_batches += 1;\n    }\n\n    if total_batches == 0 {\n        return Err(\"no batches computed for restricted loss\".to_string());\n    }\n\n    Ok(total_loss / total_batches as f64)\n}\n\npub fn compute_excluded_loss<B: Backend>(\n    model: &Transformer<B>,\n    dataset: &ModularAdditionDataset,\n    device: &B::Device,\n    top_k: usize,\n    batch_size: usize,\n) -> Result<f64, String> {\n    if batch_size == 0 {\n        return Err(\"batch_size must be greater than 0\".to_string());\n    }\n\n    let excluded_weights = excluded_token_embedding_weights(model, top_k);\n    let [vocab_size, d_model] = model.token_embedding_weights().dims();\n    let token_weights = Tensor::<B, 2>::from_data(\n        TensorData::new(excluded_weights, [vocab_size, d_model]),\n        device,\n    );\n\n    let loss_fn = CrossEntropyLossConfig::new().init(device);\n    let mut total_loss = 0.0f64;\n    let mut total_batches = 0usize;\n\n    for batch_start in (0..dataset.len()).step_by(batch_size) {\n        let batch_end = (batch_start + batch_size).min(dataset.len());\n        let batch_len = batch_end - batch_start;\n        if batch_len == 0 {\n            break;\n        }\n\n        let mut inputs_vec = Vec::with_capacity(batch_len * 3);\n        let mut targets_vec = Vec::with_capacity(batch_len);\n\n        for idx in batch_start..batch_end {\n            let (input, target) = dataset.get(idx).ok_or_else(|| {\n                format!(\"missing dataset sample at index {}\", idx)\n            })?;\n            inputs_vec.extend(input.iter().map(|value| *value as i32));\n            targets_vec.push(target as i32);\n        }\n\n        let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n            .reshape([batch_len, 3]);\n        let targets = Tensor::<B, 1, Int>::from_ints(targets_vec.as_slice(), device);\n\n        let logits = model.forward_with_token_weights(inputs, token_weights.clone());\n        let loss = loss_fn.forward(logits, targets);\n        let loss_value: f64 = loss.into_data().to_vec().unwrap().get(0).copied().unwrap_or(0.0);\n        total_loss += loss_value;\n        total_batches += 1;\n    }\n\n    if total_batches == 0 {\n        return Err(\"no batches computed for excluded loss\".to_string());\n    }\n\n    Ok(total_loss / total_batches as f64)\n}\n\n/// Find the index of the dominant frequency (excluding DC component at index 0)\nfn find_dominant_frequency(magnitudes: &[f64]) -> usize {\n    magnitudes\n        .iter()\n        .enumerate()\n        .skip(1) // Skip DC component\n        .max_by(|(_, a), (_, b)| {\n            a.partial_cmp(b).unwrap_or(std::cmp::Ordering::Equal)\n        })\n        .map(|(idx, _)| idx)\n        .unwrap_or(0)\n}\n\n/// Save FFT analysis to JSON file\npub fn save_fft_analysis(analysis: &FFTAnalysis, filename: &str) -> std::io::Result<()> {\n    let json = serde_json::to_string_pretty(analysis)?;\n    std::fs::write(filename, json)?;\n    println!(\"ðŸ’¾ FFT analysis saved to: {}\", filename);\n    Ok(())\n}\n\n/// Track training and validation loss over time\n#[derive(Debug, Serialize, Deserialize)]\npub struct LossHistory {\n    pub train_snapshots: Vec<(usize, f64)>, // (step, loss)\n    pub val_snapshots: Vec<(usize, f64)>,   // (step, loss)\n}\n\n/// Track restricted loss over time (only top-k frequencies kept)\n#[derive(Debug, Serialize, Deserialize)]\npub struct RestrictedLossHistory {\n    pub snapshots: Vec<(usize, f64)>, // (step, loss)\n}\n\nimpl RestrictedLossHistory {\n    pub fn new() -> Self {\n        Self {\n            snapshots: Vec::new(),\n        }\n    }\n\n    pub fn add_snapshot(&mut self, step: usize, loss: f64) {\n        self.snapshots.push((step, loss));\n    }\n\n    pub fn save(&self, filename: &str) -> std::io::Result<()> {\n        let json = serde_json::to_string_pretty(self)?;\n        std::fs::write(filename, json)?;\n        println!(\"ðŸ’¾ Restricted loss history saved to: {}\", filename);\n        Ok(())\n    }\n}\n\n/// Track excluded loss over time (top-k frequencies removed)\n#[derive(Debug, Serialize, Deserialize)]\npub struct ExcludedLossHistory {\n    pub snapshots: Vec<(usize, f64)>, // (step, loss)\n}\n\nimpl ExcludedLossHistory {\n    pub fn new() -> Self {\n        Self {\n            snapshots: Vec::new(),\n        }\n    }\n\n    pub fn add_snapshot(&mut self, step: usize, loss: f64) {\n        self.snapshots.push((step, loss));\n    }\n\n    pub fn save(&self, filename: &str) -> std::io::Result<()> {\n        let json = serde_json::to_string_pretty(self)?;\n        std::fs::write(filename, json)?;\n        println!(\"ðŸ’¾ Excluded loss history saved to: {}\", filename);\n        Ok(())\n    }\n}\n\nimpl LossHistory {\n    pub fn new() -> Self {\n        Self {\n            train_snapshots: Vec::new(),\n            val_snapshots: Vec::new(),\n        }\n    }\n\n    pub fn add_snapshot(&mut self, step: usize, train_loss: f64, val_loss: f64) {\n        self.train_snapshots.push((step, train_loss));\n        self.val_snapshots.push((step, val_loss));\n    }\n\n    pub fn save(&self, filename: &str) -> std::io::Result<()> {\n        let json = serde_json::to_string_pretty(self)?;\n        std::fs::write(filename, json)?;\n        println!(\"ðŸ’¾ Loss history saved to: {}\", filename);\n        Ok(())\n    }\n}\n\n/// Legacy: Single loss tracking (kept for backward compatibility)\n#[derive(Debug, Serialize, Deserialize, Clone)]\npub struct WeightNormSnapshot {\n    pub step: usize,\n    pub total_norm: f64,\n}\n\n/// Legacy: Track weight norms over training (actually stores loss)\n#[derive(Debug, Serialize, Deserialize)]\npub struct WeightNormHistory {\n    pub snapshots: Vec<WeightNormSnapshot>,\n}\n\nimpl WeightNormHistory {\n    pub fn new() -> Self {\n        Self {\n            snapshots: Vec::new(),\n        }\n    }\n\n    pub fn add_snapshot(&mut self, step: usize, norm: f64) {\n        self.snapshots.push(WeightNormSnapshot {\n            step,\n            total_norm: norm,\n        });\n    }\n\n    pub fn save(&self, filename: &str) -> std::io::Result<()> {\n        let json = serde_json::to_string_pretty(self)?;\n        std::fs::write(filename, json)?;\n        println!(\"ðŸ’¾ Weight norm history saved to: {}\", filename);\n        Ok(())\n    }\n}\n\n/// Compute the L2 norm of all model parameters\n/// This returns the Frobenius norm (sum of squared weights) across all parameters\n/// Note: This is a simplified implementation that computes the norm of token embeddings,\n/// which contain the majority of the model parameters. For a complete implementation,\n/// we would need to access all Linear layer weights via the Record trait.\npub fn compute_model_weight_norm<B: Backend>(model: &Transformer<B>) -> f64 {\n    let mut total_norm_squared = 0.0f64;\n\n    // Token embedding weights: [vocab_size, d_model]\n    let token_emb_weights = model.token_embedding_weights();\n    let token_emb_data: Vec<f32> = token_emb_weights.into_data().to_vec().unwrap();\n    for &weight in &token_emb_data {\n        total_norm_squared += (weight as f64) * (weight as f64);\n    }\n\n    // Note: Position embeddings, attention weights, MLP weights, and LM head weights\n    // are not included in this simplified implementation. For weight decay verification,\n    // tracking the token embeddings (which are the largest weight matrices) is sufficient\n    // to observe the monotonic decrease under weight decay.\n\n    total_norm_squared.sqrt()\n}\n\n/// Extract all token embeddings as a matrix [vocab_size, embedding_dim]\npub fn extract_all_embeddings<B: Backend>(model: &Transformer<B>) -> Vec<Vec<f64>> {\n    let vocab_size = ModularAdditionDataset::vocab_size();\n    let mut embeddings = Vec::with_capacity(vocab_size);\n\n    for token_id in 0..vocab_size {\n        embeddings.push(get_token_embedding(model, token_id));\n    }\n\n    embeddings\n}\n\n/// Verify that FFT analysis of embeddings shows dominant frequencies\n/// that align with the modulus structure and nontrivial spectral patterns\n///\n/// NOTE: Reserved for manual verification or future integration into analysis pipeline\n#[allow(dead_code)]\npub fn verify_fft_dominant_frequencies<B: Backend>(\n    model: &Transformer<B>,\n    config: &FFTDominantFrequencyConfig,\n) -> Result<FFTDominantFrequencyReport, String> {\n    let embeddings = extract_all_embeddings(model);\n    verify_fft_dominant_frequencies_from_embeddings(&embeddings, config)\n}\n\n/// Verify FFT dominant frequencies from pre-extracted embeddings\n///\n/// NOTE: Reserved for manual verification or future integration into analysis pipeline\n#[allow(dead_code)]\npub fn verify_fft_dominant_frequencies_from_embeddings(\n    embeddings: &[Vec<f64>],\n    config: &FFTDominantFrequencyConfig,\n) -> Result<FFTDominantFrequencyReport, String> {\n    if embeddings.is_empty() {\n        return Err(\"no embeddings provided\".to_string());\n    }\n\n    let vocab_size = embeddings.len();\n    if vocab_size < config.modulus {\n        return Err(format!(\n            \"embedding count {} less than modulus {}\",\n            vocab_size, config.modulus\n        ));\n    }\n\n    // Analyze each embedding dimension and collect dominant frequencies\n    let mut frequency_histogram: HashMap<usize, usize> = HashMap::new();\n    let embedding_dim = embeddings[0].len();\n\n    // For each embedding dimension, compute FFT across all tokens and find dominant frequency\n    for dim in 0..embedding_dim {\n        let mut signal = Vec::with_capacity(vocab_size);\n        for embedding in embeddings.iter().take(vocab_size) {\n            if dim >= embedding.len() {\n                return Err(format!(\n                    \"embedding dimension mismatch at dim {}\",\n                    dim\n                ));\n            }\n            signal.push(embedding[dim]);\n        }\n\n        // Compute FFT and find dominant frequency\n        let magnitudes = compute_fft(&signal);\n        let dominant_freq = find_dominant_frequency(&magnitudes);\n        *frequency_histogram.entry(dominant_freq).or_insert(0) += 1;\n    }\n\n    // Check 1: Modulus frequency should appear significantly\n    let modulus_count = frequency_histogram.get(&config.modulus).copied().unwrap_or(0);\n    let modulus_frequency_fraction = modulus_count as f64 / embedding_dim as f64;\n\n    if modulus_frequency_fraction < config.min_modulus_frequency_fraction {\n        return Err(format!(\n            \"modulus frequency {} appears in only {:.3} of dimensions (threshold {:.3})\",\n            config.modulus, modulus_frequency_fraction, config.min_modulus_frequency_fraction\n        ));\n    }\n\n    // Check 2: Spectrum should NOT resemble white noise (low entropy)\n    // High entropy = uniform distribution (white noise), low entropy = structured peaks\n    let total_dims = embedding_dim as f64;\n    let mut entropy = 0.0;\n    for &count in frequency_histogram.values() {\n        if count > 0 {\n            let p = count as f64 / total_dims;\n            entropy -= p * p.log2();\n        }\n    }\n    // Normalize entropy by max possible (log2 of unique frequency count)\n    let max_entropy = (frequency_histogram.len() as f64).log2();\n    let normalized_entropy = if max_entropy > 0.0 {\n        entropy / max_entropy\n    } else {\n        0.0\n    };\n\n    if normalized_entropy > config.max_spectrum_entropy {\n        return Err(format!(\n            \"spectrum entropy {:.3} too high (white noise threshold {:.3})\",\n            normalized_entropy, config.max_spectrum_entropy\n        ));\n    }\n\n    // Check 3: Peak frequencies should be significantly stronger than median\n    let mut counts: Vec<usize> = frequency_histogram.values().copied().collect();\n    counts.sort_unstable();\n    let median_count = if counts.is_empty() {\n        0\n    } else {\n        counts[counts.len() / 2]\n    };\n    let peak_count = counts.last().copied().unwrap_or(0);\n    let peak_to_median_ratio = if median_count > 0 {\n        peak_count as f64 / median_count as f64\n    } else {\n        f64::INFINITY\n    };\n\n    if peak_to_median_ratio < config.min_peak_to_median_ratio {\n        return Err(format!(\n            \"peak to median ratio {:.3} below threshold {:.3}\",\n            peak_to_median_ratio, config.min_peak_to_median_ratio\n        ));\n    }\n\n    // Collect top frequencies for the report\n    let mut freq_counts: Vec<(usize, usize)> = frequency_histogram.into_iter().collect();\n    freq_counts.sort_by(|a, b| b.1.cmp(&a.1));\n    let top_frequencies: Vec<(usize, usize)> = freq_counts.into_iter().take(5).collect();\n\n    Ok(FFTDominantFrequencyReport {\n        modulus_frequency_fraction,\n        spectrum_entropy: normalized_entropy,\n        peak_to_median_ratio,\n        top_frequencies,\n    })\n}\n\n#[derive(Debug, Clone)]\npub struct FFTDominantFrequencyConfig {\n    pub modulus: usize,\n    pub min_modulus_frequency_fraction: f64,\n    pub max_spectrum_entropy: f64,\n    pub min_peak_to_median_ratio: f64,\n}\n\nimpl FFTDominantFrequencyConfig {\n    /// NOTE: Reserved for future integration into analysis pipeline\n    #[allow(dead_code)]\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        Self {\n            modulus,\n            min_modulus_frequency_fraction: 0.15,\n            max_spectrum_entropy: 0.85,\n            min_peak_to_median_ratio: 2.0,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct FFTDominantFrequencyReport {\n    pub modulus_frequency_fraction: f64,\n    pub spectrum_entropy: f64,\n    pub peak_to_median_ratio: f64,\n    pub top_frequencies: Vec<(usize, usize)>,\n}\n\n#[derive(Debug, Clone)]\npub struct EmbeddingClockConfig {\n    pub max_radius_std_fraction: f64,\n    pub min_angular_coverage: f64,\n    pub max_mean_angle_error: f64,\n    pub min_order_fraction: f64,\n}\n\nimpl EmbeddingClockConfig {\n    pub fn default_for_modulus(_modulus: usize) -> Self {\n        Self {\n            max_radius_std_fraction: 0.1,\n            min_angular_coverage: 0.9,\n            max_mean_angle_error: 0.35,\n            min_order_fraction: 0.9,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct EmbeddingClockReport {\n    pub mean_radius: f64,\n    pub radius_std: f64,\n    pub angular_coverage: f64,\n    pub mean_angle_error: f64,\n    pub order_fraction: f64,\n}\n\npub fn project_embeddings_to_fourier_plane(\n    embeddings: &[Vec<f64>],\n    modulus: usize,\n) -> Result<Vec<(f64, f64)>, String> {\n    let p = modulus;\n    if embeddings.len() < p {\n        return Err(\"embedding matrix smaller than modulus\".to_string());\n    }\n    if p == 0 {\n        return Err(\"modulus must be positive\".to_string());\n    }\n    let d = embeddings[0].len();\n    if d == 0 {\n        return Err(\"embedding dimension must be positive\".to_string());\n    }\n    if !embeddings[..p].iter().all(|row| row.len() == d) {\n        return Err(\"embedding rows have inconsistent dimension\".to_string());\n    }\n\n    let tau = std::f64::consts::PI * 2.0;\n    let mut basis_cos = vec![0.0f64; d];\n    let mut basis_sin = vec![0.0f64; d];\n    let scale = 2.0 / p as f64;\n\n    for (token, embedding) in embeddings[..p].iter().enumerate() {\n        let angle = tau * token as f64 / p as f64;\n        let cos_val = angle.cos();\n        let sin_val = angle.sin();\n        for dim in 0..d {\n            basis_cos[dim] += cos_val * embedding[dim] * scale;\n            basis_sin[dim] += sin_val * embedding[dim] * scale;\n        }\n    }\n\n    let basis_cos = normalize_vector(&basis_cos)?;\n    let basis_sin = normalize_vector(&basis_sin)?;\n\n    let mut coords = Vec::with_capacity(p);\n    for embedding in embeddings[..p].iter() {\n        let x = dot(embedding, &basis_cos);\n        let y = dot(embedding, &basis_sin);\n        coords.push((x, y));\n    }\n\n    Ok(coords)\n}\n\npub fn verify_embedding_clock_geometry(\n    embeddings: &[Vec<f64>],\n    modulus: usize,\n    config: &EmbeddingClockConfig,\n) -> Result<EmbeddingClockReport, String> {\n    let coords = project_embeddings_to_fourier_plane(embeddings, modulus)?;\n    let p = coords.len();\n    if p == 0 {\n        return Err(\"no embedding coordinates computed\".to_string());\n    }\n\n    let mut radii = Vec::with_capacity(p);\n    let mut angles = Vec::with_capacity(p);\n    for (x, y) in &coords {\n        let radius = (x * x + y * y).sqrt();\n        radii.push(radius);\n        angles.push(y.atan2(*x));\n    }\n\n    let mean_radius = radii.iter().sum::<f64>() / p as f64;\n    if mean_radius == 0.0 {\n        return Err(\"mean radius is zero\".to_string());\n    }\n    let radius_var = radii\n        .iter()\n        .map(|r| (r - mean_radius) * (r - mean_radius))\n        .sum::<f64>()\n        / p as f64;\n    let radius_std = radius_var.sqrt();\n\n    if radius_std / mean_radius > config.max_radius_std_fraction {\n        return Err(format!(\n            \"radius variance too large (std {:.4}, mean {:.4})\",\n            radius_std, mean_radius\n        ));\n    }\n\n    let tau = std::f64::consts::PI * 2.0;\n    let (mut sum_cos, mut sum_sin) = (0.0f64, 0.0f64);\n    for (token, angle) in angles.iter().enumerate() {\n        let expected = tau * token as f64 / p as f64;\n        let delta = angle - expected;\n        sum_cos += delta.cos();\n        sum_sin += delta.sin();\n    }\n    let offset = sum_sin.atan2(sum_cos);\n\n    let mut adjusted_angles = Vec::with_capacity(p);\n    let mut angle_errors = Vec::with_capacity(p);\n    for (token, angle) in angles.iter().enumerate() {\n        let expected = tau * token as f64 / p as f64;\n        let mut adjusted = angle - offset;\n        adjusted = wrap_angle(adjusted);\n        adjusted_angles.push(adjusted);\n        let diff = wrap_angle_signed(adjusted - expected);\n        angle_errors.push(diff.abs());\n    }\n\n    let mean_angle_error = angle_errors.iter().sum::<f64>() / p as f64;\n    if mean_angle_error > config.max_mean_angle_error {\n        return Err(format!(\n            \"mean angular error too large ({:.4} rad)\",\n            mean_angle_error\n        ));\n    }\n\n    let angular_coverage = angular_coverage(&adjusted_angles);\n    if angular_coverage < config.min_angular_coverage {\n        return Err(format!(\n            \"angular coverage too small ({:.4})\",\n            angular_coverage\n        ));\n    }\n\n    let order_fraction = order_fraction(&adjusted_angles, p);\n    if order_fraction < config.min_order_fraction {\n        return Err(format!(\n            \"token ordering too weak ({:.4})\",\n            order_fraction\n        ));\n    }\n\n    Ok(EmbeddingClockReport {\n        mean_radius,\n        radius_std,\n        angular_coverage,\n        mean_angle_error,\n        order_fraction,\n    })\n}\n\nfn normalize_vector(vec: &[f64]) -> Result<Vec<f64>, String> {\n    let norm = vec.iter().map(|v| v * v).sum::<f64>().sqrt();\n    if norm == 0.0 {\n        return Err(\"basis vector norm is zero\".to_string());\n    }\n    Ok(vec.iter().map(|v| v / norm).collect())\n}\n\nfn dot(a: &[f64], b: &[f64]) -> f64 {\n    a.iter().zip(b.iter()).map(|(x, y)| x * y).sum()\n}\n\nfn wrap_angle(angle: f64) -> f64 {\n    let tau = std::f64::consts::PI * 2.0;\n    let mut wrapped = angle % tau;\n    if wrapped < 0.0 {\n        wrapped += tau;\n    }\n    wrapped\n}\n\nfn wrap_angle_signed(angle: f64) -> f64 {\n    let tau = std::f64::consts::PI * 2.0;\n    let mut wrapped = (angle + std::f64::consts::PI) % tau;\n    if wrapped < 0.0 {\n        wrapped += tau;\n    }\n    wrapped - std::f64::consts::PI\n}\n\nfn angular_coverage(angles: &[f64]) -> f64 {\n    if angles.is_empty() {\n        return 0.0;\n    }\n    let tau = std::f64::consts::PI * 2.0;\n    let mut sorted = angles.to_vec();\n    sorted.sort_by(|a, b| a.partial_cmp(b).unwrap_or(std::cmp::Ordering::Equal));\n\n    let mut max_gap = 0.0;\n    for pair in sorted.windows(2) {\n        let gap = pair[1] - pair[0];\n        if gap > max_gap {\n            max_gap = gap;\n        }\n    }\n    let wrap_gap = tau - (sorted[sorted.len() - 1] - sorted[0]);\n    if wrap_gap > max_gap {\n        max_gap = wrap_gap;\n    }\n\n    (tau - max_gap) / tau\n}\n\nfn order_fraction(angles: &[f64], modulus: usize) -> f64 {\n    if angles.len() < 2 || modulus == 0 {\n        return 0.0;\n    }\n    let mut indexed: Vec<(usize, f64)> = angles.iter().copied().enumerate().collect();\n    indexed.sort_by(|a, b| a.1.partial_cmp(&b.1).unwrap_or(std::cmp::Ordering::Equal));\n\n    let mut good = 0usize;\n    for window in indexed.windows(2) {\n        let current = window[0].0;\n        let next = window[1].0;\n        if (next + modulus - current) % modulus == 1 {\n            good += 1;\n        }\n    }\n    let last = indexed[indexed.len() - 1].0;\n    let first = indexed[0].0;\n    if (first + modulus - last) % modulus == 1 {\n        good += 1;\n    }\n\n    good as f64 / modulus as f64\n}\n\n/// Track training and validation accuracy over time\n#[derive(Debug, Serialize, Deserialize)]\npub struct AccuracyHistory {\n    pub train_snapshots: Vec<(usize, f32)>, // (step, accuracy)\n    pub val_snapshots: Vec<(usize, f32)>,   // (step, accuracy)\n}\n\nimpl AccuracyHistory {\n    pub fn new() -> Self {\n        Self {\n            train_snapshots: Vec::new(),\n            val_snapshots: Vec::new(),\n        }\n    }\n\n    pub fn add_snapshot(&mut self, step: usize, train_acc: f32, val_acc: f32) {\n        self.train_snapshots.push((step, train_acc));\n        self.val_snapshots.push((step, val_acc));\n    }\n\n    pub fn save(&self, filename: &str) -> std::io::Result<()> {\n        let json = serde_json::to_string_pretty(self)?;\n        std::fs::write(filename, json)?;\n        println!(\"ðŸ’¾ Accuracy history saved to: {}\", filename);\n        Ok(())\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    fn make_circle_embeddings(modulus: usize) -> Vec<Vec<f64>> {\n        let tau = std::f64::consts::PI * 2.0;\n        (0..modulus)\n            .map(|token| {\n                let angle = tau * token as f64 / modulus as f64;\n                vec![angle.cos(), angle.sin(), 0.0, 0.0]\n            })\n            .collect()\n    }\n\n    #[test]\n    fn top_k_frequency_indices_stable_with_ties() {\n        let magnitudes = vec![0.1, 1.0, 1.0, 0.5];\n        let first = top_k_frequency_indices(&magnitudes, 2);\n        let second = top_k_frequency_indices(&magnitudes, 2);\n        assert_eq!(first, vec![1, 2]);\n        assert_eq!(first, second);\n    }\n\n    #[test]\n    fn restrict_signal_zeroes_when_top_k_zero() {\n        let signal = vec![1.0, -1.0, 0.5, -0.5];\n        let filtered = restrict_signal_to_top_k_frequencies(&signal, 0);\n        assert_eq!(filtered, vec![0.0, 0.0, 0.0, 0.0]);\n    }\n\n    #[test]\n    fn exclude_signal_no_change_when_top_k_zero() {\n        let signal = vec![1.0, -1.0, 0.5, -0.5];\n        let filtered = exclude_top_k_frequencies(&signal, 0);\n        assert_eq!(filtered, signal);\n    }\n\n    #[test]\n    fn exclude_signal_zeroes_when_top_k_all() {\n        let signal = vec![1.0, -1.0, 0.5, -0.5];\n        let filtered = exclude_top_k_frequencies(&signal, signal.len());\n        assert_eq!(filtered, vec![0.0, 0.0, 0.0, 0.0]);\n    }\n\n    #[test]\n    fn embedding_clock_geometry_passes_on_clean_circle() {\n        let modulus = 13;\n        let embeddings = make_circle_embeddings(modulus);\n        let config = EmbeddingClockConfig::default_for_modulus(modulus);\n        let report =\n            verify_embedding_clock_geometry(&embeddings, modulus, &config).expect(\"expected pass\");\n        assert!(report.angular_coverage > 0.9);\n        assert!(report.order_fraction > 0.9);\n        assert!(report.mean_angle_error < 0.1);\n    }\n\n    #[test]\n    fn embedding_clock_geometry_flags_wrong_frequency() {\n        let modulus = 11;\n        let tau = std::f64::consts::PI * 2.0;\n        let embeddings: Vec<Vec<f64>> = (0..modulus)\n            .map(|token| {\n                let angle = tau * token as f64 / modulus as f64;\n                vec![(2.0 * angle).cos(), (2.0 * angle).sin(), 0.0, 0.0]\n            })\n            .collect();\n        let config = EmbeddingClockConfig {\n            max_radius_std_fraction: 0.5,\n            min_angular_coverage: 0.1,\n            max_mean_angle_error: 0.2,\n            min_order_fraction: 0.1,\n        };\n        let err = verify_embedding_clock_geometry(&embeddings, modulus, &config)\n            .expect_err(\"expected ordering verification to fail\");\n        assert!(err.contains(\"mean angular error\"));\n    }\n\n    #[test]\n    fn fft_dominant_frequency_passes_on_modulus_structure() {\n        // Create embeddings with strong modulus-113 frequency component\n        // Mix in some other frequencies to get a realistic peak-to-median ratio\n        let modulus = 113;\n        let vocab_size = 114; // 0-112 + equals token\n        let embedding_dim = 128;\n        let tau = std::f64::consts::PI * 2.0;\n\n        let mut embeddings = Vec::with_capacity(vocab_size);\n        for token in 0..vocab_size {\n            let mut embedding = Vec::with_capacity(embedding_dim);\n            for dim in 0..embedding_dim {\n                // 60% of dimensions have modulus frequency, 40% have other frequencies\n                let freq = if dim % 5 < 3 {\n                    modulus\n                } else {\n                    // Use various other frequencies for the remaining dims\n                    20 + (dim % 20)\n                };\n                let angle = tau * freq as f64 * token as f64 / vocab_size as f64\n                    + dim as f64 * 0.1;\n                embedding.push(angle.sin());\n            }\n            embeddings.push(embedding);\n        }\n\n        let config = FFTDominantFrequencyConfig::default_for_modulus(modulus);\n        let report = verify_fft_dominant_frequencies_from_embeddings(&embeddings, &config)\n            .expect(\"expected modulus structure to pass\");\n\n        assert!(report.modulus_frequency_fraction >= config.min_modulus_frequency_fraction);\n        assert!(report.spectrum_entropy <= config.max_spectrum_entropy);\n        assert!(report.peak_to_median_ratio >= config.min_peak_to_median_ratio);\n        assert!(report.top_frequencies.iter().any(|(freq, _)| *freq == modulus));\n    }\n\n    #[test]\n    fn fft_dominant_frequency_fails_on_white_noise() {\n        // Create random embeddings (white noise spectrum)\n        let modulus = 113;\n        let vocab_size = 114;\n        let embedding_dim = 128;\n\n        let mut embeddings = Vec::with_capacity(vocab_size);\n        for token in 0..vocab_size {\n            let mut embedding = Vec::with_capacity(embedding_dim);\n            for dim in 0..embedding_dim {\n                // Pseudo-random values\n                let value = ((token * 17 + dim * 31) % 100) as f64 / 50.0 - 1.0;\n                embedding.push(value);\n            }\n            embeddings.push(embedding);\n        }\n\n        let config = FFTDominantFrequencyConfig::default_for_modulus(modulus);\n        let err = verify_fft_dominant_frequencies_from_embeddings(&embeddings, &config)\n            .expect_err(\"expected white noise to fail\");\n\n        // Should fail on either modulus frequency fraction or spectrum entropy\n        assert!(\n            err.contains(\"modulus frequency\") || err.contains(\"spectrum entropy\")\n                || err.contains(\"peak to median\")\n        );\n    }\n\n    #[test]\n    fn fft_dominant_frequency_fails_on_wrong_frequency() {\n        // Create embeddings with dominant frequency at 57 (not 113)\n        let modulus = 113;\n        let vocab_size = 114;\n        let embedding_dim = 128;\n        let tau = std::f64::consts::PI * 2.0;\n        let wrong_frequency = 57;\n\n        let mut embeddings = Vec::with_capacity(vocab_size);\n        for token in 0..vocab_size {\n            let mut embedding = Vec::with_capacity(embedding_dim);\n            for dim in 0..embedding_dim {\n                // Each dimension has a sine wave with wrong frequency\n                let angle = tau * wrong_frequency as f64 * token as f64 / vocab_size as f64\n                    + dim as f64 * 0.1;\n                embedding.push(angle.sin());\n            }\n            embeddings.push(embedding);\n        }\n\n        let config = FFTDominantFrequencyConfig::default_for_modulus(modulus);\n        let err = verify_fft_dominant_frequencies_from_embeddings(&embeddings, &config)\n            .expect_err(\"expected wrong frequency to fail\");\n\n        assert!(err.contains(\"modulus frequency\"));\n    }\n\n    #[test]\n    fn fft_dominant_frequency_fails_on_flat_embeddings() {\n        // Create constant embeddings (DC component only, no structure)\n        let modulus = 113;\n        let vocab_size = 114;\n        let embedding_dim = 128;\n\n        let embeddings = vec![vec![1.5; embedding_dim]; vocab_size];\n\n        let config = FFTDominantFrequencyConfig::default_for_modulus(modulus);\n        let err = verify_fft_dominant_frequencies_from_embeddings(&embeddings, &config)\n            .expect_err(\"expected flat embeddings to fail\");\n\n        assert!(err.contains(\"modulus frequency\") || err.contains(\"peak to median\"));\n    }\n\n    #[test]\n    fn fft_dominant_frequency_fails_on_weak_peaks() {\n        // Create embeddings with many frequencies at similar magnitudes (no clear peaks)\n        let modulus = 113;\n        let vocab_size = 114;\n        let embedding_dim = 128;\n        let tau = std::f64::consts::PI * 2.0;\n\n        let mut embeddings = Vec::with_capacity(vocab_size);\n        for token in 0..vocab_size {\n            let mut embedding = Vec::with_capacity(embedding_dim);\n            for dim in 0..embedding_dim {\n                // Mix many frequencies with similar amplitudes\n                let mut value = 0.0;\n                for freq in 1..10 {\n                    let angle =\n                        tau * freq as f64 * token as f64 / vocab_size as f64 + dim as f64 * 0.01;\n                    value += angle.sin() / 10.0;\n                }\n                embedding.push(value);\n            }\n            embeddings.push(embedding);\n        }\n\n        let config = FFTDominantFrequencyConfig {\n            modulus,\n            min_modulus_frequency_fraction: 0.15,\n            max_spectrum_entropy: 0.85,\n            min_peak_to_median_ratio: 2.0,\n        };\n        let err = verify_fft_dominant_frequencies_from_embeddings(&embeddings, &config)\n            .expect_err(\"expected weak peaks to fail\");\n\n        assert!(\n            err.contains(\"modulus frequency\") || err.contains(\"spectrum entropy\")\n                || err.contains(\"peak to median\")\n        );\n    }\n}\n","structuredPatch":[{"oldStart":702,"oldLines":6,"newStart":702,"newLines":8,"lines":[" }"," "," impl EmbeddingClockConfig {","+    /// NOTE: Reserved for future integration into analysis pipeline","+    #[allow(dead_code)]","     pub fn default_for_modulus(_modulus: usize) -> Self {","         Self {","             max_radius_std_fraction: 0.1,"]}],"userModified":false,"replaceAll":false}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01XijQXvZio26dwRPJmxcE5H","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_018kddipMQdKkLmwURqQv9Dp","name":"Edit","input":{"replace_all":false,"file_path":"/Users/admin/MechInt/src/analysis.rs","old_string":"pub fn project_embeddings_to_fourier_plane(\n    embeddings: &[Vec<f64>],\n    modulus: usize,\n) -> Result<Vec<(f64, f64)>, String> {","new_string":"/// NOTE: Reserved for future integration into analysis pipeline\n#[allow(dead_code)]\npub fn project_embeddings_to_fourier_plane(\n    embeddings: &[Vec<f64>],\n    modulus: usize,\n) -> Result<Vec<(f64, f64)>, String> {"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":1724,"cache_read_input_tokens":47044,"cache_creation":{"ephemeral_5m_input_tokens":1724,"ephemeral_1h_input_tokens":0},"output_tokens":247,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"ed097e33-9963-4de6-9c65-ff80d9b93d29"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_018kddipMQdKkLmwURqQv9Dp","type":"tool_result","content":"The file /Users/admin/MechInt/src/analysis.rs has been updated. Here's the result of running `cat -n` on a snippet of the edited file:\n   722â†’    pub mean_angle_error: f64,\n   723â†’    pub order_fraction: f64,\n   724â†’}\n   725â†’\n   726â†’/// NOTE: Reserved for future integration into analysis pipeline\n   727â†’#[allow(dead_code)]\n   728â†’pub fn project_embeddings_to_fourier_plane(\n   729â†’    embeddings: &[Vec<f64>],\n   730â†’    modulus: usize,\n   731â†’) -> Result<Vec<(f64, f64)>, String> {\n   732â†’    let p = modulus;\n   733â†’    if embeddings.len() < p {\n   734â†’        return Err(\"embedding matrix smaller than modulus\".to_string());\n   735â†’    }"}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"c5148519-7992-44ec-8c47-13bf1c1374d8","tool_use_result":{"filePath":"/Users/admin/MechInt/src/analysis.rs","oldString":"pub fn project_embeddings_to_fourier_plane(\n    embeddings: &[Vec<f64>],\n    modulus: usize,\n) -> Result<Vec<(f64, f64)>, String> {","newString":"/// NOTE: Reserved for future integration into analysis pipeline\n#[allow(dead_code)]\npub fn project_embeddings_to_fourier_plane(\n    embeddings: &[Vec<f64>],\n    modulus: usize,\n) -> Result<Vec<(f64, f64)>, String> {","originalFile":"use burn::{\n    data::dataset::Dataset,\n    nn::loss::CrossEntropyLossConfig,\n    tensor::{backend::Backend, Int, Tensor, TensorData},\n};\nuse rustfft::{num_complex::Complex, FftPlanner};\nuse serde::{Deserialize, Serialize};\nuse std::collections::{HashMap, HashSet};\n\nuse crate::{data::ModularAdditionDataset, model::Transformer};\n\n/// FFT analysis results for embeddings\n#[derive(Debug, Serialize, Deserialize)]\npub struct FFTAnalysis {\n    pub vocab_size: usize,\n    pub embedding_dim: usize,\n    pub dominant_frequencies: Vec<(usize, f64)>, // (token_id, dominant_frequency_index)\n    pub frequency_magnitudes: HashMap<usize, Vec<f64>>, // token_id -> magnitude spectrum\n}\n\n/// Analyze the learned token embeddings using FFT\npub fn analyze_embeddings_fft<B: Backend>(model: &Transformer<B>) -> FFTAnalysis {\n    println!(\"ðŸ” Performing FFT analysis on learned embeddings...\");\n\n    let vocab_size = ModularAdditionDataset::vocab_size();\n    let embedding_dim = 128;\n\n    // Extract token embeddings by forward passing through the embedding layer\n    let mut frequency_magnitudes = HashMap::new();\n    let mut dominant_frequencies = Vec::new();\n\n    for token_id in 0..vocab_size {\n        // Get the embedding for this token\n        let embedding = get_token_embedding(model, token_id);\n\n        // Compute FFT on the embedding vector\n        let fft_result = compute_fft(&embedding);\n\n        // Find dominant frequency\n        let dominant_freq_idx = find_dominant_frequency(&fft_result);\n        dominant_frequencies.push((token_id, dominant_freq_idx as f64));\n\n        // Store magnitude spectrum\n        frequency_magnitudes.insert(token_id, fft_result);\n\n        if token_id % 10 == 0 {\n            println!(\"  Token {:2}: dominant frequency = {}\", token_id, dominant_freq_idx);\n        }\n    }\n\n    println!(\"âœ… FFT analysis complete!\");\n    println!();\n    println!(\"ðŸ“Š Dominant Frequency Analysis:\");\n\n    // Analyze patterns\n    let mut freq_histogram: HashMap<usize, usize> = HashMap::new();\n    for (_, freq) in &dominant_frequencies {\n        *freq_histogram.entry(*freq as usize).or_insert(0) += 1;\n    }\n\n    let mut freq_counts: Vec<_> = freq_histogram.iter().collect();\n    freq_counts.sort_by(|a, b| b.1.cmp(a.1));\n\n    println!(\"  Most common dominant frequencies:\");\n    for (freq, count) in freq_counts.iter().take(5) {\n        println!(\"    Frequency {}: {} tokens\", freq, count);\n    }\n\n    // Check for modular structure (expected for mod p)\n    println!();\n    println!(\"ðŸ”¬ Checking for modular structure:\");\n    let modulus = ModularAdditionDataset::modulus();\n    if freq_histogram.contains_key(&modulus) {\n        println!(\n            \"  âœ“ Frequency {} (matching modulus) appears in embeddings\",\n            modulus\n        );\n    }\n\n    FFTAnalysis {\n        vocab_size,\n        embedding_dim,\n        dominant_frequencies,\n        frequency_magnitudes,\n    }\n}\n\n/// Extract the token embedding for a single token\nfn get_token_embedding<B: Backend>(model: &Transformer<B>, token_id: usize) -> Vec<f64> {\n    // Get the token embedding directly from the model\n    let embedding_tensor = model.get_token_embedding(token_id);\n\n    // Convert to vector - shape is [1, 1, embedding_dim], we want [embedding_dim]\n    let embedding_data: Vec<f32> = embedding_tensor.into_data().to_vec().unwrap();\n\n    // Convert to f64\n    embedding_data.iter().map(|&x| x as f64).collect()\n}\n\n/// Compute FFT on a vector of real values\nfn compute_fft(data: &[f64]) -> Vec<f64> {\n    let mut planner = FftPlanner::new();\n    let fft = planner.plan_fft_forward(data.len());\n\n    // Convert to complex numbers\n    let mut buffer: Vec<Complex<f64>> = data\n        .iter()\n        .map(|&x| Complex::new(x, 0.0))\n        .collect();\n\n    // Perform FFT\n    fft.process(&mut buffer);\n\n    // Compute magnitudes\n    buffer.iter().map(|c| c.norm()).collect()\n}\n\nfn inverse_fft(data: &mut [Complex<f64>]) {\n    let mut planner = FftPlanner::new();\n    let fft = planner.plan_fft_inverse(data.len());\n    fft.process(data);\n}\n\nfn top_k_frequency_indices(magnitudes: &[f64], k: usize) -> Vec<usize> {\n    let mut indexed: Vec<(usize, f64)> = magnitudes.iter().copied().enumerate().collect();\n    indexed.sort_by(|(idx_a, mag_a), (idx_b, mag_b)| {\n        mag_b\n            .partial_cmp(mag_a)\n            .unwrap_or(std::cmp::Ordering::Equal)\n            .then_with(|| idx_a.cmp(idx_b))\n    });\n\n    let keep = k.min(indexed.len());\n    indexed.iter().take(keep).map(|(idx, _)| *idx).collect()\n}\n\nfn restrict_signal_to_top_k_frequencies(signal: &[f64], top_k: usize) -> Vec<f64> {\n    if signal.is_empty() {\n        return Vec::new();\n    }\n\n    if top_k == 0 {\n        return vec![0.0; signal.len()];\n    }\n\n    let mut planner = FftPlanner::new();\n    let fft = planner.plan_fft_forward(signal.len());\n\n    let mut buffer: Vec<Complex<f64>> = signal\n        .iter()\n        .map(|&x| Complex::new(x, 0.0))\n        .collect();\n    fft.process(&mut buffer);\n\n    let magnitudes: Vec<f64> = buffer.iter().map(|c| c.norm()).collect();\n    let keep = top_k_frequency_indices(&magnitudes, top_k);\n    let keep_set: HashSet<usize> = keep.into_iter().collect();\n\n    for (idx, value) in buffer.iter_mut().enumerate() {\n        if !keep_set.contains(&idx) {\n            *value = Complex::new(0.0, 0.0);\n        }\n    }\n\n    inverse_fft(&mut buffer);\n\n    let scale = 1.0 / signal.len() as f64;\n    buffer.iter().map(|c| c.re * scale).collect()\n}\n\nfn exclude_top_k_frequencies(signal: &[f64], top_k: usize) -> Vec<f64> {\n    if signal.is_empty() {\n        return Vec::new();\n    }\n\n    if top_k == 0 {\n        return signal.to_vec();\n    }\n\n    if top_k >= signal.len() {\n        return vec![0.0; signal.len()];\n    }\n\n    let mut planner = FftPlanner::new();\n    let fft = planner.plan_fft_forward(signal.len());\n\n    let mut buffer: Vec<Complex<f64>> = signal\n        .iter()\n        .map(|&x| Complex::new(x, 0.0))\n        .collect();\n    fft.process(&mut buffer);\n\n    let magnitudes: Vec<f64> = buffer.iter().map(|c| c.norm()).collect();\n    let remove = top_k_frequency_indices(&magnitudes, top_k);\n    let remove_set: HashSet<usize> = remove.into_iter().collect();\n\n    for (idx, value) in buffer.iter_mut().enumerate() {\n        if remove_set.contains(&idx) {\n            *value = Complex::new(0.0, 0.0);\n        }\n    }\n\n    inverse_fft(&mut buffer);\n\n    let scale = 1.0 / signal.len() as f64;\n    buffer.iter().map(|c| c.re * scale).collect()\n}\n\nfn restricted_token_embedding_weights<B: Backend>(\n    model: &Transformer<B>,\n    top_k: usize,\n) -> Vec<f32> {\n    let weight = model.token_embedding_weights();\n    let [vocab_size, d_model] = weight.dims();\n    let weight_data: Vec<f32> = weight.into_data().to_vec().unwrap();\n    let mut restricted = vec![0.0f32; weight_data.len()];\n\n    for dim in 0..d_model {\n        let mut column = Vec::with_capacity(vocab_size);\n        for token in 0..vocab_size {\n            column.push(weight_data[token * d_model + dim] as f64);\n        }\n        let filtered = restrict_signal_to_top_k_frequencies(&column, top_k);\n        for token in 0..vocab_size {\n            restricted[token * d_model + dim] = filtered[token] as f32;\n        }\n    }\n\n    restricted\n}\n\nfn excluded_token_embedding_weights<B: Backend>(\n    model: &Transformer<B>,\n    top_k: usize,\n) -> Vec<f32> {\n    let weight = model.token_embedding_weights();\n    let [vocab_size, d_model] = weight.dims();\n    let weight_data: Vec<f32> = weight.into_data().to_vec().unwrap();\n    let mut excluded = vec![0.0f32; weight_data.len()];\n\n    for dim in 0..d_model {\n        let mut column = Vec::with_capacity(vocab_size);\n        for token in 0..vocab_size {\n            column.push(weight_data[token * d_model + dim] as f64);\n        }\n        let filtered = exclude_top_k_frequencies(&column, top_k);\n        for token in 0..vocab_size {\n            excluded[token * d_model + dim] = filtered[token] as f32;\n        }\n    }\n\n    excluded\n}\n\npub fn compute_restricted_loss<B: Backend>(\n    model: &Transformer<B>,\n    dataset: &ModularAdditionDataset,\n    device: &B::Device,\n    top_k: usize,\n    batch_size: usize,\n) -> Result<f64, String> {\n    if batch_size == 0 {\n        return Err(\"batch_size must be greater than 0\".to_string());\n    }\n\n    let restricted_weights = restricted_token_embedding_weights(model, top_k);\n    let [vocab_size, d_model] = model.token_embedding_weights().dims();\n    let token_weights = Tensor::<B, 2>::from_data(\n        TensorData::new(restricted_weights, [vocab_size, d_model]),\n        device,\n    );\n\n    let loss_fn = CrossEntropyLossConfig::new().init(device);\n    let mut total_loss = 0.0f64;\n    let mut total_batches = 0usize;\n\n    for batch_start in (0..dataset.len()).step_by(batch_size) {\n        let batch_end = (batch_start + batch_size).min(dataset.len());\n        let batch_len = batch_end - batch_start;\n        if batch_len == 0 {\n            break;\n        }\n\n        let mut inputs_vec = Vec::with_capacity(batch_len * 3);\n        let mut targets_vec = Vec::with_capacity(batch_len);\n\n        for idx in batch_start..batch_end {\n            let (input, target) = dataset.get(idx).ok_or_else(|| {\n                format!(\"missing dataset sample at index {}\", idx)\n            })?;\n            inputs_vec.extend(input.iter().map(|value| *value as i32));\n            targets_vec.push(target as i32);\n        }\n\n        let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n            .reshape([batch_len, 3]);\n        let targets = Tensor::<B, 1, Int>::from_ints(targets_vec.as_slice(), device);\n\n        let logits = model.forward_with_token_weights(inputs, token_weights.clone());\n        let loss = loss_fn.forward(logits, targets);\n        let loss_value: f64 = loss.into_data().to_vec().unwrap().get(0).copied().unwrap_or(0.0);\n        total_loss += loss_value;\n        total_batches += 1;\n    }\n\n    if total_batches == 0 {\n        return Err(\"no batches computed for restricted loss\".to_string());\n    }\n\n    Ok(total_loss / total_batches as f64)\n}\n\npub fn compute_excluded_loss<B: Backend>(\n    model: &Transformer<B>,\n    dataset: &ModularAdditionDataset,\n    device: &B::Device,\n    top_k: usize,\n    batch_size: usize,\n) -> Result<f64, String> {\n    if batch_size == 0 {\n        return Err(\"batch_size must be greater than 0\".to_string());\n    }\n\n    let excluded_weights = excluded_token_embedding_weights(model, top_k);\n    let [vocab_size, d_model] = model.token_embedding_weights().dims();\n    let token_weights = Tensor::<B, 2>::from_data(\n        TensorData::new(excluded_weights, [vocab_size, d_model]),\n        device,\n    );\n\n    let loss_fn = CrossEntropyLossConfig::new().init(device);\n    let mut total_loss = 0.0f64;\n    let mut total_batches = 0usize;\n\n    for batch_start in (0..dataset.len()).step_by(batch_size) {\n        let batch_end = (batch_start + batch_size).min(dataset.len());\n        let batch_len = batch_end - batch_start;\n        if batch_len == 0 {\n            break;\n        }\n\n        let mut inputs_vec = Vec::with_capacity(batch_len * 3);\n        let mut targets_vec = Vec::with_capacity(batch_len);\n\n        for idx in batch_start..batch_end {\n            let (input, target) = dataset.get(idx).ok_or_else(|| {\n                format!(\"missing dataset sample at index {}\", idx)\n            })?;\n            inputs_vec.extend(input.iter().map(|value| *value as i32));\n            targets_vec.push(target as i32);\n        }\n\n        let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n            .reshape([batch_len, 3]);\n        let targets = Tensor::<B, 1, Int>::from_ints(targets_vec.as_slice(), device);\n\n        let logits = model.forward_with_token_weights(inputs, token_weights.clone());\n        let loss = loss_fn.forward(logits, targets);\n        let loss_value: f64 = loss.into_data().to_vec().unwrap().get(0).copied().unwrap_or(0.0);\n        total_loss += loss_value;\n        total_batches += 1;\n    }\n\n    if total_batches == 0 {\n        return Err(\"no batches computed for excluded loss\".to_string());\n    }\n\n    Ok(total_loss / total_batches as f64)\n}\n\n/// Find the index of the dominant frequency (excluding DC component at index 0)\nfn find_dominant_frequency(magnitudes: &[f64]) -> usize {\n    magnitudes\n        .iter()\n        .enumerate()\n        .skip(1) // Skip DC component\n        .max_by(|(_, a), (_, b)| {\n            a.partial_cmp(b).unwrap_or(std::cmp::Ordering::Equal)\n        })\n        .map(|(idx, _)| idx)\n        .unwrap_or(0)\n}\n\n/// Save FFT analysis to JSON file\npub fn save_fft_analysis(analysis: &FFTAnalysis, filename: &str) -> std::io::Result<()> {\n    let json = serde_json::to_string_pretty(analysis)?;\n    std::fs::write(filename, json)?;\n    println!(\"ðŸ’¾ FFT analysis saved to: {}\", filename);\n    Ok(())\n}\n\n/// Track training and validation loss over time\n#[derive(Debug, Serialize, Deserialize)]\npub struct LossHistory {\n    pub train_snapshots: Vec<(usize, f64)>, // (step, loss)\n    pub val_snapshots: Vec<(usize, f64)>,   // (step, loss)\n}\n\n/// Track restricted loss over time (only top-k frequencies kept)\n#[derive(Debug, Serialize, Deserialize)]\npub struct RestrictedLossHistory {\n    pub snapshots: Vec<(usize, f64)>, // (step, loss)\n}\n\nimpl RestrictedLossHistory {\n    pub fn new() -> Self {\n        Self {\n            snapshots: Vec::new(),\n        }\n    }\n\n    pub fn add_snapshot(&mut self, step: usize, loss: f64) {\n        self.snapshots.push((step, loss));\n    }\n\n    pub fn save(&self, filename: &str) -> std::io::Result<()> {\n        let json = serde_json::to_string_pretty(self)?;\n        std::fs::write(filename, json)?;\n        println!(\"ðŸ’¾ Restricted loss history saved to: {}\", filename);\n        Ok(())\n    }\n}\n\n/// Track excluded loss over time (top-k frequencies removed)\n#[derive(Debug, Serialize, Deserialize)]\npub struct ExcludedLossHistory {\n    pub snapshots: Vec<(usize, f64)>, // (step, loss)\n}\n\nimpl ExcludedLossHistory {\n    pub fn new() -> Self {\n        Self {\n            snapshots: Vec::new(),\n        }\n    }\n\n    pub fn add_snapshot(&mut self, step: usize, loss: f64) {\n        self.snapshots.push((step, loss));\n    }\n\n    pub fn save(&self, filename: &str) -> std::io::Result<()> {\n        let json = serde_json::to_string_pretty(self)?;\n        std::fs::write(filename, json)?;\n        println!(\"ðŸ’¾ Excluded loss history saved to: {}\", filename);\n        Ok(())\n    }\n}\n\nimpl LossHistory {\n    pub fn new() -> Self {\n        Self {\n            train_snapshots: Vec::new(),\n            val_snapshots: Vec::new(),\n        }\n    }\n\n    pub fn add_snapshot(&mut self, step: usize, train_loss: f64, val_loss: f64) {\n        self.train_snapshots.push((step, train_loss));\n        self.val_snapshots.push((step, val_loss));\n    }\n\n    pub fn save(&self, filename: &str) -> std::io::Result<()> {\n        let json = serde_json::to_string_pretty(self)?;\n        std::fs::write(filename, json)?;\n        println!(\"ðŸ’¾ Loss history saved to: {}\", filename);\n        Ok(())\n    }\n}\n\n/// Legacy: Single loss tracking (kept for backward compatibility)\n#[derive(Debug, Serialize, Deserialize, Clone)]\npub struct WeightNormSnapshot {\n    pub step: usize,\n    pub total_norm: f64,\n}\n\n/// Legacy: Track weight norms over training (actually stores loss)\n#[derive(Debug, Serialize, Deserialize)]\npub struct WeightNormHistory {\n    pub snapshots: Vec<WeightNormSnapshot>,\n}\n\nimpl WeightNormHistory {\n    pub fn new() -> Self {\n        Self {\n            snapshots: Vec::new(),\n        }\n    }\n\n    pub fn add_snapshot(&mut self, step: usize, norm: f64) {\n        self.snapshots.push(WeightNormSnapshot {\n            step,\n            total_norm: norm,\n        });\n    }\n\n    pub fn save(&self, filename: &str) -> std::io::Result<()> {\n        let json = serde_json::to_string_pretty(self)?;\n        std::fs::write(filename, json)?;\n        println!(\"ðŸ’¾ Weight norm history saved to: {}\", filename);\n        Ok(())\n    }\n}\n\n/// Compute the L2 norm of all model parameters\n/// This returns the Frobenius norm (sum of squared weights) across all parameters\n/// Note: This is a simplified implementation that computes the norm of token embeddings,\n/// which contain the majority of the model parameters. For a complete implementation,\n/// we would need to access all Linear layer weights via the Record trait.\npub fn compute_model_weight_norm<B: Backend>(model: &Transformer<B>) -> f64 {\n    let mut total_norm_squared = 0.0f64;\n\n    // Token embedding weights: [vocab_size, d_model]\n    let token_emb_weights = model.token_embedding_weights();\n    let token_emb_data: Vec<f32> = token_emb_weights.into_data().to_vec().unwrap();\n    for &weight in &token_emb_data {\n        total_norm_squared += (weight as f64) * (weight as f64);\n    }\n\n    // Note: Position embeddings, attention weights, MLP weights, and LM head weights\n    // are not included in this simplified implementation. For weight decay verification,\n    // tracking the token embeddings (which are the largest weight matrices) is sufficient\n    // to observe the monotonic decrease under weight decay.\n\n    total_norm_squared.sqrt()\n}\n\n/// Extract all token embeddings as a matrix [vocab_size, embedding_dim]\npub fn extract_all_embeddings<B: Backend>(model: &Transformer<B>) -> Vec<Vec<f64>> {\n    let vocab_size = ModularAdditionDataset::vocab_size();\n    let mut embeddings = Vec::with_capacity(vocab_size);\n\n    for token_id in 0..vocab_size {\n        embeddings.push(get_token_embedding(model, token_id));\n    }\n\n    embeddings\n}\n\n/// Verify that FFT analysis of embeddings shows dominant frequencies\n/// that align with the modulus structure and nontrivial spectral patterns\n///\n/// NOTE: Reserved for manual verification or future integration into analysis pipeline\n#[allow(dead_code)]\npub fn verify_fft_dominant_frequencies<B: Backend>(\n    model: &Transformer<B>,\n    config: &FFTDominantFrequencyConfig,\n) -> Result<FFTDominantFrequencyReport, String> {\n    let embeddings = extract_all_embeddings(model);\n    verify_fft_dominant_frequencies_from_embeddings(&embeddings, config)\n}\n\n/// Verify FFT dominant frequencies from pre-extracted embeddings\n///\n/// NOTE: Reserved for manual verification or future integration into analysis pipeline\n#[allow(dead_code)]\npub fn verify_fft_dominant_frequencies_from_embeddings(\n    embeddings: &[Vec<f64>],\n    config: &FFTDominantFrequencyConfig,\n) -> Result<FFTDominantFrequencyReport, String> {\n    if embeddings.is_empty() {\n        return Err(\"no embeddings provided\".to_string());\n    }\n\n    let vocab_size = embeddings.len();\n    if vocab_size < config.modulus {\n        return Err(format!(\n            \"embedding count {} less than modulus {}\",\n            vocab_size, config.modulus\n        ));\n    }\n\n    // Analyze each embedding dimension and collect dominant frequencies\n    let mut frequency_histogram: HashMap<usize, usize> = HashMap::new();\n    let embedding_dim = embeddings[0].len();\n\n    // For each embedding dimension, compute FFT across all tokens and find dominant frequency\n    for dim in 0..embedding_dim {\n        let mut signal = Vec::with_capacity(vocab_size);\n        for embedding in embeddings.iter().take(vocab_size) {\n            if dim >= embedding.len() {\n                return Err(format!(\n                    \"embedding dimension mismatch at dim {}\",\n                    dim\n                ));\n            }\n            signal.push(embedding[dim]);\n        }\n\n        // Compute FFT and find dominant frequency\n        let magnitudes = compute_fft(&signal);\n        let dominant_freq = find_dominant_frequency(&magnitudes);\n        *frequency_histogram.entry(dominant_freq).or_insert(0) += 1;\n    }\n\n    // Check 1: Modulus frequency should appear significantly\n    let modulus_count = frequency_histogram.get(&config.modulus).copied().unwrap_or(0);\n    let modulus_frequency_fraction = modulus_count as f64 / embedding_dim as f64;\n\n    if modulus_frequency_fraction < config.min_modulus_frequency_fraction {\n        return Err(format!(\n            \"modulus frequency {} appears in only {:.3} of dimensions (threshold {:.3})\",\n            config.modulus, modulus_frequency_fraction, config.min_modulus_frequency_fraction\n        ));\n    }\n\n    // Check 2: Spectrum should NOT resemble white noise (low entropy)\n    // High entropy = uniform distribution (white noise), low entropy = structured peaks\n    let total_dims = embedding_dim as f64;\n    let mut entropy = 0.0;\n    for &count in frequency_histogram.values() {\n        if count > 0 {\n            let p = count as f64 / total_dims;\n            entropy -= p * p.log2();\n        }\n    }\n    // Normalize entropy by max possible (log2 of unique frequency count)\n    let max_entropy = (frequency_histogram.len() as f64).log2();\n    let normalized_entropy = if max_entropy > 0.0 {\n        entropy / max_entropy\n    } else {\n        0.0\n    };\n\n    if normalized_entropy > config.max_spectrum_entropy {\n        return Err(format!(\n            \"spectrum entropy {:.3} too high (white noise threshold {:.3})\",\n            normalized_entropy, config.max_spectrum_entropy\n        ));\n    }\n\n    // Check 3: Peak frequencies should be significantly stronger than median\n    let mut counts: Vec<usize> = frequency_histogram.values().copied().collect();\n    counts.sort_unstable();\n    let median_count = if counts.is_empty() {\n        0\n    } else {\n        counts[counts.len() / 2]\n    };\n    let peak_count = counts.last().copied().unwrap_or(0);\n    let peak_to_median_ratio = if median_count > 0 {\n        peak_count as f64 / median_count as f64\n    } else {\n        f64::INFINITY\n    };\n\n    if peak_to_median_ratio < config.min_peak_to_median_ratio {\n        return Err(format!(\n            \"peak to median ratio {:.3} below threshold {:.3}\",\n            peak_to_median_ratio, config.min_peak_to_median_ratio\n        ));\n    }\n\n    // Collect top frequencies for the report\n    let mut freq_counts: Vec<(usize, usize)> = frequency_histogram.into_iter().collect();\n    freq_counts.sort_by(|a, b| b.1.cmp(&a.1));\n    let top_frequencies: Vec<(usize, usize)> = freq_counts.into_iter().take(5).collect();\n\n    Ok(FFTDominantFrequencyReport {\n        modulus_frequency_fraction,\n        spectrum_entropy: normalized_entropy,\n        peak_to_median_ratio,\n        top_frequencies,\n    })\n}\n\n#[derive(Debug, Clone)]\npub struct FFTDominantFrequencyConfig {\n    pub modulus: usize,\n    pub min_modulus_frequency_fraction: f64,\n    pub max_spectrum_entropy: f64,\n    pub min_peak_to_median_ratio: f64,\n}\n\nimpl FFTDominantFrequencyConfig {\n    /// NOTE: Reserved for future integration into analysis pipeline\n    #[allow(dead_code)]\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        Self {\n            modulus,\n            min_modulus_frequency_fraction: 0.15,\n            max_spectrum_entropy: 0.85,\n            min_peak_to_median_ratio: 2.0,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct FFTDominantFrequencyReport {\n    pub modulus_frequency_fraction: f64,\n    pub spectrum_entropy: f64,\n    pub peak_to_median_ratio: f64,\n    pub top_frequencies: Vec<(usize, usize)>,\n}\n\n#[derive(Debug, Clone)]\npub struct EmbeddingClockConfig {\n    pub max_radius_std_fraction: f64,\n    pub min_angular_coverage: f64,\n    pub max_mean_angle_error: f64,\n    pub min_order_fraction: f64,\n}\n\nimpl EmbeddingClockConfig {\n    /// NOTE: Reserved for future integration into analysis pipeline\n    #[allow(dead_code)]\n    pub fn default_for_modulus(_modulus: usize) -> Self {\n        Self {\n            max_radius_std_fraction: 0.1,\n            min_angular_coverage: 0.9,\n            max_mean_angle_error: 0.35,\n            min_order_fraction: 0.9,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct EmbeddingClockReport {\n    pub mean_radius: f64,\n    pub radius_std: f64,\n    pub angular_coverage: f64,\n    pub mean_angle_error: f64,\n    pub order_fraction: f64,\n}\n\npub fn project_embeddings_to_fourier_plane(\n    embeddings: &[Vec<f64>],\n    modulus: usize,\n) -> Result<Vec<(f64, f64)>, String> {\n    let p = modulus;\n    if embeddings.len() < p {\n        return Err(\"embedding matrix smaller than modulus\".to_string());\n    }\n    if p == 0 {\n        return Err(\"modulus must be positive\".to_string());\n    }\n    let d = embeddings[0].len();\n    if d == 0 {\n        return Err(\"embedding dimension must be positive\".to_string());\n    }\n    if !embeddings[..p].iter().all(|row| row.len() == d) {\n        return Err(\"embedding rows have inconsistent dimension\".to_string());\n    }\n\n    let tau = std::f64::consts::PI * 2.0;\n    let mut basis_cos = vec![0.0f64; d];\n    let mut basis_sin = vec![0.0f64; d];\n    let scale = 2.0 / p as f64;\n\n    for (token, embedding) in embeddings[..p].iter().enumerate() {\n        let angle = tau * token as f64 / p as f64;\n        let cos_val = angle.cos();\n        let sin_val = angle.sin();\n        for dim in 0..d {\n            basis_cos[dim] += cos_val * embedding[dim] * scale;\n            basis_sin[dim] += sin_val * embedding[dim] * scale;\n        }\n    }\n\n    let basis_cos = normalize_vector(&basis_cos)?;\n    let basis_sin = normalize_vector(&basis_sin)?;\n\n    let mut coords = Vec::with_capacity(p);\n    for embedding in embeddings[..p].iter() {\n        let x = dot(embedding, &basis_cos);\n        let y = dot(embedding, &basis_sin);\n        coords.push((x, y));\n    }\n\n    Ok(coords)\n}\n\npub fn verify_embedding_clock_geometry(\n    embeddings: &[Vec<f64>],\n    modulus: usize,\n    config: &EmbeddingClockConfig,\n) -> Result<EmbeddingClockReport, String> {\n    let coords = project_embeddings_to_fourier_plane(embeddings, modulus)?;\n    let p = coords.len();\n    if p == 0 {\n        return Err(\"no embedding coordinates computed\".to_string());\n    }\n\n    let mut radii = Vec::with_capacity(p);\n    let mut angles = Vec::with_capacity(p);\n    for (x, y) in &coords {\n        let radius = (x * x + y * y).sqrt();\n        radii.push(radius);\n        angles.push(y.atan2(*x));\n    }\n\n    let mean_radius = radii.iter().sum::<f64>() / p as f64;\n    if mean_radius == 0.0 {\n        return Err(\"mean radius is zero\".to_string());\n    }\n    let radius_var = radii\n        .iter()\n        .map(|r| (r - mean_radius) * (r - mean_radius))\n        .sum::<f64>()\n        / p as f64;\n    let radius_std = radius_var.sqrt();\n\n    if radius_std / mean_radius > config.max_radius_std_fraction {\n        return Err(format!(\n            \"radius variance too large (std {:.4}, mean {:.4})\",\n            radius_std, mean_radius\n        ));\n    }\n\n    let tau = std::f64::consts::PI * 2.0;\n    let (mut sum_cos, mut sum_sin) = (0.0f64, 0.0f64);\n    for (token, angle) in angles.iter().enumerate() {\n        let expected = tau * token as f64 / p as f64;\n        let delta = angle - expected;\n        sum_cos += delta.cos();\n        sum_sin += delta.sin();\n    }\n    let offset = sum_sin.atan2(sum_cos);\n\n    let mut adjusted_angles = Vec::with_capacity(p);\n    let mut angle_errors = Vec::with_capacity(p);\n    for (token, angle) in angles.iter().enumerate() {\n        let expected = tau * token as f64 / p as f64;\n        let mut adjusted = angle - offset;\n        adjusted = wrap_angle(adjusted);\n        adjusted_angles.push(adjusted);\n        let diff = wrap_angle_signed(adjusted - expected);\n        angle_errors.push(diff.abs());\n    }\n\n    let mean_angle_error = angle_errors.iter().sum::<f64>() / p as f64;\n    if mean_angle_error > config.max_mean_angle_error {\n        return Err(format!(\n            \"mean angular error too large ({:.4} rad)\",\n            mean_angle_error\n        ));\n    }\n\n    let angular_coverage = angular_coverage(&adjusted_angles);\n    if angular_coverage < config.min_angular_coverage {\n        return Err(format!(\n            \"angular coverage too small ({:.4})\",\n            angular_coverage\n        ));\n    }\n\n    let order_fraction = order_fraction(&adjusted_angles, p);\n    if order_fraction < config.min_order_fraction {\n        return Err(format!(\n            \"token ordering too weak ({:.4})\",\n            order_fraction\n        ));\n    }\n\n    Ok(EmbeddingClockReport {\n        mean_radius,\n        radius_std,\n        angular_coverage,\n        mean_angle_error,\n        order_fraction,\n    })\n}\n\nfn normalize_vector(vec: &[f64]) -> Result<Vec<f64>, String> {\n    let norm = vec.iter().map(|v| v * v).sum::<f64>().sqrt();\n    if norm == 0.0 {\n        return Err(\"basis vector norm is zero\".to_string());\n    }\n    Ok(vec.iter().map(|v| v / norm).collect())\n}\n\nfn dot(a: &[f64], b: &[f64]) -> f64 {\n    a.iter().zip(b.iter()).map(|(x, y)| x * y).sum()\n}\n\nfn wrap_angle(angle: f64) -> f64 {\n    let tau = std::f64::consts::PI * 2.0;\n    let mut wrapped = angle % tau;\n    if wrapped < 0.0 {\n        wrapped += tau;\n    }\n    wrapped\n}\n\nfn wrap_angle_signed(angle: f64) -> f64 {\n    let tau = std::f64::consts::PI * 2.0;\n    let mut wrapped = (angle + std::f64::consts::PI) % tau;\n    if wrapped < 0.0 {\n        wrapped += tau;\n    }\n    wrapped - std::f64::consts::PI\n}\n\nfn angular_coverage(angles: &[f64]) -> f64 {\n    if angles.is_empty() {\n        return 0.0;\n    }\n    let tau = std::f64::consts::PI * 2.0;\n    let mut sorted = angles.to_vec();\n    sorted.sort_by(|a, b| a.partial_cmp(b).unwrap_or(std::cmp::Ordering::Equal));\n\n    let mut max_gap = 0.0;\n    for pair in sorted.windows(2) {\n        let gap = pair[1] - pair[0];\n        if gap > max_gap {\n            max_gap = gap;\n        }\n    }\n    let wrap_gap = tau - (sorted[sorted.len() - 1] - sorted[0]);\n    if wrap_gap > max_gap {\n        max_gap = wrap_gap;\n    }\n\n    (tau - max_gap) / tau\n}\n\nfn order_fraction(angles: &[f64], modulus: usize) -> f64 {\n    if angles.len() < 2 || modulus == 0 {\n        return 0.0;\n    }\n    let mut indexed: Vec<(usize, f64)> = angles.iter().copied().enumerate().collect();\n    indexed.sort_by(|a, b| a.1.partial_cmp(&b.1).unwrap_or(std::cmp::Ordering::Equal));\n\n    let mut good = 0usize;\n    for window in indexed.windows(2) {\n        let current = window[0].0;\n        let next = window[1].0;\n        if (next + modulus - current) % modulus == 1 {\n            good += 1;\n        }\n    }\n    let last = indexed[indexed.len() - 1].0;\n    let first = indexed[0].0;\n    if (first + modulus - last) % modulus == 1 {\n        good += 1;\n    }\n\n    good as f64 / modulus as f64\n}\n\n/// Track training and validation accuracy over time\n#[derive(Debug, Serialize, Deserialize)]\npub struct AccuracyHistory {\n    pub train_snapshots: Vec<(usize, f32)>, // (step, accuracy)\n    pub val_snapshots: Vec<(usize, f32)>,   // (step, accuracy)\n}\n\nimpl AccuracyHistory {\n    pub fn new() -> Self {\n        Self {\n            train_snapshots: Vec::new(),\n            val_snapshots: Vec::new(),\n        }\n    }\n\n    pub fn add_snapshot(&mut self, step: usize, train_acc: f32, val_acc: f32) {\n        self.train_snapshots.push((step, train_acc));\n        self.val_snapshots.push((step, val_acc));\n    }\n\n    pub fn save(&self, filename: &str) -> std::io::Result<()> {\n        let json = serde_json::to_string_pretty(self)?;\n        std::fs::write(filename, json)?;\n        println!(\"ðŸ’¾ Accuracy history saved to: {}\", filename);\n        Ok(())\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    fn make_circle_embeddings(modulus: usize) -> Vec<Vec<f64>> {\n        let tau = std::f64::consts::PI * 2.0;\n        (0..modulus)\n            .map(|token| {\n                let angle = tau * token as f64 / modulus as f64;\n                vec![angle.cos(), angle.sin(), 0.0, 0.0]\n            })\n            .collect()\n    }\n\n    #[test]\n    fn top_k_frequency_indices_stable_with_ties() {\n        let magnitudes = vec![0.1, 1.0, 1.0, 0.5];\n        let first = top_k_frequency_indices(&magnitudes, 2);\n        let second = top_k_frequency_indices(&magnitudes, 2);\n        assert_eq!(first, vec![1, 2]);\n        assert_eq!(first, second);\n    }\n\n    #[test]\n    fn restrict_signal_zeroes_when_top_k_zero() {\n        let signal = vec![1.0, -1.0, 0.5, -0.5];\n        let filtered = restrict_signal_to_top_k_frequencies(&signal, 0);\n        assert_eq!(filtered, vec![0.0, 0.0, 0.0, 0.0]);\n    }\n\n    #[test]\n    fn exclude_signal_no_change_when_top_k_zero() {\n        let signal = vec![1.0, -1.0, 0.5, -0.5];\n        let filtered = exclude_top_k_frequencies(&signal, 0);\n        assert_eq!(filtered, signal);\n    }\n\n    #[test]\n    fn exclude_signal_zeroes_when_top_k_all() {\n        let signal = vec![1.0, -1.0, 0.5, -0.5];\n        let filtered = exclude_top_k_frequencies(&signal, signal.len());\n        assert_eq!(filtered, vec![0.0, 0.0, 0.0, 0.0]);\n    }\n\n    #[test]\n    fn embedding_clock_geometry_passes_on_clean_circle() {\n        let modulus = 13;\n        let embeddings = make_circle_embeddings(modulus);\n        let config = EmbeddingClockConfig::default_for_modulus(modulus);\n        let report =\n            verify_embedding_clock_geometry(&embeddings, modulus, &config).expect(\"expected pass\");\n        assert!(report.angular_coverage > 0.9);\n        assert!(report.order_fraction > 0.9);\n        assert!(report.mean_angle_error < 0.1);\n    }\n\n    #[test]\n    fn embedding_clock_geometry_flags_wrong_frequency() {\n        let modulus = 11;\n        let tau = std::f64::consts::PI * 2.0;\n        let embeddings: Vec<Vec<f64>> = (0..modulus)\n            .map(|token| {\n                let angle = tau * token as f64 / modulus as f64;\n                vec![(2.0 * angle).cos(), (2.0 * angle).sin(), 0.0, 0.0]\n            })\n            .collect();\n        let config = EmbeddingClockConfig {\n            max_radius_std_fraction: 0.5,\n            min_angular_coverage: 0.1,\n            max_mean_angle_error: 0.2,\n            min_order_fraction: 0.1,\n        };\n        let err = verify_embedding_clock_geometry(&embeddings, modulus, &config)\n            .expect_err(\"expected ordering verification to fail\");\n        assert!(err.contains(\"mean angular error\"));\n    }\n\n    #[test]\n    fn fft_dominant_frequency_passes_on_modulus_structure() {\n        // Create embeddings with strong modulus-113 frequency component\n        // Mix in some other frequencies to get a realistic peak-to-median ratio\n        let modulus = 113;\n        let vocab_size = 114; // 0-112 + equals token\n        let embedding_dim = 128;\n        let tau = std::f64::consts::PI * 2.0;\n\n        let mut embeddings = Vec::with_capacity(vocab_size);\n        for token in 0..vocab_size {\n            let mut embedding = Vec::with_capacity(embedding_dim);\n            for dim in 0..embedding_dim {\n                // 60% of dimensions have modulus frequency, 40% have other frequencies\n                let freq = if dim % 5 < 3 {\n                    modulus\n                } else {\n                    // Use various other frequencies for the remaining dims\n                    20 + (dim % 20)\n                };\n                let angle = tau * freq as f64 * token as f64 / vocab_size as f64\n                    + dim as f64 * 0.1;\n                embedding.push(angle.sin());\n            }\n            embeddings.push(embedding);\n        }\n\n        let config = FFTDominantFrequencyConfig::default_for_modulus(modulus);\n        let report = verify_fft_dominant_frequencies_from_embeddings(&embeddings, &config)\n            .expect(\"expected modulus structure to pass\");\n\n        assert!(report.modulus_frequency_fraction >= config.min_modulus_frequency_fraction);\n        assert!(report.spectrum_entropy <= config.max_spectrum_entropy);\n        assert!(report.peak_to_median_ratio >= config.min_peak_to_median_ratio);\n        assert!(report.top_frequencies.iter().any(|(freq, _)| *freq == modulus));\n    }\n\n    #[test]\n    fn fft_dominant_frequency_fails_on_white_noise() {\n        // Create random embeddings (white noise spectrum)\n        let modulus = 113;\n        let vocab_size = 114;\n        let embedding_dim = 128;\n\n        let mut embeddings = Vec::with_capacity(vocab_size);\n        for token in 0..vocab_size {\n            let mut embedding = Vec::with_capacity(embedding_dim);\n            for dim in 0..embedding_dim {\n                // Pseudo-random values\n                let value = ((token * 17 + dim * 31) % 100) as f64 / 50.0 - 1.0;\n                embedding.push(value);\n            }\n            embeddings.push(embedding);\n        }\n\n        let config = FFTDominantFrequencyConfig::default_for_modulus(modulus);\n        let err = verify_fft_dominant_frequencies_from_embeddings(&embeddings, &config)\n            .expect_err(\"expected white noise to fail\");\n\n        // Should fail on either modulus frequency fraction or spectrum entropy\n        assert!(\n            err.contains(\"modulus frequency\") || err.contains(\"spectrum entropy\")\n                || err.contains(\"peak to median\")\n        );\n    }\n\n    #[test]\n    fn fft_dominant_frequency_fails_on_wrong_frequency() {\n        // Create embeddings with dominant frequency at 57 (not 113)\n        let modulus = 113;\n        let vocab_size = 114;\n        let embedding_dim = 128;\n        let tau = std::f64::consts::PI * 2.0;\n        let wrong_frequency = 57;\n\n        let mut embeddings = Vec::with_capacity(vocab_size);\n        for token in 0..vocab_size {\n            let mut embedding = Vec::with_capacity(embedding_dim);\n            for dim in 0..embedding_dim {\n                // Each dimension has a sine wave with wrong frequency\n                let angle = tau * wrong_frequency as f64 * token as f64 / vocab_size as f64\n                    + dim as f64 * 0.1;\n                embedding.push(angle.sin());\n            }\n            embeddings.push(embedding);\n        }\n\n        let config = FFTDominantFrequencyConfig::default_for_modulus(modulus);\n        let err = verify_fft_dominant_frequencies_from_embeddings(&embeddings, &config)\n            .expect_err(\"expected wrong frequency to fail\");\n\n        assert!(err.contains(\"modulus frequency\"));\n    }\n\n    #[test]\n    fn fft_dominant_frequency_fails_on_flat_embeddings() {\n        // Create constant embeddings (DC component only, no structure)\n        let modulus = 113;\n        let vocab_size = 114;\n        let embedding_dim = 128;\n\n        let embeddings = vec![vec![1.5; embedding_dim]; vocab_size];\n\n        let config = FFTDominantFrequencyConfig::default_for_modulus(modulus);\n        let err = verify_fft_dominant_frequencies_from_embeddings(&embeddings, &config)\n            .expect_err(\"expected flat embeddings to fail\");\n\n        assert!(err.contains(\"modulus frequency\") || err.contains(\"peak to median\"));\n    }\n\n    #[test]\n    fn fft_dominant_frequency_fails_on_weak_peaks() {\n        // Create embeddings with many frequencies at similar magnitudes (no clear peaks)\n        let modulus = 113;\n        let vocab_size = 114;\n        let embedding_dim = 128;\n        let tau = std::f64::consts::PI * 2.0;\n\n        let mut embeddings = Vec::with_capacity(vocab_size);\n        for token in 0..vocab_size {\n            let mut embedding = Vec::with_capacity(embedding_dim);\n            for dim in 0..embedding_dim {\n                // Mix many frequencies with similar amplitudes\n                let mut value = 0.0;\n                for freq in 1..10 {\n                    let angle =\n                        tau * freq as f64 * token as f64 / vocab_size as f64 + dim as f64 * 0.01;\n                    value += angle.sin() / 10.0;\n                }\n                embedding.push(value);\n            }\n            embeddings.push(embedding);\n        }\n\n        let config = FFTDominantFrequencyConfig {\n            modulus,\n            min_modulus_frequency_fraction: 0.15,\n            max_spectrum_entropy: 0.85,\n            min_peak_to_median_ratio: 2.0,\n        };\n        let err = verify_fft_dominant_frequencies_from_embeddings(&embeddings, &config)\n            .expect_err(\"expected weak peaks to fail\");\n\n        assert!(\n            err.contains(\"modulus frequency\") || err.contains(\"spectrum entropy\")\n                || err.contains(\"peak to median\")\n        );\n    }\n}\n","structuredPatch":[{"oldStart":723,"oldLines":6,"newStart":723,"newLines":8,"lines":["     pub order_fraction: f64,"," }"," ","+/// NOTE: Reserved for future integration into analysis pipeline","+#[allow(dead_code)]"," pub fn project_embeddings_to_fourier_plane(","     embeddings: &[Vec<f64>],","     modulus: usize,"]}],"userModified":false,"replaceAll":false}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01CvofvUG59ueQaEkp86SUzb","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_0143zo5NRbnZi63nSKNS5zKT","name":"Edit","input":{"replace_all":false,"file_path":"/Users/admin/MechInt/src/analysis.rs","old_string":"pub fn verify_embedding_clock_geometry(\n    embeddings: &[Vec<f64>],\n    modulus: usize,\n    config: &EmbeddingClockConfig,\n) -> Result<EmbeddingClockReport, String> {","new_string":"/// NOTE: Reserved for future integration into analysis pipeline\n#[allow(dead_code)]\npub fn verify_embedding_clock_geometry(\n    embeddings: &[Vec<f64>],\n    modulus: usize,\n    config: &EmbeddingClockConfig,\n) -> Result<EmbeddingClockReport, String> {"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":1571,"cache_read_input_tokens":48768,"cache_creation":{"ephemeral_5m_input_tokens":1571,"ephemeral_1h_input_tokens":0},"output_tokens":261,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"c214d42c-10c2-4d8e-8a8f-0cd122c6fe20"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_0143zo5NRbnZi63nSKNS5zKT","type":"tool_result","content":"The file /Users/admin/MechInt/src/analysis.rs has been updated. Here's the result of running `cat -n` on a snippet of the edited file:\n   771â†’\n   772â†’    Ok(coords)\n   773â†’}\n   774â†’\n   775â†’/// NOTE: Reserved for future integration into analysis pipeline\n   776â†’#[allow(dead_code)]\n   777â†’pub fn verify_embedding_clock_geometry(\n   778â†’    embeddings: &[Vec<f64>],\n   779â†’    modulus: usize,\n   780â†’    config: &EmbeddingClockConfig,\n   781â†’) -> Result<EmbeddingClockReport, String> {\n   782â†’    let coords = project_embeddings_to_fourier_plane(embeddings, modulus)?;\n   783â†’    let p = coords.len();\n   784â†’    if p == 0 {\n   785â†’        return Err(\"no embedding coordinates computed\".to_string());"}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"9bb96614-c933-4372-b2d3-8b535ad88176","tool_use_result":{"filePath":"/Users/admin/MechInt/src/analysis.rs","oldString":"pub fn verify_embedding_clock_geometry(\n    embeddings: &[Vec<f64>],\n    modulus: usize,\n    config: &EmbeddingClockConfig,\n) -> Result<EmbeddingClockReport, String> {","newString":"/// NOTE: Reserved for future integration into analysis pipeline\n#[allow(dead_code)]\npub fn verify_embedding_clock_geometry(\n    embeddings: &[Vec<f64>],\n    modulus: usize,\n    config: &EmbeddingClockConfig,\n) -> Result<EmbeddingClockReport, String> {","originalFile":"use burn::{\n    data::dataset::Dataset,\n    nn::loss::CrossEntropyLossConfig,\n    tensor::{backend::Backend, Int, Tensor, TensorData},\n};\nuse rustfft::{num_complex::Complex, FftPlanner};\nuse serde::{Deserialize, Serialize};\nuse std::collections::{HashMap, HashSet};\n\nuse crate::{data::ModularAdditionDataset, model::Transformer};\n\n/// FFT analysis results for embeddings\n#[derive(Debug, Serialize, Deserialize)]\npub struct FFTAnalysis {\n    pub vocab_size: usize,\n    pub embedding_dim: usize,\n    pub dominant_frequencies: Vec<(usize, f64)>, // (token_id, dominant_frequency_index)\n    pub frequency_magnitudes: HashMap<usize, Vec<f64>>, // token_id -> magnitude spectrum\n}\n\n/// Analyze the learned token embeddings using FFT\npub fn analyze_embeddings_fft<B: Backend>(model: &Transformer<B>) -> FFTAnalysis {\n    println!(\"ðŸ” Performing FFT analysis on learned embeddings...\");\n\n    let vocab_size = ModularAdditionDataset::vocab_size();\n    let embedding_dim = 128;\n\n    // Extract token embeddings by forward passing through the embedding layer\n    let mut frequency_magnitudes = HashMap::new();\n    let mut dominant_frequencies = Vec::new();\n\n    for token_id in 0..vocab_size {\n        // Get the embedding for this token\n        let embedding = get_token_embedding(model, token_id);\n\n        // Compute FFT on the embedding vector\n        let fft_result = compute_fft(&embedding);\n\n        // Find dominant frequency\n        let dominant_freq_idx = find_dominant_frequency(&fft_result);\n        dominant_frequencies.push((token_id, dominant_freq_idx as f64));\n\n        // Store magnitude spectrum\n        frequency_magnitudes.insert(token_id, fft_result);\n\n        if token_id % 10 == 0 {\n            println!(\"  Token {:2}: dominant frequency = {}\", token_id, dominant_freq_idx);\n        }\n    }\n\n    println!(\"âœ… FFT analysis complete!\");\n    println!();\n    println!(\"ðŸ“Š Dominant Frequency Analysis:\");\n\n    // Analyze patterns\n    let mut freq_histogram: HashMap<usize, usize> = HashMap::new();\n    for (_, freq) in &dominant_frequencies {\n        *freq_histogram.entry(*freq as usize).or_insert(0) += 1;\n    }\n\n    let mut freq_counts: Vec<_> = freq_histogram.iter().collect();\n    freq_counts.sort_by(|a, b| b.1.cmp(a.1));\n\n    println!(\"  Most common dominant frequencies:\");\n    for (freq, count) in freq_counts.iter().take(5) {\n        println!(\"    Frequency {}: {} tokens\", freq, count);\n    }\n\n    // Check for modular structure (expected for mod p)\n    println!();\n    println!(\"ðŸ”¬ Checking for modular structure:\");\n    let modulus = ModularAdditionDataset::modulus();\n    if freq_histogram.contains_key(&modulus) {\n        println!(\n            \"  âœ“ Frequency {} (matching modulus) appears in embeddings\",\n            modulus\n        );\n    }\n\n    FFTAnalysis {\n        vocab_size,\n        embedding_dim,\n        dominant_frequencies,\n        frequency_magnitudes,\n    }\n}\n\n/// Extract the token embedding for a single token\nfn get_token_embedding<B: Backend>(model: &Transformer<B>, token_id: usize) -> Vec<f64> {\n    // Get the token embedding directly from the model\n    let embedding_tensor = model.get_token_embedding(token_id);\n\n    // Convert to vector - shape is [1, 1, embedding_dim], we want [embedding_dim]\n    let embedding_data: Vec<f32> = embedding_tensor.into_data().to_vec().unwrap();\n\n    // Convert to f64\n    embedding_data.iter().map(|&x| x as f64).collect()\n}\n\n/// Compute FFT on a vector of real values\nfn compute_fft(data: &[f64]) -> Vec<f64> {\n    let mut planner = FftPlanner::new();\n    let fft = planner.plan_fft_forward(data.len());\n\n    // Convert to complex numbers\n    let mut buffer: Vec<Complex<f64>> = data\n        .iter()\n        .map(|&x| Complex::new(x, 0.0))\n        .collect();\n\n    // Perform FFT\n    fft.process(&mut buffer);\n\n    // Compute magnitudes\n    buffer.iter().map(|c| c.norm()).collect()\n}\n\nfn inverse_fft(data: &mut [Complex<f64>]) {\n    let mut planner = FftPlanner::new();\n    let fft = planner.plan_fft_inverse(data.len());\n    fft.process(data);\n}\n\nfn top_k_frequency_indices(magnitudes: &[f64], k: usize) -> Vec<usize> {\n    let mut indexed: Vec<(usize, f64)> = magnitudes.iter().copied().enumerate().collect();\n    indexed.sort_by(|(idx_a, mag_a), (idx_b, mag_b)| {\n        mag_b\n            .partial_cmp(mag_a)\n            .unwrap_or(std::cmp::Ordering::Equal)\n            .then_with(|| idx_a.cmp(idx_b))\n    });\n\n    let keep = k.min(indexed.len());\n    indexed.iter().take(keep).map(|(idx, _)| *idx).collect()\n}\n\nfn restrict_signal_to_top_k_frequencies(signal: &[f64], top_k: usize) -> Vec<f64> {\n    if signal.is_empty() {\n        return Vec::new();\n    }\n\n    if top_k == 0 {\n        return vec![0.0; signal.len()];\n    }\n\n    let mut planner = FftPlanner::new();\n    let fft = planner.plan_fft_forward(signal.len());\n\n    let mut buffer: Vec<Complex<f64>> = signal\n        .iter()\n        .map(|&x| Complex::new(x, 0.0))\n        .collect();\n    fft.process(&mut buffer);\n\n    let magnitudes: Vec<f64> = buffer.iter().map(|c| c.norm()).collect();\n    let keep = top_k_frequency_indices(&magnitudes, top_k);\n    let keep_set: HashSet<usize> = keep.into_iter().collect();\n\n    for (idx, value) in buffer.iter_mut().enumerate() {\n        if !keep_set.contains(&idx) {\n            *value = Complex::new(0.0, 0.0);\n        }\n    }\n\n    inverse_fft(&mut buffer);\n\n    let scale = 1.0 / signal.len() as f64;\n    buffer.iter().map(|c| c.re * scale).collect()\n}\n\nfn exclude_top_k_frequencies(signal: &[f64], top_k: usize) -> Vec<f64> {\n    if signal.is_empty() {\n        return Vec::new();\n    }\n\n    if top_k == 0 {\n        return signal.to_vec();\n    }\n\n    if top_k >= signal.len() {\n        return vec![0.0; signal.len()];\n    }\n\n    let mut planner = FftPlanner::new();\n    let fft = planner.plan_fft_forward(signal.len());\n\n    let mut buffer: Vec<Complex<f64>> = signal\n        .iter()\n        .map(|&x| Complex::new(x, 0.0))\n        .collect();\n    fft.process(&mut buffer);\n\n    let magnitudes: Vec<f64> = buffer.iter().map(|c| c.norm()).collect();\n    let remove = top_k_frequency_indices(&magnitudes, top_k);\n    let remove_set: HashSet<usize> = remove.into_iter().collect();\n\n    for (idx, value) in buffer.iter_mut().enumerate() {\n        if remove_set.contains(&idx) {\n            *value = Complex::new(0.0, 0.0);\n        }\n    }\n\n    inverse_fft(&mut buffer);\n\n    let scale = 1.0 / signal.len() as f64;\n    buffer.iter().map(|c| c.re * scale).collect()\n}\n\nfn restricted_token_embedding_weights<B: Backend>(\n    model: &Transformer<B>,\n    top_k: usize,\n) -> Vec<f32> {\n    let weight = model.token_embedding_weights();\n    let [vocab_size, d_model] = weight.dims();\n    let weight_data: Vec<f32> = weight.into_data().to_vec().unwrap();\n    let mut restricted = vec![0.0f32; weight_data.len()];\n\n    for dim in 0..d_model {\n        let mut column = Vec::with_capacity(vocab_size);\n        for token in 0..vocab_size {\n            column.push(weight_data[token * d_model + dim] as f64);\n        }\n        let filtered = restrict_signal_to_top_k_frequencies(&column, top_k);\n        for token in 0..vocab_size {\n            restricted[token * d_model + dim] = filtered[token] as f32;\n        }\n    }\n\n    restricted\n}\n\nfn excluded_token_embedding_weights<B: Backend>(\n    model: &Transformer<B>,\n    top_k: usize,\n) -> Vec<f32> {\n    let weight = model.token_embedding_weights();\n    let [vocab_size, d_model] = weight.dims();\n    let weight_data: Vec<f32> = weight.into_data().to_vec().unwrap();\n    let mut excluded = vec![0.0f32; weight_data.len()];\n\n    for dim in 0..d_model {\n        let mut column = Vec::with_capacity(vocab_size);\n        for token in 0..vocab_size {\n            column.push(weight_data[token * d_model + dim] as f64);\n        }\n        let filtered = exclude_top_k_frequencies(&column, top_k);\n        for token in 0..vocab_size {\n            excluded[token * d_model + dim] = filtered[token] as f32;\n        }\n    }\n\n    excluded\n}\n\npub fn compute_restricted_loss<B: Backend>(\n    model: &Transformer<B>,\n    dataset: &ModularAdditionDataset,\n    device: &B::Device,\n    top_k: usize,\n    batch_size: usize,\n) -> Result<f64, String> {\n    if batch_size == 0 {\n        return Err(\"batch_size must be greater than 0\".to_string());\n    }\n\n    let restricted_weights = restricted_token_embedding_weights(model, top_k);\n    let [vocab_size, d_model] = model.token_embedding_weights().dims();\n    let token_weights = Tensor::<B, 2>::from_data(\n        TensorData::new(restricted_weights, [vocab_size, d_model]),\n        device,\n    );\n\n    let loss_fn = CrossEntropyLossConfig::new().init(device);\n    let mut total_loss = 0.0f64;\n    let mut total_batches = 0usize;\n\n    for batch_start in (0..dataset.len()).step_by(batch_size) {\n        let batch_end = (batch_start + batch_size).min(dataset.len());\n        let batch_len = batch_end - batch_start;\n        if batch_len == 0 {\n            break;\n        }\n\n        let mut inputs_vec = Vec::with_capacity(batch_len * 3);\n        let mut targets_vec = Vec::with_capacity(batch_len);\n\n        for idx in batch_start..batch_end {\n            let (input, target) = dataset.get(idx).ok_or_else(|| {\n                format!(\"missing dataset sample at index {}\", idx)\n            })?;\n            inputs_vec.extend(input.iter().map(|value| *value as i32));\n            targets_vec.push(target as i32);\n        }\n\n        let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n            .reshape([batch_len, 3]);\n        let targets = Tensor::<B, 1, Int>::from_ints(targets_vec.as_slice(), device);\n\n        let logits = model.forward_with_token_weights(inputs, token_weights.clone());\n        let loss = loss_fn.forward(logits, targets);\n        let loss_value: f64 = loss.into_data().to_vec().unwrap().get(0).copied().unwrap_or(0.0);\n        total_loss += loss_value;\n        total_batches += 1;\n    }\n\n    if total_batches == 0 {\n        return Err(\"no batches computed for restricted loss\".to_string());\n    }\n\n    Ok(total_loss / total_batches as f64)\n}\n\npub fn compute_excluded_loss<B: Backend>(\n    model: &Transformer<B>,\n    dataset: &ModularAdditionDataset,\n    device: &B::Device,\n    top_k: usize,\n    batch_size: usize,\n) -> Result<f64, String> {\n    if batch_size == 0 {\n        return Err(\"batch_size must be greater than 0\".to_string());\n    }\n\n    let excluded_weights = excluded_token_embedding_weights(model, top_k);\n    let [vocab_size, d_model] = model.token_embedding_weights().dims();\n    let token_weights = Tensor::<B, 2>::from_data(\n        TensorData::new(excluded_weights, [vocab_size, d_model]),\n        device,\n    );\n\n    let loss_fn = CrossEntropyLossConfig::new().init(device);\n    let mut total_loss = 0.0f64;\n    let mut total_batches = 0usize;\n\n    for batch_start in (0..dataset.len()).step_by(batch_size) {\n        let batch_end = (batch_start + batch_size).min(dataset.len());\n        let batch_len = batch_end - batch_start;\n        if batch_len == 0 {\n            break;\n        }\n\n        let mut inputs_vec = Vec::with_capacity(batch_len * 3);\n        let mut targets_vec = Vec::with_capacity(batch_len);\n\n        for idx in batch_start..batch_end {\n            let (input, target) = dataset.get(idx).ok_or_else(|| {\n                format!(\"missing dataset sample at index {}\", idx)\n            })?;\n            inputs_vec.extend(input.iter().map(|value| *value as i32));\n            targets_vec.push(target as i32);\n        }\n\n        let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n            .reshape([batch_len, 3]);\n        let targets = Tensor::<B, 1, Int>::from_ints(targets_vec.as_slice(), device);\n\n        let logits = model.forward_with_token_weights(inputs, token_weights.clone());\n        let loss = loss_fn.forward(logits, targets);\n        let loss_value: f64 = loss.into_data().to_vec().unwrap().get(0).copied().unwrap_or(0.0);\n        total_loss += loss_value;\n        total_batches += 1;\n    }\n\n    if total_batches == 0 {\n        return Err(\"no batches computed for excluded loss\".to_string());\n    }\n\n    Ok(total_loss / total_batches as f64)\n}\n\n/// Find the index of the dominant frequency (excluding DC component at index 0)\nfn find_dominant_frequency(magnitudes: &[f64]) -> usize {\n    magnitudes\n        .iter()\n        .enumerate()\n        .skip(1) // Skip DC component\n        .max_by(|(_, a), (_, b)| {\n            a.partial_cmp(b).unwrap_or(std::cmp::Ordering::Equal)\n        })\n        .map(|(idx, _)| idx)\n        .unwrap_or(0)\n}\n\n/// Save FFT analysis to JSON file\npub fn save_fft_analysis(analysis: &FFTAnalysis, filename: &str) -> std::io::Result<()> {\n    let json = serde_json::to_string_pretty(analysis)?;\n    std::fs::write(filename, json)?;\n    println!(\"ðŸ’¾ FFT analysis saved to: {}\", filename);\n    Ok(())\n}\n\n/// Track training and validation loss over time\n#[derive(Debug, Serialize, Deserialize)]\npub struct LossHistory {\n    pub train_snapshots: Vec<(usize, f64)>, // (step, loss)\n    pub val_snapshots: Vec<(usize, f64)>,   // (step, loss)\n}\n\n/// Track restricted loss over time (only top-k frequencies kept)\n#[derive(Debug, Serialize, Deserialize)]\npub struct RestrictedLossHistory {\n    pub snapshots: Vec<(usize, f64)>, // (step, loss)\n}\n\nimpl RestrictedLossHistory {\n    pub fn new() -> Self {\n        Self {\n            snapshots: Vec::new(),\n        }\n    }\n\n    pub fn add_snapshot(&mut self, step: usize, loss: f64) {\n        self.snapshots.push((step, loss));\n    }\n\n    pub fn save(&self, filename: &str) -> std::io::Result<()> {\n        let json = serde_json::to_string_pretty(self)?;\n        std::fs::write(filename, json)?;\n        println!(\"ðŸ’¾ Restricted loss history saved to: {}\", filename);\n        Ok(())\n    }\n}\n\n/// Track excluded loss over time (top-k frequencies removed)\n#[derive(Debug, Serialize, Deserialize)]\npub struct ExcludedLossHistory {\n    pub snapshots: Vec<(usize, f64)>, // (step, loss)\n}\n\nimpl ExcludedLossHistory {\n    pub fn new() -> Self {\n        Self {\n            snapshots: Vec::new(),\n        }\n    }\n\n    pub fn add_snapshot(&mut self, step: usize, loss: f64) {\n        self.snapshots.push((step, loss));\n    }\n\n    pub fn save(&self, filename: &str) -> std::io::Result<()> {\n        let json = serde_json::to_string_pretty(self)?;\n        std::fs::write(filename, json)?;\n        println!(\"ðŸ’¾ Excluded loss history saved to: {}\", filename);\n        Ok(())\n    }\n}\n\nimpl LossHistory {\n    pub fn new() -> Self {\n        Self {\n            train_snapshots: Vec::new(),\n            val_snapshots: Vec::new(),\n        }\n    }\n\n    pub fn add_snapshot(&mut self, step: usize, train_loss: f64, val_loss: f64) {\n        self.train_snapshots.push((step, train_loss));\n        self.val_snapshots.push((step, val_loss));\n    }\n\n    pub fn save(&self, filename: &str) -> std::io::Result<()> {\n        let json = serde_json::to_string_pretty(self)?;\n        std::fs::write(filename, json)?;\n        println!(\"ðŸ’¾ Loss history saved to: {}\", filename);\n        Ok(())\n    }\n}\n\n/// Legacy: Single loss tracking (kept for backward compatibility)\n#[derive(Debug, Serialize, Deserialize, Clone)]\npub struct WeightNormSnapshot {\n    pub step: usize,\n    pub total_norm: f64,\n}\n\n/// Legacy: Track weight norms over training (actually stores loss)\n#[derive(Debug, Serialize, Deserialize)]\npub struct WeightNormHistory {\n    pub snapshots: Vec<WeightNormSnapshot>,\n}\n\nimpl WeightNormHistory {\n    pub fn new() -> Self {\n        Self {\n            snapshots: Vec::new(),\n        }\n    }\n\n    pub fn add_snapshot(&mut self, step: usize, norm: f64) {\n        self.snapshots.push(WeightNormSnapshot {\n            step,\n            total_norm: norm,\n        });\n    }\n\n    pub fn save(&self, filename: &str) -> std::io::Result<()> {\n        let json = serde_json::to_string_pretty(self)?;\n        std::fs::write(filename, json)?;\n        println!(\"ðŸ’¾ Weight norm history saved to: {}\", filename);\n        Ok(())\n    }\n}\n\n/// Compute the L2 norm of all model parameters\n/// This returns the Frobenius norm (sum of squared weights) across all parameters\n/// Note: This is a simplified implementation that computes the norm of token embeddings,\n/// which contain the majority of the model parameters. For a complete implementation,\n/// we would need to access all Linear layer weights via the Record trait.\npub fn compute_model_weight_norm<B: Backend>(model: &Transformer<B>) -> f64 {\n    let mut total_norm_squared = 0.0f64;\n\n    // Token embedding weights: [vocab_size, d_model]\n    let token_emb_weights = model.token_embedding_weights();\n    let token_emb_data: Vec<f32> = token_emb_weights.into_data().to_vec().unwrap();\n    for &weight in &token_emb_data {\n        total_norm_squared += (weight as f64) * (weight as f64);\n    }\n\n    // Note: Position embeddings, attention weights, MLP weights, and LM head weights\n    // are not included in this simplified implementation. For weight decay verification,\n    // tracking the token embeddings (which are the largest weight matrices) is sufficient\n    // to observe the monotonic decrease under weight decay.\n\n    total_norm_squared.sqrt()\n}\n\n/// Extract all token embeddings as a matrix [vocab_size, embedding_dim]\npub fn extract_all_embeddings<B: Backend>(model: &Transformer<B>) -> Vec<Vec<f64>> {\n    let vocab_size = ModularAdditionDataset::vocab_size();\n    let mut embeddings = Vec::with_capacity(vocab_size);\n\n    for token_id in 0..vocab_size {\n        embeddings.push(get_token_embedding(model, token_id));\n    }\n\n    embeddings\n}\n\n/// Verify that FFT analysis of embeddings shows dominant frequencies\n/// that align with the modulus structure and nontrivial spectral patterns\n///\n/// NOTE: Reserved for manual verification or future integration into analysis pipeline\n#[allow(dead_code)]\npub fn verify_fft_dominant_frequencies<B: Backend>(\n    model: &Transformer<B>,\n    config: &FFTDominantFrequencyConfig,\n) -> Result<FFTDominantFrequencyReport, String> {\n    let embeddings = extract_all_embeddings(model);\n    verify_fft_dominant_frequencies_from_embeddings(&embeddings, config)\n}\n\n/// Verify FFT dominant frequencies from pre-extracted embeddings\n///\n/// NOTE: Reserved for manual verification or future integration into analysis pipeline\n#[allow(dead_code)]\npub fn verify_fft_dominant_frequencies_from_embeddings(\n    embeddings: &[Vec<f64>],\n    config: &FFTDominantFrequencyConfig,\n) -> Result<FFTDominantFrequencyReport, String> {\n    if embeddings.is_empty() {\n        return Err(\"no embeddings provided\".to_string());\n    }\n\n    let vocab_size = embeddings.len();\n    if vocab_size < config.modulus {\n        return Err(format!(\n            \"embedding count {} less than modulus {}\",\n            vocab_size, config.modulus\n        ));\n    }\n\n    // Analyze each embedding dimension and collect dominant frequencies\n    let mut frequency_histogram: HashMap<usize, usize> = HashMap::new();\n    let embedding_dim = embeddings[0].len();\n\n    // For each embedding dimension, compute FFT across all tokens and find dominant frequency\n    for dim in 0..embedding_dim {\n        let mut signal = Vec::with_capacity(vocab_size);\n        for embedding in embeddings.iter().take(vocab_size) {\n            if dim >= embedding.len() {\n                return Err(format!(\n                    \"embedding dimension mismatch at dim {}\",\n                    dim\n                ));\n            }\n            signal.push(embedding[dim]);\n        }\n\n        // Compute FFT and find dominant frequency\n        let magnitudes = compute_fft(&signal);\n        let dominant_freq = find_dominant_frequency(&magnitudes);\n        *frequency_histogram.entry(dominant_freq).or_insert(0) += 1;\n    }\n\n    // Check 1: Modulus frequency should appear significantly\n    let modulus_count = frequency_histogram.get(&config.modulus).copied().unwrap_or(0);\n    let modulus_frequency_fraction = modulus_count as f64 / embedding_dim as f64;\n\n    if modulus_frequency_fraction < config.min_modulus_frequency_fraction {\n        return Err(format!(\n            \"modulus frequency {} appears in only {:.3} of dimensions (threshold {:.3})\",\n            config.modulus, modulus_frequency_fraction, config.min_modulus_frequency_fraction\n        ));\n    }\n\n    // Check 2: Spectrum should NOT resemble white noise (low entropy)\n    // High entropy = uniform distribution (white noise), low entropy = structured peaks\n    let total_dims = embedding_dim as f64;\n    let mut entropy = 0.0;\n    for &count in frequency_histogram.values() {\n        if count > 0 {\n            let p = count as f64 / total_dims;\n            entropy -= p * p.log2();\n        }\n    }\n    // Normalize entropy by max possible (log2 of unique frequency count)\n    let max_entropy = (frequency_histogram.len() as f64).log2();\n    let normalized_entropy = if max_entropy > 0.0 {\n        entropy / max_entropy\n    } else {\n        0.0\n    };\n\n    if normalized_entropy > config.max_spectrum_entropy {\n        return Err(format!(\n            \"spectrum entropy {:.3} too high (white noise threshold {:.3})\",\n            normalized_entropy, config.max_spectrum_entropy\n        ));\n    }\n\n    // Check 3: Peak frequencies should be significantly stronger than median\n    let mut counts: Vec<usize> = frequency_histogram.values().copied().collect();\n    counts.sort_unstable();\n    let median_count = if counts.is_empty() {\n        0\n    } else {\n        counts[counts.len() / 2]\n    };\n    let peak_count = counts.last().copied().unwrap_or(0);\n    let peak_to_median_ratio = if median_count > 0 {\n        peak_count as f64 / median_count as f64\n    } else {\n        f64::INFINITY\n    };\n\n    if peak_to_median_ratio < config.min_peak_to_median_ratio {\n        return Err(format!(\n            \"peak to median ratio {:.3} below threshold {:.3}\",\n            peak_to_median_ratio, config.min_peak_to_median_ratio\n        ));\n    }\n\n    // Collect top frequencies for the report\n    let mut freq_counts: Vec<(usize, usize)> = frequency_histogram.into_iter().collect();\n    freq_counts.sort_by(|a, b| b.1.cmp(&a.1));\n    let top_frequencies: Vec<(usize, usize)> = freq_counts.into_iter().take(5).collect();\n\n    Ok(FFTDominantFrequencyReport {\n        modulus_frequency_fraction,\n        spectrum_entropy: normalized_entropy,\n        peak_to_median_ratio,\n        top_frequencies,\n    })\n}\n\n#[derive(Debug, Clone)]\npub struct FFTDominantFrequencyConfig {\n    pub modulus: usize,\n    pub min_modulus_frequency_fraction: f64,\n    pub max_spectrum_entropy: f64,\n    pub min_peak_to_median_ratio: f64,\n}\n\nimpl FFTDominantFrequencyConfig {\n    /// NOTE: Reserved for future integration into analysis pipeline\n    #[allow(dead_code)]\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        Self {\n            modulus,\n            min_modulus_frequency_fraction: 0.15,\n            max_spectrum_entropy: 0.85,\n            min_peak_to_median_ratio: 2.0,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct FFTDominantFrequencyReport {\n    pub modulus_frequency_fraction: f64,\n    pub spectrum_entropy: f64,\n    pub peak_to_median_ratio: f64,\n    pub top_frequencies: Vec<(usize, usize)>,\n}\n\n#[derive(Debug, Clone)]\npub struct EmbeddingClockConfig {\n    pub max_radius_std_fraction: f64,\n    pub min_angular_coverage: f64,\n    pub max_mean_angle_error: f64,\n    pub min_order_fraction: f64,\n}\n\nimpl EmbeddingClockConfig {\n    /// NOTE: Reserved for future integration into analysis pipeline\n    #[allow(dead_code)]\n    pub fn default_for_modulus(_modulus: usize) -> Self {\n        Self {\n            max_radius_std_fraction: 0.1,\n            min_angular_coverage: 0.9,\n            max_mean_angle_error: 0.35,\n            min_order_fraction: 0.9,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct EmbeddingClockReport {\n    pub mean_radius: f64,\n    pub radius_std: f64,\n    pub angular_coverage: f64,\n    pub mean_angle_error: f64,\n    pub order_fraction: f64,\n}\n\n/// NOTE: Reserved for future integration into analysis pipeline\n#[allow(dead_code)]\npub fn project_embeddings_to_fourier_plane(\n    embeddings: &[Vec<f64>],\n    modulus: usize,\n) -> Result<Vec<(f64, f64)>, String> {\n    let p = modulus;\n    if embeddings.len() < p {\n        return Err(\"embedding matrix smaller than modulus\".to_string());\n    }\n    if p == 0 {\n        return Err(\"modulus must be positive\".to_string());\n    }\n    let d = embeddings[0].len();\n    if d == 0 {\n        return Err(\"embedding dimension must be positive\".to_string());\n    }\n    if !embeddings[..p].iter().all(|row| row.len() == d) {\n        return Err(\"embedding rows have inconsistent dimension\".to_string());\n    }\n\n    let tau = std::f64::consts::PI * 2.0;\n    let mut basis_cos = vec![0.0f64; d];\n    let mut basis_sin = vec![0.0f64; d];\n    let scale = 2.0 / p as f64;\n\n    for (token, embedding) in embeddings[..p].iter().enumerate() {\n        let angle = tau * token as f64 / p as f64;\n        let cos_val = angle.cos();\n        let sin_val = angle.sin();\n        for dim in 0..d {\n            basis_cos[dim] += cos_val * embedding[dim] * scale;\n            basis_sin[dim] += sin_val * embedding[dim] * scale;\n        }\n    }\n\n    let basis_cos = normalize_vector(&basis_cos)?;\n    let basis_sin = normalize_vector(&basis_sin)?;\n\n    let mut coords = Vec::with_capacity(p);\n    for embedding in embeddings[..p].iter() {\n        let x = dot(embedding, &basis_cos);\n        let y = dot(embedding, &basis_sin);\n        coords.push((x, y));\n    }\n\n    Ok(coords)\n}\n\npub fn verify_embedding_clock_geometry(\n    embeddings: &[Vec<f64>],\n    modulus: usize,\n    config: &EmbeddingClockConfig,\n) -> Result<EmbeddingClockReport, String> {\n    let coords = project_embeddings_to_fourier_plane(embeddings, modulus)?;\n    let p = coords.len();\n    if p == 0 {\n        return Err(\"no embedding coordinates computed\".to_string());\n    }\n\n    let mut radii = Vec::with_capacity(p);\n    let mut angles = Vec::with_capacity(p);\n    for (x, y) in &coords {\n        let radius = (x * x + y * y).sqrt();\n        radii.push(radius);\n        angles.push(y.atan2(*x));\n    }\n\n    let mean_radius = radii.iter().sum::<f64>() / p as f64;\n    if mean_radius == 0.0 {\n        return Err(\"mean radius is zero\".to_string());\n    }\n    let radius_var = radii\n        .iter()\n        .map(|r| (r - mean_radius) * (r - mean_radius))\n        .sum::<f64>()\n        / p as f64;\n    let radius_std = radius_var.sqrt();\n\n    if radius_std / mean_radius > config.max_radius_std_fraction {\n        return Err(format!(\n            \"radius variance too large (std {:.4}, mean {:.4})\",\n            radius_std, mean_radius\n        ));\n    }\n\n    let tau = std::f64::consts::PI * 2.0;\n    let (mut sum_cos, mut sum_sin) = (0.0f64, 0.0f64);\n    for (token, angle) in angles.iter().enumerate() {\n        let expected = tau * token as f64 / p as f64;\n        let delta = angle - expected;\n        sum_cos += delta.cos();\n        sum_sin += delta.sin();\n    }\n    let offset = sum_sin.atan2(sum_cos);\n\n    let mut adjusted_angles = Vec::with_capacity(p);\n    let mut angle_errors = Vec::with_capacity(p);\n    for (token, angle) in angles.iter().enumerate() {\n        let expected = tau * token as f64 / p as f64;\n        let mut adjusted = angle - offset;\n        adjusted = wrap_angle(adjusted);\n        adjusted_angles.push(adjusted);\n        let diff = wrap_angle_signed(adjusted - expected);\n        angle_errors.push(diff.abs());\n    }\n\n    let mean_angle_error = angle_errors.iter().sum::<f64>() / p as f64;\n    if mean_angle_error > config.max_mean_angle_error {\n        return Err(format!(\n            \"mean angular error too large ({:.4} rad)\",\n            mean_angle_error\n        ));\n    }\n\n    let angular_coverage = angular_coverage(&adjusted_angles);\n    if angular_coverage < config.min_angular_coverage {\n        return Err(format!(\n            \"angular coverage too small ({:.4})\",\n            angular_coverage\n        ));\n    }\n\n    let order_fraction = order_fraction(&adjusted_angles, p);\n    if order_fraction < config.min_order_fraction {\n        return Err(format!(\n            \"token ordering too weak ({:.4})\",\n            order_fraction\n        ));\n    }\n\n    Ok(EmbeddingClockReport {\n        mean_radius,\n        radius_std,\n        angular_coverage,\n        mean_angle_error,\n        order_fraction,\n    })\n}\n\nfn normalize_vector(vec: &[f64]) -> Result<Vec<f64>, String> {\n    let norm = vec.iter().map(|v| v * v).sum::<f64>().sqrt();\n    if norm == 0.0 {\n        return Err(\"basis vector norm is zero\".to_string());\n    }\n    Ok(vec.iter().map(|v| v / norm).collect())\n}\n\nfn dot(a: &[f64], b: &[f64]) -> f64 {\n    a.iter().zip(b.iter()).map(|(x, y)| x * y).sum()\n}\n\nfn wrap_angle(angle: f64) -> f64 {\n    let tau = std::f64::consts::PI * 2.0;\n    let mut wrapped = angle % tau;\n    if wrapped < 0.0 {\n        wrapped += tau;\n    }\n    wrapped\n}\n\nfn wrap_angle_signed(angle: f64) -> f64 {\n    let tau = std::f64::consts::PI * 2.0;\n    let mut wrapped = (angle + std::f64::consts::PI) % tau;\n    if wrapped < 0.0 {\n        wrapped += tau;\n    }\n    wrapped - std::f64::consts::PI\n}\n\nfn angular_coverage(angles: &[f64]) -> f64 {\n    if angles.is_empty() {\n        return 0.0;\n    }\n    let tau = std::f64::consts::PI * 2.0;\n    let mut sorted = angles.to_vec();\n    sorted.sort_by(|a, b| a.partial_cmp(b).unwrap_or(std::cmp::Ordering::Equal));\n\n    let mut max_gap = 0.0;\n    for pair in sorted.windows(2) {\n        let gap = pair[1] - pair[0];\n        if gap > max_gap {\n            max_gap = gap;\n        }\n    }\n    let wrap_gap = tau - (sorted[sorted.len() - 1] - sorted[0]);\n    if wrap_gap > max_gap {\n        max_gap = wrap_gap;\n    }\n\n    (tau - max_gap) / tau\n}\n\nfn order_fraction(angles: &[f64], modulus: usize) -> f64 {\n    if angles.len() < 2 || modulus == 0 {\n        return 0.0;\n    }\n    let mut indexed: Vec<(usize, f64)> = angles.iter().copied().enumerate().collect();\n    indexed.sort_by(|a, b| a.1.partial_cmp(&b.1).unwrap_or(std::cmp::Ordering::Equal));\n\n    let mut good = 0usize;\n    for window in indexed.windows(2) {\n        let current = window[0].0;\n        let next = window[1].0;\n        if (next + modulus - current) % modulus == 1 {\n            good += 1;\n        }\n    }\n    let last = indexed[indexed.len() - 1].0;\n    let first = indexed[0].0;\n    if (first + modulus - last) % modulus == 1 {\n        good += 1;\n    }\n\n    good as f64 / modulus as f64\n}\n\n/// Track training and validation accuracy over time\n#[derive(Debug, Serialize, Deserialize)]\npub struct AccuracyHistory {\n    pub train_snapshots: Vec<(usize, f32)>, // (step, accuracy)\n    pub val_snapshots: Vec<(usize, f32)>,   // (step, accuracy)\n}\n\nimpl AccuracyHistory {\n    pub fn new() -> Self {\n        Self {\n            train_snapshots: Vec::new(),\n            val_snapshots: Vec::new(),\n        }\n    }\n\n    pub fn add_snapshot(&mut self, step: usize, train_acc: f32, val_acc: f32) {\n        self.train_snapshots.push((step, train_acc));\n        self.val_snapshots.push((step, val_acc));\n    }\n\n    pub fn save(&self, filename: &str) -> std::io::Result<()> {\n        let json = serde_json::to_string_pretty(self)?;\n        std::fs::write(filename, json)?;\n        println!(\"ðŸ’¾ Accuracy history saved to: {}\", filename);\n        Ok(())\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    fn make_circle_embeddings(modulus: usize) -> Vec<Vec<f64>> {\n        let tau = std::f64::consts::PI * 2.0;\n        (0..modulus)\n            .map(|token| {\n                let angle = tau * token as f64 / modulus as f64;\n                vec![angle.cos(), angle.sin(), 0.0, 0.0]\n            })\n            .collect()\n    }\n\n    #[test]\n    fn top_k_frequency_indices_stable_with_ties() {\n        let magnitudes = vec![0.1, 1.0, 1.0, 0.5];\n        let first = top_k_frequency_indices(&magnitudes, 2);\n        let second = top_k_frequency_indices(&magnitudes, 2);\n        assert_eq!(first, vec![1, 2]);\n        assert_eq!(first, second);\n    }\n\n    #[test]\n    fn restrict_signal_zeroes_when_top_k_zero() {\n        let signal = vec![1.0, -1.0, 0.5, -0.5];\n        let filtered = restrict_signal_to_top_k_frequencies(&signal, 0);\n        assert_eq!(filtered, vec![0.0, 0.0, 0.0, 0.0]);\n    }\n\n    #[test]\n    fn exclude_signal_no_change_when_top_k_zero() {\n        let signal = vec![1.0, -1.0, 0.5, -0.5];\n        let filtered = exclude_top_k_frequencies(&signal, 0);\n        assert_eq!(filtered, signal);\n    }\n\n    #[test]\n    fn exclude_signal_zeroes_when_top_k_all() {\n        let signal = vec![1.0, -1.0, 0.5, -0.5];\n        let filtered = exclude_top_k_frequencies(&signal, signal.len());\n        assert_eq!(filtered, vec![0.0, 0.0, 0.0, 0.0]);\n    }\n\n    #[test]\n    fn embedding_clock_geometry_passes_on_clean_circle() {\n        let modulus = 13;\n        let embeddings = make_circle_embeddings(modulus);\n        let config = EmbeddingClockConfig::default_for_modulus(modulus);\n        let report =\n            verify_embedding_clock_geometry(&embeddings, modulus, &config).expect(\"expected pass\");\n        assert!(report.angular_coverage > 0.9);\n        assert!(report.order_fraction > 0.9);\n        assert!(report.mean_angle_error < 0.1);\n    }\n\n    #[test]\n    fn embedding_clock_geometry_flags_wrong_frequency() {\n        let modulus = 11;\n        let tau = std::f64::consts::PI * 2.0;\n        let embeddings: Vec<Vec<f64>> = (0..modulus)\n            .map(|token| {\n                let angle = tau * token as f64 / modulus as f64;\n                vec![(2.0 * angle).cos(), (2.0 * angle).sin(), 0.0, 0.0]\n            })\n            .collect();\n        let config = EmbeddingClockConfig {\n            max_radius_std_fraction: 0.5,\n            min_angular_coverage: 0.1,\n            max_mean_angle_error: 0.2,\n            min_order_fraction: 0.1,\n        };\n        let err = verify_embedding_clock_geometry(&embeddings, modulus, &config)\n            .expect_err(\"expected ordering verification to fail\");\n        assert!(err.contains(\"mean angular error\"));\n    }\n\n    #[test]\n    fn fft_dominant_frequency_passes_on_modulus_structure() {\n        // Create embeddings with strong modulus-113 frequency component\n        // Mix in some other frequencies to get a realistic peak-to-median ratio\n        let modulus = 113;\n        let vocab_size = 114; // 0-112 + equals token\n        let embedding_dim = 128;\n        let tau = std::f64::consts::PI * 2.0;\n\n        let mut embeddings = Vec::with_capacity(vocab_size);\n        for token in 0..vocab_size {\n            let mut embedding = Vec::with_capacity(embedding_dim);\n            for dim in 0..embedding_dim {\n                // 60% of dimensions have modulus frequency, 40% have other frequencies\n                let freq = if dim % 5 < 3 {\n                    modulus\n                } else {\n                    // Use various other frequencies for the remaining dims\n                    20 + (dim % 20)\n                };\n                let angle = tau * freq as f64 * token as f64 / vocab_size as f64\n                    + dim as f64 * 0.1;\n                embedding.push(angle.sin());\n            }\n            embeddings.push(embedding);\n        }\n\n        let config = FFTDominantFrequencyConfig::default_for_modulus(modulus);\n        let report = verify_fft_dominant_frequencies_from_embeddings(&embeddings, &config)\n            .expect(\"expected modulus structure to pass\");\n\n        assert!(report.modulus_frequency_fraction >= config.min_modulus_frequency_fraction);\n        assert!(report.spectrum_entropy <= config.max_spectrum_entropy);\n        assert!(report.peak_to_median_ratio >= config.min_peak_to_median_ratio);\n        assert!(report.top_frequencies.iter().any(|(freq, _)| *freq == modulus));\n    }\n\n    #[test]\n    fn fft_dominant_frequency_fails_on_white_noise() {\n        // Create random embeddings (white noise spectrum)\n        let modulus = 113;\n        let vocab_size = 114;\n        let embedding_dim = 128;\n\n        let mut embeddings = Vec::with_capacity(vocab_size);\n        for token in 0..vocab_size {\n            let mut embedding = Vec::with_capacity(embedding_dim);\n            for dim in 0..embedding_dim {\n                // Pseudo-random values\n                let value = ((token * 17 + dim * 31) % 100) as f64 / 50.0 - 1.0;\n                embedding.push(value);\n            }\n            embeddings.push(embedding);\n        }\n\n        let config = FFTDominantFrequencyConfig::default_for_modulus(modulus);\n        let err = verify_fft_dominant_frequencies_from_embeddings(&embeddings, &config)\n            .expect_err(\"expected white noise to fail\");\n\n        // Should fail on either modulus frequency fraction or spectrum entropy\n        assert!(\n            err.contains(\"modulus frequency\") || err.contains(\"spectrum entropy\")\n                || err.contains(\"peak to median\")\n        );\n    }\n\n    #[test]\n    fn fft_dominant_frequency_fails_on_wrong_frequency() {\n        // Create embeddings with dominant frequency at 57 (not 113)\n        let modulus = 113;\n        let vocab_size = 114;\n        let embedding_dim = 128;\n        let tau = std::f64::consts::PI * 2.0;\n        let wrong_frequency = 57;\n\n        let mut embeddings = Vec::with_capacity(vocab_size);\n        for token in 0..vocab_size {\n            let mut embedding = Vec::with_capacity(embedding_dim);\n            for dim in 0..embedding_dim {\n                // Each dimension has a sine wave with wrong frequency\n                let angle = tau * wrong_frequency as f64 * token as f64 / vocab_size as f64\n                    + dim as f64 * 0.1;\n                embedding.push(angle.sin());\n            }\n            embeddings.push(embedding);\n        }\n\n        let config = FFTDominantFrequencyConfig::default_for_modulus(modulus);\n        let err = verify_fft_dominant_frequencies_from_embeddings(&embeddings, &config)\n            .expect_err(\"expected wrong frequency to fail\");\n\n        assert!(err.contains(\"modulus frequency\"));\n    }\n\n    #[test]\n    fn fft_dominant_frequency_fails_on_flat_embeddings() {\n        // Create constant embeddings (DC component only, no structure)\n        let modulus = 113;\n        let vocab_size = 114;\n        let embedding_dim = 128;\n\n        let embeddings = vec![vec![1.5; embedding_dim]; vocab_size];\n\n        let config = FFTDominantFrequencyConfig::default_for_modulus(modulus);\n        let err = verify_fft_dominant_frequencies_from_embeddings(&embeddings, &config)\n            .expect_err(\"expected flat embeddings to fail\");\n\n        assert!(err.contains(\"modulus frequency\") || err.contains(\"peak to median\"));\n    }\n\n    #[test]\n    fn fft_dominant_frequency_fails_on_weak_peaks() {\n        // Create embeddings with many frequencies at similar magnitudes (no clear peaks)\n        let modulus = 113;\n        let vocab_size = 114;\n        let embedding_dim = 128;\n        let tau = std::f64::consts::PI * 2.0;\n\n        let mut embeddings = Vec::with_capacity(vocab_size);\n        for token in 0..vocab_size {\n            let mut embedding = Vec::with_capacity(embedding_dim);\n            for dim in 0..embedding_dim {\n                // Mix many frequencies with similar amplitudes\n                let mut value = 0.0;\n                for freq in 1..10 {\n                    let angle =\n                        tau * freq as f64 * token as f64 / vocab_size as f64 + dim as f64 * 0.01;\n                    value += angle.sin() / 10.0;\n                }\n                embedding.push(value);\n            }\n            embeddings.push(embedding);\n        }\n\n        let config = FFTDominantFrequencyConfig {\n            modulus,\n            min_modulus_frequency_fraction: 0.15,\n            max_spectrum_entropy: 0.85,\n            min_peak_to_median_ratio: 2.0,\n        };\n        let err = verify_fft_dominant_frequencies_from_embeddings(&embeddings, &config)\n            .expect_err(\"expected weak peaks to fail\");\n\n        assert!(\n            err.contains(\"modulus frequency\") || err.contains(\"spectrum entropy\")\n                || err.contains(\"peak to median\")\n        );\n    }\n}\n","structuredPatch":[{"oldStart":772,"oldLines":6,"newStart":772,"newLines":8,"lines":["     Ok(coords)"," }"," ","+/// NOTE: Reserved for future integration into analysis pipeline","+#[allow(dead_code)]"," pub fn verify_embedding_clock_geometry(","     embeddings: &[Vec<f64>],","     modulus: usize,"]}],"userModified":false,"replaceAll":false}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01Rz9WjhqmM5wq2EM1zBxWDk","type":"message","role":"assistant","content":[{"type":"text","text":"Now let me tackle the utility functions that are never used anywhere. I'll remove the genuinely unused ones:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":1046,"cache_read_input_tokens":50339,"cache_creation":{"ephemeral_5m_input_tokens":1046,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"20aa3204-b857-4ec8-895c-40a2f11579b2"}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01Rz9WjhqmM5wq2EM1zBxWDk","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01WqpdLryUtFx7Dyjn4mVeC4","name":"Read","input":{"file_path":"/Users/admin/MechInt/src/analysis.rs","offset":850,"limit":80}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":1046,"cache_read_input_tokens":50339,"cache_creation":{"ephemeral_5m_input_tokens":1046,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"81118319-a82b-4a18-a6b1-c45bf9357aad"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01WqpdLryUtFx7Dyjn4mVeC4","type":"tool_result","content":"   850â†’\n   851â†’    let order_fraction = order_fraction(&adjusted_angles, p);\n   852â†’    if order_fraction < config.min_order_fraction {\n   853â†’        return Err(format!(\n   854â†’            \"token ordering too weak ({:.4})\",\n   855â†’            order_fraction\n   856â†’        ));\n   857â†’    }\n   858â†’\n   859â†’    Ok(EmbeddingClockReport {\n   860â†’        mean_radius,\n   861â†’        radius_std,\n   862â†’        angular_coverage,\n   863â†’        mean_angle_error,\n   864â†’        order_fraction,\n   865â†’    })\n   866â†’}\n   867â†’\n   868â†’fn normalize_vector(vec: &[f64]) -> Result<Vec<f64>, String> {\n   869â†’    let norm = vec.iter().map(|v| v * v).sum::<f64>().sqrt();\n   870â†’    if norm == 0.0 {\n   871â†’        return Err(\"basis vector norm is zero\".to_string());\n   872â†’    }\n   873â†’    Ok(vec.iter().map(|v| v / norm).collect())\n   874â†’}\n   875â†’\n   876â†’fn dot(a: &[f64], b: &[f64]) -> f64 {\n   877â†’    a.iter().zip(b.iter()).map(|(x, y)| x * y).sum()\n   878â†’}\n   879â†’\n   880â†’fn wrap_angle(angle: f64) -> f64 {\n   881â†’    let tau = std::f64::consts::PI * 2.0;\n   882â†’    let mut wrapped = angle % tau;\n   883â†’    if wrapped < 0.0 {\n   884â†’        wrapped += tau;\n   885â†’    }\n   886â†’    wrapped\n   887â†’}\n   888â†’\n   889â†’fn wrap_angle_signed(angle: f64) -> f64 {\n   890â†’    let tau = std::f64::consts::PI * 2.0;\n   891â†’    let mut wrapped = (angle + std::f64::consts::PI) % tau;\n   892â†’    if wrapped < 0.0 {\n   893â†’        wrapped += tau;\n   894â†’    }\n   895â†’    wrapped - std::f64::consts::PI\n   896â†’}\n   897â†’\n   898â†’fn angular_coverage(angles: &[f64]) -> f64 {\n   899â†’    if angles.is_empty() {\n   900â†’        return 0.0;\n   901â†’    }\n   902â†’    let tau = std::f64::consts::PI * 2.0;\n   903â†’    let mut sorted = angles.to_vec();\n   904â†’    sorted.sort_by(|a, b| a.partial_cmp(b).unwrap_or(std::cmp::Ordering::Equal));\n   905â†’\n   906â†’    let mut max_gap = 0.0;\n   907â†’    for pair in sorted.windows(2) {\n   908â†’        let gap = pair[1] - pair[0];\n   909â†’        if gap > max_gap {\n   910â†’            max_gap = gap;\n   911â†’        }\n   912â†’    }\n   913â†’    let wrap_gap = tau - (sorted[sorted.len() - 1] - sorted[0]);\n   914â†’    if wrap_gap > max_gap {\n   915â†’        max_gap = wrap_gap;\n   916â†’    }\n   917â†’\n   918â†’    (tau - max_gap) / tau\n   919â†’}\n   920â†’\n   921â†’fn order_fraction(angles: &[f64], modulus: usize) -> f64 {\n   922â†’    if angles.len() < 2 || modulus == 0 {\n   923â†’        return 0.0;\n   924â†’    }\n   925â†’    let mut indexed: Vec<(usize, f64)> = angles.iter().copied().enumerate().collect();\n   926â†’    indexed.sort_by(|a, b| a.1.partial_cmp(&b.1).unwrap_or(std::cmp::Ordering::Equal));\n   927â†’\n   928â†’    let mut good = 0usize;\n   929â†’    for window in indexed.windows(2) {\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"4a0486d3-0077-4dbc-b1af-4a15218272ae","tool_use_result":{"type":"text","file":{"filePath":"/Users/admin/MechInt/src/analysis.rs","content":"\n    let order_fraction = order_fraction(&adjusted_angles, p);\n    if order_fraction < config.min_order_fraction {\n        return Err(format!(\n            \"token ordering too weak ({:.4})\",\n            order_fraction\n        ));\n    }\n\n    Ok(EmbeddingClockReport {\n        mean_radius,\n        radius_std,\n        angular_coverage,\n        mean_angle_error,\n        order_fraction,\n    })\n}\n\nfn normalize_vector(vec: &[f64]) -> Result<Vec<f64>, String> {\n    let norm = vec.iter().map(|v| v * v).sum::<f64>().sqrt();\n    if norm == 0.0 {\n        return Err(\"basis vector norm is zero\".to_string());\n    }\n    Ok(vec.iter().map(|v| v / norm).collect())\n}\n\nfn dot(a: &[f64], b: &[f64]) -> f64 {\n    a.iter().zip(b.iter()).map(|(x, y)| x * y).sum()\n}\n\nfn wrap_angle(angle: f64) -> f64 {\n    let tau = std::f64::consts::PI * 2.0;\n    let mut wrapped = angle % tau;\n    if wrapped < 0.0 {\n        wrapped += tau;\n    }\n    wrapped\n}\n\nfn wrap_angle_signed(angle: f64) -> f64 {\n    let tau = std::f64::consts::PI * 2.0;\n    let mut wrapped = (angle + std::f64::consts::PI) % tau;\n    if wrapped < 0.0 {\n        wrapped += tau;\n    }\n    wrapped - std::f64::consts::PI\n}\n\nfn angular_coverage(angles: &[f64]) -> f64 {\n    if angles.is_empty() {\n        return 0.0;\n    }\n    let tau = std::f64::consts::PI * 2.0;\n    let mut sorted = angles.to_vec();\n    sorted.sort_by(|a, b| a.partial_cmp(b).unwrap_or(std::cmp::Ordering::Equal));\n\n    let mut max_gap = 0.0;\n    for pair in sorted.windows(2) {\n        let gap = pair[1] - pair[0];\n        if gap > max_gap {\n            max_gap = gap;\n        }\n    }\n    let wrap_gap = tau - (sorted[sorted.len() - 1] - sorted[0]);\n    if wrap_gap > max_gap {\n        max_gap = wrap_gap;\n    }\n\n    (tau - max_gap) / tau\n}\n\nfn order_fraction(angles: &[f64], modulus: usize) -> f64 {\n    if angles.len() < 2 || modulus == 0 {\n        return 0.0;\n    }\n    let mut indexed: Vec<(usize, f64)> = angles.iter().copied().enumerate().collect();\n    indexed.sort_by(|a, b| a.1.partial_cmp(&b.1).unwrap_or(std::cmp::Ordering::Equal));\n\n    let mut good = 0usize;\n    for window in indexed.windows(2) {","numLines":80,"startLine":850,"totalLines":1199}}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01T1QRc5db4oCSRpDgEz11aQ","type":"message","role":"assistant","content":[{"type":"text","text":"These utility functions (normalize_vector, dot, wrap_angle, etc.) are actually used by `verify_embedding_clock_geometry` which is marked for future use. So they should stay. Let me mark them:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":1939,"cache_read_input_tokens":51385,"cache_creation":{"ephemeral_5m_input_tokens":1939,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"98902de0-5791-42b4-9d01-12aabd7b20ac"}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01T1QRc5db4oCSRpDgEz11aQ","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_019TteMQY3hC8uWNgo4Bd3UR","name":"Edit","input":{"replace_all":false,"file_path":"/Users/admin/MechInt/src/analysis.rs","old_string":"fn normalize_vector(vec: &[f64]) -> Result<Vec<f64>, String> {\n    let norm = vec.iter().map(|v| v * v).sum::<f64>().sqrt();\n    if norm == 0.0 {\n        return Err(\"basis vector norm is zero\".to_string());\n    }\n    Ok(vec.iter().map(|v| v / norm).collect())\n}\n\nfn dot(a: &[f64], b: &[f64]) -> f64 {\n    a.iter().zip(b.iter()).map(|(x, y)| x * y).sum()\n}\n\nfn wrap_angle(angle: f64) -> f64 {\n    let tau = std::f64::consts::PI * 2.0;\n    let mut wrapped = angle % tau;\n    if wrapped < 0.0 {\n        wrapped += tau;\n    }\n    wrapped\n}\n\nfn wrap_angle_signed(angle: f64) -> f64 {\n    let tau = std::f64::consts::PI * 2.0;\n    let mut wrapped = (angle + std::f64::consts::PI) % tau;\n    if wrapped < 0.0 {\n        wrapped += tau;\n    }\n    wrapped - std::f64::consts::PI\n}\n\nfn angular_coverage(angles: &[f64]) -> f64 {","new_string":"// Helper functions for embedding clock geometry verification\n#[allow(dead_code)]\nfn normalize_vector(vec: &[f64]) -> Result<Vec<f64>, String> {\n    let norm = vec.iter().map(|v| v * v).sum::<f64>().sqrt();\n    if norm == 0.0 {\n        return Err(\"basis vector norm is zero\".to_string());\n    }\n    Ok(vec.iter().map(|v| v / norm).collect())\n}\n\n#[allow(dead_code)]\nfn dot(a: &[f64], b: &[f64]) -> f64 {\n    a.iter().zip(b.iter()).map(|(x, y)| x * y).sum()\n}\n\n#[allow(dead_code)]\nfn wrap_angle(angle: f64) -> f64 {\n    let tau = std::f64::consts::PI * 2.0;\n    let mut wrapped = angle % tau;\n    if wrapped < 0.0 {\n        wrapped += tau;\n    }\n    wrapped\n}\n\n#[allow(dead_code)]\nfn wrap_angle_signed(angle: f64) -> f64 {\n    let tau = std::f64::consts::PI * 2.0;\n    let mut wrapped = (angle + std::f64::consts::PI) % tau;\n    if wrapped < 0.0 {\n        wrapped += tau;\n    }\n    wrapped - std::f64::consts::PI\n}\n\n#[allow(dead_code)]\nfn angular_coverage(angles: &[f64]) -> f64 {"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":1939,"cache_read_input_tokens":51385,"cache_creation":{"ephemeral_5m_input_tokens":1939,"ephemeral_1h_input_tokens":0},"output_tokens":894,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"eb5367e3-4409-4c68-ba40-35e015ce0a26"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_019TteMQY3hC8uWNgo4Bd3UR","type":"tool_result","content":"The file /Users/admin/MechInt/src/analysis.rs has been updated. Here's the result of running `cat -n` on a snippet of the edited file:\n   864â†’        order_fraction,\n   865â†’    })\n   866â†’}\n   867â†’\n   868â†’// Helper functions for embedding clock geometry verification\n   869â†’#[allow(dead_code)]\n   870â†’fn normalize_vector(vec: &[f64]) -> Result<Vec<f64>, String> {\n   871â†’    let norm = vec.iter().map(|v| v * v).sum::<f64>().sqrt();\n   872â†’    if norm == 0.0 {\n   873â†’        return Err(\"basis vector norm is zero\".to_string());\n   874â†’    }\n   875â†’    Ok(vec.iter().map(|v| v / norm).collect())\n   876â†’}\n   877â†’\n   878â†’#[allow(dead_code)]\n   879â†’fn dot(a: &[f64], b: &[f64]) -> f64 {\n   880â†’    a.iter().zip(b.iter()).map(|(x, y)| x * y).sum()\n   881â†’}\n   882â†’\n   883â†’#[allow(dead_code)]\n   884â†’fn wrap_angle(angle: f64) -> f64 {\n   885â†’    let tau = std::f64::consts::PI * 2.0;\n   886â†’    let mut wrapped = angle % tau;\n   887â†’    if wrapped < 0.0 {\n   888â†’        wrapped += tau;\n   889â†’    }\n   890â†’    wrapped\n   891â†’}\n   892â†’\n   893â†’#[allow(dead_code)]\n   894â†’fn wrap_angle_signed(angle: f64) -> f64 {\n   895â†’    let tau = std::f64::consts::PI * 2.0;\n   896â†’    let mut wrapped = (angle + std::f64::consts::PI) % tau;\n   897â†’    if wrapped < 0.0 {\n   898â†’        wrapped += tau;\n   899â†’    }\n   900â†’    wrapped - std::f64::consts::PI\n   901â†’}\n   902â†’\n   903â†’#[allow(dead_code)]\n   904â†’fn angular_coverage(angles: &[f64]) -> f64 {\n   905â†’    if angles.is_empty() {\n   906â†’        return 0.0;\n   907â†’    }\n   908â†’    let tau = std::f64::consts::PI * 2.0;"}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"d7bcb778-bcac-4287-8e52-a6c69b3707fb","tool_use_result":{"filePath":"/Users/admin/MechInt/src/analysis.rs","oldString":"fn normalize_vector(vec: &[f64]) -> Result<Vec<f64>, String> {\n    let norm = vec.iter().map(|v| v * v).sum::<f64>().sqrt();\n    if norm == 0.0 {\n        return Err(\"basis vector norm is zero\".to_string());\n    }\n    Ok(vec.iter().map(|v| v / norm).collect())\n}\n\nfn dot(a: &[f64], b: &[f64]) -> f64 {\n    a.iter().zip(b.iter()).map(|(x, y)| x * y).sum()\n}\n\nfn wrap_angle(angle: f64) -> f64 {\n    let tau = std::f64::consts::PI * 2.0;\n    let mut wrapped = angle % tau;\n    if wrapped < 0.0 {\n        wrapped += tau;\n    }\n    wrapped\n}\n\nfn wrap_angle_signed(angle: f64) -> f64 {\n    let tau = std::f64::consts::PI * 2.0;\n    let mut wrapped = (angle + std::f64::consts::PI) % tau;\n    if wrapped < 0.0 {\n        wrapped += tau;\n    }\n    wrapped - std::f64::consts::PI\n}\n\nfn angular_coverage(angles: &[f64]) -> f64 {","newString":"// Helper functions for embedding clock geometry verification\n#[allow(dead_code)]\nfn normalize_vector(vec: &[f64]) -> Result<Vec<f64>, String> {\n    let norm = vec.iter().map(|v| v * v).sum::<f64>().sqrt();\n    if norm == 0.0 {\n        return Err(\"basis vector norm is zero\".to_string());\n    }\n    Ok(vec.iter().map(|v| v / norm).collect())\n}\n\n#[allow(dead_code)]\nfn dot(a: &[f64], b: &[f64]) -> f64 {\n    a.iter().zip(b.iter()).map(|(x, y)| x * y).sum()\n}\n\n#[allow(dead_code)]\nfn wrap_angle(angle: f64) -> f64 {\n    let tau = std::f64::consts::PI * 2.0;\n    let mut wrapped = angle % tau;\n    if wrapped < 0.0 {\n        wrapped += tau;\n    }\n    wrapped\n}\n\n#[allow(dead_code)]\nfn wrap_angle_signed(angle: f64) -> f64 {\n    let tau = std::f64::consts::PI * 2.0;\n    let mut wrapped = (angle + std::f64::consts::PI) % tau;\n    if wrapped < 0.0 {\n        wrapped += tau;\n    }\n    wrapped - std::f64::consts::PI\n}\n\n#[allow(dead_code)]\nfn angular_coverage(angles: &[f64]) -> f64 {","originalFile":"use burn::{\n    data::dataset::Dataset,\n    nn::loss::CrossEntropyLossConfig,\n    tensor::{backend::Backend, Int, Tensor, TensorData},\n};\nuse rustfft::{num_complex::Complex, FftPlanner};\nuse serde::{Deserialize, Serialize};\nuse std::collections::{HashMap, HashSet};\n\nuse crate::{data::ModularAdditionDataset, model::Transformer};\n\n/// FFT analysis results for embeddings\n#[derive(Debug, Serialize, Deserialize)]\npub struct FFTAnalysis {\n    pub vocab_size: usize,\n    pub embedding_dim: usize,\n    pub dominant_frequencies: Vec<(usize, f64)>, // (token_id, dominant_frequency_index)\n    pub frequency_magnitudes: HashMap<usize, Vec<f64>>, // token_id -> magnitude spectrum\n}\n\n/// Analyze the learned token embeddings using FFT\npub fn analyze_embeddings_fft<B: Backend>(model: &Transformer<B>) -> FFTAnalysis {\n    println!(\"ðŸ” Performing FFT analysis on learned embeddings...\");\n\n    let vocab_size = ModularAdditionDataset::vocab_size();\n    let embedding_dim = 128;\n\n    // Extract token embeddings by forward passing through the embedding layer\n    let mut frequency_magnitudes = HashMap::new();\n    let mut dominant_frequencies = Vec::new();\n\n    for token_id in 0..vocab_size {\n        // Get the embedding for this token\n        let embedding = get_token_embedding(model, token_id);\n\n        // Compute FFT on the embedding vector\n        let fft_result = compute_fft(&embedding);\n\n        // Find dominant frequency\n        let dominant_freq_idx = find_dominant_frequency(&fft_result);\n        dominant_frequencies.push((token_id, dominant_freq_idx as f64));\n\n        // Store magnitude spectrum\n        frequency_magnitudes.insert(token_id, fft_result);\n\n        if token_id % 10 == 0 {\n            println!(\"  Token {:2}: dominant frequency = {}\", token_id, dominant_freq_idx);\n        }\n    }\n\n    println!(\"âœ… FFT analysis complete!\");\n    println!();\n    println!(\"ðŸ“Š Dominant Frequency Analysis:\");\n\n    // Analyze patterns\n    let mut freq_histogram: HashMap<usize, usize> = HashMap::new();\n    for (_, freq) in &dominant_frequencies {\n        *freq_histogram.entry(*freq as usize).or_insert(0) += 1;\n    }\n\n    let mut freq_counts: Vec<_> = freq_histogram.iter().collect();\n    freq_counts.sort_by(|a, b| b.1.cmp(a.1));\n\n    println!(\"  Most common dominant frequencies:\");\n    for (freq, count) in freq_counts.iter().take(5) {\n        println!(\"    Frequency {}: {} tokens\", freq, count);\n    }\n\n    // Check for modular structure (expected for mod p)\n    println!();\n    println!(\"ðŸ”¬ Checking for modular structure:\");\n    let modulus = ModularAdditionDataset::modulus();\n    if freq_histogram.contains_key(&modulus) {\n        println!(\n            \"  âœ“ Frequency {} (matching modulus) appears in embeddings\",\n            modulus\n        );\n    }\n\n    FFTAnalysis {\n        vocab_size,\n        embedding_dim,\n        dominant_frequencies,\n        frequency_magnitudes,\n    }\n}\n\n/// Extract the token embedding for a single token\nfn get_token_embedding<B: Backend>(model: &Transformer<B>, token_id: usize) -> Vec<f64> {\n    // Get the token embedding directly from the model\n    let embedding_tensor = model.get_token_embedding(token_id);\n\n    // Convert to vector - shape is [1, 1, embedding_dim], we want [embedding_dim]\n    let embedding_data: Vec<f32> = embedding_tensor.into_data().to_vec().unwrap();\n\n    // Convert to f64\n    embedding_data.iter().map(|&x| x as f64).collect()\n}\n\n/// Compute FFT on a vector of real values\nfn compute_fft(data: &[f64]) -> Vec<f64> {\n    let mut planner = FftPlanner::new();\n    let fft = planner.plan_fft_forward(data.len());\n\n    // Convert to complex numbers\n    let mut buffer: Vec<Complex<f64>> = data\n        .iter()\n        .map(|&x| Complex::new(x, 0.0))\n        .collect();\n\n    // Perform FFT\n    fft.process(&mut buffer);\n\n    // Compute magnitudes\n    buffer.iter().map(|c| c.norm()).collect()\n}\n\nfn inverse_fft(data: &mut [Complex<f64>]) {\n    let mut planner = FftPlanner::new();\n    let fft = planner.plan_fft_inverse(data.len());\n    fft.process(data);\n}\n\nfn top_k_frequency_indices(magnitudes: &[f64], k: usize) -> Vec<usize> {\n    let mut indexed: Vec<(usize, f64)> = magnitudes.iter().copied().enumerate().collect();\n    indexed.sort_by(|(idx_a, mag_a), (idx_b, mag_b)| {\n        mag_b\n            .partial_cmp(mag_a)\n            .unwrap_or(std::cmp::Ordering::Equal)\n            .then_with(|| idx_a.cmp(idx_b))\n    });\n\n    let keep = k.min(indexed.len());\n    indexed.iter().take(keep).map(|(idx, _)| *idx).collect()\n}\n\nfn restrict_signal_to_top_k_frequencies(signal: &[f64], top_k: usize) -> Vec<f64> {\n    if signal.is_empty() {\n        return Vec::new();\n    }\n\n    if top_k == 0 {\n        return vec![0.0; signal.len()];\n    }\n\n    let mut planner = FftPlanner::new();\n    let fft = planner.plan_fft_forward(signal.len());\n\n    let mut buffer: Vec<Complex<f64>> = signal\n        .iter()\n        .map(|&x| Complex::new(x, 0.0))\n        .collect();\n    fft.process(&mut buffer);\n\n    let magnitudes: Vec<f64> = buffer.iter().map(|c| c.norm()).collect();\n    let keep = top_k_frequency_indices(&magnitudes, top_k);\n    let keep_set: HashSet<usize> = keep.into_iter().collect();\n\n    for (idx, value) in buffer.iter_mut().enumerate() {\n        if !keep_set.contains(&idx) {\n            *value = Complex::new(0.0, 0.0);\n        }\n    }\n\n    inverse_fft(&mut buffer);\n\n    let scale = 1.0 / signal.len() as f64;\n    buffer.iter().map(|c| c.re * scale).collect()\n}\n\nfn exclude_top_k_frequencies(signal: &[f64], top_k: usize) -> Vec<f64> {\n    if signal.is_empty() {\n        return Vec::new();\n    }\n\n    if top_k == 0 {\n        return signal.to_vec();\n    }\n\n    if top_k >= signal.len() {\n        return vec![0.0; signal.len()];\n    }\n\n    let mut planner = FftPlanner::new();\n    let fft = planner.plan_fft_forward(signal.len());\n\n    let mut buffer: Vec<Complex<f64>> = signal\n        .iter()\n        .map(|&x| Complex::new(x, 0.0))\n        .collect();\n    fft.process(&mut buffer);\n\n    let magnitudes: Vec<f64> = buffer.iter().map(|c| c.norm()).collect();\n    let remove = top_k_frequency_indices(&magnitudes, top_k);\n    let remove_set: HashSet<usize> = remove.into_iter().collect();\n\n    for (idx, value) in buffer.iter_mut().enumerate() {\n        if remove_set.contains(&idx) {\n            *value = Complex::new(0.0, 0.0);\n        }\n    }\n\n    inverse_fft(&mut buffer);\n\n    let scale = 1.0 / signal.len() as f64;\n    buffer.iter().map(|c| c.re * scale).collect()\n}\n\nfn restricted_token_embedding_weights<B: Backend>(\n    model: &Transformer<B>,\n    top_k: usize,\n) -> Vec<f32> {\n    let weight = model.token_embedding_weights();\n    let [vocab_size, d_model] = weight.dims();\n    let weight_data: Vec<f32> = weight.into_data().to_vec().unwrap();\n    let mut restricted = vec![0.0f32; weight_data.len()];\n\n    for dim in 0..d_model {\n        let mut column = Vec::with_capacity(vocab_size);\n        for token in 0..vocab_size {\n            column.push(weight_data[token * d_model + dim] as f64);\n        }\n        let filtered = restrict_signal_to_top_k_frequencies(&column, top_k);\n        for token in 0..vocab_size {\n            restricted[token * d_model + dim] = filtered[token] as f32;\n        }\n    }\n\n    restricted\n}\n\nfn excluded_token_embedding_weights<B: Backend>(\n    model: &Transformer<B>,\n    top_k: usize,\n) -> Vec<f32> {\n    let weight = model.token_embedding_weights();\n    let [vocab_size, d_model] = weight.dims();\n    let weight_data: Vec<f32> = weight.into_data().to_vec().unwrap();\n    let mut excluded = vec![0.0f32; weight_data.len()];\n\n    for dim in 0..d_model {\n        let mut column = Vec::with_capacity(vocab_size);\n        for token in 0..vocab_size {\n            column.push(weight_data[token * d_model + dim] as f64);\n        }\n        let filtered = exclude_top_k_frequencies(&column, top_k);\n        for token in 0..vocab_size {\n            excluded[token * d_model + dim] = filtered[token] as f32;\n        }\n    }\n\n    excluded\n}\n\npub fn compute_restricted_loss<B: Backend>(\n    model: &Transformer<B>,\n    dataset: &ModularAdditionDataset,\n    device: &B::Device,\n    top_k: usize,\n    batch_size: usize,\n) -> Result<f64, String> {\n    if batch_size == 0 {\n        return Err(\"batch_size must be greater than 0\".to_string());\n    }\n\n    let restricted_weights = restricted_token_embedding_weights(model, top_k);\n    let [vocab_size, d_model] = model.token_embedding_weights().dims();\n    let token_weights = Tensor::<B, 2>::from_data(\n        TensorData::new(restricted_weights, [vocab_size, d_model]),\n        device,\n    );\n\n    let loss_fn = CrossEntropyLossConfig::new().init(device);\n    let mut total_loss = 0.0f64;\n    let mut total_batches = 0usize;\n\n    for batch_start in (0..dataset.len()).step_by(batch_size) {\n        let batch_end = (batch_start + batch_size).min(dataset.len());\n        let batch_len = batch_end - batch_start;\n        if batch_len == 0 {\n            break;\n        }\n\n        let mut inputs_vec = Vec::with_capacity(batch_len * 3);\n        let mut targets_vec = Vec::with_capacity(batch_len);\n\n        for idx in batch_start..batch_end {\n            let (input, target) = dataset.get(idx).ok_or_else(|| {\n                format!(\"missing dataset sample at index {}\", idx)\n            })?;\n            inputs_vec.extend(input.iter().map(|value| *value as i32));\n            targets_vec.push(target as i32);\n        }\n\n        let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n            .reshape([batch_len, 3]);\n        let targets = Tensor::<B, 1, Int>::from_ints(targets_vec.as_slice(), device);\n\n        let logits = model.forward_with_token_weights(inputs, token_weights.clone());\n        let loss = loss_fn.forward(logits, targets);\n        let loss_value: f64 = loss.into_data().to_vec().unwrap().get(0).copied().unwrap_or(0.0);\n        total_loss += loss_value;\n        total_batches += 1;\n    }\n\n    if total_batches == 0 {\n        return Err(\"no batches computed for restricted loss\".to_string());\n    }\n\n    Ok(total_loss / total_batches as f64)\n}\n\npub fn compute_excluded_loss<B: Backend>(\n    model: &Transformer<B>,\n    dataset: &ModularAdditionDataset,\n    device: &B::Device,\n    top_k: usize,\n    batch_size: usize,\n) -> Result<f64, String> {\n    if batch_size == 0 {\n        return Err(\"batch_size must be greater than 0\".to_string());\n    }\n\n    let excluded_weights = excluded_token_embedding_weights(model, top_k);\n    let [vocab_size, d_model] = model.token_embedding_weights().dims();\n    let token_weights = Tensor::<B, 2>::from_data(\n        TensorData::new(excluded_weights, [vocab_size, d_model]),\n        device,\n    );\n\n    let loss_fn = CrossEntropyLossConfig::new().init(device);\n    let mut total_loss = 0.0f64;\n    let mut total_batches = 0usize;\n\n    for batch_start in (0..dataset.len()).step_by(batch_size) {\n        let batch_end = (batch_start + batch_size).min(dataset.len());\n        let batch_len = batch_end - batch_start;\n        if batch_len == 0 {\n            break;\n        }\n\n        let mut inputs_vec = Vec::with_capacity(batch_len * 3);\n        let mut targets_vec = Vec::with_capacity(batch_len);\n\n        for idx in batch_start..batch_end {\n            let (input, target) = dataset.get(idx).ok_or_else(|| {\n                format!(\"missing dataset sample at index {}\", idx)\n            })?;\n            inputs_vec.extend(input.iter().map(|value| *value as i32));\n            targets_vec.push(target as i32);\n        }\n\n        let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n            .reshape([batch_len, 3]);\n        let targets = Tensor::<B, 1, Int>::from_ints(targets_vec.as_slice(), device);\n\n        let logits = model.forward_with_token_weights(inputs, token_weights.clone());\n        let loss = loss_fn.forward(logits, targets);\n        let loss_value: f64 = loss.into_data().to_vec().unwrap().get(0).copied().unwrap_or(0.0);\n        total_loss += loss_value;\n        total_batches += 1;\n    }\n\n    if total_batches == 0 {\n        return Err(\"no batches computed for excluded loss\".to_string());\n    }\n\n    Ok(total_loss / total_batches as f64)\n}\n\n/// Find the index of the dominant frequency (excluding DC component at index 0)\nfn find_dominant_frequency(magnitudes: &[f64]) -> usize {\n    magnitudes\n        .iter()\n        .enumerate()\n        .skip(1) // Skip DC component\n        .max_by(|(_, a), (_, b)| {\n            a.partial_cmp(b).unwrap_or(std::cmp::Ordering::Equal)\n        })\n        .map(|(idx, _)| idx)\n        .unwrap_or(0)\n}\n\n/// Save FFT analysis to JSON file\npub fn save_fft_analysis(analysis: &FFTAnalysis, filename: &str) -> std::io::Result<()> {\n    let json = serde_json::to_string_pretty(analysis)?;\n    std::fs::write(filename, json)?;\n    println!(\"ðŸ’¾ FFT analysis saved to: {}\", filename);\n    Ok(())\n}\n\n/// Track training and validation loss over time\n#[derive(Debug, Serialize, Deserialize)]\npub struct LossHistory {\n    pub train_snapshots: Vec<(usize, f64)>, // (step, loss)\n    pub val_snapshots: Vec<(usize, f64)>,   // (step, loss)\n}\n\n/// Track restricted loss over time (only top-k frequencies kept)\n#[derive(Debug, Serialize, Deserialize)]\npub struct RestrictedLossHistory {\n    pub snapshots: Vec<(usize, f64)>, // (step, loss)\n}\n\nimpl RestrictedLossHistory {\n    pub fn new() -> Self {\n        Self {\n            snapshots: Vec::new(),\n        }\n    }\n\n    pub fn add_snapshot(&mut self, step: usize, loss: f64) {\n        self.snapshots.push((step, loss));\n    }\n\n    pub fn save(&self, filename: &str) -> std::io::Result<()> {\n        let json = serde_json::to_string_pretty(self)?;\n        std::fs::write(filename, json)?;\n        println!(\"ðŸ’¾ Restricted loss history saved to: {}\", filename);\n        Ok(())\n    }\n}\n\n/// Track excluded loss over time (top-k frequencies removed)\n#[derive(Debug, Serialize, Deserialize)]\npub struct ExcludedLossHistory {\n    pub snapshots: Vec<(usize, f64)>, // (step, loss)\n}\n\nimpl ExcludedLossHistory {\n    pub fn new() -> Self {\n        Self {\n            snapshots: Vec::new(),\n        }\n    }\n\n    pub fn add_snapshot(&mut self, step: usize, loss: f64) {\n        self.snapshots.push((step, loss));\n    }\n\n    pub fn save(&self, filename: &str) -> std::io::Result<()> {\n        let json = serde_json::to_string_pretty(self)?;\n        std::fs::write(filename, json)?;\n        println!(\"ðŸ’¾ Excluded loss history saved to: {}\", filename);\n        Ok(())\n    }\n}\n\nimpl LossHistory {\n    pub fn new() -> Self {\n        Self {\n            train_snapshots: Vec::new(),\n            val_snapshots: Vec::new(),\n        }\n    }\n\n    pub fn add_snapshot(&mut self, step: usize, train_loss: f64, val_loss: f64) {\n        self.train_snapshots.push((step, train_loss));\n        self.val_snapshots.push((step, val_loss));\n    }\n\n    pub fn save(&self, filename: &str) -> std::io::Result<()> {\n        let json = serde_json::to_string_pretty(self)?;\n        std::fs::write(filename, json)?;\n        println!(\"ðŸ’¾ Loss history saved to: {}\", filename);\n        Ok(())\n    }\n}\n\n/// Legacy: Single loss tracking (kept for backward compatibility)\n#[derive(Debug, Serialize, Deserialize, Clone)]\npub struct WeightNormSnapshot {\n    pub step: usize,\n    pub total_norm: f64,\n}\n\n/// Legacy: Track weight norms over training (actually stores loss)\n#[derive(Debug, Serialize, Deserialize)]\npub struct WeightNormHistory {\n    pub snapshots: Vec<WeightNormSnapshot>,\n}\n\nimpl WeightNormHistory {\n    pub fn new() -> Self {\n        Self {\n            snapshots: Vec::new(),\n        }\n    }\n\n    pub fn add_snapshot(&mut self, step: usize, norm: f64) {\n        self.snapshots.push(WeightNormSnapshot {\n            step,\n            total_norm: norm,\n        });\n    }\n\n    pub fn save(&self, filename: &str) -> std::io::Result<()> {\n        let json = serde_json::to_string_pretty(self)?;\n        std::fs::write(filename, json)?;\n        println!(\"ðŸ’¾ Weight norm history saved to: {}\", filename);\n        Ok(())\n    }\n}\n\n/// Compute the L2 norm of all model parameters\n/// This returns the Frobenius norm (sum of squared weights) across all parameters\n/// Note: This is a simplified implementation that computes the norm of token embeddings,\n/// which contain the majority of the model parameters. For a complete implementation,\n/// we would need to access all Linear layer weights via the Record trait.\npub fn compute_model_weight_norm<B: Backend>(model: &Transformer<B>) -> f64 {\n    let mut total_norm_squared = 0.0f64;\n\n    // Token embedding weights: [vocab_size, d_model]\n    let token_emb_weights = model.token_embedding_weights();\n    let token_emb_data: Vec<f32> = token_emb_weights.into_data().to_vec().unwrap();\n    for &weight in &token_emb_data {\n        total_norm_squared += (weight as f64) * (weight as f64);\n    }\n\n    // Note: Position embeddings, attention weights, MLP weights, and LM head weights\n    // are not included in this simplified implementation. For weight decay verification,\n    // tracking the token embeddings (which are the largest weight matrices) is sufficient\n    // to observe the monotonic decrease under weight decay.\n\n    total_norm_squared.sqrt()\n}\n\n/// Extract all token embeddings as a matrix [vocab_size, embedding_dim]\npub fn extract_all_embeddings<B: Backend>(model: &Transformer<B>) -> Vec<Vec<f64>> {\n    let vocab_size = ModularAdditionDataset::vocab_size();\n    let mut embeddings = Vec::with_capacity(vocab_size);\n\n    for token_id in 0..vocab_size {\n        embeddings.push(get_token_embedding(model, token_id));\n    }\n\n    embeddings\n}\n\n/// Verify that FFT analysis of embeddings shows dominant frequencies\n/// that align with the modulus structure and nontrivial spectral patterns\n///\n/// NOTE: Reserved for manual verification or future integration into analysis pipeline\n#[allow(dead_code)]\npub fn verify_fft_dominant_frequencies<B: Backend>(\n    model: &Transformer<B>,\n    config: &FFTDominantFrequencyConfig,\n) -> Result<FFTDominantFrequencyReport, String> {\n    let embeddings = extract_all_embeddings(model);\n    verify_fft_dominant_frequencies_from_embeddings(&embeddings, config)\n}\n\n/// Verify FFT dominant frequencies from pre-extracted embeddings\n///\n/// NOTE: Reserved for manual verification or future integration into analysis pipeline\n#[allow(dead_code)]\npub fn verify_fft_dominant_frequencies_from_embeddings(\n    embeddings: &[Vec<f64>],\n    config: &FFTDominantFrequencyConfig,\n) -> Result<FFTDominantFrequencyReport, String> {\n    if embeddings.is_empty() {\n        return Err(\"no embeddings provided\".to_string());\n    }\n\n    let vocab_size = embeddings.len();\n    if vocab_size < config.modulus {\n        return Err(format!(\n            \"embedding count {} less than modulus {}\",\n            vocab_size, config.modulus\n        ));\n    }\n\n    // Analyze each embedding dimension and collect dominant frequencies\n    let mut frequency_histogram: HashMap<usize, usize> = HashMap::new();\n    let embedding_dim = embeddings[0].len();\n\n    // For each embedding dimension, compute FFT across all tokens and find dominant frequency\n    for dim in 0..embedding_dim {\n        let mut signal = Vec::with_capacity(vocab_size);\n        for embedding in embeddings.iter().take(vocab_size) {\n            if dim >= embedding.len() {\n                return Err(format!(\n                    \"embedding dimension mismatch at dim {}\",\n                    dim\n                ));\n            }\n            signal.push(embedding[dim]);\n        }\n\n        // Compute FFT and find dominant frequency\n        let magnitudes = compute_fft(&signal);\n        let dominant_freq = find_dominant_frequency(&magnitudes);\n        *frequency_histogram.entry(dominant_freq).or_insert(0) += 1;\n    }\n\n    // Check 1: Modulus frequency should appear significantly\n    let modulus_count = frequency_histogram.get(&config.modulus).copied().unwrap_or(0);\n    let modulus_frequency_fraction = modulus_count as f64 / embedding_dim as f64;\n\n    if modulus_frequency_fraction < config.min_modulus_frequency_fraction {\n        return Err(format!(\n            \"modulus frequency {} appears in only {:.3} of dimensions (threshold {:.3})\",\n            config.modulus, modulus_frequency_fraction, config.min_modulus_frequency_fraction\n        ));\n    }\n\n    // Check 2: Spectrum should NOT resemble white noise (low entropy)\n    // High entropy = uniform distribution (white noise), low entropy = structured peaks\n    let total_dims = embedding_dim as f64;\n    let mut entropy = 0.0;\n    for &count in frequency_histogram.values() {\n        if count > 0 {\n            let p = count as f64 / total_dims;\n            entropy -= p * p.log2();\n        }\n    }\n    // Normalize entropy by max possible (log2 of unique frequency count)\n    let max_entropy = (frequency_histogram.len() as f64).log2();\n    let normalized_entropy = if max_entropy > 0.0 {\n        entropy / max_entropy\n    } else {\n        0.0\n    };\n\n    if normalized_entropy > config.max_spectrum_entropy {\n        return Err(format!(\n            \"spectrum entropy {:.3} too high (white noise threshold {:.3})\",\n            normalized_entropy, config.max_spectrum_entropy\n        ));\n    }\n\n    // Check 3: Peak frequencies should be significantly stronger than median\n    let mut counts: Vec<usize> = frequency_histogram.values().copied().collect();\n    counts.sort_unstable();\n    let median_count = if counts.is_empty() {\n        0\n    } else {\n        counts[counts.len() / 2]\n    };\n    let peak_count = counts.last().copied().unwrap_or(0);\n    let peak_to_median_ratio = if median_count > 0 {\n        peak_count as f64 / median_count as f64\n    } else {\n        f64::INFINITY\n    };\n\n    if peak_to_median_ratio < config.min_peak_to_median_ratio {\n        return Err(format!(\n            \"peak to median ratio {:.3} below threshold {:.3}\",\n            peak_to_median_ratio, config.min_peak_to_median_ratio\n        ));\n    }\n\n    // Collect top frequencies for the report\n    let mut freq_counts: Vec<(usize, usize)> = frequency_histogram.into_iter().collect();\n    freq_counts.sort_by(|a, b| b.1.cmp(&a.1));\n    let top_frequencies: Vec<(usize, usize)> = freq_counts.into_iter().take(5).collect();\n\n    Ok(FFTDominantFrequencyReport {\n        modulus_frequency_fraction,\n        spectrum_entropy: normalized_entropy,\n        peak_to_median_ratio,\n        top_frequencies,\n    })\n}\n\n#[derive(Debug, Clone)]\npub struct FFTDominantFrequencyConfig {\n    pub modulus: usize,\n    pub min_modulus_frequency_fraction: f64,\n    pub max_spectrum_entropy: f64,\n    pub min_peak_to_median_ratio: f64,\n}\n\nimpl FFTDominantFrequencyConfig {\n    /// NOTE: Reserved for future integration into analysis pipeline\n    #[allow(dead_code)]\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        Self {\n            modulus,\n            min_modulus_frequency_fraction: 0.15,\n            max_spectrum_entropy: 0.85,\n            min_peak_to_median_ratio: 2.0,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct FFTDominantFrequencyReport {\n    pub modulus_frequency_fraction: f64,\n    pub spectrum_entropy: f64,\n    pub peak_to_median_ratio: f64,\n    pub top_frequencies: Vec<(usize, usize)>,\n}\n\n#[derive(Debug, Clone)]\npub struct EmbeddingClockConfig {\n    pub max_radius_std_fraction: f64,\n    pub min_angular_coverage: f64,\n    pub max_mean_angle_error: f64,\n    pub min_order_fraction: f64,\n}\n\nimpl EmbeddingClockConfig {\n    /// NOTE: Reserved for future integration into analysis pipeline\n    #[allow(dead_code)]\n    pub fn default_for_modulus(_modulus: usize) -> Self {\n        Self {\n            max_radius_std_fraction: 0.1,\n            min_angular_coverage: 0.9,\n            max_mean_angle_error: 0.35,\n            min_order_fraction: 0.9,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct EmbeddingClockReport {\n    pub mean_radius: f64,\n    pub radius_std: f64,\n    pub angular_coverage: f64,\n    pub mean_angle_error: f64,\n    pub order_fraction: f64,\n}\n\n/// NOTE: Reserved for future integration into analysis pipeline\n#[allow(dead_code)]\npub fn project_embeddings_to_fourier_plane(\n    embeddings: &[Vec<f64>],\n    modulus: usize,\n) -> Result<Vec<(f64, f64)>, String> {\n    let p = modulus;\n    if embeddings.len() < p {\n        return Err(\"embedding matrix smaller than modulus\".to_string());\n    }\n    if p == 0 {\n        return Err(\"modulus must be positive\".to_string());\n    }\n    let d = embeddings[0].len();\n    if d == 0 {\n        return Err(\"embedding dimension must be positive\".to_string());\n    }\n    if !embeddings[..p].iter().all(|row| row.len() == d) {\n        return Err(\"embedding rows have inconsistent dimension\".to_string());\n    }\n\n    let tau = std::f64::consts::PI * 2.0;\n    let mut basis_cos = vec![0.0f64; d];\n    let mut basis_sin = vec![0.0f64; d];\n    let scale = 2.0 / p as f64;\n\n    for (token, embedding) in embeddings[..p].iter().enumerate() {\n        let angle = tau * token as f64 / p as f64;\n        let cos_val = angle.cos();\n        let sin_val = angle.sin();\n        for dim in 0..d {\n            basis_cos[dim] += cos_val * embedding[dim] * scale;\n            basis_sin[dim] += sin_val * embedding[dim] * scale;\n        }\n    }\n\n    let basis_cos = normalize_vector(&basis_cos)?;\n    let basis_sin = normalize_vector(&basis_sin)?;\n\n    let mut coords = Vec::with_capacity(p);\n    for embedding in embeddings[..p].iter() {\n        let x = dot(embedding, &basis_cos);\n        let y = dot(embedding, &basis_sin);\n        coords.push((x, y));\n    }\n\n    Ok(coords)\n}\n\n/// NOTE: Reserved for future integration into analysis pipeline\n#[allow(dead_code)]\npub fn verify_embedding_clock_geometry(\n    embeddings: &[Vec<f64>],\n    modulus: usize,\n    config: &EmbeddingClockConfig,\n) -> Result<EmbeddingClockReport, String> {\n    let coords = project_embeddings_to_fourier_plane(embeddings, modulus)?;\n    let p = coords.len();\n    if p == 0 {\n        return Err(\"no embedding coordinates computed\".to_string());\n    }\n\n    let mut radii = Vec::with_capacity(p);\n    let mut angles = Vec::with_capacity(p);\n    for (x, y) in &coords {\n        let radius = (x * x + y * y).sqrt();\n        radii.push(radius);\n        angles.push(y.atan2(*x));\n    }\n\n    let mean_radius = radii.iter().sum::<f64>() / p as f64;\n    if mean_radius == 0.0 {\n        return Err(\"mean radius is zero\".to_string());\n    }\n    let radius_var = radii\n        .iter()\n        .map(|r| (r - mean_radius) * (r - mean_radius))\n        .sum::<f64>()\n        / p as f64;\n    let radius_std = radius_var.sqrt();\n\n    if radius_std / mean_radius > config.max_radius_std_fraction {\n        return Err(format!(\n            \"radius variance too large (std {:.4}, mean {:.4})\",\n            radius_std, mean_radius\n        ));\n    }\n\n    let tau = std::f64::consts::PI * 2.0;\n    let (mut sum_cos, mut sum_sin) = (0.0f64, 0.0f64);\n    for (token, angle) in angles.iter().enumerate() {\n        let expected = tau * token as f64 / p as f64;\n        let delta = angle - expected;\n        sum_cos += delta.cos();\n        sum_sin += delta.sin();\n    }\n    let offset = sum_sin.atan2(sum_cos);\n\n    let mut adjusted_angles = Vec::with_capacity(p);\n    let mut angle_errors = Vec::with_capacity(p);\n    for (token, angle) in angles.iter().enumerate() {\n        let expected = tau * token as f64 / p as f64;\n        let mut adjusted = angle - offset;\n        adjusted = wrap_angle(adjusted);\n        adjusted_angles.push(adjusted);\n        let diff = wrap_angle_signed(adjusted - expected);\n        angle_errors.push(diff.abs());\n    }\n\n    let mean_angle_error = angle_errors.iter().sum::<f64>() / p as f64;\n    if mean_angle_error > config.max_mean_angle_error {\n        return Err(format!(\n            \"mean angular error too large ({:.4} rad)\",\n            mean_angle_error\n        ));\n    }\n\n    let angular_coverage = angular_coverage(&adjusted_angles);\n    if angular_coverage < config.min_angular_coverage {\n        return Err(format!(\n            \"angular coverage too small ({:.4})\",\n            angular_coverage\n        ));\n    }\n\n    let order_fraction = order_fraction(&adjusted_angles, p);\n    if order_fraction < config.min_order_fraction {\n        return Err(format!(\n            \"token ordering too weak ({:.4})\",\n            order_fraction\n        ));\n    }\n\n    Ok(EmbeddingClockReport {\n        mean_radius,\n        radius_std,\n        angular_coverage,\n        mean_angle_error,\n        order_fraction,\n    })\n}\n\nfn normalize_vector(vec: &[f64]) -> Result<Vec<f64>, String> {\n    let norm = vec.iter().map(|v| v * v).sum::<f64>().sqrt();\n    if norm == 0.0 {\n        return Err(\"basis vector norm is zero\".to_string());\n    }\n    Ok(vec.iter().map(|v| v / norm).collect())\n}\n\nfn dot(a: &[f64], b: &[f64]) -> f64 {\n    a.iter().zip(b.iter()).map(|(x, y)| x * y).sum()\n}\n\nfn wrap_angle(angle: f64) -> f64 {\n    let tau = std::f64::consts::PI * 2.0;\n    let mut wrapped = angle % tau;\n    if wrapped < 0.0 {\n        wrapped += tau;\n    }\n    wrapped\n}\n\nfn wrap_angle_signed(angle: f64) -> f64 {\n    let tau = std::f64::consts::PI * 2.0;\n    let mut wrapped = (angle + std::f64::consts::PI) % tau;\n    if wrapped < 0.0 {\n        wrapped += tau;\n    }\n    wrapped - std::f64::consts::PI\n}\n\nfn angular_coverage(angles: &[f64]) -> f64 {\n    if angles.is_empty() {\n        return 0.0;\n    }\n    let tau = std::f64::consts::PI * 2.0;\n    let mut sorted = angles.to_vec();\n    sorted.sort_by(|a, b| a.partial_cmp(b).unwrap_or(std::cmp::Ordering::Equal));\n\n    let mut max_gap = 0.0;\n    for pair in sorted.windows(2) {\n        let gap = pair[1] - pair[0];\n        if gap > max_gap {\n            max_gap = gap;\n        }\n    }\n    let wrap_gap = tau - (sorted[sorted.len() - 1] - sorted[0]);\n    if wrap_gap > max_gap {\n        max_gap = wrap_gap;\n    }\n\n    (tau - max_gap) / tau\n}\n\nfn order_fraction(angles: &[f64], modulus: usize) -> f64 {\n    if angles.len() < 2 || modulus == 0 {\n        return 0.0;\n    }\n    let mut indexed: Vec<(usize, f64)> = angles.iter().copied().enumerate().collect();\n    indexed.sort_by(|a, b| a.1.partial_cmp(&b.1).unwrap_or(std::cmp::Ordering::Equal));\n\n    let mut good = 0usize;\n    for window in indexed.windows(2) {\n        let current = window[0].0;\n        let next = window[1].0;\n        if (next + modulus - current) % modulus == 1 {\n            good += 1;\n        }\n    }\n    let last = indexed[indexed.len() - 1].0;\n    let first = indexed[0].0;\n    if (first + modulus - last) % modulus == 1 {\n        good += 1;\n    }\n\n    good as f64 / modulus as f64\n}\n\n/// Track training and validation accuracy over time\n#[derive(Debug, Serialize, Deserialize)]\npub struct AccuracyHistory {\n    pub train_snapshots: Vec<(usize, f32)>, // (step, accuracy)\n    pub val_snapshots: Vec<(usize, f32)>,   // (step, accuracy)\n}\n\nimpl AccuracyHistory {\n    pub fn new() -> Self {\n        Self {\n            train_snapshots: Vec::new(),\n            val_snapshots: Vec::new(),\n        }\n    }\n\n    pub fn add_snapshot(&mut self, step: usize, train_acc: f32, val_acc: f32) {\n        self.train_snapshots.push((step, train_acc));\n        self.val_snapshots.push((step, val_acc));\n    }\n\n    pub fn save(&self, filename: &str) -> std::io::Result<()> {\n        let json = serde_json::to_string_pretty(self)?;\n        std::fs::write(filename, json)?;\n        println!(\"ðŸ’¾ Accuracy history saved to: {}\", filename);\n        Ok(())\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    fn make_circle_embeddings(modulus: usize) -> Vec<Vec<f64>> {\n        let tau = std::f64::consts::PI * 2.0;\n        (0..modulus)\n            .map(|token| {\n                let angle = tau * token as f64 / modulus as f64;\n                vec![angle.cos(), angle.sin(), 0.0, 0.0]\n            })\n            .collect()\n    }\n\n    #[test]\n    fn top_k_frequency_indices_stable_with_ties() {\n        let magnitudes = vec![0.1, 1.0, 1.0, 0.5];\n        let first = top_k_frequency_indices(&magnitudes, 2);\n        let second = top_k_frequency_indices(&magnitudes, 2);\n        assert_eq!(first, vec![1, 2]);\n        assert_eq!(first, second);\n    }\n\n    #[test]\n    fn restrict_signal_zeroes_when_top_k_zero() {\n        let signal = vec![1.0, -1.0, 0.5, -0.5];\n        let filtered = restrict_signal_to_top_k_frequencies(&signal, 0);\n        assert_eq!(filtered, vec![0.0, 0.0, 0.0, 0.0]);\n    }\n\n    #[test]\n    fn exclude_signal_no_change_when_top_k_zero() {\n        let signal = vec![1.0, -1.0, 0.5, -0.5];\n        let filtered = exclude_top_k_frequencies(&signal, 0);\n        assert_eq!(filtered, signal);\n    }\n\n    #[test]\n    fn exclude_signal_zeroes_when_top_k_all() {\n        let signal = vec![1.0, -1.0, 0.5, -0.5];\n        let filtered = exclude_top_k_frequencies(&signal, signal.len());\n        assert_eq!(filtered, vec![0.0, 0.0, 0.0, 0.0]);\n    }\n\n    #[test]\n    fn embedding_clock_geometry_passes_on_clean_circle() {\n        let modulus = 13;\n        let embeddings = make_circle_embeddings(modulus);\n        let config = EmbeddingClockConfig::default_for_modulus(modulus);\n        let report =\n            verify_embedding_clock_geometry(&embeddings, modulus, &config).expect(\"expected pass\");\n        assert!(report.angular_coverage > 0.9);\n        assert!(report.order_fraction > 0.9);\n        assert!(report.mean_angle_error < 0.1);\n    }\n\n    #[test]\n    fn embedding_clock_geometry_flags_wrong_frequency() {\n        let modulus = 11;\n        let tau = std::f64::consts::PI * 2.0;\n        let embeddings: Vec<Vec<f64>> = (0..modulus)\n            .map(|token| {\n                let angle = tau * token as f64 / modulus as f64;\n                vec![(2.0 * angle).cos(), (2.0 * angle).sin(), 0.0, 0.0]\n            })\n            .collect();\n        let config = EmbeddingClockConfig {\n            max_radius_std_fraction: 0.5,\n            min_angular_coverage: 0.1,\n            max_mean_angle_error: 0.2,\n            min_order_fraction: 0.1,\n        };\n        let err = verify_embedding_clock_geometry(&embeddings, modulus, &config)\n            .expect_err(\"expected ordering verification to fail\");\n        assert!(err.contains(\"mean angular error\"));\n    }\n\n    #[test]\n    fn fft_dominant_frequency_passes_on_modulus_structure() {\n        // Create embeddings with strong modulus-113 frequency component\n        // Mix in some other frequencies to get a realistic peak-to-median ratio\n        let modulus = 113;\n        let vocab_size = 114; // 0-112 + equals token\n        let embedding_dim = 128;\n        let tau = std::f64::consts::PI * 2.0;\n\n        let mut embeddings = Vec::with_capacity(vocab_size);\n        for token in 0..vocab_size {\n            let mut embedding = Vec::with_capacity(embedding_dim);\n            for dim in 0..embedding_dim {\n                // 60% of dimensions have modulus frequency, 40% have other frequencies\n                let freq = if dim % 5 < 3 {\n                    modulus\n                } else {\n                    // Use various other frequencies for the remaining dims\n                    20 + (dim % 20)\n                };\n                let angle = tau * freq as f64 * token as f64 / vocab_size as f64\n                    + dim as f64 * 0.1;\n                embedding.push(angle.sin());\n            }\n            embeddings.push(embedding);\n        }\n\n        let config = FFTDominantFrequencyConfig::default_for_modulus(modulus);\n        let report = verify_fft_dominant_frequencies_from_embeddings(&embeddings, &config)\n            .expect(\"expected modulus structure to pass\");\n\n        assert!(report.modulus_frequency_fraction >= config.min_modulus_frequency_fraction);\n        assert!(report.spectrum_entropy <= config.max_spectrum_entropy);\n        assert!(report.peak_to_median_ratio >= config.min_peak_to_median_ratio);\n        assert!(report.top_frequencies.iter().any(|(freq, _)| *freq == modulus));\n    }\n\n    #[test]\n    fn fft_dominant_frequency_fails_on_white_noise() {\n        // Create random embeddings (white noise spectrum)\n        let modulus = 113;\n        let vocab_size = 114;\n        let embedding_dim = 128;\n\n        let mut embeddings = Vec::with_capacity(vocab_size);\n        for token in 0..vocab_size {\n            let mut embedding = Vec::with_capacity(embedding_dim);\n            for dim in 0..embedding_dim {\n                // Pseudo-random values\n                let value = ((token * 17 + dim * 31) % 100) as f64 / 50.0 - 1.0;\n                embedding.push(value);\n            }\n            embeddings.push(embedding);\n        }\n\n        let config = FFTDominantFrequencyConfig::default_for_modulus(modulus);\n        let err = verify_fft_dominant_frequencies_from_embeddings(&embeddings, &config)\n            .expect_err(\"expected white noise to fail\");\n\n        // Should fail on either modulus frequency fraction or spectrum entropy\n        assert!(\n            err.contains(\"modulus frequency\") || err.contains(\"spectrum entropy\")\n                || err.contains(\"peak to median\")\n        );\n    }\n\n    #[test]\n    fn fft_dominant_frequency_fails_on_wrong_frequency() {\n        // Create embeddings with dominant frequency at 57 (not 113)\n        let modulus = 113;\n        let vocab_size = 114;\n        let embedding_dim = 128;\n        let tau = std::f64::consts::PI * 2.0;\n        let wrong_frequency = 57;\n\n        let mut embeddings = Vec::with_capacity(vocab_size);\n        for token in 0..vocab_size {\n            let mut embedding = Vec::with_capacity(embedding_dim);\n            for dim in 0..embedding_dim {\n                // Each dimension has a sine wave with wrong frequency\n                let angle = tau * wrong_frequency as f64 * token as f64 / vocab_size as f64\n                    + dim as f64 * 0.1;\n                embedding.push(angle.sin());\n            }\n            embeddings.push(embedding);\n        }\n\n        let config = FFTDominantFrequencyConfig::default_for_modulus(modulus);\n        let err = verify_fft_dominant_frequencies_from_embeddings(&embeddings, &config)\n            .expect_err(\"expected wrong frequency to fail\");\n\n        assert!(err.contains(\"modulus frequency\"));\n    }\n\n    #[test]\n    fn fft_dominant_frequency_fails_on_flat_embeddings() {\n        // Create constant embeddings (DC component only, no structure)\n        let modulus = 113;\n        let vocab_size = 114;\n        let embedding_dim = 128;\n\n        let embeddings = vec![vec![1.5; embedding_dim]; vocab_size];\n\n        let config = FFTDominantFrequencyConfig::default_for_modulus(modulus);\n        let err = verify_fft_dominant_frequencies_from_embeddings(&embeddings, &config)\n            .expect_err(\"expected flat embeddings to fail\");\n\n        assert!(err.contains(\"modulus frequency\") || err.contains(\"peak to median\"));\n    }\n\n    #[test]\n    fn fft_dominant_frequency_fails_on_weak_peaks() {\n        // Create embeddings with many frequencies at similar magnitudes (no clear peaks)\n        let modulus = 113;\n        let vocab_size = 114;\n        let embedding_dim = 128;\n        let tau = std::f64::consts::PI * 2.0;\n\n        let mut embeddings = Vec::with_capacity(vocab_size);\n        for token in 0..vocab_size {\n            let mut embedding = Vec::with_capacity(embedding_dim);\n            for dim in 0..embedding_dim {\n                // Mix many frequencies with similar amplitudes\n                let mut value = 0.0;\n                for freq in 1..10 {\n                    let angle =\n                        tau * freq as f64 * token as f64 / vocab_size as f64 + dim as f64 * 0.01;\n                    value += angle.sin() / 10.0;\n                }\n                embedding.push(value);\n            }\n            embeddings.push(embedding);\n        }\n\n        let config = FFTDominantFrequencyConfig {\n            modulus,\n            min_modulus_frequency_fraction: 0.15,\n            max_spectrum_entropy: 0.85,\n            min_peak_to_median_ratio: 2.0,\n        };\n        let err = verify_fft_dominant_frequencies_from_embeddings(&embeddings, &config)\n            .expect_err(\"expected weak peaks to fail\");\n\n        assert!(\n            err.contains(\"modulus frequency\") || err.contains(\"spectrum entropy\")\n                || err.contains(\"peak to median\")\n        );\n    }\n}\n","structuredPatch":[{"oldStart":865,"oldLines":6,"newStart":865,"newLines":8,"lines":["     })"," }"," ","+// Helper functions for embedding clock geometry verification","+#[allow(dead_code)]"," fn normalize_vector(vec: &[f64]) -> Result<Vec<f64>, String> {","     let norm = vec.iter().map(|v| v * v).sum::<f64>().sqrt();","     if norm == 0.0 {"]},{"oldStart":873,"oldLines":10,"newStart":875,"newLines":12,"lines":["     Ok(vec.iter().map(|v| v / norm).collect())"," }"," ","+#[allow(dead_code)]"," fn dot(a: &[f64], b: &[f64]) -> f64 {","     a.iter().zip(b.iter()).map(|(x, y)| x * y).sum()"," }"," ","+#[allow(dead_code)]"," fn wrap_angle(angle: f64) -> f64 {","     let tau = std::f64::consts::PI * 2.0;","     let mut wrapped = angle % tau;"]},{"oldStart":886,"oldLines":6,"newStart":890,"newLines":7,"lines":["     wrapped"," }"," ","+#[allow(dead_code)]"," fn wrap_angle_signed(angle: f64) -> f64 {","     let tau = std::f64::consts::PI * 2.0;","     let mut wrapped = (angle + std::f64::consts::PI) % tau;"]},{"oldStart":895,"oldLines":6,"newStart":900,"newLines":7,"lines":["     wrapped - std::f64::consts::PI"," }"," ","+#[allow(dead_code)]"," fn angular_coverage(angles: &[f64]) -> f64 {","     if angles.is_empty() {","         return 0.0;"]}],"userModified":false,"replaceAll":false}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_017AukwjJv5sXnPxsNLt7Yde","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01DSzvrV66MzCMmNa2EuWf5L","name":"Edit","input":{"replace_all":false,"file_path":"/Users/admin/MechInt/src/analysis.rs","old_string":"fn order_fraction(angles: &[f64], modulus: usize) -> f64 {","new_string":"#[allow(dead_code)]\nfn order_fraction(angles: &[f64], modulus: usize) -> f64 {"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":1527,"cache_read_input_tokens":53324,"cache_creation":{"ephemeral_5m_input_tokens":1527,"ephemeral_1h_input_tokens":0},"output_tokens":176,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"6d087cbf-8e4d-4082-9f09-058087c93370"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01DSzvrV66MzCMmNa2EuWf5L","type":"tool_result","content":"The file /Users/admin/MechInt/src/analysis.rs has been updated. Here's the result of running `cat -n` on a snippet of the edited file:\n   923â†’\n   924â†’    (tau - max_gap) / tau\n   925â†’}\n   926â†’\n   927â†’#[allow(dead_code)]\n   928â†’fn order_fraction(angles: &[f64], modulus: usize) -> f64 {\n   929â†’    if angles.len() < 2 || modulus == 0 {\n   930â†’        return 0.0;\n   931â†’    }\n   932â†’    let mut indexed: Vec<(usize, f64)> = angles.iter().copied().enumerate().collect();"}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"3f910dcc-b294-4969-96b6-da61f462f8dc","tool_use_result":{"filePath":"/Users/admin/MechInt/src/analysis.rs","oldString":"fn order_fraction(angles: &[f64], modulus: usize) -> f64 {","newString":"#[allow(dead_code)]\nfn order_fraction(angles: &[f64], modulus: usize) -> f64 {","originalFile":"use burn::{\n    data::dataset::Dataset,\n    nn::loss::CrossEntropyLossConfig,\n    tensor::{backend::Backend, Int, Tensor, TensorData},\n};\nuse rustfft::{num_complex::Complex, FftPlanner};\nuse serde::{Deserialize, Serialize};\nuse std::collections::{HashMap, HashSet};\n\nuse crate::{data::ModularAdditionDataset, model::Transformer};\n\n/// FFT analysis results for embeddings\n#[derive(Debug, Serialize, Deserialize)]\npub struct FFTAnalysis {\n    pub vocab_size: usize,\n    pub embedding_dim: usize,\n    pub dominant_frequencies: Vec<(usize, f64)>, // (token_id, dominant_frequency_index)\n    pub frequency_magnitudes: HashMap<usize, Vec<f64>>, // token_id -> magnitude spectrum\n}\n\n/// Analyze the learned token embeddings using FFT\npub fn analyze_embeddings_fft<B: Backend>(model: &Transformer<B>) -> FFTAnalysis {\n    println!(\"ðŸ” Performing FFT analysis on learned embeddings...\");\n\n    let vocab_size = ModularAdditionDataset::vocab_size();\n    let embedding_dim = 128;\n\n    // Extract token embeddings by forward passing through the embedding layer\n    let mut frequency_magnitudes = HashMap::new();\n    let mut dominant_frequencies = Vec::new();\n\n    for token_id in 0..vocab_size {\n        // Get the embedding for this token\n        let embedding = get_token_embedding(model, token_id);\n\n        // Compute FFT on the embedding vector\n        let fft_result = compute_fft(&embedding);\n\n        // Find dominant frequency\n        let dominant_freq_idx = find_dominant_frequency(&fft_result);\n        dominant_frequencies.push((token_id, dominant_freq_idx as f64));\n\n        // Store magnitude spectrum\n        frequency_magnitudes.insert(token_id, fft_result);\n\n        if token_id % 10 == 0 {\n            println!(\"  Token {:2}: dominant frequency = {}\", token_id, dominant_freq_idx);\n        }\n    }\n\n    println!(\"âœ… FFT analysis complete!\");\n    println!();\n    println!(\"ðŸ“Š Dominant Frequency Analysis:\");\n\n    // Analyze patterns\n    let mut freq_histogram: HashMap<usize, usize> = HashMap::new();\n    for (_, freq) in &dominant_frequencies {\n        *freq_histogram.entry(*freq as usize).or_insert(0) += 1;\n    }\n\n    let mut freq_counts: Vec<_> = freq_histogram.iter().collect();\n    freq_counts.sort_by(|a, b| b.1.cmp(a.1));\n\n    println!(\"  Most common dominant frequencies:\");\n    for (freq, count) in freq_counts.iter().take(5) {\n        println!(\"    Frequency {}: {} tokens\", freq, count);\n    }\n\n    // Check for modular structure (expected for mod p)\n    println!();\n    println!(\"ðŸ”¬ Checking for modular structure:\");\n    let modulus = ModularAdditionDataset::modulus();\n    if freq_histogram.contains_key(&modulus) {\n        println!(\n            \"  âœ“ Frequency {} (matching modulus) appears in embeddings\",\n            modulus\n        );\n    }\n\n    FFTAnalysis {\n        vocab_size,\n        embedding_dim,\n        dominant_frequencies,\n        frequency_magnitudes,\n    }\n}\n\n/// Extract the token embedding for a single token\nfn get_token_embedding<B: Backend>(model: &Transformer<B>, token_id: usize) -> Vec<f64> {\n    // Get the token embedding directly from the model\n    let embedding_tensor = model.get_token_embedding(token_id);\n\n    // Convert to vector - shape is [1, 1, embedding_dim], we want [embedding_dim]\n    let embedding_data: Vec<f32> = embedding_tensor.into_data().to_vec().unwrap();\n\n    // Convert to f64\n    embedding_data.iter().map(|&x| x as f64).collect()\n}\n\n/// Compute FFT on a vector of real values\nfn compute_fft(data: &[f64]) -> Vec<f64> {\n    let mut planner = FftPlanner::new();\n    let fft = planner.plan_fft_forward(data.len());\n\n    // Convert to complex numbers\n    let mut buffer: Vec<Complex<f64>> = data\n        .iter()\n        .map(|&x| Complex::new(x, 0.0))\n        .collect();\n\n    // Perform FFT\n    fft.process(&mut buffer);\n\n    // Compute magnitudes\n    buffer.iter().map(|c| c.norm()).collect()\n}\n\nfn inverse_fft(data: &mut [Complex<f64>]) {\n    let mut planner = FftPlanner::new();\n    let fft = planner.plan_fft_inverse(data.len());\n    fft.process(data);\n}\n\nfn top_k_frequency_indices(magnitudes: &[f64], k: usize) -> Vec<usize> {\n    let mut indexed: Vec<(usize, f64)> = magnitudes.iter().copied().enumerate().collect();\n    indexed.sort_by(|(idx_a, mag_a), (idx_b, mag_b)| {\n        mag_b\n            .partial_cmp(mag_a)\n            .unwrap_or(std::cmp::Ordering::Equal)\n            .then_with(|| idx_a.cmp(idx_b))\n    });\n\n    let keep = k.min(indexed.len());\n    indexed.iter().take(keep).map(|(idx, _)| *idx).collect()\n}\n\nfn restrict_signal_to_top_k_frequencies(signal: &[f64], top_k: usize) -> Vec<f64> {\n    if signal.is_empty() {\n        return Vec::new();\n    }\n\n    if top_k == 0 {\n        return vec![0.0; signal.len()];\n    }\n\n    let mut planner = FftPlanner::new();\n    let fft = planner.plan_fft_forward(signal.len());\n\n    let mut buffer: Vec<Complex<f64>> = signal\n        .iter()\n        .map(|&x| Complex::new(x, 0.0))\n        .collect();\n    fft.process(&mut buffer);\n\n    let magnitudes: Vec<f64> = buffer.iter().map(|c| c.norm()).collect();\n    let keep = top_k_frequency_indices(&magnitudes, top_k);\n    let keep_set: HashSet<usize> = keep.into_iter().collect();\n\n    for (idx, value) in buffer.iter_mut().enumerate() {\n        if !keep_set.contains(&idx) {\n            *value = Complex::new(0.0, 0.0);\n        }\n    }\n\n    inverse_fft(&mut buffer);\n\n    let scale = 1.0 / signal.len() as f64;\n    buffer.iter().map(|c| c.re * scale).collect()\n}\n\nfn exclude_top_k_frequencies(signal: &[f64], top_k: usize) -> Vec<f64> {\n    if signal.is_empty() {\n        return Vec::new();\n    }\n\n    if top_k == 0 {\n        return signal.to_vec();\n    }\n\n    if top_k >= signal.len() {\n        return vec![0.0; signal.len()];\n    }\n\n    let mut planner = FftPlanner::new();\n    let fft = planner.plan_fft_forward(signal.len());\n\n    let mut buffer: Vec<Complex<f64>> = signal\n        .iter()\n        .map(|&x| Complex::new(x, 0.0))\n        .collect();\n    fft.process(&mut buffer);\n\n    let magnitudes: Vec<f64> = buffer.iter().map(|c| c.norm()).collect();\n    let remove = top_k_frequency_indices(&magnitudes, top_k);\n    let remove_set: HashSet<usize> = remove.into_iter().collect();\n\n    for (idx, value) in buffer.iter_mut().enumerate() {\n        if remove_set.contains(&idx) {\n            *value = Complex::new(0.0, 0.0);\n        }\n    }\n\n    inverse_fft(&mut buffer);\n\n    let scale = 1.0 / signal.len() as f64;\n    buffer.iter().map(|c| c.re * scale).collect()\n}\n\nfn restricted_token_embedding_weights<B: Backend>(\n    model: &Transformer<B>,\n    top_k: usize,\n) -> Vec<f32> {\n    let weight = model.token_embedding_weights();\n    let [vocab_size, d_model] = weight.dims();\n    let weight_data: Vec<f32> = weight.into_data().to_vec().unwrap();\n    let mut restricted = vec![0.0f32; weight_data.len()];\n\n    for dim in 0..d_model {\n        let mut column = Vec::with_capacity(vocab_size);\n        for token in 0..vocab_size {\n            column.push(weight_data[token * d_model + dim] as f64);\n        }\n        let filtered = restrict_signal_to_top_k_frequencies(&column, top_k);\n        for token in 0..vocab_size {\n            restricted[token * d_model + dim] = filtered[token] as f32;\n        }\n    }\n\n    restricted\n}\n\nfn excluded_token_embedding_weights<B: Backend>(\n    model: &Transformer<B>,\n    top_k: usize,\n) -> Vec<f32> {\n    let weight = model.token_embedding_weights();\n    let [vocab_size, d_model] = weight.dims();\n    let weight_data: Vec<f32> = weight.into_data().to_vec().unwrap();\n    let mut excluded = vec![0.0f32; weight_data.len()];\n\n    for dim in 0..d_model {\n        let mut column = Vec::with_capacity(vocab_size);\n        for token in 0..vocab_size {\n            column.push(weight_data[token * d_model + dim] as f64);\n        }\n        let filtered = exclude_top_k_frequencies(&column, top_k);\n        for token in 0..vocab_size {\n            excluded[token * d_model + dim] = filtered[token] as f32;\n        }\n    }\n\n    excluded\n}\n\npub fn compute_restricted_loss<B: Backend>(\n    model: &Transformer<B>,\n    dataset: &ModularAdditionDataset,\n    device: &B::Device,\n    top_k: usize,\n    batch_size: usize,\n) -> Result<f64, String> {\n    if batch_size == 0 {\n        return Err(\"batch_size must be greater than 0\".to_string());\n    }\n\n    let restricted_weights = restricted_token_embedding_weights(model, top_k);\n    let [vocab_size, d_model] = model.token_embedding_weights().dims();\n    let token_weights = Tensor::<B, 2>::from_data(\n        TensorData::new(restricted_weights, [vocab_size, d_model]),\n        device,\n    );\n\n    let loss_fn = CrossEntropyLossConfig::new().init(device);\n    let mut total_loss = 0.0f64;\n    let mut total_batches = 0usize;\n\n    for batch_start in (0..dataset.len()).step_by(batch_size) {\n        let batch_end = (batch_start + batch_size).min(dataset.len());\n        let batch_len = batch_end - batch_start;\n        if batch_len == 0 {\n            break;\n        }\n\n        let mut inputs_vec = Vec::with_capacity(batch_len * 3);\n        let mut targets_vec = Vec::with_capacity(batch_len);\n\n        for idx in batch_start..batch_end {\n            let (input, target) = dataset.get(idx).ok_or_else(|| {\n                format!(\"missing dataset sample at index {}\", idx)\n            })?;\n            inputs_vec.extend(input.iter().map(|value| *value as i32));\n            targets_vec.push(target as i32);\n        }\n\n        let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n            .reshape([batch_len, 3]);\n        let targets = Tensor::<B, 1, Int>::from_ints(targets_vec.as_slice(), device);\n\n        let logits = model.forward_with_token_weights(inputs, token_weights.clone());\n        let loss = loss_fn.forward(logits, targets);\n        let loss_value: f64 = loss.into_data().to_vec().unwrap().get(0).copied().unwrap_or(0.0);\n        total_loss += loss_value;\n        total_batches += 1;\n    }\n\n    if total_batches == 0 {\n        return Err(\"no batches computed for restricted loss\".to_string());\n    }\n\n    Ok(total_loss / total_batches as f64)\n}\n\npub fn compute_excluded_loss<B: Backend>(\n    model: &Transformer<B>,\n    dataset: &ModularAdditionDataset,\n    device: &B::Device,\n    top_k: usize,\n    batch_size: usize,\n) -> Result<f64, String> {\n    if batch_size == 0 {\n        return Err(\"batch_size must be greater than 0\".to_string());\n    }\n\n    let excluded_weights = excluded_token_embedding_weights(model, top_k);\n    let [vocab_size, d_model] = model.token_embedding_weights().dims();\n    let token_weights = Tensor::<B, 2>::from_data(\n        TensorData::new(excluded_weights, [vocab_size, d_model]),\n        device,\n    );\n\n    let loss_fn = CrossEntropyLossConfig::new().init(device);\n    let mut total_loss = 0.0f64;\n    let mut total_batches = 0usize;\n\n    for batch_start in (0..dataset.len()).step_by(batch_size) {\n        let batch_end = (batch_start + batch_size).min(dataset.len());\n        let batch_len = batch_end - batch_start;\n        if batch_len == 0 {\n            break;\n        }\n\n        let mut inputs_vec = Vec::with_capacity(batch_len * 3);\n        let mut targets_vec = Vec::with_capacity(batch_len);\n\n        for idx in batch_start..batch_end {\n            let (input, target) = dataset.get(idx).ok_or_else(|| {\n                format!(\"missing dataset sample at index {}\", idx)\n            })?;\n            inputs_vec.extend(input.iter().map(|value| *value as i32));\n            targets_vec.push(target as i32);\n        }\n\n        let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n            .reshape([batch_len, 3]);\n        let targets = Tensor::<B, 1, Int>::from_ints(targets_vec.as_slice(), device);\n\n        let logits = model.forward_with_token_weights(inputs, token_weights.clone());\n        let loss = loss_fn.forward(logits, targets);\n        let loss_value: f64 = loss.into_data().to_vec().unwrap().get(0).copied().unwrap_or(0.0);\n        total_loss += loss_value;\n        total_batches += 1;\n    }\n\n    if total_batches == 0 {\n        return Err(\"no batches computed for excluded loss\".to_string());\n    }\n\n    Ok(total_loss / total_batches as f64)\n}\n\n/// Find the index of the dominant frequency (excluding DC component at index 0)\nfn find_dominant_frequency(magnitudes: &[f64]) -> usize {\n    magnitudes\n        .iter()\n        .enumerate()\n        .skip(1) // Skip DC component\n        .max_by(|(_, a), (_, b)| {\n            a.partial_cmp(b).unwrap_or(std::cmp::Ordering::Equal)\n        })\n        .map(|(idx, _)| idx)\n        .unwrap_or(0)\n}\n\n/// Save FFT analysis to JSON file\npub fn save_fft_analysis(analysis: &FFTAnalysis, filename: &str) -> std::io::Result<()> {\n    let json = serde_json::to_string_pretty(analysis)?;\n    std::fs::write(filename, json)?;\n    println!(\"ðŸ’¾ FFT analysis saved to: {}\", filename);\n    Ok(())\n}\n\n/// Track training and validation loss over time\n#[derive(Debug, Serialize, Deserialize)]\npub struct LossHistory {\n    pub train_snapshots: Vec<(usize, f64)>, // (step, loss)\n    pub val_snapshots: Vec<(usize, f64)>,   // (step, loss)\n}\n\n/// Track restricted loss over time (only top-k frequencies kept)\n#[derive(Debug, Serialize, Deserialize)]\npub struct RestrictedLossHistory {\n    pub snapshots: Vec<(usize, f64)>, // (step, loss)\n}\n\nimpl RestrictedLossHistory {\n    pub fn new() -> Self {\n        Self {\n            snapshots: Vec::new(),\n        }\n    }\n\n    pub fn add_snapshot(&mut self, step: usize, loss: f64) {\n        self.snapshots.push((step, loss));\n    }\n\n    pub fn save(&self, filename: &str) -> std::io::Result<()> {\n        let json = serde_json::to_string_pretty(self)?;\n        std::fs::write(filename, json)?;\n        println!(\"ðŸ’¾ Restricted loss history saved to: {}\", filename);\n        Ok(())\n    }\n}\n\n/// Track excluded loss over time (top-k frequencies removed)\n#[derive(Debug, Serialize, Deserialize)]\npub struct ExcludedLossHistory {\n    pub snapshots: Vec<(usize, f64)>, // (step, loss)\n}\n\nimpl ExcludedLossHistory {\n    pub fn new() -> Self {\n        Self {\n            snapshots: Vec::new(),\n        }\n    }\n\n    pub fn add_snapshot(&mut self, step: usize, loss: f64) {\n        self.snapshots.push((step, loss));\n    }\n\n    pub fn save(&self, filename: &str) -> std::io::Result<()> {\n        let json = serde_json::to_string_pretty(self)?;\n        std::fs::write(filename, json)?;\n        println!(\"ðŸ’¾ Excluded loss history saved to: {}\", filename);\n        Ok(())\n    }\n}\n\nimpl LossHistory {\n    pub fn new() -> Self {\n        Self {\n            train_snapshots: Vec::new(),\n            val_snapshots: Vec::new(),\n        }\n    }\n\n    pub fn add_snapshot(&mut self, step: usize, train_loss: f64, val_loss: f64) {\n        self.train_snapshots.push((step, train_loss));\n        self.val_snapshots.push((step, val_loss));\n    }\n\n    pub fn save(&self, filename: &str) -> std::io::Result<()> {\n        let json = serde_json::to_string_pretty(self)?;\n        std::fs::write(filename, json)?;\n        println!(\"ðŸ’¾ Loss history saved to: {}\", filename);\n        Ok(())\n    }\n}\n\n/// Legacy: Single loss tracking (kept for backward compatibility)\n#[derive(Debug, Serialize, Deserialize, Clone)]\npub struct WeightNormSnapshot {\n    pub step: usize,\n    pub total_norm: f64,\n}\n\n/// Legacy: Track weight norms over training (actually stores loss)\n#[derive(Debug, Serialize, Deserialize)]\npub struct WeightNormHistory {\n    pub snapshots: Vec<WeightNormSnapshot>,\n}\n\nimpl WeightNormHistory {\n    pub fn new() -> Self {\n        Self {\n            snapshots: Vec::new(),\n        }\n    }\n\n    pub fn add_snapshot(&mut self, step: usize, norm: f64) {\n        self.snapshots.push(WeightNormSnapshot {\n            step,\n            total_norm: norm,\n        });\n    }\n\n    pub fn save(&self, filename: &str) -> std::io::Result<()> {\n        let json = serde_json::to_string_pretty(self)?;\n        std::fs::write(filename, json)?;\n        println!(\"ðŸ’¾ Weight norm history saved to: {}\", filename);\n        Ok(())\n    }\n}\n\n/// Compute the L2 norm of all model parameters\n/// This returns the Frobenius norm (sum of squared weights) across all parameters\n/// Note: This is a simplified implementation that computes the norm of token embeddings,\n/// which contain the majority of the model parameters. For a complete implementation,\n/// we would need to access all Linear layer weights via the Record trait.\npub fn compute_model_weight_norm<B: Backend>(model: &Transformer<B>) -> f64 {\n    let mut total_norm_squared = 0.0f64;\n\n    // Token embedding weights: [vocab_size, d_model]\n    let token_emb_weights = model.token_embedding_weights();\n    let token_emb_data: Vec<f32> = token_emb_weights.into_data().to_vec().unwrap();\n    for &weight in &token_emb_data {\n        total_norm_squared += (weight as f64) * (weight as f64);\n    }\n\n    // Note: Position embeddings, attention weights, MLP weights, and LM head weights\n    // are not included in this simplified implementation. For weight decay verification,\n    // tracking the token embeddings (which are the largest weight matrices) is sufficient\n    // to observe the monotonic decrease under weight decay.\n\n    total_norm_squared.sqrt()\n}\n\n/// Extract all token embeddings as a matrix [vocab_size, embedding_dim]\npub fn extract_all_embeddings<B: Backend>(model: &Transformer<B>) -> Vec<Vec<f64>> {\n    let vocab_size = ModularAdditionDataset::vocab_size();\n    let mut embeddings = Vec::with_capacity(vocab_size);\n\n    for token_id in 0..vocab_size {\n        embeddings.push(get_token_embedding(model, token_id));\n    }\n\n    embeddings\n}\n\n/// Verify that FFT analysis of embeddings shows dominant frequencies\n/// that align with the modulus structure and nontrivial spectral patterns\n///\n/// NOTE: Reserved for manual verification or future integration into analysis pipeline\n#[allow(dead_code)]\npub fn verify_fft_dominant_frequencies<B: Backend>(\n    model: &Transformer<B>,\n    config: &FFTDominantFrequencyConfig,\n) -> Result<FFTDominantFrequencyReport, String> {\n    let embeddings = extract_all_embeddings(model);\n    verify_fft_dominant_frequencies_from_embeddings(&embeddings, config)\n}\n\n/// Verify FFT dominant frequencies from pre-extracted embeddings\n///\n/// NOTE: Reserved for manual verification or future integration into analysis pipeline\n#[allow(dead_code)]\npub fn verify_fft_dominant_frequencies_from_embeddings(\n    embeddings: &[Vec<f64>],\n    config: &FFTDominantFrequencyConfig,\n) -> Result<FFTDominantFrequencyReport, String> {\n    if embeddings.is_empty() {\n        return Err(\"no embeddings provided\".to_string());\n    }\n\n    let vocab_size = embeddings.len();\n    if vocab_size < config.modulus {\n        return Err(format!(\n            \"embedding count {} less than modulus {}\",\n            vocab_size, config.modulus\n        ));\n    }\n\n    // Analyze each embedding dimension and collect dominant frequencies\n    let mut frequency_histogram: HashMap<usize, usize> = HashMap::new();\n    let embedding_dim = embeddings[0].len();\n\n    // For each embedding dimension, compute FFT across all tokens and find dominant frequency\n    for dim in 0..embedding_dim {\n        let mut signal = Vec::with_capacity(vocab_size);\n        for embedding in embeddings.iter().take(vocab_size) {\n            if dim >= embedding.len() {\n                return Err(format!(\n                    \"embedding dimension mismatch at dim {}\",\n                    dim\n                ));\n            }\n            signal.push(embedding[dim]);\n        }\n\n        // Compute FFT and find dominant frequency\n        let magnitudes = compute_fft(&signal);\n        let dominant_freq = find_dominant_frequency(&magnitudes);\n        *frequency_histogram.entry(dominant_freq).or_insert(0) += 1;\n    }\n\n    // Check 1: Modulus frequency should appear significantly\n    let modulus_count = frequency_histogram.get(&config.modulus).copied().unwrap_or(0);\n    let modulus_frequency_fraction = modulus_count as f64 / embedding_dim as f64;\n\n    if modulus_frequency_fraction < config.min_modulus_frequency_fraction {\n        return Err(format!(\n            \"modulus frequency {} appears in only {:.3} of dimensions (threshold {:.3})\",\n            config.modulus, modulus_frequency_fraction, config.min_modulus_frequency_fraction\n        ));\n    }\n\n    // Check 2: Spectrum should NOT resemble white noise (low entropy)\n    // High entropy = uniform distribution (white noise), low entropy = structured peaks\n    let total_dims = embedding_dim as f64;\n    let mut entropy = 0.0;\n    for &count in frequency_histogram.values() {\n        if count > 0 {\n            let p = count as f64 / total_dims;\n            entropy -= p * p.log2();\n        }\n    }\n    // Normalize entropy by max possible (log2 of unique frequency count)\n    let max_entropy = (frequency_histogram.len() as f64).log2();\n    let normalized_entropy = if max_entropy > 0.0 {\n        entropy / max_entropy\n    } else {\n        0.0\n    };\n\n    if normalized_entropy > config.max_spectrum_entropy {\n        return Err(format!(\n            \"spectrum entropy {:.3} too high (white noise threshold {:.3})\",\n            normalized_entropy, config.max_spectrum_entropy\n        ));\n    }\n\n    // Check 3: Peak frequencies should be significantly stronger than median\n    let mut counts: Vec<usize> = frequency_histogram.values().copied().collect();\n    counts.sort_unstable();\n    let median_count = if counts.is_empty() {\n        0\n    } else {\n        counts[counts.len() / 2]\n    };\n    let peak_count = counts.last().copied().unwrap_or(0);\n    let peak_to_median_ratio = if median_count > 0 {\n        peak_count as f64 / median_count as f64\n    } else {\n        f64::INFINITY\n    };\n\n    if peak_to_median_ratio < config.min_peak_to_median_ratio {\n        return Err(format!(\n            \"peak to median ratio {:.3} below threshold {:.3}\",\n            peak_to_median_ratio, config.min_peak_to_median_ratio\n        ));\n    }\n\n    // Collect top frequencies for the report\n    let mut freq_counts: Vec<(usize, usize)> = frequency_histogram.into_iter().collect();\n    freq_counts.sort_by(|a, b| b.1.cmp(&a.1));\n    let top_frequencies: Vec<(usize, usize)> = freq_counts.into_iter().take(5).collect();\n\n    Ok(FFTDominantFrequencyReport {\n        modulus_frequency_fraction,\n        spectrum_entropy: normalized_entropy,\n        peak_to_median_ratio,\n        top_frequencies,\n    })\n}\n\n#[derive(Debug, Clone)]\npub struct FFTDominantFrequencyConfig {\n    pub modulus: usize,\n    pub min_modulus_frequency_fraction: f64,\n    pub max_spectrum_entropy: f64,\n    pub min_peak_to_median_ratio: f64,\n}\n\nimpl FFTDominantFrequencyConfig {\n    /// NOTE: Reserved for future integration into analysis pipeline\n    #[allow(dead_code)]\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        Self {\n            modulus,\n            min_modulus_frequency_fraction: 0.15,\n            max_spectrum_entropy: 0.85,\n            min_peak_to_median_ratio: 2.0,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct FFTDominantFrequencyReport {\n    pub modulus_frequency_fraction: f64,\n    pub spectrum_entropy: f64,\n    pub peak_to_median_ratio: f64,\n    pub top_frequencies: Vec<(usize, usize)>,\n}\n\n#[derive(Debug, Clone)]\npub struct EmbeddingClockConfig {\n    pub max_radius_std_fraction: f64,\n    pub min_angular_coverage: f64,\n    pub max_mean_angle_error: f64,\n    pub min_order_fraction: f64,\n}\n\nimpl EmbeddingClockConfig {\n    /// NOTE: Reserved for future integration into analysis pipeline\n    #[allow(dead_code)]\n    pub fn default_for_modulus(_modulus: usize) -> Self {\n        Self {\n            max_radius_std_fraction: 0.1,\n            min_angular_coverage: 0.9,\n            max_mean_angle_error: 0.35,\n            min_order_fraction: 0.9,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct EmbeddingClockReport {\n    pub mean_radius: f64,\n    pub radius_std: f64,\n    pub angular_coverage: f64,\n    pub mean_angle_error: f64,\n    pub order_fraction: f64,\n}\n\n/// NOTE: Reserved for future integration into analysis pipeline\n#[allow(dead_code)]\npub fn project_embeddings_to_fourier_plane(\n    embeddings: &[Vec<f64>],\n    modulus: usize,\n) -> Result<Vec<(f64, f64)>, String> {\n    let p = modulus;\n    if embeddings.len() < p {\n        return Err(\"embedding matrix smaller than modulus\".to_string());\n    }\n    if p == 0 {\n        return Err(\"modulus must be positive\".to_string());\n    }\n    let d = embeddings[0].len();\n    if d == 0 {\n        return Err(\"embedding dimension must be positive\".to_string());\n    }\n    if !embeddings[..p].iter().all(|row| row.len() == d) {\n        return Err(\"embedding rows have inconsistent dimension\".to_string());\n    }\n\n    let tau = std::f64::consts::PI * 2.0;\n    let mut basis_cos = vec![0.0f64; d];\n    let mut basis_sin = vec![0.0f64; d];\n    let scale = 2.0 / p as f64;\n\n    for (token, embedding) in embeddings[..p].iter().enumerate() {\n        let angle = tau * token as f64 / p as f64;\n        let cos_val = angle.cos();\n        let sin_val = angle.sin();\n        for dim in 0..d {\n            basis_cos[dim] += cos_val * embedding[dim] * scale;\n            basis_sin[dim] += sin_val * embedding[dim] * scale;\n        }\n    }\n\n    let basis_cos = normalize_vector(&basis_cos)?;\n    let basis_sin = normalize_vector(&basis_sin)?;\n\n    let mut coords = Vec::with_capacity(p);\n    for embedding in embeddings[..p].iter() {\n        let x = dot(embedding, &basis_cos);\n        let y = dot(embedding, &basis_sin);\n        coords.push((x, y));\n    }\n\n    Ok(coords)\n}\n\n/// NOTE: Reserved for future integration into analysis pipeline\n#[allow(dead_code)]\npub fn verify_embedding_clock_geometry(\n    embeddings: &[Vec<f64>],\n    modulus: usize,\n    config: &EmbeddingClockConfig,\n) -> Result<EmbeddingClockReport, String> {\n    let coords = project_embeddings_to_fourier_plane(embeddings, modulus)?;\n    let p = coords.len();\n    if p == 0 {\n        return Err(\"no embedding coordinates computed\".to_string());\n    }\n\n    let mut radii = Vec::with_capacity(p);\n    let mut angles = Vec::with_capacity(p);\n    for (x, y) in &coords {\n        let radius = (x * x + y * y).sqrt();\n        radii.push(radius);\n        angles.push(y.atan2(*x));\n    }\n\n    let mean_radius = radii.iter().sum::<f64>() / p as f64;\n    if mean_radius == 0.0 {\n        return Err(\"mean radius is zero\".to_string());\n    }\n    let radius_var = radii\n        .iter()\n        .map(|r| (r - mean_radius) * (r - mean_radius))\n        .sum::<f64>()\n        / p as f64;\n    let radius_std = radius_var.sqrt();\n\n    if radius_std / mean_radius > config.max_radius_std_fraction {\n        return Err(format!(\n            \"radius variance too large (std {:.4}, mean {:.4})\",\n            radius_std, mean_radius\n        ));\n    }\n\n    let tau = std::f64::consts::PI * 2.0;\n    let (mut sum_cos, mut sum_sin) = (0.0f64, 0.0f64);\n    for (token, angle) in angles.iter().enumerate() {\n        let expected = tau * token as f64 / p as f64;\n        let delta = angle - expected;\n        sum_cos += delta.cos();\n        sum_sin += delta.sin();\n    }\n    let offset = sum_sin.atan2(sum_cos);\n\n    let mut adjusted_angles = Vec::with_capacity(p);\n    let mut angle_errors = Vec::with_capacity(p);\n    for (token, angle) in angles.iter().enumerate() {\n        let expected = tau * token as f64 / p as f64;\n        let mut adjusted = angle - offset;\n        adjusted = wrap_angle(adjusted);\n        adjusted_angles.push(adjusted);\n        let diff = wrap_angle_signed(adjusted - expected);\n        angle_errors.push(diff.abs());\n    }\n\n    let mean_angle_error = angle_errors.iter().sum::<f64>() / p as f64;\n    if mean_angle_error > config.max_mean_angle_error {\n        return Err(format!(\n            \"mean angular error too large ({:.4} rad)\",\n            mean_angle_error\n        ));\n    }\n\n    let angular_coverage = angular_coverage(&adjusted_angles);\n    if angular_coverage < config.min_angular_coverage {\n        return Err(format!(\n            \"angular coverage too small ({:.4})\",\n            angular_coverage\n        ));\n    }\n\n    let order_fraction = order_fraction(&adjusted_angles, p);\n    if order_fraction < config.min_order_fraction {\n        return Err(format!(\n            \"token ordering too weak ({:.4})\",\n            order_fraction\n        ));\n    }\n\n    Ok(EmbeddingClockReport {\n        mean_radius,\n        radius_std,\n        angular_coverage,\n        mean_angle_error,\n        order_fraction,\n    })\n}\n\n// Helper functions for embedding clock geometry verification\n#[allow(dead_code)]\nfn normalize_vector(vec: &[f64]) -> Result<Vec<f64>, String> {\n    let norm = vec.iter().map(|v| v * v).sum::<f64>().sqrt();\n    if norm == 0.0 {\n        return Err(\"basis vector norm is zero\".to_string());\n    }\n    Ok(vec.iter().map(|v| v / norm).collect())\n}\n\n#[allow(dead_code)]\nfn dot(a: &[f64], b: &[f64]) -> f64 {\n    a.iter().zip(b.iter()).map(|(x, y)| x * y).sum()\n}\n\n#[allow(dead_code)]\nfn wrap_angle(angle: f64) -> f64 {\n    let tau = std::f64::consts::PI * 2.0;\n    let mut wrapped = angle % tau;\n    if wrapped < 0.0 {\n        wrapped += tau;\n    }\n    wrapped\n}\n\n#[allow(dead_code)]\nfn wrap_angle_signed(angle: f64) -> f64 {\n    let tau = std::f64::consts::PI * 2.0;\n    let mut wrapped = (angle + std::f64::consts::PI) % tau;\n    if wrapped < 0.0 {\n        wrapped += tau;\n    }\n    wrapped - std::f64::consts::PI\n}\n\n#[allow(dead_code)]\nfn angular_coverage(angles: &[f64]) -> f64 {\n    if angles.is_empty() {\n        return 0.0;\n    }\n    let tau = std::f64::consts::PI * 2.0;\n    let mut sorted = angles.to_vec();\n    sorted.sort_by(|a, b| a.partial_cmp(b).unwrap_or(std::cmp::Ordering::Equal));\n\n    let mut max_gap = 0.0;\n    for pair in sorted.windows(2) {\n        let gap = pair[1] - pair[0];\n        if gap > max_gap {\n            max_gap = gap;\n        }\n    }\n    let wrap_gap = tau - (sorted[sorted.len() - 1] - sorted[0]);\n    if wrap_gap > max_gap {\n        max_gap = wrap_gap;\n    }\n\n    (tau - max_gap) / tau\n}\n\nfn order_fraction(angles: &[f64], modulus: usize) -> f64 {\n    if angles.len() < 2 || modulus == 0 {\n        return 0.0;\n    }\n    let mut indexed: Vec<(usize, f64)> = angles.iter().copied().enumerate().collect();\n    indexed.sort_by(|a, b| a.1.partial_cmp(&b.1).unwrap_or(std::cmp::Ordering::Equal));\n\n    let mut good = 0usize;\n    for window in indexed.windows(2) {\n        let current = window[0].0;\n        let next = window[1].0;\n        if (next + modulus - current) % modulus == 1 {\n            good += 1;\n        }\n    }\n    let last = indexed[indexed.len() - 1].0;\n    let first = indexed[0].0;\n    if (first + modulus - last) % modulus == 1 {\n        good += 1;\n    }\n\n    good as f64 / modulus as f64\n}\n\n/// Track training and validation accuracy over time\n#[derive(Debug, Serialize, Deserialize)]\npub struct AccuracyHistory {\n    pub train_snapshots: Vec<(usize, f32)>, // (step, accuracy)\n    pub val_snapshots: Vec<(usize, f32)>,   // (step, accuracy)\n}\n\nimpl AccuracyHistory {\n    pub fn new() -> Self {\n        Self {\n            train_snapshots: Vec::new(),\n            val_snapshots: Vec::new(),\n        }\n    }\n\n    pub fn add_snapshot(&mut self, step: usize, train_acc: f32, val_acc: f32) {\n        self.train_snapshots.push((step, train_acc));\n        self.val_snapshots.push((step, val_acc));\n    }\n\n    pub fn save(&self, filename: &str) -> std::io::Result<()> {\n        let json = serde_json::to_string_pretty(self)?;\n        std::fs::write(filename, json)?;\n        println!(\"ðŸ’¾ Accuracy history saved to: {}\", filename);\n        Ok(())\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    fn make_circle_embeddings(modulus: usize) -> Vec<Vec<f64>> {\n        let tau = std::f64::consts::PI * 2.0;\n        (0..modulus)\n            .map(|token| {\n                let angle = tau * token as f64 / modulus as f64;\n                vec![angle.cos(), angle.sin(), 0.0, 0.0]\n            })\n            .collect()\n    }\n\n    #[test]\n    fn top_k_frequency_indices_stable_with_ties() {\n        let magnitudes = vec![0.1, 1.0, 1.0, 0.5];\n        let first = top_k_frequency_indices(&magnitudes, 2);\n        let second = top_k_frequency_indices(&magnitudes, 2);\n        assert_eq!(first, vec![1, 2]);\n        assert_eq!(first, second);\n    }\n\n    #[test]\n    fn restrict_signal_zeroes_when_top_k_zero() {\n        let signal = vec![1.0, -1.0, 0.5, -0.5];\n        let filtered = restrict_signal_to_top_k_frequencies(&signal, 0);\n        assert_eq!(filtered, vec![0.0, 0.0, 0.0, 0.0]);\n    }\n\n    #[test]\n    fn exclude_signal_no_change_when_top_k_zero() {\n        let signal = vec![1.0, -1.0, 0.5, -0.5];\n        let filtered = exclude_top_k_frequencies(&signal, 0);\n        assert_eq!(filtered, signal);\n    }\n\n    #[test]\n    fn exclude_signal_zeroes_when_top_k_all() {\n        let signal = vec![1.0, -1.0, 0.5, -0.5];\n        let filtered = exclude_top_k_frequencies(&signal, signal.len());\n        assert_eq!(filtered, vec![0.0, 0.0, 0.0, 0.0]);\n    }\n\n    #[test]\n    fn embedding_clock_geometry_passes_on_clean_circle() {\n        let modulus = 13;\n        let embeddings = make_circle_embeddings(modulus);\n        let config = EmbeddingClockConfig::default_for_modulus(modulus);\n        let report =\n            verify_embedding_clock_geometry(&embeddings, modulus, &config).expect(\"expected pass\");\n        assert!(report.angular_coverage > 0.9);\n        assert!(report.order_fraction > 0.9);\n        assert!(report.mean_angle_error < 0.1);\n    }\n\n    #[test]\n    fn embedding_clock_geometry_flags_wrong_frequency() {\n        let modulus = 11;\n        let tau = std::f64::consts::PI * 2.0;\n        let embeddings: Vec<Vec<f64>> = (0..modulus)\n            .map(|token| {\n                let angle = tau * token as f64 / modulus as f64;\n                vec![(2.0 * angle).cos(), (2.0 * angle).sin(), 0.0, 0.0]\n            })\n            .collect();\n        let config = EmbeddingClockConfig {\n            max_radius_std_fraction: 0.5,\n            min_angular_coverage: 0.1,\n            max_mean_angle_error: 0.2,\n            min_order_fraction: 0.1,\n        };\n        let err = verify_embedding_clock_geometry(&embeddings, modulus, &config)\n            .expect_err(\"expected ordering verification to fail\");\n        assert!(err.contains(\"mean angular error\"));\n    }\n\n    #[test]\n    fn fft_dominant_frequency_passes_on_modulus_structure() {\n        // Create embeddings with strong modulus-113 frequency component\n        // Mix in some other frequencies to get a realistic peak-to-median ratio\n        let modulus = 113;\n        let vocab_size = 114; // 0-112 + equals token\n        let embedding_dim = 128;\n        let tau = std::f64::consts::PI * 2.0;\n\n        let mut embeddings = Vec::with_capacity(vocab_size);\n        for token in 0..vocab_size {\n            let mut embedding = Vec::with_capacity(embedding_dim);\n            for dim in 0..embedding_dim {\n                // 60% of dimensions have modulus frequency, 40% have other frequencies\n                let freq = if dim % 5 < 3 {\n                    modulus\n                } else {\n                    // Use various other frequencies for the remaining dims\n                    20 + (dim % 20)\n                };\n                let angle = tau * freq as f64 * token as f64 / vocab_size as f64\n                    + dim as f64 * 0.1;\n                embedding.push(angle.sin());\n            }\n            embeddings.push(embedding);\n        }\n\n        let config = FFTDominantFrequencyConfig::default_for_modulus(modulus);\n        let report = verify_fft_dominant_frequencies_from_embeddings(&embeddings, &config)\n            .expect(\"expected modulus structure to pass\");\n\n        assert!(report.modulus_frequency_fraction >= config.min_modulus_frequency_fraction);\n        assert!(report.spectrum_entropy <= config.max_spectrum_entropy);\n        assert!(report.peak_to_median_ratio >= config.min_peak_to_median_ratio);\n        assert!(report.top_frequencies.iter().any(|(freq, _)| *freq == modulus));\n    }\n\n    #[test]\n    fn fft_dominant_frequency_fails_on_white_noise() {\n        // Create random embeddings (white noise spectrum)\n        let modulus = 113;\n        let vocab_size = 114;\n        let embedding_dim = 128;\n\n        let mut embeddings = Vec::with_capacity(vocab_size);\n        for token in 0..vocab_size {\n            let mut embedding = Vec::with_capacity(embedding_dim);\n            for dim in 0..embedding_dim {\n                // Pseudo-random values\n                let value = ((token * 17 + dim * 31) % 100) as f64 / 50.0 - 1.0;\n                embedding.push(value);\n            }\n            embeddings.push(embedding);\n        }\n\n        let config = FFTDominantFrequencyConfig::default_for_modulus(modulus);\n        let err = verify_fft_dominant_frequencies_from_embeddings(&embeddings, &config)\n            .expect_err(\"expected white noise to fail\");\n\n        // Should fail on either modulus frequency fraction or spectrum entropy\n        assert!(\n            err.contains(\"modulus frequency\") || err.contains(\"spectrum entropy\")\n                || err.contains(\"peak to median\")\n        );\n    }\n\n    #[test]\n    fn fft_dominant_frequency_fails_on_wrong_frequency() {\n        // Create embeddings with dominant frequency at 57 (not 113)\n        let modulus = 113;\n        let vocab_size = 114;\n        let embedding_dim = 128;\n        let tau = std::f64::consts::PI * 2.0;\n        let wrong_frequency = 57;\n\n        let mut embeddings = Vec::with_capacity(vocab_size);\n        for token in 0..vocab_size {\n            let mut embedding = Vec::with_capacity(embedding_dim);\n            for dim in 0..embedding_dim {\n                // Each dimension has a sine wave with wrong frequency\n                let angle = tau * wrong_frequency as f64 * token as f64 / vocab_size as f64\n                    + dim as f64 * 0.1;\n                embedding.push(angle.sin());\n            }\n            embeddings.push(embedding);\n        }\n\n        let config = FFTDominantFrequencyConfig::default_for_modulus(modulus);\n        let err = verify_fft_dominant_frequencies_from_embeddings(&embeddings, &config)\n            .expect_err(\"expected wrong frequency to fail\");\n\n        assert!(err.contains(\"modulus frequency\"));\n    }\n\n    #[test]\n    fn fft_dominant_frequency_fails_on_flat_embeddings() {\n        // Create constant embeddings (DC component only, no structure)\n        let modulus = 113;\n        let vocab_size = 114;\n        let embedding_dim = 128;\n\n        let embeddings = vec![vec![1.5; embedding_dim]; vocab_size];\n\n        let config = FFTDominantFrequencyConfig::default_for_modulus(modulus);\n        let err = verify_fft_dominant_frequencies_from_embeddings(&embeddings, &config)\n            .expect_err(\"expected flat embeddings to fail\");\n\n        assert!(err.contains(\"modulus frequency\") || err.contains(\"peak to median\"));\n    }\n\n    #[test]\n    fn fft_dominant_frequency_fails_on_weak_peaks() {\n        // Create embeddings with many frequencies at similar magnitudes (no clear peaks)\n        let modulus = 113;\n        let vocab_size = 114;\n        let embedding_dim = 128;\n        let tau = std::f64::consts::PI * 2.0;\n\n        let mut embeddings = Vec::with_capacity(vocab_size);\n        for token in 0..vocab_size {\n            let mut embedding = Vec::with_capacity(embedding_dim);\n            for dim in 0..embedding_dim {\n                // Mix many frequencies with similar amplitudes\n                let mut value = 0.0;\n                for freq in 1..10 {\n                    let angle =\n                        tau * freq as f64 * token as f64 / vocab_size as f64 + dim as f64 * 0.01;\n                    value += angle.sin() / 10.0;\n                }\n                embedding.push(value);\n            }\n            embeddings.push(embedding);\n        }\n\n        let config = FFTDominantFrequencyConfig {\n            modulus,\n            min_modulus_frequency_fraction: 0.15,\n            max_spectrum_entropy: 0.85,\n            min_peak_to_median_ratio: 2.0,\n        };\n        let err = verify_fft_dominant_frequencies_from_embeddings(&embeddings, &config)\n            .expect_err(\"expected weak peaks to fail\");\n\n        assert!(\n            err.contains(\"modulus frequency\") || err.contains(\"spectrum entropy\")\n                || err.contains(\"peak to median\")\n        );\n    }\n}\n","structuredPatch":[{"oldStart":924,"oldLines":6,"newStart":924,"newLines":7,"lines":["     (tau - max_gap) / tau"," }"," ","+#[allow(dead_code)]"," fn order_fraction(angles: &[f64], modulus: usize) -> f64 {","     if angles.len() < 2 || modulus == 0 {","         return 0.0;"]}],"userModified":false,"replaceAll":false}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01Ly4cqyZczCSj2DdjKKJ3dh","type":"message","role":"assistant","content":[{"type":"text","text":"Now let me find and mark the add_snapshot method:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":455,"cache_read_input_tokens":54851,"cache_creation":{"ephemeral_5m_input_tokens":455,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"cbcc6409-95c4-4ab8-b71c-6bcef5391e7d"}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01Ly4cqyZczCSj2DdjKKJ3dh","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_0162nsDHkbgcEXw9GsV3epU7","name":"Bash","input":{"command":"grep -n \"fn add_snapshot\" src/analysis.rs","description":"Find add_snapshot methods"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":455,"cache_read_input_tokens":54851,"cache_creation":{"ephemeral_5m_input_tokens":455,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"9c2c44f9-607c-4eb8-b99e-aed21f044e2d"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_0162nsDHkbgcEXw9GsV3epU7","type":"tool_result","content":"412:    pub fn add_snapshot(&mut self, step: usize, loss: f64) {\n437:    pub fn add_snapshot(&mut self, step: usize, loss: f64) {\n457:    pub fn add_snapshot(&mut self, step: usize, train_loss: f64, val_loss: f64) {\n490:    pub fn add_snapshot(&mut self, step: usize, norm: f64) {\n967:    pub fn add_snapshot(&mut self, step: usize, train_acc: f32, val_acc: f32) {","is_error":false}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"b97d9541-7083-496f-bd58-9242538ff81b","tool_use_result":{"stdout":"412:    pub fn add_snapshot(&mut self, step: usize, loss: f64) {\n437:    pub fn add_snapshot(&mut self, step: usize, loss: f64) {\n457:    pub fn add_snapshot(&mut self, step: usize, train_loss: f64, val_loss: f64) {\n490:    pub fn add_snapshot(&mut self, step: usize, norm: f64) {\n967:    pub fn add_snapshot(&mut self, step: usize, train_acc: f32, val_acc: f32) {","stderr":"","interrupted":false,"isImage":false}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01ToaxcLa9hfkdHLnn5yeYzB","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_017Hrucnounryvxe6X244n62","name":"Read","input":{"file_path":"/Users/admin/MechInt/src/analysis.rs","offset":960,"limit":30}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":341,"cache_read_input_tokens":55306,"cache_creation":{"ephemeral_5m_input_tokens":341,"ephemeral_1h_input_tokens":0},"output_tokens":18,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"363eb578-3782-4d61-873e-f02b7cb2e4b5"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_017Hrucnounryvxe6X244n62","type":"tool_result","content":"   960â†’    pub fn new() -> Self {\n   961â†’        Self {\n   962â†’            train_snapshots: Vec::new(),\n   963â†’            val_snapshots: Vec::new(),\n   964â†’        }\n   965â†’    }\n   966â†’\n   967â†’    pub fn add_snapshot(&mut self, step: usize, train_acc: f32, val_acc: f32) {\n   968â†’        self.train_snapshots.push((step, train_acc));\n   969â†’        self.val_snapshots.push((step, val_acc));\n   970â†’    }\n   971â†’\n   972â†’    pub fn save(&self, filename: &str) -> std::io::Result<()> {\n   973â†’        let json = serde_json::to_string_pretty(self)?;\n   974â†’        std::fs::write(filename, json)?;\n   975â†’        println!(\"ðŸ’¾ Accuracy history saved to: {}\", filename);\n   976â†’        Ok(())\n   977â†’    }\n   978â†’}\n   979â†’\n   980â†’#[cfg(test)]\n   981â†’mod tests {\n   982â†’    use super::*;\n   983â†’\n   984â†’    fn make_circle_embeddings(modulus: usize) -> Vec<Vec<f64>> {\n   985â†’        let tau = std::f64::consts::PI * 2.0;\n   986â†’        (0..modulus)\n   987â†’            .map(|token| {\n   988â†’                let angle = tau * token as f64 / modulus as f64;\n   989â†’                vec![angle.cos(), angle.sin(), 0.0, 0.0]\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"144c8b26-5899-47a7-80fd-d21cd6de9c7f","tool_use_result":{"type":"text","file":{"filePath":"/Users/admin/MechInt/src/analysis.rs","content":"    pub fn new() -> Self {\n        Self {\n            train_snapshots: Vec::new(),\n            val_snapshots: Vec::new(),\n        }\n    }\n\n    pub fn add_snapshot(&mut self, step: usize, train_acc: f32, val_acc: f32) {\n        self.train_snapshots.push((step, train_acc));\n        self.val_snapshots.push((step, val_acc));\n    }\n\n    pub fn save(&self, filename: &str) -> std::io::Result<()> {\n        let json = serde_json::to_string_pretty(self)?;\n        std::fs::write(filename, json)?;\n        println!(\"ðŸ’¾ Accuracy history saved to: {}\", filename);\n        Ok(())\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    fn make_circle_embeddings(modulus: usize) -> Vec<Vec<f64>> {\n        let tau = std::f64::consts::PI * 2.0;\n        (0..modulus)\n            .map(|token| {\n                let angle = tau * token as f64 / modulus as f64;\n                vec![angle.cos(), angle.sin(), 0.0, 0.0]","numLines":30,"startLine":960,"totalLines":1206}}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01P2WyiX7CV1gnhRTZ4d7hje","type":"message","role":"assistant","content":[{"type":"text","text":"The add_snapshot method for AccuracyHistory is not used because the accuracy tracking is done differently in the current implementation. Let me mark it:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":598,"cache_read_input_tokens":55647,"cache_creation":{"ephemeral_5m_input_tokens":598,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"97244c1c-b575-44b6-98d5-fe9998f25ba3"}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01P2WyiX7CV1gnhRTZ4d7hje","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_013MudRXEc1GTdDwWGWmbNNk","name":"Edit","input":{"replace_all":false,"file_path":"/Users/admin/MechInt/src/analysis.rs","old_string":"    pub fn add_snapshot(&mut self, step: usize, train_acc: f32, val_acc: f32) {\n        self.train_snapshots.push((step, train_acc));\n        self.val_snapshots.push((step, val_acc));\n    }","new_string":"    /// NOTE: Currently unused - accuracy tracked directly in main loop\n    #[allow(dead_code)]\n    pub fn add_snapshot(&mut self, step: usize, train_acc: f32, val_acc: f32) {\n        self.train_snapshots.push((step, train_acc));\n        self.val_snapshots.push((step, val_acc));\n    }"}}],"stop_reason":"tool_use","stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":598,"cache_read_input_tokens":55647,"cache_creation":{"ephemeral_5m_input_tokens":598,"ephemeral_1h_input_tokens":0},"output_tokens":311,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"3edf2541-f2b0-4be0-b149-90166636e34d"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_013MudRXEc1GTdDwWGWmbNNk","type":"tool_result","content":"The file /Users/admin/MechInt/src/analysis.rs has been updated. Here's the result of running `cat -n` on a snippet of the edited file:\n   963â†’            val_snapshots: Vec::new(),\n   964â†’        }\n   965â†’    }\n   966â†’\n   967â†’    /// NOTE: Currently unused - accuracy tracked directly in main loop\n   968â†’    #[allow(dead_code)]\n   969â†’    pub fn add_snapshot(&mut self, step: usize, train_acc: f32, val_acc: f32) {\n   970â†’        self.train_snapshots.push((step, train_acc));\n   971â†’        self.val_snapshots.push((step, val_acc));\n   972â†’    }\n   973â†’\n   974â†’    pub fn save(&self, filename: &str) -> std::io::Result<()> {\n   975â†’        let json = serde_json::to_string_pretty(self)?;\n   976â†’        std::fs::write(filename, json)?;"}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"571ef4ef-167e-46c5-9403-9c4a47ea8a80","tool_use_result":{"filePath":"/Users/admin/MechInt/src/analysis.rs","oldString":"    pub fn add_snapshot(&mut self, step: usize, train_acc: f32, val_acc: f32) {\n        self.train_snapshots.push((step, train_acc));\n        self.val_snapshots.push((step, val_acc));\n    }","newString":"    /// NOTE: Currently unused - accuracy tracked directly in main loop\n    #[allow(dead_code)]\n    pub fn add_snapshot(&mut self, step: usize, train_acc: f32, val_acc: f32) {\n        self.train_snapshots.push((step, train_acc));\n        self.val_snapshots.push((step, val_acc));\n    }","originalFile":"use burn::{\n    data::dataset::Dataset,\n    nn::loss::CrossEntropyLossConfig,\n    tensor::{backend::Backend, Int, Tensor, TensorData},\n};\nuse rustfft::{num_complex::Complex, FftPlanner};\nuse serde::{Deserialize, Serialize};\nuse std::collections::{HashMap, HashSet};\n\nuse crate::{data::ModularAdditionDataset, model::Transformer};\n\n/// FFT analysis results for embeddings\n#[derive(Debug, Serialize, Deserialize)]\npub struct FFTAnalysis {\n    pub vocab_size: usize,\n    pub embedding_dim: usize,\n    pub dominant_frequencies: Vec<(usize, f64)>, // (token_id, dominant_frequency_index)\n    pub frequency_magnitudes: HashMap<usize, Vec<f64>>, // token_id -> magnitude spectrum\n}\n\n/// Analyze the learned token embeddings using FFT\npub fn analyze_embeddings_fft<B: Backend>(model: &Transformer<B>) -> FFTAnalysis {\n    println!(\"ðŸ” Performing FFT analysis on learned embeddings...\");\n\n    let vocab_size = ModularAdditionDataset::vocab_size();\n    let embedding_dim = 128;\n\n    // Extract token embeddings by forward passing through the embedding layer\n    let mut frequency_magnitudes = HashMap::new();\n    let mut dominant_frequencies = Vec::new();\n\n    for token_id in 0..vocab_size {\n        // Get the embedding for this token\n        let embedding = get_token_embedding(model, token_id);\n\n        // Compute FFT on the embedding vector\n        let fft_result = compute_fft(&embedding);\n\n        // Find dominant frequency\n        let dominant_freq_idx = find_dominant_frequency(&fft_result);\n        dominant_frequencies.push((token_id, dominant_freq_idx as f64));\n\n        // Store magnitude spectrum\n        frequency_magnitudes.insert(token_id, fft_result);\n\n        if token_id % 10 == 0 {\n            println!(\"  Token {:2}: dominant frequency = {}\", token_id, dominant_freq_idx);\n        }\n    }\n\n    println!(\"âœ… FFT analysis complete!\");\n    println!();\n    println!(\"ðŸ“Š Dominant Frequency Analysis:\");\n\n    // Analyze patterns\n    let mut freq_histogram: HashMap<usize, usize> = HashMap::new();\n    for (_, freq) in &dominant_frequencies {\n        *freq_histogram.entry(*freq as usize).or_insert(0) += 1;\n    }\n\n    let mut freq_counts: Vec<_> = freq_histogram.iter().collect();\n    freq_counts.sort_by(|a, b| b.1.cmp(a.1));\n\n    println!(\"  Most common dominant frequencies:\");\n    for (freq, count) in freq_counts.iter().take(5) {\n        println!(\"    Frequency {}: {} tokens\", freq, count);\n    }\n\n    // Check for modular structure (expected for mod p)\n    println!();\n    println!(\"ðŸ”¬ Checking for modular structure:\");\n    let modulus = ModularAdditionDataset::modulus();\n    if freq_histogram.contains_key(&modulus) {\n        println!(\n            \"  âœ“ Frequency {} (matching modulus) appears in embeddings\",\n            modulus\n        );\n    }\n\n    FFTAnalysis {\n        vocab_size,\n        embedding_dim,\n        dominant_frequencies,\n        frequency_magnitudes,\n    }\n}\n\n/// Extract the token embedding for a single token\nfn get_token_embedding<B: Backend>(model: &Transformer<B>, token_id: usize) -> Vec<f64> {\n    // Get the token embedding directly from the model\n    let embedding_tensor = model.get_token_embedding(token_id);\n\n    // Convert to vector - shape is [1, 1, embedding_dim], we want [embedding_dim]\n    let embedding_data: Vec<f32> = embedding_tensor.into_data().to_vec().unwrap();\n\n    // Convert to f64\n    embedding_data.iter().map(|&x| x as f64).collect()\n}\n\n/// Compute FFT on a vector of real values\nfn compute_fft(data: &[f64]) -> Vec<f64> {\n    let mut planner = FftPlanner::new();\n    let fft = planner.plan_fft_forward(data.len());\n\n    // Convert to complex numbers\n    let mut buffer: Vec<Complex<f64>> = data\n        .iter()\n        .map(|&x| Complex::new(x, 0.0))\n        .collect();\n\n    // Perform FFT\n    fft.process(&mut buffer);\n\n    // Compute magnitudes\n    buffer.iter().map(|c| c.norm()).collect()\n}\n\nfn inverse_fft(data: &mut [Complex<f64>]) {\n    let mut planner = FftPlanner::new();\n    let fft = planner.plan_fft_inverse(data.len());\n    fft.process(data);\n}\n\nfn top_k_frequency_indices(magnitudes: &[f64], k: usize) -> Vec<usize> {\n    let mut indexed: Vec<(usize, f64)> = magnitudes.iter().copied().enumerate().collect();\n    indexed.sort_by(|(idx_a, mag_a), (idx_b, mag_b)| {\n        mag_b\n            .partial_cmp(mag_a)\n            .unwrap_or(std::cmp::Ordering::Equal)\n            .then_with(|| idx_a.cmp(idx_b))\n    });\n\n    let keep = k.min(indexed.len());\n    indexed.iter().take(keep).map(|(idx, _)| *idx).collect()\n}\n\nfn restrict_signal_to_top_k_frequencies(signal: &[f64], top_k: usize) -> Vec<f64> {\n    if signal.is_empty() {\n        return Vec::new();\n    }\n\n    if top_k == 0 {\n        return vec![0.0; signal.len()];\n    }\n\n    let mut planner = FftPlanner::new();\n    let fft = planner.plan_fft_forward(signal.len());\n\n    let mut buffer: Vec<Complex<f64>> = signal\n        .iter()\n        .map(|&x| Complex::new(x, 0.0))\n        .collect();\n    fft.process(&mut buffer);\n\n    let magnitudes: Vec<f64> = buffer.iter().map(|c| c.norm()).collect();\n    let keep = top_k_frequency_indices(&magnitudes, top_k);\n    let keep_set: HashSet<usize> = keep.into_iter().collect();\n\n    for (idx, value) in buffer.iter_mut().enumerate() {\n        if !keep_set.contains(&idx) {\n            *value = Complex::new(0.0, 0.0);\n        }\n    }\n\n    inverse_fft(&mut buffer);\n\n    let scale = 1.0 / signal.len() as f64;\n    buffer.iter().map(|c| c.re * scale).collect()\n}\n\nfn exclude_top_k_frequencies(signal: &[f64], top_k: usize) -> Vec<f64> {\n    if signal.is_empty() {\n        return Vec::new();\n    }\n\n    if top_k == 0 {\n        return signal.to_vec();\n    }\n\n    if top_k >= signal.len() {\n        return vec![0.0; signal.len()];\n    }\n\n    let mut planner = FftPlanner::new();\n    let fft = planner.plan_fft_forward(signal.len());\n\n    let mut buffer: Vec<Complex<f64>> = signal\n        .iter()\n        .map(|&x| Complex::new(x, 0.0))\n        .collect();\n    fft.process(&mut buffer);\n\n    let magnitudes: Vec<f64> = buffer.iter().map(|c| c.norm()).collect();\n    let remove = top_k_frequency_indices(&magnitudes, top_k);\n    let remove_set: HashSet<usize> = remove.into_iter().collect();\n\n    for (idx, value) in buffer.iter_mut().enumerate() {\n        if remove_set.contains(&idx) {\n            *value = Complex::new(0.0, 0.0);\n        }\n    }\n\n    inverse_fft(&mut buffer);\n\n    let scale = 1.0 / signal.len() as f64;\n    buffer.iter().map(|c| c.re * scale).collect()\n}\n\nfn restricted_token_embedding_weights<B: Backend>(\n    model: &Transformer<B>,\n    top_k: usize,\n) -> Vec<f32> {\n    let weight = model.token_embedding_weights();\n    let [vocab_size, d_model] = weight.dims();\n    let weight_data: Vec<f32> = weight.into_data().to_vec().unwrap();\n    let mut restricted = vec![0.0f32; weight_data.len()];\n\n    for dim in 0..d_model {\n        let mut column = Vec::with_capacity(vocab_size);\n        for token in 0..vocab_size {\n            column.push(weight_data[token * d_model + dim] as f64);\n        }\n        let filtered = restrict_signal_to_top_k_frequencies(&column, top_k);\n        for token in 0..vocab_size {\n            restricted[token * d_model + dim] = filtered[token] as f32;\n        }\n    }\n\n    restricted\n}\n\nfn excluded_token_embedding_weights<B: Backend>(\n    model: &Transformer<B>,\n    top_k: usize,\n) -> Vec<f32> {\n    let weight = model.token_embedding_weights();\n    let [vocab_size, d_model] = weight.dims();\n    let weight_data: Vec<f32> = weight.into_data().to_vec().unwrap();\n    let mut excluded = vec![0.0f32; weight_data.len()];\n\n    for dim in 0..d_model {\n        let mut column = Vec::with_capacity(vocab_size);\n        for token in 0..vocab_size {\n            column.push(weight_data[token * d_model + dim] as f64);\n        }\n        let filtered = exclude_top_k_frequencies(&column, top_k);\n        for token in 0..vocab_size {\n            excluded[token * d_model + dim] = filtered[token] as f32;\n        }\n    }\n\n    excluded\n}\n\npub fn compute_restricted_loss<B: Backend>(\n    model: &Transformer<B>,\n    dataset: &ModularAdditionDataset,\n    device: &B::Device,\n    top_k: usize,\n    batch_size: usize,\n) -> Result<f64, String> {\n    if batch_size == 0 {\n        return Err(\"batch_size must be greater than 0\".to_string());\n    }\n\n    let restricted_weights = restricted_token_embedding_weights(model, top_k);\n    let [vocab_size, d_model] = model.token_embedding_weights().dims();\n    let token_weights = Tensor::<B, 2>::from_data(\n        TensorData::new(restricted_weights, [vocab_size, d_model]),\n        device,\n    );\n\n    let loss_fn = CrossEntropyLossConfig::new().init(device);\n    let mut total_loss = 0.0f64;\n    let mut total_batches = 0usize;\n\n    for batch_start in (0..dataset.len()).step_by(batch_size) {\n        let batch_end = (batch_start + batch_size).min(dataset.len());\n        let batch_len = batch_end - batch_start;\n        if batch_len == 0 {\n            break;\n        }\n\n        let mut inputs_vec = Vec::with_capacity(batch_len * 3);\n        let mut targets_vec = Vec::with_capacity(batch_len);\n\n        for idx in batch_start..batch_end {\n            let (input, target) = dataset.get(idx).ok_or_else(|| {\n                format!(\"missing dataset sample at index {}\", idx)\n            })?;\n            inputs_vec.extend(input.iter().map(|value| *value as i32));\n            targets_vec.push(target as i32);\n        }\n\n        let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n            .reshape([batch_len, 3]);\n        let targets = Tensor::<B, 1, Int>::from_ints(targets_vec.as_slice(), device);\n\n        let logits = model.forward_with_token_weights(inputs, token_weights.clone());\n        let loss = loss_fn.forward(logits, targets);\n        let loss_value: f64 = loss.into_data().to_vec().unwrap().get(0).copied().unwrap_or(0.0);\n        total_loss += loss_value;\n        total_batches += 1;\n    }\n\n    if total_batches == 0 {\n        return Err(\"no batches computed for restricted loss\".to_string());\n    }\n\n    Ok(total_loss / total_batches as f64)\n}\n\npub fn compute_excluded_loss<B: Backend>(\n    model: &Transformer<B>,\n    dataset: &ModularAdditionDataset,\n    device: &B::Device,\n    top_k: usize,\n    batch_size: usize,\n) -> Result<f64, String> {\n    if batch_size == 0 {\n        return Err(\"batch_size must be greater than 0\".to_string());\n    }\n\n    let excluded_weights = excluded_token_embedding_weights(model, top_k);\n    let [vocab_size, d_model] = model.token_embedding_weights().dims();\n    let token_weights = Tensor::<B, 2>::from_data(\n        TensorData::new(excluded_weights, [vocab_size, d_model]),\n        device,\n    );\n\n    let loss_fn = CrossEntropyLossConfig::new().init(device);\n    let mut total_loss = 0.0f64;\n    let mut total_batches = 0usize;\n\n    for batch_start in (0..dataset.len()).step_by(batch_size) {\n        let batch_end = (batch_start + batch_size).min(dataset.len());\n        let batch_len = batch_end - batch_start;\n        if batch_len == 0 {\n            break;\n        }\n\n        let mut inputs_vec = Vec::with_capacity(batch_len * 3);\n        let mut targets_vec = Vec::with_capacity(batch_len);\n\n        for idx in batch_start..batch_end {\n            let (input, target) = dataset.get(idx).ok_or_else(|| {\n                format!(\"missing dataset sample at index {}\", idx)\n            })?;\n            inputs_vec.extend(input.iter().map(|value| *value as i32));\n            targets_vec.push(target as i32);\n        }\n\n        let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n            .reshape([batch_len, 3]);\n        let targets = Tensor::<B, 1, Int>::from_ints(targets_vec.as_slice(), device);\n\n        let logits = model.forward_with_token_weights(inputs, token_weights.clone());\n        let loss = loss_fn.forward(logits, targets);\n        let loss_value: f64 = loss.into_data().to_vec().unwrap().get(0).copied().unwrap_or(0.0);\n        total_loss += loss_value;\n        total_batches += 1;\n    }\n\n    if total_batches == 0 {\n        return Err(\"no batches computed for excluded loss\".to_string());\n    }\n\n    Ok(total_loss / total_batches as f64)\n}\n\n/// Find the index of the dominant frequency (excluding DC component at index 0)\nfn find_dominant_frequency(magnitudes: &[f64]) -> usize {\n    magnitudes\n        .iter()\n        .enumerate()\n        .skip(1) // Skip DC component\n        .max_by(|(_, a), (_, b)| {\n            a.partial_cmp(b).unwrap_or(std::cmp::Ordering::Equal)\n        })\n        .map(|(idx, _)| idx)\n        .unwrap_or(0)\n}\n\n/// Save FFT analysis to JSON file\npub fn save_fft_analysis(analysis: &FFTAnalysis, filename: &str) -> std::io::Result<()> {\n    let json = serde_json::to_string_pretty(analysis)?;\n    std::fs::write(filename, json)?;\n    println!(\"ðŸ’¾ FFT analysis saved to: {}\", filename);\n    Ok(())\n}\n\n/// Track training and validation loss over time\n#[derive(Debug, Serialize, Deserialize)]\npub struct LossHistory {\n    pub train_snapshots: Vec<(usize, f64)>, // (step, loss)\n    pub val_snapshots: Vec<(usize, f64)>,   // (step, loss)\n}\n\n/// Track restricted loss over time (only top-k frequencies kept)\n#[derive(Debug, Serialize, Deserialize)]\npub struct RestrictedLossHistory {\n    pub snapshots: Vec<(usize, f64)>, // (step, loss)\n}\n\nimpl RestrictedLossHistory {\n    pub fn new() -> Self {\n        Self {\n            snapshots: Vec::new(),\n        }\n    }\n\n    pub fn add_snapshot(&mut self, step: usize, loss: f64) {\n        self.snapshots.push((step, loss));\n    }\n\n    pub fn save(&self, filename: &str) -> std::io::Result<()> {\n        let json = serde_json::to_string_pretty(self)?;\n        std::fs::write(filename, json)?;\n        println!(\"ðŸ’¾ Restricted loss history saved to: {}\", filename);\n        Ok(())\n    }\n}\n\n/// Track excluded loss over time (top-k frequencies removed)\n#[derive(Debug, Serialize, Deserialize)]\npub struct ExcludedLossHistory {\n    pub snapshots: Vec<(usize, f64)>, // (step, loss)\n}\n\nimpl ExcludedLossHistory {\n    pub fn new() -> Self {\n        Self {\n            snapshots: Vec::new(),\n        }\n    }\n\n    pub fn add_snapshot(&mut self, step: usize, loss: f64) {\n        self.snapshots.push((step, loss));\n    }\n\n    pub fn save(&self, filename: &str) -> std::io::Result<()> {\n        let json = serde_json::to_string_pretty(self)?;\n        std::fs::write(filename, json)?;\n        println!(\"ðŸ’¾ Excluded loss history saved to: {}\", filename);\n        Ok(())\n    }\n}\n\nimpl LossHistory {\n    pub fn new() -> Self {\n        Self {\n            train_snapshots: Vec::new(),\n            val_snapshots: Vec::new(),\n        }\n    }\n\n    pub fn add_snapshot(&mut self, step: usize, train_loss: f64, val_loss: f64) {\n        self.train_snapshots.push((step, train_loss));\n        self.val_snapshots.push((step, val_loss));\n    }\n\n    pub fn save(&self, filename: &str) -> std::io::Result<()> {\n        let json = serde_json::to_string_pretty(self)?;\n        std::fs::write(filename, json)?;\n        println!(\"ðŸ’¾ Loss history saved to: {}\", filename);\n        Ok(())\n    }\n}\n\n/// Legacy: Single loss tracking (kept for backward compatibility)\n#[derive(Debug, Serialize, Deserialize, Clone)]\npub struct WeightNormSnapshot {\n    pub step: usize,\n    pub total_norm: f64,\n}\n\n/// Legacy: Track weight norms over training (actually stores loss)\n#[derive(Debug, Serialize, Deserialize)]\npub struct WeightNormHistory {\n    pub snapshots: Vec<WeightNormSnapshot>,\n}\n\nimpl WeightNormHistory {\n    pub fn new() -> Self {\n        Self {\n            snapshots: Vec::new(),\n        }\n    }\n\n    pub fn add_snapshot(&mut self, step: usize, norm: f64) {\n        self.snapshots.push(WeightNormSnapshot {\n            step,\n            total_norm: norm,\n        });\n    }\n\n    pub fn save(&self, filename: &str) -> std::io::Result<()> {\n        let json = serde_json::to_string_pretty(self)?;\n        std::fs::write(filename, json)?;\n        println!(\"ðŸ’¾ Weight norm history saved to: {}\", filename);\n        Ok(())\n    }\n}\n\n/// Compute the L2 norm of all model parameters\n/// This returns the Frobenius norm (sum of squared weights) across all parameters\n/// Note: This is a simplified implementation that computes the norm of token embeddings,\n/// which contain the majority of the model parameters. For a complete implementation,\n/// we would need to access all Linear layer weights via the Record trait.\npub fn compute_model_weight_norm<B: Backend>(model: &Transformer<B>) -> f64 {\n    let mut total_norm_squared = 0.0f64;\n\n    // Token embedding weights: [vocab_size, d_model]\n    let token_emb_weights = model.token_embedding_weights();\n    let token_emb_data: Vec<f32> = token_emb_weights.into_data().to_vec().unwrap();\n    for &weight in &token_emb_data {\n        total_norm_squared += (weight as f64) * (weight as f64);\n    }\n\n    // Note: Position embeddings, attention weights, MLP weights, and LM head weights\n    // are not included in this simplified implementation. For weight decay verification,\n    // tracking the token embeddings (which are the largest weight matrices) is sufficient\n    // to observe the monotonic decrease under weight decay.\n\n    total_norm_squared.sqrt()\n}\n\n/// Extract all token embeddings as a matrix [vocab_size, embedding_dim]\npub fn extract_all_embeddings<B: Backend>(model: &Transformer<B>) -> Vec<Vec<f64>> {\n    let vocab_size = ModularAdditionDataset::vocab_size();\n    let mut embeddings = Vec::with_capacity(vocab_size);\n\n    for token_id in 0..vocab_size {\n        embeddings.push(get_token_embedding(model, token_id));\n    }\n\n    embeddings\n}\n\n/// Verify that FFT analysis of embeddings shows dominant frequencies\n/// that align with the modulus structure and nontrivial spectral patterns\n///\n/// NOTE: Reserved for manual verification or future integration into analysis pipeline\n#[allow(dead_code)]\npub fn verify_fft_dominant_frequencies<B: Backend>(\n    model: &Transformer<B>,\n    config: &FFTDominantFrequencyConfig,\n) -> Result<FFTDominantFrequencyReport, String> {\n    let embeddings = extract_all_embeddings(model);\n    verify_fft_dominant_frequencies_from_embeddings(&embeddings, config)\n}\n\n/// Verify FFT dominant frequencies from pre-extracted embeddings\n///\n/// NOTE: Reserved for manual verification or future integration into analysis pipeline\n#[allow(dead_code)]\npub fn verify_fft_dominant_frequencies_from_embeddings(\n    embeddings: &[Vec<f64>],\n    config: &FFTDominantFrequencyConfig,\n) -> Result<FFTDominantFrequencyReport, String> {\n    if embeddings.is_empty() {\n        return Err(\"no embeddings provided\".to_string());\n    }\n\n    let vocab_size = embeddings.len();\n    if vocab_size < config.modulus {\n        return Err(format!(\n            \"embedding count {} less than modulus {}\",\n            vocab_size, config.modulus\n        ));\n    }\n\n    // Analyze each embedding dimension and collect dominant frequencies\n    let mut frequency_histogram: HashMap<usize, usize> = HashMap::new();\n    let embedding_dim = embeddings[0].len();\n\n    // For each embedding dimension, compute FFT across all tokens and find dominant frequency\n    for dim in 0..embedding_dim {\n        let mut signal = Vec::with_capacity(vocab_size);\n        for embedding in embeddings.iter().take(vocab_size) {\n            if dim >= embedding.len() {\n                return Err(format!(\n                    \"embedding dimension mismatch at dim {}\",\n                    dim\n                ));\n            }\n            signal.push(embedding[dim]);\n        }\n\n        // Compute FFT and find dominant frequency\n        let magnitudes = compute_fft(&signal);\n        let dominant_freq = find_dominant_frequency(&magnitudes);\n        *frequency_histogram.entry(dominant_freq).or_insert(0) += 1;\n    }\n\n    // Check 1: Modulus frequency should appear significantly\n    let modulus_count = frequency_histogram.get(&config.modulus).copied().unwrap_or(0);\n    let modulus_frequency_fraction = modulus_count as f64 / embedding_dim as f64;\n\n    if modulus_frequency_fraction < config.min_modulus_frequency_fraction {\n        return Err(format!(\n            \"modulus frequency {} appears in only {:.3} of dimensions (threshold {:.3})\",\n            config.modulus, modulus_frequency_fraction, config.min_modulus_frequency_fraction\n        ));\n    }\n\n    // Check 2: Spectrum should NOT resemble white noise (low entropy)\n    // High entropy = uniform distribution (white noise), low entropy = structured peaks\n    let total_dims = embedding_dim as f64;\n    let mut entropy = 0.0;\n    for &count in frequency_histogram.values() {\n        if count > 0 {\n            let p = count as f64 / total_dims;\n            entropy -= p * p.log2();\n        }\n    }\n    // Normalize entropy by max possible (log2 of unique frequency count)\n    let max_entropy = (frequency_histogram.len() as f64).log2();\n    let normalized_entropy = if max_entropy > 0.0 {\n        entropy / max_entropy\n    } else {\n        0.0\n    };\n\n    if normalized_entropy > config.max_spectrum_entropy {\n        return Err(format!(\n            \"spectrum entropy {:.3} too high (white noise threshold {:.3})\",\n            normalized_entropy, config.max_spectrum_entropy\n        ));\n    }\n\n    // Check 3: Peak frequencies should be significantly stronger than median\n    let mut counts: Vec<usize> = frequency_histogram.values().copied().collect();\n    counts.sort_unstable();\n    let median_count = if counts.is_empty() {\n        0\n    } else {\n        counts[counts.len() / 2]\n    };\n    let peak_count = counts.last().copied().unwrap_or(0);\n    let peak_to_median_ratio = if median_count > 0 {\n        peak_count as f64 / median_count as f64\n    } else {\n        f64::INFINITY\n    };\n\n    if peak_to_median_ratio < config.min_peak_to_median_ratio {\n        return Err(format!(\n            \"peak to median ratio {:.3} below threshold {:.3}\",\n            peak_to_median_ratio, config.min_peak_to_median_ratio\n        ));\n    }\n\n    // Collect top frequencies for the report\n    let mut freq_counts: Vec<(usize, usize)> = frequency_histogram.into_iter().collect();\n    freq_counts.sort_by(|a, b| b.1.cmp(&a.1));\n    let top_frequencies: Vec<(usize, usize)> = freq_counts.into_iter().take(5).collect();\n\n    Ok(FFTDominantFrequencyReport {\n        modulus_frequency_fraction,\n        spectrum_entropy: normalized_entropy,\n        peak_to_median_ratio,\n        top_frequencies,\n    })\n}\n\n#[derive(Debug, Clone)]\npub struct FFTDominantFrequencyConfig {\n    pub modulus: usize,\n    pub min_modulus_frequency_fraction: f64,\n    pub max_spectrum_entropy: f64,\n    pub min_peak_to_median_ratio: f64,\n}\n\nimpl FFTDominantFrequencyConfig {\n    /// NOTE: Reserved for future integration into analysis pipeline\n    #[allow(dead_code)]\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        Self {\n            modulus,\n            min_modulus_frequency_fraction: 0.15,\n            max_spectrum_entropy: 0.85,\n            min_peak_to_median_ratio: 2.0,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct FFTDominantFrequencyReport {\n    pub modulus_frequency_fraction: f64,\n    pub spectrum_entropy: f64,\n    pub peak_to_median_ratio: f64,\n    pub top_frequencies: Vec<(usize, usize)>,\n}\n\n#[derive(Debug, Clone)]\npub struct EmbeddingClockConfig {\n    pub max_radius_std_fraction: f64,\n    pub min_angular_coverage: f64,\n    pub max_mean_angle_error: f64,\n    pub min_order_fraction: f64,\n}\n\nimpl EmbeddingClockConfig {\n    /// NOTE: Reserved for future integration into analysis pipeline\n    #[allow(dead_code)]\n    pub fn default_for_modulus(_modulus: usize) -> Self {\n        Self {\n            max_radius_std_fraction: 0.1,\n            min_angular_coverage: 0.9,\n            max_mean_angle_error: 0.35,\n            min_order_fraction: 0.9,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct EmbeddingClockReport {\n    pub mean_radius: f64,\n    pub radius_std: f64,\n    pub angular_coverage: f64,\n    pub mean_angle_error: f64,\n    pub order_fraction: f64,\n}\n\n/// NOTE: Reserved for future integration into analysis pipeline\n#[allow(dead_code)]\npub fn project_embeddings_to_fourier_plane(\n    embeddings: &[Vec<f64>],\n    modulus: usize,\n) -> Result<Vec<(f64, f64)>, String> {\n    let p = modulus;\n    if embeddings.len() < p {\n        return Err(\"embedding matrix smaller than modulus\".to_string());\n    }\n    if p == 0 {\n        return Err(\"modulus must be positive\".to_string());\n    }\n    let d = embeddings[0].len();\n    if d == 0 {\n        return Err(\"embedding dimension must be positive\".to_string());\n    }\n    if !embeddings[..p].iter().all(|row| row.len() == d) {\n        return Err(\"embedding rows have inconsistent dimension\".to_string());\n    }\n\n    let tau = std::f64::consts::PI * 2.0;\n    let mut basis_cos = vec![0.0f64; d];\n    let mut basis_sin = vec![0.0f64; d];\n    let scale = 2.0 / p as f64;\n\n    for (token, embedding) in embeddings[..p].iter().enumerate() {\n        let angle = tau * token as f64 / p as f64;\n        let cos_val = angle.cos();\n        let sin_val = angle.sin();\n        for dim in 0..d {\n            basis_cos[dim] += cos_val * embedding[dim] * scale;\n            basis_sin[dim] += sin_val * embedding[dim] * scale;\n        }\n    }\n\n    let basis_cos = normalize_vector(&basis_cos)?;\n    let basis_sin = normalize_vector(&basis_sin)?;\n\n    let mut coords = Vec::with_capacity(p);\n    for embedding in embeddings[..p].iter() {\n        let x = dot(embedding, &basis_cos);\n        let y = dot(embedding, &basis_sin);\n        coords.push((x, y));\n    }\n\n    Ok(coords)\n}\n\n/// NOTE: Reserved for future integration into analysis pipeline\n#[allow(dead_code)]\npub fn verify_embedding_clock_geometry(\n    embeddings: &[Vec<f64>],\n    modulus: usize,\n    config: &EmbeddingClockConfig,\n) -> Result<EmbeddingClockReport, String> {\n    let coords = project_embeddings_to_fourier_plane(embeddings, modulus)?;\n    let p = coords.len();\n    if p == 0 {\n        return Err(\"no embedding coordinates computed\".to_string());\n    }\n\n    let mut radii = Vec::with_capacity(p);\n    let mut angles = Vec::with_capacity(p);\n    for (x, y) in &coords {\n        let radius = (x * x + y * y).sqrt();\n        radii.push(radius);\n        angles.push(y.atan2(*x));\n    }\n\n    let mean_radius = radii.iter().sum::<f64>() / p as f64;\n    if mean_radius == 0.0 {\n        return Err(\"mean radius is zero\".to_string());\n    }\n    let radius_var = radii\n        .iter()\n        .map(|r| (r - mean_radius) * (r - mean_radius))\n        .sum::<f64>()\n        / p as f64;\n    let radius_std = radius_var.sqrt();\n\n    if radius_std / mean_radius > config.max_radius_std_fraction {\n        return Err(format!(\n            \"radius variance too large (std {:.4}, mean {:.4})\",\n            radius_std, mean_radius\n        ));\n    }\n\n    let tau = std::f64::consts::PI * 2.0;\n    let (mut sum_cos, mut sum_sin) = (0.0f64, 0.0f64);\n    for (token, angle) in angles.iter().enumerate() {\n        let expected = tau * token as f64 / p as f64;\n        let delta = angle - expected;\n        sum_cos += delta.cos();\n        sum_sin += delta.sin();\n    }\n    let offset = sum_sin.atan2(sum_cos);\n\n    let mut adjusted_angles = Vec::with_capacity(p);\n    let mut angle_errors = Vec::with_capacity(p);\n    for (token, angle) in angles.iter().enumerate() {\n        let expected = tau * token as f64 / p as f64;\n        let mut adjusted = angle - offset;\n        adjusted = wrap_angle(adjusted);\n        adjusted_angles.push(adjusted);\n        let diff = wrap_angle_signed(adjusted - expected);\n        angle_errors.push(diff.abs());\n    }\n\n    let mean_angle_error = angle_errors.iter().sum::<f64>() / p as f64;\n    if mean_angle_error > config.max_mean_angle_error {\n        return Err(format!(\n            \"mean angular error too large ({:.4} rad)\",\n            mean_angle_error\n        ));\n    }\n\n    let angular_coverage = angular_coverage(&adjusted_angles);\n    if angular_coverage < config.min_angular_coverage {\n        return Err(format!(\n            \"angular coverage too small ({:.4})\",\n            angular_coverage\n        ));\n    }\n\n    let order_fraction = order_fraction(&adjusted_angles, p);\n    if order_fraction < config.min_order_fraction {\n        return Err(format!(\n            \"token ordering too weak ({:.4})\",\n            order_fraction\n        ));\n    }\n\n    Ok(EmbeddingClockReport {\n        mean_radius,\n        radius_std,\n        angular_coverage,\n        mean_angle_error,\n        order_fraction,\n    })\n}\n\n// Helper functions for embedding clock geometry verification\n#[allow(dead_code)]\nfn normalize_vector(vec: &[f64]) -> Result<Vec<f64>, String> {\n    let norm = vec.iter().map(|v| v * v).sum::<f64>().sqrt();\n    if norm == 0.0 {\n        return Err(\"basis vector norm is zero\".to_string());\n    }\n    Ok(vec.iter().map(|v| v / norm).collect())\n}\n\n#[allow(dead_code)]\nfn dot(a: &[f64], b: &[f64]) -> f64 {\n    a.iter().zip(b.iter()).map(|(x, y)| x * y).sum()\n}\n\n#[allow(dead_code)]\nfn wrap_angle(angle: f64) -> f64 {\n    let tau = std::f64::consts::PI * 2.0;\n    let mut wrapped = angle % tau;\n    if wrapped < 0.0 {\n        wrapped += tau;\n    }\n    wrapped\n}\n\n#[allow(dead_code)]\nfn wrap_angle_signed(angle: f64) -> f64 {\n    let tau = std::f64::consts::PI * 2.0;\n    let mut wrapped = (angle + std::f64::consts::PI) % tau;\n    if wrapped < 0.0 {\n        wrapped += tau;\n    }\n    wrapped - std::f64::consts::PI\n}\n\n#[allow(dead_code)]\nfn angular_coverage(angles: &[f64]) -> f64 {\n    if angles.is_empty() {\n        return 0.0;\n    }\n    let tau = std::f64::consts::PI * 2.0;\n    let mut sorted = angles.to_vec();\n    sorted.sort_by(|a, b| a.partial_cmp(b).unwrap_or(std::cmp::Ordering::Equal));\n\n    let mut max_gap = 0.0;\n    for pair in sorted.windows(2) {\n        let gap = pair[1] - pair[0];\n        if gap > max_gap {\n            max_gap = gap;\n        }\n    }\n    let wrap_gap = tau - (sorted[sorted.len() - 1] - sorted[0]);\n    if wrap_gap > max_gap {\n        max_gap = wrap_gap;\n    }\n\n    (tau - max_gap) / tau\n}\n\n#[allow(dead_code)]\nfn order_fraction(angles: &[f64], modulus: usize) -> f64 {\n    if angles.len() < 2 || modulus == 0 {\n        return 0.0;\n    }\n    let mut indexed: Vec<(usize, f64)> = angles.iter().copied().enumerate().collect();\n    indexed.sort_by(|a, b| a.1.partial_cmp(&b.1).unwrap_or(std::cmp::Ordering::Equal));\n\n    let mut good = 0usize;\n    for window in indexed.windows(2) {\n        let current = window[0].0;\n        let next = window[1].0;\n        if (next + modulus - current) % modulus == 1 {\n            good += 1;\n        }\n    }\n    let last = indexed[indexed.len() - 1].0;\n    let first = indexed[0].0;\n    if (first + modulus - last) % modulus == 1 {\n        good += 1;\n    }\n\n    good as f64 / modulus as f64\n}\n\n/// Track training and validation accuracy over time\n#[derive(Debug, Serialize, Deserialize)]\npub struct AccuracyHistory {\n    pub train_snapshots: Vec<(usize, f32)>, // (step, accuracy)\n    pub val_snapshots: Vec<(usize, f32)>,   // (step, accuracy)\n}\n\nimpl AccuracyHistory {\n    pub fn new() -> Self {\n        Self {\n            train_snapshots: Vec::new(),\n            val_snapshots: Vec::new(),\n        }\n    }\n\n    pub fn add_snapshot(&mut self, step: usize, train_acc: f32, val_acc: f32) {\n        self.train_snapshots.push((step, train_acc));\n        self.val_snapshots.push((step, val_acc));\n    }\n\n    pub fn save(&self, filename: &str) -> std::io::Result<()> {\n        let json = serde_json::to_string_pretty(self)?;\n        std::fs::write(filename, json)?;\n        println!(\"ðŸ’¾ Accuracy history saved to: {}\", filename);\n        Ok(())\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    fn make_circle_embeddings(modulus: usize) -> Vec<Vec<f64>> {\n        let tau = std::f64::consts::PI * 2.0;\n        (0..modulus)\n            .map(|token| {\n                let angle = tau * token as f64 / modulus as f64;\n                vec![angle.cos(), angle.sin(), 0.0, 0.0]\n            })\n            .collect()\n    }\n\n    #[test]\n    fn top_k_frequency_indices_stable_with_ties() {\n        let magnitudes = vec![0.1, 1.0, 1.0, 0.5];\n        let first = top_k_frequency_indices(&magnitudes, 2);\n        let second = top_k_frequency_indices(&magnitudes, 2);\n        assert_eq!(first, vec![1, 2]);\n        assert_eq!(first, second);\n    }\n\n    #[test]\n    fn restrict_signal_zeroes_when_top_k_zero() {\n        let signal = vec![1.0, -1.0, 0.5, -0.5];\n        let filtered = restrict_signal_to_top_k_frequencies(&signal, 0);\n        assert_eq!(filtered, vec![0.0, 0.0, 0.0, 0.0]);\n    }\n\n    #[test]\n    fn exclude_signal_no_change_when_top_k_zero() {\n        let signal = vec![1.0, -1.0, 0.5, -0.5];\n        let filtered = exclude_top_k_frequencies(&signal, 0);\n        assert_eq!(filtered, signal);\n    }\n\n    #[test]\n    fn exclude_signal_zeroes_when_top_k_all() {\n        let signal = vec![1.0, -1.0, 0.5, -0.5];\n        let filtered = exclude_top_k_frequencies(&signal, signal.len());\n        assert_eq!(filtered, vec![0.0, 0.0, 0.0, 0.0]);\n    }\n\n    #[test]\n    fn embedding_clock_geometry_passes_on_clean_circle() {\n        let modulus = 13;\n        let embeddings = make_circle_embeddings(modulus);\n        let config = EmbeddingClockConfig::default_for_modulus(modulus);\n        let report =\n            verify_embedding_clock_geometry(&embeddings, modulus, &config).expect(\"expected pass\");\n        assert!(report.angular_coverage > 0.9);\n        assert!(report.order_fraction > 0.9);\n        assert!(report.mean_angle_error < 0.1);\n    }\n\n    #[test]\n    fn embedding_clock_geometry_flags_wrong_frequency() {\n        let modulus = 11;\n        let tau = std::f64::consts::PI * 2.0;\n        let embeddings: Vec<Vec<f64>> = (0..modulus)\n            .map(|token| {\n                let angle = tau * token as f64 / modulus as f64;\n                vec![(2.0 * angle).cos(), (2.0 * angle).sin(), 0.0, 0.0]\n            })\n            .collect();\n        let config = EmbeddingClockConfig {\n            max_radius_std_fraction: 0.5,\n            min_angular_coverage: 0.1,\n            max_mean_angle_error: 0.2,\n            min_order_fraction: 0.1,\n        };\n        let err = verify_embedding_clock_geometry(&embeddings, modulus, &config)\n            .expect_err(\"expected ordering verification to fail\");\n        assert!(err.contains(\"mean angular error\"));\n    }\n\n    #[test]\n    fn fft_dominant_frequency_passes_on_modulus_structure() {\n        // Create embeddings with strong modulus-113 frequency component\n        // Mix in some other frequencies to get a realistic peak-to-median ratio\n        let modulus = 113;\n        let vocab_size = 114; // 0-112 + equals token\n        let embedding_dim = 128;\n        let tau = std::f64::consts::PI * 2.0;\n\n        let mut embeddings = Vec::with_capacity(vocab_size);\n        for token in 0..vocab_size {\n            let mut embedding = Vec::with_capacity(embedding_dim);\n            for dim in 0..embedding_dim {\n                // 60% of dimensions have modulus frequency, 40% have other frequencies\n                let freq = if dim % 5 < 3 {\n                    modulus\n                } else {\n                    // Use various other frequencies for the remaining dims\n                    20 + (dim % 20)\n                };\n                let angle = tau * freq as f64 * token as f64 / vocab_size as f64\n                    + dim as f64 * 0.1;\n                embedding.push(angle.sin());\n            }\n            embeddings.push(embedding);\n        }\n\n        let config = FFTDominantFrequencyConfig::default_for_modulus(modulus);\n        let report = verify_fft_dominant_frequencies_from_embeddings(&embeddings, &config)\n            .expect(\"expected modulus structure to pass\");\n\n        assert!(report.modulus_frequency_fraction >= config.min_modulus_frequency_fraction);\n        assert!(report.spectrum_entropy <= config.max_spectrum_entropy);\n        assert!(report.peak_to_median_ratio >= config.min_peak_to_median_ratio);\n        assert!(report.top_frequencies.iter().any(|(freq, _)| *freq == modulus));\n    }\n\n    #[test]\n    fn fft_dominant_frequency_fails_on_white_noise() {\n        // Create random embeddings (white noise spectrum)\n        let modulus = 113;\n        let vocab_size = 114;\n        let embedding_dim = 128;\n\n        let mut embeddings = Vec::with_capacity(vocab_size);\n        for token in 0..vocab_size {\n            let mut embedding = Vec::with_capacity(embedding_dim);\n            for dim in 0..embedding_dim {\n                // Pseudo-random values\n                let value = ((token * 17 + dim * 31) % 100) as f64 / 50.0 - 1.0;\n                embedding.push(value);\n            }\n            embeddings.push(embedding);\n        }\n\n        let config = FFTDominantFrequencyConfig::default_for_modulus(modulus);\n        let err = verify_fft_dominant_frequencies_from_embeddings(&embeddings, &config)\n            .expect_err(\"expected white noise to fail\");\n\n        // Should fail on either modulus frequency fraction or spectrum entropy\n        assert!(\n            err.contains(\"modulus frequency\") || err.contains(\"spectrum entropy\")\n                || err.contains(\"peak to median\")\n        );\n    }\n\n    #[test]\n    fn fft_dominant_frequency_fails_on_wrong_frequency() {\n        // Create embeddings with dominant frequency at 57 (not 113)\n        let modulus = 113;\n        let vocab_size = 114;\n        let embedding_dim = 128;\n        let tau = std::f64::consts::PI * 2.0;\n        let wrong_frequency = 57;\n\n        let mut embeddings = Vec::with_capacity(vocab_size);\n        for token in 0..vocab_size {\n            let mut embedding = Vec::with_capacity(embedding_dim);\n            for dim in 0..embedding_dim {\n                // Each dimension has a sine wave with wrong frequency\n                let angle = tau * wrong_frequency as f64 * token as f64 / vocab_size as f64\n                    + dim as f64 * 0.1;\n                embedding.push(angle.sin());\n            }\n            embeddings.push(embedding);\n        }\n\n        let config = FFTDominantFrequencyConfig::default_for_modulus(modulus);\n        let err = verify_fft_dominant_frequencies_from_embeddings(&embeddings, &config)\n            .expect_err(\"expected wrong frequency to fail\");\n\n        assert!(err.contains(\"modulus frequency\"));\n    }\n\n    #[test]\n    fn fft_dominant_frequency_fails_on_flat_embeddings() {\n        // Create constant embeddings (DC component only, no structure)\n        let modulus = 113;\n        let vocab_size = 114;\n        let embedding_dim = 128;\n\n        let embeddings = vec![vec![1.5; embedding_dim]; vocab_size];\n\n        let config = FFTDominantFrequencyConfig::default_for_modulus(modulus);\n        let err = verify_fft_dominant_frequencies_from_embeddings(&embeddings, &config)\n            .expect_err(\"expected flat embeddings to fail\");\n\n        assert!(err.contains(\"modulus frequency\") || err.contains(\"peak to median\"));\n    }\n\n    #[test]\n    fn fft_dominant_frequency_fails_on_weak_peaks() {\n        // Create embeddings with many frequencies at similar magnitudes (no clear peaks)\n        let modulus = 113;\n        let vocab_size = 114;\n        let embedding_dim = 128;\n        let tau = std::f64::consts::PI * 2.0;\n\n        let mut embeddings = Vec::with_capacity(vocab_size);\n        for token in 0..vocab_size {\n            let mut embedding = Vec::with_capacity(embedding_dim);\n            for dim in 0..embedding_dim {\n                // Mix many frequencies with similar amplitudes\n                let mut value = 0.0;\n                for freq in 1..10 {\n                    let angle =\n                        tau * freq as f64 * token as f64 / vocab_size as f64 + dim as f64 * 0.01;\n                    value += angle.sin() / 10.0;\n                }\n                embedding.push(value);\n            }\n            embeddings.push(embedding);\n        }\n\n        let config = FFTDominantFrequencyConfig {\n            modulus,\n            min_modulus_frequency_fraction: 0.15,\n            max_spectrum_entropy: 0.85,\n            min_peak_to_median_ratio: 2.0,\n        };\n        let err = verify_fft_dominant_frequencies_from_embeddings(&embeddings, &config)\n            .expect_err(\"expected weak peaks to fail\");\n\n        assert!(\n            err.contains(\"modulus frequency\") || err.contains(\"spectrum entropy\")\n                || err.contains(\"peak to median\")\n        );\n    }\n}\n","structuredPatch":[{"oldStart":964,"oldLines":6,"newStart":964,"newLines":8,"lines":["         }","     }"," ","+    /// NOTE: Currently unused - accuracy tracked directly in main loop","+    #[allow(dead_code)]","     pub fn add_snapshot(&mut self, step: usize, train_acc: f32, val_acc: f32) {","         self.train_snapshots.push((step, train_acc));","         self.val_snapshots.push((step, val_acc));"]}],"userModified":false,"replaceAll":false}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01LPw82KLepxaeYrjYZHgyta","type":"message","role":"assistant","content":[{"type":"text","text":"Now let me move on to the verify.rs file. Let me check what needs to be marked there:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":569,"cache_read_input_tokens":56245,"cache_creation":{"ephemeral_5m_input_tokens":569,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"ac240250-6a47-46a5-87ee-0bf351e45983"}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01LPw82KLepxaeYrjYZHgyta","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01LqUervSgAmpUPW7Hc6FPe5","name":"Bash","input":{"command":"grep -n \"pub fn verify_restricted_loss_early_drop\\|pub fn verify_mlp_activation_waves\\|pub fn verify_mlp_activation_waves_from_signals\\|pub fn verify_constructive_interference_surface_from_grid\\|pub fn verify_snake_curve_shape\" src/verify.rs | head -10","description":"Find verification functions in verify.rs"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":569,"cache_read_input_tokens":56245,"cache_creation":{"ephemeral_5m_input_tokens":569,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"75a05299-de68-4d25-b46d-3eb42628971a"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01LqUervSgAmpUPW7Hc6FPe5","type":"tool_result","content":"396:pub fn verify_restricted_loss_early_drop(\n513:pub fn verify_mlp_activation_waves<B: Backend>(\n542:pub fn verify_mlp_activation_waves_from_signals(\n658:pub fn verify_constructive_interference_surface_from_grid(\n1344:pub fn verify_snake_curve_shape(","is_error":false}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"a40fcd16-90bf-40a3-be32-fdde575ca2a4","tool_use_result":{"stdout":"396:pub fn verify_restricted_loss_early_drop(\n513:pub fn verify_mlp_activation_waves<B: Backend>(\n542:pub fn verify_mlp_activation_waves_from_signals(\n658:pub fn verify_constructive_interference_surface_from_grid(\n1344:pub fn verify_snake_curve_shape(","stderr":"","interrupted":false,"isImage":false}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01VwhEBHgof3wWZSQmoTWP4c","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_0179DFD32KVVj33isu9YMEWK","name":"Read","input":{"file_path":"/Users/admin/MechInt/src/verify.rs","offset":395,"limit":10}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":374,"cache_read_input_tokens":56814,"cache_creation":{"ephemeral_5m_input_tokens":374,"ephemeral_1h_input_tokens":0},"output_tokens":25,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"23ac8fe7-dc6b-47fc-b6d1-9a4890d22006"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_0179DFD32KVVj33isu9YMEWK","type":"tool_result","content":"   395â†’\n   396â†’pub fn verify_restricted_loss_early_drop(\n   397â†’    full_losses: &[(usize, f64)],\n   398â†’    restricted_losses: &[(usize, f64)],\n   399â†’    config: &RestrictedLossVerificationConfig,\n   400â†’) -> Result<RestrictedLossReport, String> {\n   401â†’    let restricted_baseline =\n   402â†’        baseline_loss(restricted_losses, config.plateau_min_step, config.drop_window)?;\n   403â†’    let full_baseline = baseline_loss(full_losses, config.plateau_min_step, config.drop_window)?;\n   404â†’\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"cc1f66d1-7513-41c5-9bbb-5fea92bc7527","tool_use_result":{"type":"text","file":{"filePath":"/Users/admin/MechInt/src/verify.rs","content":"\npub fn verify_restricted_loss_early_drop(\n    full_losses: &[(usize, f64)],\n    restricted_losses: &[(usize, f64)],\n    config: &RestrictedLossVerificationConfig,\n) -> Result<RestrictedLossReport, String> {\n    let restricted_baseline =\n        baseline_loss(restricted_losses, config.plateau_min_step, config.drop_window)?;\n    let full_baseline = baseline_loss(full_losses, config.plateau_min_step, config.drop_window)?;\n","numLines":10,"startLine":395,"totalLines":2190}}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01TZPu1WBPnhjE2mWPHvxNPQ","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01SnGD5rP2YjjFusDyDc8udF","name":"Edit","input":{"replace_all":false,"file_path":"/Users/admin/MechInt/src/verify.rs","old_string":"pub fn verify_restricted_loss_early_drop(\n    full_losses: &[(usize, f64)],\n    restricted_losses: &[(usize, f64)],\n    config: &RestrictedLossVerificationConfig,\n) -> Result<RestrictedLossReport, String> {","new_string":"/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_restricted_loss_early_drop(\n    full_losses: &[(usize, f64)],\n    restricted_losses: &[(usize, f64)],\n    config: &RestrictedLossVerificationConfig,\n) -> Result<RestrictedLossReport, String> {"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":357,"cache_read_input_tokens":57188,"cache_creation":{"ephemeral_5m_input_tokens":357,"ephemeral_1h_input_tokens":0},"output_tokens":286,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"5f98c766-46e6-4556-a3ba-e800031d4f39"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01SnGD5rP2YjjFusDyDc8udF","type":"tool_result","content":"The file /Users/admin/MechInt/src/verify.rs has been updated. Here's the result of running `cat -n` on a snippet of the edited file:\n   392â†’\n   393â†’    Ok(after_avg / before_avg)\n   394â†’}\n   395â†’\n   396â†’/// NOTE: Verification function reserved for future integration or manual analysis\n   397â†’#[allow(dead_code)]\n   398â†’pub fn verify_restricted_loss_early_drop(\n   399â†’    full_losses: &[(usize, f64)],\n   400â†’    restricted_losses: &[(usize, f64)],\n   401â†’    config: &RestrictedLossVerificationConfig,\n   402â†’) -> Result<RestrictedLossReport, String> {\n   403â†’    let restricted_baseline =\n   404â†’        baseline_loss(restricted_losses, config.plateau_min_step, config.drop_window)?;\n   405â†’    let full_baseline = baseline_loss(full_losses, config.plateau_min_step, config.drop_window)?;\n   406â†’"}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"a7d26ef7-0968-449a-81ac-cb1a857b182f","tool_use_result":{"filePath":"/Users/admin/MechInt/src/verify.rs","oldString":"pub fn verify_restricted_loss_early_drop(\n    full_losses: &[(usize, f64)],\n    restricted_losses: &[(usize, f64)],\n    config: &RestrictedLossVerificationConfig,\n) -> Result<RestrictedLossReport, String> {","newString":"/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_restricted_loss_early_drop(\n    full_losses: &[(usize, f64)],\n    restricted_losses: &[(usize, f64)],\n    config: &RestrictedLossVerificationConfig,\n) -> Result<RestrictedLossReport, String> {","originalFile":"use burn::{\n    data::dataset::Dataset,\n    tensor::{backend::Backend, Int, Tensor},\n};\nuse rustfft::{num_complex::Complex, FftPlanner};\n\nuse crate::analysis::{AccuracyHistory, LossHistory};\nuse crate::data::ModularAdditionDataset;\nuse crate::model::Transformer;\n\n#[derive(Debug, Clone)]\npub struct GrokkingVerificationConfig {\n    pub early_train_acc_threshold: f32,\n    pub early_step_max: usize,\n    pub plateau_min_step: usize,\n    pub generalization_window: usize,\n    pub chance_accuracy: f32,\n    pub chance_tolerance: f32,\n    pub target_val_acc_threshold: f32,\n    pub accuracy_window: usize,\n    pub loss_window: usize,\n    pub loss_drop_fraction: f64,\n}\n\nimpl GrokkingVerificationConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        Self {\n            early_train_acc_threshold: 0.99,\n            early_step_max: 1000,\n            plateau_min_step: 1000,\n            generalization_window: 10_000,\n            chance_accuracy: 1.0 / modulus as f32,\n            chance_tolerance: 0.02,\n            target_val_acc_threshold: 0.99,\n            accuracy_window: 5,\n            loss_window: 5,\n            loss_drop_fraction: 0.5,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct GrokkingPhaseReport {\n    pub early_train_step: usize,\n    pub transition_step: usize,\n    pub plateau_max_val_acc: f32,\n    pub loss_drop_ratio: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossVerificationConfig {\n    pub plateau_min_step: usize,\n    pub drop_window: usize,\n    pub drop_fraction: f64,\n    pub min_step_lead: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossReport {\n    pub restricted_drop_step: usize,\n    pub full_drop_step: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossDropConfig {\n    pub min_relative_decrease: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossDropReport {\n    pub drop_step: usize,\n    pub drop_loss: f64,\n    pub baseline_loss: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct ExcludedLossSpikeConfig {\n    pub min_relative_increase: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct ExcludedLossSpikeReport {\n    pub spike_step: usize,\n    pub spike_loss: f64,\n    pub baseline_loss: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct MlpWaveVerificationConfig {\n    pub max_negative_fraction: f64,\n    pub negative_tolerance: f64,\n    pub min_dominant_ratio: f64,\n    pub min_correlation: f64,\n    pub phase_steps: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct MlpWaveNeuronReport {\n    pub neuron_index: usize,\n    pub dominant_frequency: usize,\n    pub dominant_ratio: f64,\n    pub negative_fraction: f64,\n    pub best_correlation: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct MlpWaveReport {\n    pub neuron_reports: Vec<MlpWaveNeuronReport>,\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldConfig {\n    pub fixed_y: usize,\n    pub neuron_indices: Vec<usize>,\n    pub min_post_score: f64,\n    pub max_pre_score: f64,\n    pub max_axis_ratio: f64,\n    pub min_pairs_passing: usize,\n}\n\nimpl PairwiseManifoldConfig {\n    pub fn default_for_modulus(_modulus: usize) -> Self {\n        Self {\n            fixed_y: 0,\n            neuron_indices: (0..7).collect(),\n            min_post_score: 0.75,\n            max_pre_score: 0.55,\n            max_axis_ratio: 8.0,\n            min_pairs_passing: 10,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldPairReport {\n    pub neuron_a: usize,\n    pub neuron_b: usize,\n    pub circularity_score: f64,\n    pub radius_cv: f64,\n    pub axis_ratio: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldReport {\n    pub pair_reports: Vec<PairwiseManifoldPairReport>,\n    pub average_score: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldTransitionReport {\n    pub pre_average_score: f64,\n    pub post_average_score: f64,\n    pub passing_pairs: usize,\n    pub total_pairs: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct DiagonalRidgeConfig {\n    pub neuron_index: usize,\n    pub max_lag: usize,\n    pub min_diagonal_ratio: f64,\n    pub min_diagonal_score: f64,\n}\n\nimpl DiagonalRidgeConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        let max_lag = (modulus / 10).max(4).min(20);\n        Self {\n            neuron_index: 10,\n            max_lag,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct DiagonalRidgeReport {\n    pub neuron_index: usize,\n    pub diagonal_score: f64,\n    pub axis_score: f64,\n    pub ratio: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct SnakeCurveConfig {\n    pub early_step_max: usize,\n    pub train_convergence_threshold: f32,\n    pub val_plateau_min_step: usize,\n    pub val_plateau_max_acc: f32,\n    pub val_final_threshold: f32,\n}\n\nimpl SnakeCurveConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        Self {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 1000,\n            val_plateau_max_acc: 1.0 / modulus as f32 + 0.05,\n            val_final_threshold: 0.90,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct WeightNormDecayConfig {\n    pub min_relative_decrease: f64,\n    pub plateau_min_step: usize,\n    pub grok_step_window: usize,\n}\n\nimpl WeightNormDecayConfig {\n    pub fn default_for_grokking() -> Self {\n        Self {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct WeightNormDecayReport {\n    pub initial_norm: f64,\n    pub final_norm: f64,\n    pub relative_decrease: f64,\n    pub grok_step_norm: Option<f64>,\n}\n\n#[derive(Debug, Clone)]\npub struct SnakeCurveReport {\n    pub train_converged_by_step: usize,\n    pub val_plateau_max_acc: f32,\n    pub val_grok_step: usize,\n    pub curve_shape_valid: bool,\n}\n\npub fn verify_grokking_phase_transition(\n    loss_history: &LossHistory,\n    accuracy_history: &AccuracyHistory,\n    config: &GrokkingVerificationConfig,\n) -> Result<GrokkingPhaseReport, String> {\n    let (early_train_step, _) = accuracy_history\n        .train_snapshots\n        .iter()\n        .find(|(_, acc)| *acc >= config.early_train_acc_threshold)\n        .ok_or_else(|| \"train accuracy never reaches threshold\".to_string())?;\n\n    if *early_train_step > config.early_step_max {\n        return Err(format!(\n            \"train accuracy reaches {:.2} after step {}, expected before {}\",\n            config.early_train_acc_threshold, early_train_step, config.early_step_max\n        ));\n    }\n\n    let plateau_max_val_acc = accuracy_history\n        .val_snapshots\n        .iter()\n        .filter(|(step, _)| *step <= config.plateau_min_step)\n        .map(|(_, acc)| *acc)\n        .fold(None::<f32>, |accum, value| {\n            Some(accum.map_or(value, |max_val| max_val.max(value)))\n        })\n        .ok_or_else(|| \"no validation accuracy samples before plateau_min_step\".to_string())?;\n\n    if plateau_max_val_acc > config.chance_accuracy + config.chance_tolerance {\n        return Err(format!(\n            \"validation accuracy before step {} exceeds chance (max {:.4})\",\n            config.plateau_min_step, plateau_max_val_acc\n        ));\n    }\n\n    let (transition_step, _) = accuracy_history\n        .val_snapshots\n        .iter()\n        .find(|(step, acc)| {\n            *step >= config.plateau_min_step && *acc >= config.target_val_acc_threshold\n        })\n        .ok_or_else(|| \"validation accuracy never reaches target threshold\".to_string())?;\n\n    let latest_allowed = config.plateau_min_step + config.generalization_window;\n    if *transition_step > latest_allowed {\n        return Err(format!(\n            \"validation accuracy reaches {:.2} at step {}, expected before {}\",\n            config.target_val_acc_threshold, transition_step, latest_allowed\n        ));\n    }\n\n    verify_accuracy_jump(\n        &accuracy_history.val_snapshots,\n        *transition_step,\n        config.accuracy_window,\n        config.chance_accuracy,\n        config.chance_tolerance,\n        config.target_val_acc_threshold,\n    )?;\n\n    let loss_drop_ratio = verify_loss_drop(\n        &loss_history.val_snapshots,\n        *transition_step,\n        config.loss_window,\n        config.loss_drop_fraction,\n    )?;\n\n    Ok(GrokkingPhaseReport {\n        early_train_step: *early_train_step,\n        transition_step: *transition_step,\n        plateau_max_val_acc,\n        loss_drop_ratio,\n    })\n}\n\nfn verify_accuracy_jump(\n    val_accuracy: &[(usize, f32)],\n    transition_step: usize,\n    window: usize,\n    chance_accuracy: f32,\n    chance_tolerance: f32,\n    target_val_acc_threshold: f32,\n) -> Result<(), String> {\n    let before_values: Vec<f32> = val_accuracy\n        .iter()\n        .filter(|(step, _)| *step < transition_step)\n        .map(|(_, acc)| *acc)\n        .collect();\n\n    let after_values: Vec<f32> = val_accuracy\n        .iter()\n        .filter(|(step, _)| *step >= transition_step)\n        .map(|(_, acc)| *acc)\n        .collect();\n\n    if before_values.len() < window || after_values.len() < window {\n        return Err(\"insufficient accuracy samples around transition\".to_string());\n    }\n\n    let before_avg: f32 =\n        before_values[before_values.len() - window..].iter().sum::<f32>() / window as f32;\n    let after_avg: f32 = after_values[..window].iter().sum::<f32>() / window as f32;\n\n    if before_avg > chance_accuracy + chance_tolerance {\n        return Err(format!(\n            \"validation accuracy plateau too high before transition (avg {:.4})\",\n            before_avg\n        ));\n    }\n\n    if after_avg < target_val_acc_threshold {\n        return Err(format!(\n            \"validation accuracy after transition too low (avg {:.4})\",\n            after_avg\n        ));\n    }\n\n    Ok(())\n}\n\nfn verify_loss_drop(\n    val_losses: &[(usize, f64)],\n    transition_step: usize,\n    loss_window: usize,\n    loss_drop_fraction: f64,\n) -> Result<f64, String> {\n    let before_losses: Vec<f64> = val_losses\n        .iter()\n        .filter(|(step, _)| *step < transition_step)\n        .map(|(_, loss)| *loss)\n        .collect();\n\n    let after_losses: Vec<f64> = val_losses\n        .iter()\n        .filter(|(step, _)| *step >= transition_step)\n        .map(|(_, loss)| *loss)\n        .collect();\n\n    if before_losses.len() < loss_window || after_losses.len() < loss_window {\n        return Err(\"insufficient loss samples around transition\".to_string());\n    }\n\n    let before_avg: f64 =\n        before_losses[before_losses.len() - loss_window..].iter().sum::<f64>()\n            / loss_window as f64;\n    let after_avg: f64 = after_losses[..loss_window].iter().sum::<f64>() / loss_window as f64;\n\n    if after_avg > before_avg * (1.0 - loss_drop_fraction) {\n        return Err(format!(\n            \"validation loss drop too small (before {:.4}, after {:.4})\",\n            before_avg, after_avg\n        ));\n    }\n\n    Ok(after_avg / before_avg)\n}\n\npub fn verify_restricted_loss_early_drop(\n    full_losses: &[(usize, f64)],\n    restricted_losses: &[(usize, f64)],\n    config: &RestrictedLossVerificationConfig,\n) -> Result<RestrictedLossReport, String> {\n    let restricted_baseline =\n        baseline_loss(restricted_losses, config.plateau_min_step, config.drop_window)?;\n    let full_baseline = baseline_loss(full_losses, config.plateau_min_step, config.drop_window)?;\n\n    let restricted_drop_step = find_drop_step(\n        restricted_losses,\n        config.plateau_min_step,\n        restricted_baseline,\n        config.drop_fraction,\n    )?;\n    let full_drop_step = find_drop_step(\n        full_losses,\n        config.plateau_min_step,\n        full_baseline,\n        config.drop_fraction,\n    )?;\n\n    if restricted_drop_step + config.min_step_lead > full_drop_step {\n        return Err(format!(\n            \"restricted loss drops at step {}, expected at least {} steps before full loss (step {})\",\n            restricted_drop_step, config.min_step_lead, full_drop_step\n        ));\n    }\n\n    Ok(RestrictedLossReport {\n        restricted_drop_step,\n        full_drop_step,\n    })\n}\n\npub fn verify_restricted_loss_drop(\n    restricted_losses: &[(usize, f64)],\n    config: &RestrictedLossDropConfig,\n) -> Result<RestrictedLossDropReport, String> {\n    if restricted_losses.len() < 3 {\n        return Err(\"restricted loss history requires at least 3 points\".to_string());\n    }\n\n    let first = restricted_losses.first().ok_or_else(|| {\n        \"restricted loss history is empty\".to_string()\n    })?;\n    let baseline_loss = first.1;\n\n    let (min_idx, (min_step, min_loss)) = restricted_losses\n        .iter()\n        .enumerate()\n        .min_by(|(_, a), (_, b)| {\n            a.1.partial_cmp(&b.1).unwrap_or(std::cmp::Ordering::Equal)\n        })\n        .ok_or_else(|| \"restricted loss history is empty\".to_string())?;\n\n    if min_idx == 0 {\n        return Err(\"restricted loss minimum occurs at start (no drop detected)\".to_string());\n    }\n\n    let relative_decrease = (baseline_loss - min_loss) / baseline_loss;\n    if relative_decrease < config.min_relative_decrease {\n        return Err(format!(\n            \"restricted loss decrease too small (baseline {:.4}, min {:.4}, decrease {:.1}%)\",\n            baseline_loss, min_loss, relative_decrease * 100.0\n        ));\n    }\n\n    Ok(RestrictedLossDropReport {\n        drop_step: *min_step,\n        drop_loss: *min_loss,\n        baseline_loss,\n    })\n}\n\npub fn verify_excluded_loss_spike(\n    excluded_losses: &[(usize, f64)],\n    config: &ExcludedLossSpikeConfig,\n) -> Result<ExcludedLossSpikeReport, String> {\n    if excluded_losses.len() < 3 {\n        return Err(\"excluded loss history requires at least 3 points\".to_string());\n    }\n\n    let first = excluded_losses.first().ok_or_else(|| {\n        \"excluded loss history is empty\".to_string()\n    })?;\n    let last = excluded_losses.last().ok_or_else(|| {\n        \"excluded loss history is empty\".to_string()\n    })?;\n    let baseline_loss = (first.1 + last.1) / 2.0;\n\n    let (peak_idx, (peak_step, peak_loss)) = excluded_losses\n        .iter()\n        .enumerate()\n        .max_by(|(_, a), (_, b)| {\n            a.1.partial_cmp(&b.1).unwrap_or(std::cmp::Ordering::Equal)\n        })\n        .ok_or_else(|| \"excluded loss history is empty\".to_string())?;\n\n    if peak_idx == 0 || peak_idx + 1 == excluded_losses.len() {\n        return Err(\"excluded loss spike occurs at boundary\".to_string());\n    }\n\n    if peak_loss < &(baseline_loss * (1.0 + config.min_relative_increase)) {\n        return Err(format!(\n            \"excluded loss spike too small (baseline {:.4}, peak {:.4})\",\n            baseline_loss, peak_loss\n        ));\n    }\n\n    Ok(ExcludedLossSpikeReport {\n        spike_step: *peak_step,\n        spike_loss: *peak_loss,\n        baseline_loss,\n    })\n}\n\npub fn verify_mlp_activation_waves<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n    fixed_y: usize,\n    neuron_indices: &[usize],\n    config: &MlpWaveVerificationConfig,\n) -> Result<MlpWaveReport, String> {\n    let activations = collect_mlp_post_relu_activations(model, device, fixed_y)?;\n    if neuron_indices.is_empty() {\n        return Err(\"neuron indices list is empty\".to_string());\n    }\n\n    let d_ff = activations.len();\n    for &idx in neuron_indices {\n        if idx >= d_ff {\n            return Err(format!(\n                \"neuron index {} out of range (d_ff={})\",\n                idx, d_ff\n            ));\n        }\n    }\n\n    let signals: Vec<(usize, Vec<f64>)> = neuron_indices\n        .iter()\n        .map(|&idx| (idx, activations[idx].clone()))\n        .collect();\n    verify_mlp_activation_waves_from_signals(&signals, config)\n}\n\npub fn verify_mlp_activation_waves_from_signals(\n    signals: &[(usize, Vec<f64>)],\n    config: &MlpWaveVerificationConfig,\n) -> Result<MlpWaveReport, String> {\n    if signals.is_empty() {\n        return Err(\"no activation signals provided\".to_string());\n    }\n\n    let mut reports = Vec::with_capacity(signals.len());\n    for (neuron_index, signal) in signals {\n        let report = analyze_mlp_activation_wave(*neuron_index, signal, config)?;\n        reports.push(report);\n    }\n\n    Ok(MlpWaveReport {\n        neuron_reports: reports,\n    })\n}\n\npub fn verify_mlp_pairwise_manifold_transition<B: Backend>(\n    pre_model: &Transformer<B>,\n    post_model: &Transformer<B>,\n    device: &B::Device,\n    config: &PairwiseManifoldConfig,\n) -> Result<PairwiseManifoldTransitionReport, String> {\n    let modulus = ModularAdditionDataset::modulus();\n    if config.fixed_y >= modulus {\n        return Err(format!(\n            \"fixed_y {} out of range (modulus {})\",\n            config.fixed_y, modulus\n        ));\n    }\n\n    let pre_activations = collect_mlp_post_relu_activations(pre_model, device, config.fixed_y)?;\n    let post_activations = collect_mlp_post_relu_activations(post_model, device, config.fixed_y)?;\n\n    let d_ff = pre_activations.len();\n    if post_activations.len() != d_ff {\n        return Err(\"post activations shape mismatch\".to_string());\n    }\n\n    if config.neuron_indices.len() < 2 {\n        return Err(\"need at least two neuron indices\".to_string());\n    }\n\n    for &idx in &config.neuron_indices {\n        if idx >= d_ff {\n            return Err(format!(\n                \"neuron index {} out of range (d_ff={})\",\n                idx, d_ff\n            ));\n        }\n    }\n\n    let pre_signals: Vec<(usize, Vec<f64>)> = config\n        .neuron_indices\n        .iter()\n        .map(|&idx| (idx, pre_activations[idx].clone()))\n        .collect();\n    let post_signals: Vec<(usize, Vec<f64>)> = config\n        .neuron_indices\n        .iter()\n        .map(|&idx| (idx, post_activations[idx].clone()))\n        .collect();\n\n    verify_pairwise_manifold_transition_from_signals(&pre_signals, &post_signals, config)\n}\n\npub fn collect_mlp_activation_surface<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n    neuron_index: usize,\n) -> Result<Vec<Vec<f64>>, String> {\n    let modulus = ModularAdditionDataset::modulus();\n    let equals_token = ModularAdditionDataset::equals_token();\n\n    let mut inputs_vec = Vec::with_capacity(modulus * modulus * 3);\n    for x in 0..modulus {\n        for y in 0..modulus {\n            inputs_vec.push(x as i32);\n            inputs_vec.push(y as i32);\n            inputs_vec.push(equals_token as i32);\n        }\n    }\n\n    let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n        .reshape([modulus * modulus, 3]);\n    let (_, mlp_acts) = model.forward_with_mlp_activations(inputs);\n    let [batch_size, d_ff] = mlp_acts.dims();\n    if batch_size != modulus * modulus {\n        return Err(format!(\n            \"unexpected activation batch size {} (expected {})\",\n            batch_size,\n            modulus * modulus\n        ));\n    }\n    if neuron_index >= d_ff {\n        return Err(format!(\n            \"neuron index {} out of range (d_ff={})\",\n            neuron_index, d_ff\n        ));\n    }\n\n    let data: Vec<f32> = mlp_acts.into_data().to_vec().unwrap();\n    let mut surface = vec![vec![0.0f64; modulus]; modulus];\n    for x in 0..modulus {\n        for y in 0..modulus {\n            let batch = x * modulus + y;\n            let idx = batch * d_ff + neuron_index;\n            surface[x][y] = data[idx] as f64;\n        }\n    }\n\n    Ok(surface)\n}\n\npub fn verify_constructive_interference_surface_from_grid(\n    surface: &[Vec<f64>],\n    config: &DiagonalRidgeConfig,\n) -> Result<DiagonalRidgeReport, String> {\n    let (diag_main, diag_anti, axis_x, axis_y) =\n        surface_autocorrelation_scores(surface, config.max_lag)?;\n\n    let diagonal_score = diag_main.max(diag_anti);\n    let axis_score = axis_x.max(axis_y);\n    let ratio = if axis_score > 0.0 {\n        diagonal_score / axis_score\n    } else {\n        f64::INFINITY\n    };\n\n    if diagonal_score < config.min_diagonal_score {\n        return Err(format!(\n            \"diagonal autocorrelation {:.3} below threshold {:.3}\",\n            diagonal_score, config.min_diagonal_score\n        ));\n    }\n\n    if ratio < config.min_diagonal_ratio {\n        return Err(format!(\n            \"diagonal/axis ratio {:.3} below threshold {:.3}\",\n            ratio, config.min_diagonal_ratio\n        ));\n    }\n\n    Ok(DiagonalRidgeReport {\n        neuron_index: config.neuron_index,\n        diagonal_score,\n        axis_score,\n        ratio,\n    })\n}\n\npub fn verify_pairwise_manifold_transition_from_signals(\n    pre_signals: &[(usize, Vec<f64>)],\n    post_signals: &[(usize, Vec<f64>)],\n    config: &PairwiseManifoldConfig,\n) -> Result<PairwiseManifoldTransitionReport, String> {\n    let pre_report = evaluate_pairwise_manifolds_from_signals(pre_signals)?;\n    let post_report = evaluate_pairwise_manifolds_from_signals(post_signals)?;\n\n    if pre_report.average_score > config.max_pre_score {\n        return Err(format!(\n            \"pre-grok average circularity {:.3} exceeds threshold {:.3}\",\n            pre_report.average_score, config.max_pre_score\n        ));\n    }\n\n    let passing_pairs = post_report\n        .pair_reports\n        .iter()\n        .filter(|pair| {\n            pair.circularity_score >= config.min_post_score\n                && pair.axis_ratio <= config.max_axis_ratio\n        })\n        .count();\n    if passing_pairs < config.min_pairs_passing {\n        return Err(format!(\n            \"post-grok only {} pairs meet circularity threshold (need {})\",\n            passing_pairs, config.min_pairs_passing\n        ));\n    }\n\n    Ok(PairwiseManifoldTransitionReport {\n        pre_average_score: pre_report.average_score,\n        post_average_score: post_report.average_score,\n        passing_pairs,\n        total_pairs: post_report.pair_reports.len(),\n    })\n}\n\nfn evaluate_pairwise_manifolds_from_signals(\n    signals: &[(usize, Vec<f64>)],\n) -> Result<PairwiseManifoldReport, String> {\n    if signals.len() < 2 {\n        return Err(\"need at least two activation signals\".to_string());\n    }\n\n    let signal_len = signals[0].1.len();\n    if signal_len < 3 {\n        return Err(\"activation signal too short\".to_string());\n    }\n\n    for (idx, signal) in signals {\n        if signal.len() != signal_len {\n            return Err(format!(\n                \"signal length mismatch for neuron {}\",\n                idx\n            ));\n        }\n    }\n\n    let mut reports = Vec::new();\n    for i in 0..signals.len() {\n        for j in (i + 1)..signals.len() {\n            let (neuron_a, signal_a) = &signals[i];\n            let (neuron_b, signal_b) = &signals[j];\n            let points: Vec<(f64, f64)> = signal_a\n                .iter()\n                .zip(signal_b.iter())\n                .map(|(x, y)| (*x, *y))\n                .collect();\n            let (score, radius_cv, axis_ratio) =\n                match pairwise_circularity_metrics(&points) {\n                    Ok(metrics) => metrics,\n                    Err(_) => (0.0, 1.0, f64::INFINITY),\n                };\n            reports.push(PairwiseManifoldPairReport {\n                neuron_a: *neuron_a,\n                neuron_b: *neuron_b,\n                circularity_score: score,\n                radius_cv,\n                axis_ratio,\n            });\n        }\n    }\n\n    let average_score = reports\n        .iter()\n        .map(|pair| pair.circularity_score)\n        .sum::<f64>()\n        / reports.len() as f64;\n\n    Ok(PairwiseManifoldReport {\n        pair_reports: reports,\n        average_score,\n    })\n}\n\nfn pairwise_circularity_metrics(points: &[(f64, f64)]) -> Result<(f64, f64, f64), String> {\n    if points.len() < 3 {\n        return Err(\"not enough points for circularity\".to_string());\n    }\n\n    let n = points.len() as f64;\n    let mean_x = points.iter().map(|(x, _)| *x).sum::<f64>() / n;\n    let mean_y = points.iter().map(|(_, y)| *y).sum::<f64>() / n;\n\n    let mut var_x = 0.0;\n    let mut var_y = 0.0;\n    let mut cov_xy = 0.0;\n    for (x, y) in points {\n        let dx = x - mean_x;\n        let dy = y - mean_y;\n        var_x += dx * dx;\n        var_y += dy * dy;\n        cov_xy += dx * dy;\n    }\n    var_x /= n;\n    var_y /= n;\n    cov_xy /= n;\n\n    let trace = var_x + var_y;\n    let det = var_x * var_y - cov_xy * cov_xy;\n    if det <= 1.0e-12 || trace <= 0.0 {\n        return Err(\"degenerate covariance\".to_string());\n    }\n\n    let half_trace = trace * 0.5;\n    let disc = (half_trace * half_trace - det).max(0.0).sqrt();\n    let lambda1 = half_trace + disc;\n    let lambda2 = half_trace - disc;\n    if lambda2 <= 0.0 {\n        return Err(\"degenerate eigenvalues\".to_string());\n    }\n\n    let axis_ratio = (lambda1 / lambda2).sqrt();\n\n    let mut v1_x = cov_xy;\n    let mut v1_y = lambda1 - var_x;\n    if v1_x.abs() < 1.0e-12 && v1_y.abs() < 1.0e-12 {\n        v1_x = 1.0;\n        v1_y = 0.0;\n    }\n    let norm = (v1_x * v1_x + v1_y * v1_y).sqrt();\n    v1_x /= norm;\n    v1_y /= norm;\n    let v2_x = -v1_y;\n    let v2_y = v1_x;\n\n    let mut radii = Vec::with_capacity(points.len());\n    for (x, y) in points {\n        let dx = x - mean_x;\n        let dy = y - mean_y;\n        let p1 = dx * v1_x + dy * v1_y;\n        let p2 = dx * v2_x + dy * v2_y;\n        let w1 = p1 / lambda1.sqrt();\n        let w2 = p2 / lambda2.sqrt();\n        radii.push((w1 * w1 + w2 * w2).sqrt());\n    }\n\n    let mean_radius = radii.iter().sum::<f64>() / radii.len() as f64;\n    if mean_radius == 0.0 {\n        return Err(\"zero mean radius\".to_string());\n    }\n\n    let mut variance = 0.0;\n    for r in &radii {\n        let diff = r - mean_radius;\n        variance += diff * diff;\n    }\n    let std = (variance / radii.len() as f64).sqrt();\n    let radius_cv = std / mean_radius;\n    let score = 1.0 / (1.0 + radius_cv);\n\n    Ok((score, radius_cv, axis_ratio))\n}\n\nfn baseline_loss(\n    losses: &[(usize, f64)],\n    plateau_min_step: usize,\n    drop_window: usize,\n) -> Result<f64, String> {\n    let before: Vec<f64> = losses\n        .iter()\n        .filter(|(step, _)| *step <= plateau_min_step)\n        .map(|(_, loss)| *loss)\n        .collect();\n\n    if before.len() < drop_window || drop_window == 0 {\n        return Err(\"insufficient loss samples for baseline\".to_string());\n    }\n\n    let window = &before[before.len() - drop_window..];\n    Ok(window.iter().sum::<f64>() / drop_window as f64)\n}\n\nfn find_drop_step(\n    losses: &[(usize, f64)],\n    plateau_min_step: usize,\n    baseline: f64,\n    drop_fraction: f64,\n) -> Result<usize, String> {\n    losses\n        .iter()\n        .filter(|(step, _)| *step >= plateau_min_step)\n        .find(|(_, loss)| *loss <= baseline * (1.0 - drop_fraction))\n        .map(|(step, _)| *step)\n        .ok_or_else(|| \"loss never drops below threshold\".to_string())\n}\n\nfn collect_mlp_post_relu_activations<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n    fixed_y: usize,\n) -> Result<Vec<Vec<f64>>, String> {\n    let modulus = ModularAdditionDataset::modulus();\n    let equals_token = ModularAdditionDataset::equals_token();\n\n    let mut inputs_vec = Vec::with_capacity(modulus * 3);\n    for x in 0..modulus {\n        inputs_vec.push(x as i32);\n        inputs_vec.push(fixed_y as i32);\n        inputs_vec.push(equals_token as i32);\n    }\n\n    let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n        .reshape([modulus, 3]);\n    let (_, mlp_acts) = model.forward_with_mlp_activations(inputs);\n    let [batch_size, d_ff] = mlp_acts.dims();\n\n    if batch_size != modulus {\n        return Err(format!(\n            \"unexpected activation batch size {} (expected {})\",\n            batch_size, modulus\n        ));\n    }\n\n    let data: Vec<f32> = mlp_acts.into_data().to_vec().unwrap();\n    let mut per_neuron = vec![Vec::with_capacity(batch_size); d_ff];\n    for batch in 0..batch_size {\n        for neuron in 0..d_ff {\n            let idx = batch * d_ff + neuron;\n            per_neuron[neuron].push(data[idx] as f64);\n        }\n    }\n\n    Ok(per_neuron)\n}\n\nfn surface_autocorrelation_scores(\n    surface: &[Vec<f64>],\n    max_lag: usize,\n) -> Result<(f64, f64, f64, f64), String> {\n    let (height, width) = surface_dimensions(surface)?;\n    if height < 2 || width < 2 {\n        return Err(\"surface grid too small\".to_string());\n    }\n\n    let (mean, var) = surface_mean_variance(surface)?;\n    if var == 0.0 {\n        return Err(\"surface variance is zero\".to_string());\n    }\n\n    let max_lag = max_lag.min(height.saturating_sub(1)).min(width.saturating_sub(1));\n    if max_lag == 0 {\n        return Err(\"max_lag too small for surface\".to_string());\n    }\n\n    let diag_main = average_autocorrelation(surface, mean, var, max_lag, 1, 1)?;\n    let diag_anti = average_autocorrelation(surface, mean, var, max_lag, 1, -1)?;\n    let axis_x = average_autocorrelation(surface, mean, var, max_lag, 1, 0)?;\n    let axis_y = average_autocorrelation(surface, mean, var, max_lag, 0, 1)?;\n\n    Ok((diag_main, diag_anti, axis_x, axis_y))\n}\n\nfn surface_dimensions(surface: &[Vec<f64>]) -> Result<(usize, usize), String> {\n    let height = surface.len();\n    if height == 0 {\n        return Err(\"surface is empty\".to_string());\n    }\n    let width = surface[0].len();\n    if width == 0 {\n        return Err(\"surface has empty rows\".to_string());\n    }\n    for (idx, row) in surface.iter().enumerate() {\n        if row.len() != width {\n            return Err(format!(\n                \"surface row {} has length {}, expected {}\",\n                idx,\n                row.len(),\n                width\n            ));\n        }\n    }\n    Ok((height, width))\n}\n\nfn surface_mean_variance(surface: &[Vec<f64>]) -> Result<(f64, f64), String> {\n    let (height, width) = surface_dimensions(surface)?;\n    let mut sum = 0.0;\n    for row in surface {\n        for value in row {\n            sum += *value;\n        }\n    }\n    let count = (height * width) as f64;\n    let mean = sum / count;\n    let mut variance = 0.0;\n    for row in surface {\n        for value in row {\n            let diff = *value - mean;\n            variance += diff * diff;\n        }\n    }\n    variance /= count;\n    Ok((mean, variance))\n}\n\nfn average_autocorrelation(\n    surface: &[Vec<f64>],\n    mean: f64,\n    variance: f64,\n    max_lag: usize,\n    dx: i32,\n    dy: i32,\n) -> Result<f64, String> {\n    let (height, width) = surface_dimensions(surface)?;\n    let mut total = 0.0;\n    let mut lag_count = 0usize;\n\n    for lag in 1..=max_lag {\n        let mut sum = 0.0;\n        let mut count = 0usize;\n        let lag = lag as i32;\n        for x in 0..height {\n            for y in 0..width {\n                let x2 = x as i32 + dx * lag;\n                let y2 = y as i32 + dy * lag;\n                if x2 < 0 || y2 < 0 {\n                    continue;\n                }\n                let x2 = x2 as usize;\n                let y2 = y2 as usize;\n                if x2 >= height || y2 >= width {\n                    continue;\n                }\n                let a = surface[x][y] - mean;\n                let b = surface[x2][y2] - mean;\n                sum += a * b;\n                count += 1;\n            }\n        }\n        if count == 0 {\n            continue;\n        }\n        let corr = sum / (count as f64 * variance);\n        total += corr.abs();\n        lag_count += 1;\n    }\n\n    if lag_count == 0 {\n        return Err(\"no valid lags for autocorrelation\".to_string());\n    }\n\n    Ok(total / lag_count as f64)\n}\n\nfn analyze_mlp_activation_wave(\n    neuron_index: usize,\n    signal: &[f64],\n    config: &MlpWaveVerificationConfig,\n) -> Result<MlpWaveNeuronReport, String> {\n    if signal.len() < 3 {\n        return Err(format!(\n            \"activation signal for neuron {} is too short\",\n            neuron_index\n        ));\n    }\n\n    let negative_count = signal\n        .iter()\n        .filter(|value| **value < -config.negative_tolerance)\n        .count();\n    let negative_fraction = negative_count as f64 / signal.len() as f64;\n    if negative_fraction > config.max_negative_fraction {\n        return Err(format!(\n            \"neuron {} has {:.3} negative activations\",\n            neuron_index, negative_fraction\n        ));\n    }\n\n    let (dominant_frequency, dominant_ratio) = dominant_frequency_ratio(signal)?;\n    if dominant_ratio < config.min_dominant_ratio {\n        return Err(format!(\n            \"neuron {} dominant frequency ratio {:.3} below threshold {:.3}\",\n            neuron_index, dominant_ratio, config.min_dominant_ratio\n        ));\n    }\n\n    let best_correlation =\n        best_rectified_sine_correlation(signal, dominant_frequency, config.phase_steps);\n    if best_correlation < config.min_correlation {\n        return Err(format!(\n            \"neuron {} rectified sine correlation {:.3} below threshold {:.3}\",\n            neuron_index, best_correlation, config.min_correlation\n        ));\n    }\n\n    Ok(MlpWaveNeuronReport {\n        neuron_index,\n        dominant_frequency,\n        dominant_ratio,\n        negative_fraction,\n        best_correlation,\n    })\n}\n\nfn dominant_frequency_ratio(signal: &[f64]) -> Result<(usize, f64), String> {\n    if signal.len() < 2 {\n        return Err(\"signal too short for FFT\".to_string());\n    }\n\n    let mut planner = FftPlanner::new();\n    let fft = planner.plan_fft_forward(signal.len());\n    let mut buffer: Vec<Complex<f64>> = signal\n        .iter()\n        .map(|&x| Complex::new(x, 0.0))\n        .collect();\n    fft.process(&mut buffer);\n    let magnitudes: Vec<f64> = buffer.iter().map(|c| c.norm()).collect();\n\n    if magnitudes.len() < 2 {\n        return Err(\"insufficient FFT bins\".to_string());\n    }\n\n    let mut dominant_idx = 1usize;\n    let mut dominant_mag = magnitudes[1];\n    for (idx, &mag) in magnitudes.iter().enumerate().skip(2) {\n        if mag > dominant_mag {\n            dominant_mag = mag;\n            dominant_idx = idx;\n        }\n    }\n\n    if dominant_idx == 0 {\n        return Err(\"dominant frequency is zero\".to_string());\n    }\n\n    let mut nonzero = magnitudes[1..].to_vec();\n    nonzero.sort_by(|a, b| a.partial_cmp(b).unwrap_or(std::cmp::Ordering::Equal));\n    let median_mag = nonzero[nonzero.len() / 2];\n    let ratio = if median_mag == 0.0 {\n        dominant_mag\n    } else {\n        dominant_mag / median_mag\n    };\n\n    Ok((dominant_idx, ratio))\n}\n\nfn best_rectified_sine_correlation(signal: &[f64], frequency: usize, phase_steps: usize) -> f64 {\n    if signal.is_empty() || frequency == 0 || phase_steps == 0 {\n        return 0.0;\n    }\n\n    let tau = std::f64::consts::PI * 2.0;\n    let mut best = -1.0;\n    for step in 0..phase_steps {\n        let phase = tau * step as f64 / phase_steps as f64;\n        let wave: Vec<f64> = (0..signal.len())\n            .map(|idx| {\n                let angle = tau * frequency as f64 * idx as f64 / signal.len() as f64 + phase;\n                angle.sin().max(0.0)\n            })\n            .collect();\n        let corr = pearson_correlation(signal, &wave);\n        if corr > best {\n            best = corr;\n        }\n    }\n\n    best\n}\n\nfn pearson_correlation(a: &[f64], b: &[f64]) -> f64 {\n    if a.len() != b.len() || a.is_empty() {\n        return 0.0;\n    }\n\n    let mean_a = a.iter().sum::<f64>() / a.len() as f64;\n    let mean_b = b.iter().sum::<f64>() / b.len() as f64;\n\n    let mut num = 0.0;\n    let mut denom_a = 0.0;\n    let mut denom_b = 0.0;\n    for (x, y) in a.iter().zip(b.iter()) {\n        let da = x - mean_a;\n        let db = y - mean_b;\n        num += da * db;\n        denom_a += da * da;\n        denom_b += db * db;\n    }\n\n    if denom_a == 0.0 || denom_b == 0.0 {\n        return 0.0;\n    }\n\n    num / (denom_a.sqrt() * denom_b.sqrt())\n}\n\n/// Verify that the model actually learned modular addition\n/// by testing it on ALL possible examples systematically\npub fn verify_full_accuracy<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n) -> (f32, f32) {\n    println!(\"ðŸ” Verifying model on COMPLETE dataset (no sampling)...\");\n\n    let train_dataset = ModularAdditionDataset::new(true, 42);\n    let val_dataset = ModularAdditionDataset::new(false, 42);\n\n    let train_acc = test_all_examples(model, &train_dataset, device);\n    let val_acc = test_all_examples(model, &val_dataset, device);\n\n    println!(\"ðŸ“Š FULL Dataset Results:\");\n    println!(\"   Training: {}/{} = {:.2}%\",\n        (train_acc * train_dataset.len() as f32) as usize,\n        train_dataset.len(),\n        train_acc * 100.0);\n    println!(\"   Validation: {}/{} = {:.2}%\",\n        (val_acc * val_dataset.len() as f32) as usize,\n        val_dataset.len(),\n        val_acc * 100.0);\n\n    (train_acc, val_acc)\n}\n\n/// Test model on ALL examples in dataset (no random sampling)\nfn test_all_examples<B: Backend>(\n    model: &Transformer<B>,\n    dataset: &ModularAdditionDataset,\n    device: &B::Device,\n) -> f32 {\n    let mut total_correct = 0;\n    let total = dataset.len();\n\n    // Test in batches to avoid memory issues\n    let batch_size = 100;\n    for batch_start in (0..total).step_by(batch_size) {\n        let batch_end = (batch_start + batch_size).min(total);\n        let batch_len = batch_end - batch_start;\n\n        let mut inputs_vec = Vec::new();\n        let mut targets_vec = Vec::new();\n\n        for idx in batch_start..batch_end {\n            let (input, target) = dataset.get(idx).unwrap();\n            inputs_vec.extend(input);\n            targets_vec.push(target as i32);\n        }\n\n        let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n            .reshape([batch_len, 3]);\n\n        let logits = model.forward(inputs);\n        let predictions = logits.argmax(1).squeeze::<1>();\n        let predictions_vec: Vec<i32> = predictions.into_data().to_vec().unwrap();\n\n        let correct = predictions_vec\n            .iter()\n            .zip(targets_vec.iter())\n            .filter(|(p, t)| p == t)\n            .count();\n\n        total_correct += correct;\n    }\n\n    total_correct as f32 / total as f32\n}\n\n/// Verify that weight norms decrease over time with weight decay\n/// and show correlation with the grokking transition\npub fn verify_weight_norm_decay(\n    weight_norm_history: &[(usize, f64)],\n    grok_step: Option<usize>,\n    config: &WeightNormDecayConfig,\n) -> Result<WeightNormDecayReport, String> {\n    if weight_norm_history.len() < 2 {\n        return Err(\"weight norm history requires at least 2 snapshots\".to_string());\n    }\n\n    let initial_norm = weight_norm_history\n        .first()\n        .ok_or_else(|| \"weight norm history is empty\".to_string())?\n        .1;\n    let final_norm = weight_norm_history\n        .last()\n        .ok_or_else(|| \"weight norm history is empty\".to_string())?\n        .1;\n\n    if initial_norm <= 0.0 {\n        return Err(\"initial weight norm must be positive\".to_string());\n    }\n\n    // Check that norms are generally decreasing (with weight decay)\n    let relative_decrease = (initial_norm - final_norm) / initial_norm;\n    if relative_decrease < config.min_relative_decrease {\n        return Err(format!(\n            \"weight norm decrease {:.4} below threshold {:.4} (initial {:.4}, final {:.4})\",\n            relative_decrease, config.min_relative_decrease, initial_norm, final_norm\n        ));\n    }\n\n    // Check that norms are not increasing (weight decay should monotonically decrease norms)\n    for window in weight_norm_history.windows(2) {\n        let prev_norm = window[0].1;\n        let next_norm = window[1].1;\n        if next_norm > prev_norm * 1.01 {\n            // Allow 1% tolerance for numerical noise\n            return Err(format!(\n                \"weight norm increased from {:.4} to {:.4} at step {}\",\n                prev_norm, next_norm, window[1].0\n            ));\n        }\n    }\n\n    // If grok step is provided, check that norms show a shift around grokking\n    let grok_step_norm = if let Some(step) = grok_step {\n        weight_norm_history\n            .iter()\n            .filter(|(s, _)| {\n                *s >= step.saturating_sub(config.grok_step_window)\n                    && *s <= step + config.grok_step_window\n            })\n            .map(|(_, norm)| *norm)\n            .next()\n    } else {\n        None\n    };\n\n    Ok(WeightNormDecayReport {\n        initial_norm,\n        final_norm,\n        relative_decrease,\n        grok_step_norm,\n    })\n}\n\n/// Verify the characteristic \"snake curve\" shape of grokking phenomenon\n/// on log-scale plots: train accuracy rises early, validation stays flat then jumps\npub fn verify_snake_curve_shape(\n    train_acc: &[(usize, f32)],\n    val_acc: &[(usize, f32)],\n    config: &SnakeCurveConfig,\n) -> Result<SnakeCurveReport, String> {\n    if train_acc.is_empty() || val_acc.is_empty() {\n        return Err(\"empty accuracy history\".to_string());\n    }\n\n    // Phase 1: Verify early train convergence (immediate drop on log scale)\n    let train_converged_step = train_acc\n        .iter()\n        .find(|(_, acc)| *acc >= config.train_convergence_threshold)\n        .map(|(step, _)| *step)\n        .ok_or_else(|| {\n            format!(\n                \"train accuracy never reaches {} threshold\",\n                config.train_convergence_threshold\n            )\n        })?;\n\n    if train_converged_step > config.early_step_max {\n        return Err(format!(\n            \"train accuracy converges at step {}, expected before {}\",\n            train_converged_step, config.early_step_max\n        ));\n    }\n\n    // Phase 2: Verify validation plateau (flat on log scale)\n    let plateau_samples: Vec<f32> = val_acc\n        .iter()\n        .filter(|(step, _)| *step >= 1 && *step <= config.val_plateau_min_step)\n        .map(|(_, acc)| *acc)\n        .collect();\n\n    if plateau_samples.is_empty() {\n        return Err(\"no validation samples in plateau region\".to_string());\n    }\n\n    let val_plateau_max = plateau_samples\n        .iter()\n        .fold(0.0f32, |max_val, &acc| max_val.max(acc));\n\n    if val_plateau_max > config.val_plateau_max_acc {\n        return Err(format!(\n            \"validation accuracy in plateau region too high: max {:.4} exceeds {:.4}\",\n            val_plateau_max, config.val_plateau_max_acc\n        ));\n    }\n\n    // Phase 3: Verify delayed validation jump (crash up on log scale)\n    let val_grok_step = val_acc\n        .iter()\n        .filter(|(step, _)| *step > config.val_plateau_min_step)\n        .find(|(_, acc)| *acc >= config.val_final_threshold)\n        .map(|(step, _)| *step)\n        .ok_or_else(|| {\n            format!(\n                \"validation accuracy never reaches {} after plateau\",\n                config.val_final_threshold\n            )\n        })?;\n\n    // Phase 4: Verify phase ordering (snake shape: early train, late val)\n    let curve_shape_valid = train_converged_step < config.val_plateau_min_step\n        && val_grok_step > config.val_plateau_min_step\n        && train_converged_step < val_grok_step;\n\n    if !curve_shape_valid {\n        return Err(format!(\n            \"invalid curve shape: train converged at {}, val plateau until {}, val grokked at {}\",\n            train_converged_step, config.val_plateau_min_step, val_grok_step\n        ));\n    }\n\n    Ok(SnakeCurveReport {\n        train_converged_by_step: train_converged_step,\n        val_plateau_max_acc: val_plateau_max,\n        val_grok_step,\n        curve_shape_valid,\n    })\n}\n\n/// Test specific examples to see if model actually computes mod p\npub fn test_specific_examples<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n) {\n    println!();\n    println!(\"ðŸ§ª Testing specific examples:\");\n\n    let modulus = ModularAdditionDataset::modulus();\n    let equals_token = ModularAdditionDataset::equals_token();\n    let test_cases = vec![\n        (0, 0, 0),\n        (1, 1, 2),\n        (50, 50, (50 + 50) % modulus),\n        (112, 1, (112 + 1) % modulus),\n        (56, 57, (56 + 57) % modulus),\n        (112, 112, (112 + 112) % modulus),\n        (10, 20, 30),\n        (60, 60, (60 + 60) % modulus),\n    ];\n\n    for (a, b, expected) in test_cases {\n        let input_vec = vec![a as i32, b as i32, equals_token as i32];\n        let input = Tensor::<B, 1, Int>::from_ints(input_vec.as_slice(), device)\n            .reshape([1, 3]);\n\n        let logits = model.forward(input);\n        let prediction = logits.argmax(1).squeeze::<1>();\n        let pred_value: i32 = prediction.into_data().to_vec().unwrap()[0];\n\n        let correct = if pred_value == expected as i32 { \"âœ“\" } else { \"âœ—\" };\n        println!(\n            \"   {} + {} mod {} = {} | Model: {} {}\",\n            a, b, modulus, expected, pred_value, correct\n        );\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    fn make_histories(\n        train_acc: &[(usize, f32)],\n        val_acc: &[(usize, f32)],\n        val_loss: &[(usize, f64)],\n    ) -> (LossHistory, AccuracyHistory) {\n        let mut loss_history = LossHistory::new();\n        loss_history.val_snapshots = val_loss.to_vec();\n        let mut accuracy_history = AccuracyHistory::new();\n        accuracy_history.train_snapshots = train_acc.to_vec();\n        accuracy_history.val_snapshots = val_acc.to_vec();\n        (loss_history, accuracy_history)\n    }\n\n    #[test]\n    fn grokking_phase_verification_passes_on_synthetic_signal() {\n        let train_acc = vec![(0, 0.2), (500, 0.995), (1000, 0.999)];\n        let val_acc = vec![\n            (0, 0.01),\n            (500, 0.01),\n            (1500, 0.01),\n            (2200, 0.995),\n            (2500, 0.999),\n        ];\n        let val_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 2.0),\n            (2200, 0.2),\n            (2500, 0.15),\n        ];\n\n        let (loss_history, accuracy_history) = make_histories(&train_acc, &val_acc, &val_loss);\n        let config = GrokkingVerificationConfig {\n            early_step_max: 1000,\n            plateau_min_step: 1500,\n            generalization_window: 1500,\n            accuracy_window: 2,\n            loss_window: 2,\n            ..GrokkingVerificationConfig::default_for_modulus(ModularAdditionDataset::modulus())\n        };\n\n        let report = verify_grokking_phase_transition(&loss_history, &accuracy_history, &config)\n            .expect(\"expected grokking verification to pass\");\n        assert!(report.transition_step >= config.plateau_min_step);\n        assert!(report.early_train_step <= config.early_step_max);\n    }\n\n    #[test]\n    fn grokking_phase_verification_fails_without_transition() {\n        let train_acc = vec![(0, 0.2), (500, 0.995)];\n        let val_acc = vec![(0, 0.01), (500, 0.01), (1500, 0.02)];\n        let val_loss = vec![(0, 2.0), (500, 2.0), (1500, 1.9)];\n\n        let (loss_history, accuracy_history) = make_histories(&train_acc, &val_acc, &val_loss);\n        let config = GrokkingVerificationConfig {\n            early_step_max: 1000,\n            plateau_min_step: 1000,\n            generalization_window: 1000,\n            accuracy_window: 2,\n            loss_window: 2,\n            ..GrokkingVerificationConfig::default_for_modulus(ModularAdditionDataset::modulus())\n        };\n\n        let err = verify_grokking_phase_transition(&loss_history, &accuracy_history, &config)\n            .expect_err(\"expected grokking verification to fail\");\n        assert!(err.contains(\"validation accuracy never reaches target\"));\n    }\n\n    #[test]\n    fn grokking_phase_verification_fails_with_slow_accuracy_rise() {\n        let train_acc = vec![(0, 0.2), (500, 0.995), (1000, 0.999)];\n        let val_acc = vec![\n            (0, 0.01),\n            (500, 0.01),\n            (1500, 0.5),\n            (2000, 0.6),\n            (2200, 0.995),\n            (2500, 0.999),\n        ];\n        let val_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 2.0),\n            (2200, 0.2),\n            (2500, 0.15),\n        ];\n\n        let (loss_history, accuracy_history) = make_histories(&train_acc, &val_acc, &val_loss);\n        let config = GrokkingVerificationConfig {\n            early_step_max: 1000,\n            plateau_min_step: 1000,\n            generalization_window: 2000,\n            accuracy_window: 2,\n            loss_window: 2,\n            ..GrokkingVerificationConfig::default_for_modulus(ModularAdditionDataset::modulus())\n        };\n\n        let err = verify_grokking_phase_transition(&loss_history, &accuracy_history, &config)\n            .expect_err(\"expected grokking verification to fail\");\n        assert!(err.contains(\"plateau too high\"));\n    }\n\n    #[test]\n    fn restricted_loss_detection_passes_with_early_drop() {\n        let full_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 2.0),\n            (2500, 2.0),\n            (3500, 0.3),\n        ];\n        let restricted_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 0.8),\n            (2500, 0.4),\n            (3500, 0.3),\n        ];\n\n        let config = RestrictedLossVerificationConfig {\n            plateau_min_step: 1000,\n            drop_window: 2,\n            drop_fraction: 0.5,\n            min_step_lead: 500,\n        };\n\n        let report = verify_restricted_loss_early_drop(&full_loss, &restricted_loss, &config)\n            .expect(\"expected restricted loss verification to pass\");\n        assert!(report.restricted_drop_step < report.full_drop_step);\n    }\n\n    #[test]\n    fn restricted_loss_detection_fails_when_full_drops_first() {\n        let full_loss = vec![(0, 2.0), (1000, 1.0), (1500, 0.8)];\n        let restricted_loss = vec![(0, 2.0), (1000, 2.0), (1500, 0.8)];\n\n        let config = RestrictedLossVerificationConfig {\n            plateau_min_step: 500,\n            drop_window: 1,\n            drop_fraction: 0.3,\n            min_step_lead: 200,\n        };\n\n        let err = verify_restricted_loss_early_drop(&full_loss, &restricted_loss, &config)\n            .expect_err(\"expected restricted loss verification to fail\");\n        assert!(err.contains(\"restricted loss drops\"));\n    }\n\n    #[test]\n    fn excluded_loss_spike_verification_passes() {\n        let excluded_loss = vec![(0, 1.0), (1500, 2.2), (3000, 1.1)];\n        let config = ExcludedLossSpikeConfig {\n            min_relative_increase: 0.5,\n        };\n\n        let report = verify_excluded_loss_spike(&excluded_loss, &config)\n            .expect(\"expected excluded loss spike verification to pass\");\n        assert_eq!(report.spike_step, 1500);\n    }\n\n    #[test]\n    fn excluded_loss_spike_verification_fails_without_spike() {\n        let excluded_loss = vec![(0, 1.0), (1500, 1.1), (3000, 1.0)];\n        let config = ExcludedLossSpikeConfig {\n            min_relative_increase: 0.5,\n        };\n\n        let err = verify_excluded_loss_spike(&excluded_loss, &config)\n            .expect_err(\"expected excluded loss spike verification to fail\");\n        assert!(err.contains(\"excluded loss spike too small\"));\n    }\n\n    #[test]\n    fn restricted_loss_drop_verification_passes() {\n        let restricted_loss = vec![\n            (0, 2.0),\n            (1000, 1.8),\n            (5000, 1.2),\n            (7000, 0.8),\n            (10000, 0.9),\n        ];\n        let config = RestrictedLossDropConfig {\n            min_relative_decrease: 0.3,\n        };\n\n        let report = verify_restricted_loss_drop(&restricted_loss, &config)\n            .expect(\"expected restricted loss drop verification to pass\");\n        assert_eq!(report.drop_step, 7000);\n        assert!(report.baseline_loss > report.drop_loss);\n        assert!((report.baseline_loss - report.drop_loss) / report.baseline_loss >= 0.3);\n    }\n\n    #[test]\n    fn restricted_loss_drop_verification_fails_without_drop() {\n        let restricted_loss = vec![\n            (0, 2.0),\n            (1000, 2.1),\n            (5000, 1.9),\n            (7000, 2.0),\n            (10000, 2.05),\n        ];\n        let config = RestrictedLossDropConfig {\n            min_relative_decrease: 0.3,\n        };\n\n        let err = verify_restricted_loss_drop(&restricted_loss, &config)\n            .expect_err(\"expected restricted loss drop verification to fail\");\n        assert!(err.contains(\"restricted loss decrease too small\"));\n    }\n\n    #[test]\n    fn mlp_activation_wave_verification_passes_on_rectified_sine() {\n        let n = ModularAdditionDataset::modulus();\n        let freq = 5.0;\n        let mut signal = Vec::with_capacity(n);\n        for idx in 0..n {\n            let angle = 2.0 * std::f64::consts::PI * freq * idx as f64 / n as f64;\n            let rectified = angle.sin().max(0.0);\n            let wobble = (2.0 * std::f64::consts::PI * 2.0 * idx as f64 / n as f64)\n                .sin()\n                .abs()\n                * 0.02;\n            signal.push(rectified + wobble);\n        }\n\n        let config = MlpWaveVerificationConfig {\n            max_negative_fraction: 0.0,\n            negative_tolerance: 1e-6,\n            min_dominant_ratio: 3.0,\n            min_correlation: 0.7,\n            phase_steps: 32,\n        };\n\n        let report =\n            verify_mlp_activation_waves_from_signals(&[(0, signal)], &config)\n                .expect(\"expected rectified sine to pass\");\n        assert_eq!(report.neuron_reports.len(), 1);\n    }\n\n    #[test]\n    fn mlp_activation_wave_verification_fails_on_noise() {\n        let n = ModularAdditionDataset::modulus();\n        let signal: Vec<f64> = (0..n).map(|idx| idx as f64 / n as f64).collect();\n\n        let config = MlpWaveVerificationConfig {\n            max_negative_fraction: 0.0,\n            negative_tolerance: 1e-6,\n            min_dominant_ratio: 4.0,\n            min_correlation: 0.8,\n            phase_steps: 16,\n        };\n\n        let err =\n            verify_mlp_activation_waves_from_signals(&[(0, signal)], &config)\n                .expect_err(\"expected linear signal to fail\");\n        assert!(err.contains(\"dominant frequency ratio\") || err.contains(\"correlation\"));\n    }\n\n    #[test]\n    fn pairwise_manifold_transition_passes_on_elliptic_signals() {\n        let n = ModularAdditionDataset::modulus();\n        let freq = 5.0;\n        let mut post_signals = Vec::new();\n        for neuron_idx in 0..7usize {\n            let phase = 2.0 * std::f64::consts::PI * neuron_idx as f64 / 7.0;\n            let signal: Vec<f64> = (0..n)\n                .map(|idx| {\n                    let angle = 2.0 * std::f64::consts::PI * freq * idx as f64 / n as f64 + phase;\n                    angle.sin()\n                })\n                .collect();\n            post_signals.push((neuron_idx, signal));\n        }\n\n        let mut pre_signals = Vec::new();\n        for neuron_idx in 0..7usize {\n            let signal: Vec<f64> = (0..n)\n                .map(|idx| idx as f64 / n as f64 + neuron_idx as f64 * 0.01)\n                .collect();\n            pre_signals.push((neuron_idx, signal));\n        }\n\n        let config = PairwiseManifoldConfig {\n            fixed_y: 0,\n            neuron_indices: (0..7).collect(),\n            min_post_score: 0.75,\n            max_pre_score: 0.55,\n            max_axis_ratio: 8.0,\n            min_pairs_passing: 8,\n        };\n\n        let report = verify_pairwise_manifold_transition_from_signals(\n            &pre_signals,\n            &post_signals,\n            &config,\n        )\n        .expect(\"expected pairwise manifold transition to pass\");\n        assert!(report.passing_pairs >= config.min_pairs_passing);\n    }\n\n    #[test]\n    fn pairwise_manifold_transition_fails_without_rings() {\n        let n = ModularAdditionDataset::modulus();\n        let mut signals = Vec::new();\n        for neuron_idx in 0..7usize {\n            let signal: Vec<f64> = (0..n)\n                .map(|idx| idx as f64 / n as f64 + neuron_idx as f64 * 0.02)\n                .collect();\n            signals.push((neuron_idx, signal));\n        }\n\n        let config = PairwiseManifoldConfig {\n            fixed_y: 0,\n            neuron_indices: (0..7).collect(),\n            min_post_score: 0.8,\n            max_pre_score: 0.55,\n            max_axis_ratio: 4.0,\n            min_pairs_passing: 5,\n        };\n\n        let err = verify_pairwise_manifold_transition_from_signals(\n            &signals,\n            &signals,\n            &config,\n        )\n        .expect_err(\"expected pairwise manifold transition to fail\");\n        assert!(err.contains(\"post-grok\") || err.contains(\"pre-grok\"));\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_passes_on_diagonal_corrugation() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create diagonal ridges: f(x,y) = sin(2Ï€ * freq * (x+y) / n)\n        let freq = 3.0;\n        for x in 0..n {\n            for y in 0..n {\n                let angle = 2.0 * std::f64::consts::PI * freq * (x + y) as f64 / n as f64;\n                surface[x][y] = angle.sin();\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let report = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect(\"expected diagonal ridge verification to pass\");\n        assert!(report.diagonal_score > report.axis_score);\n        assert!(report.ratio >= config.min_diagonal_ratio);\n        assert!(report.diagonal_score >= config.min_diagonal_score);\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_passes_on_anti_diagonal_pattern() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create anti-diagonal ridges: f(x,y) = sin(2Ï€ * freq * (x-y) / n)\n        let freq = 3.0;\n        for x in 0..n {\n            for y in 0..n {\n                let angle = 2.0 * std::f64::consts::PI * freq * (x as i32 - y as i32) as f64 / n as f64;\n                surface[x][y] = angle.sin();\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let report = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect(\"expected anti-diagonal ridge verification to pass\");\n        assert!(report.diagonal_score > report.axis_score);\n        assert!(report.ratio >= config.min_diagonal_ratio);\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_fails_on_axis_aligned_pattern() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create axis-aligned ridges: f(x,y) = sin(2Ï€ * freq * x / n)\n        let freq = 3.0;\n        for x in 0..n {\n            for y in 0..n {\n                let angle = 2.0 * std::f64::consts::PI * freq * x as f64 / n as f64;\n                surface[x][y] = angle.sin();\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let err = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect_err(\"expected axis-aligned pattern to fail diagonal verification\");\n        assert!(err.contains(\"diagonal/axis ratio\") || err.contains(\"diagonal autocorrelation\"));\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_fails_on_flat_surface() {\n        let n = 50;\n        let surface = vec![vec![1.5; n]; n];\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let err = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect_err(\"expected flat surface to fail\");\n        assert!(err.contains(\"surface variance is zero\"));\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_fails_on_random_noise() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create random noise (low autocorrelation everywhere)\n        for x in 0..n {\n            for y in 0..n {\n                surface[x][y] = ((x * 17 + y * 31) % 100) as f64 / 100.0;\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let err = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect_err(\"expected random noise to fail diagonal verification\");\n        assert!(err.contains(\"diagonal/axis ratio\") || err.contains(\"diagonal autocorrelation\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_passes_on_valid_grokking_curve() {\n        // Simulate typical grokking: train converges early, val plateaus then jumps\n        let train_acc = vec![\n            (1, 0.10),\n            (10, 0.50),\n            (100, 0.80),\n            (500, 0.98),\n            (1000, 0.99),\n            (2000, 0.995),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (10, 0.009),\n            (100, 0.01),\n            (500, 0.011),\n            (1000, 0.009),\n            (1500, 0.01),\n            (2000, 0.92),\n            (3000, 0.98),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 1500,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let report = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect(\"expected snake curve verification to pass\");\n\n        assert_eq!(report.train_converged_by_step, 500);\n        assert!(report.val_plateau_max_acc < config.val_plateau_max_acc);\n        assert_eq!(report.val_grok_step, 2000);\n        assert!(report.curve_shape_valid);\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_on_simultaneous_convergence() {\n        // Both train and val converge at the same time (no grokking)\n        let train_acc = vec![\n            (1, 0.10),\n            (100, 0.98),\n            (500, 0.99),\n        ];\n        let val_acc = vec![\n            (1, 0.10),\n            (100, 0.97),\n            (500, 0.98),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 200,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let err = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect_err(\"expected snake curve verification to fail\");\n        assert!(err.contains(\"plateau region too high\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_on_slow_train_convergence() {\n        // Train takes too long to converge (not the typical snake curve)\n        let train_acc = vec![\n            (1, 0.10),\n            (500, 0.50),\n            (1000, 0.70),\n            (2000, 0.98),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (500, 0.01),\n            (1000, 0.01),\n            (2000, 0.92),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 1500,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let err = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect_err(\"expected snake curve verification to fail\");\n        assert!(err.contains(\"converges at step 2000\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_without_val_jump() {\n        // Validation never jumps up (no generalization)\n        let train_acc = vec![\n            (1, 0.10),\n            (100, 0.98),\n            (500, 0.99),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (100, 0.01),\n            (500, 0.01),\n            (2000, 0.02),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 200,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let err = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect_err(\"expected snake curve verification to fail\");\n        assert!(err.contains(\"validation accuracy never reaches\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_on_wrong_phase_ordering() {\n        // Val jumps before train converges (backwards snake)\n        let train_acc = vec![\n            (1, 0.10),\n            (500, 0.50),\n            (2000, 0.98),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (100, 0.95),\n            (500, 0.98),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 200,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let result = verify_snake_curve_shape(&train_acc, &val_acc, &config);\n        // Should fail because val jumps before train converges (not a proper snake curve)\n        // This can fail for multiple reasons, but should definitely fail\n        assert!(result.is_err(), \"Expected snake curve verification to fail but it passed\");\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_passes_on_decreasing_norms() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 85.0),\n            (1000, 72.0),\n            (1500, 63.0),\n            (2000, 58.0),\n            (2500, 55.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let report = verify_weight_norm_decay(&weight_norms, Some(2000), &config)\n            .expect(\"expected weight norm decay verification to pass\");\n\n        assert_eq!(report.initial_norm, 100.0);\n        assert_eq!(report.final_norm, 55.0);\n        assert!((report.relative_decrease - 0.45).abs() < 0.01);\n        assert!(report.grok_step_norm.is_some());\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_fails_on_flat_norms() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 99.0),\n            (1000, 98.5),\n            (1500, 98.0),\n            (2000, 97.5),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let err = verify_weight_norm_decay(&weight_norms, None, &config)\n            .expect_err(\"expected weight norm decay verification to fail\");\n        assert!(err.contains(\"weight norm decrease\"));\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_fails_on_increasing_norms() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 95.0),\n            (1000, 105.0), // Increase here\n            (1500, 90.0),\n            (2000, 85.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let err = verify_weight_norm_decay(&weight_norms, None, &config)\n            .expect_err(\"expected weight norm decay verification to fail\");\n        assert!(err.contains(\"weight norm increased\"));\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_allows_small_noise() {\n        // Weight decay with small numerical noise (within 1% tolerance)\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 95.0),\n            (1000, 90.5),  // Small increase within tolerance\n            (1500, 90.0),\n            (2000, 85.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let report = verify_weight_norm_decay(&weight_norms, None, &config)\n            .expect(\"expected weight norm decay with small noise to pass\");\n\n        assert_eq!(report.initial_norm, 100.0);\n        assert_eq!(report.final_norm, 85.0);\n        assert!((report.relative_decrease - 0.15).abs() < 0.01);\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_finds_grok_step_norm() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (1000, 80.0),\n            (1800, 68.0),  // Within window of grok step (2200 - 500 = 1700)\n            (2000, 65.0),\n            (2200, 62.0),  // Around grok step\n            (3000, 55.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let report = verify_weight_norm_decay(&weight_norms, Some(2200), &config)\n            .expect(\"expected weight norm decay verification to pass\");\n\n        assert!(report.grok_step_norm.is_some());\n        let grok_norm = report.grok_step_norm.unwrap();\n        // Should find the first norm in the window [1700, 2700], which is 68.0 at step 1800\n        assert!((grok_norm - 68.0).abs() < 0.01);\n    }\n}\n","structuredPatch":[{"oldStart":393,"oldLines":6,"newStart":393,"newLines":8,"lines":["     Ok(after_avg / before_avg)"," }"," ","+/// NOTE: Verification function reserved for future integration or manual analysis","+#[allow(dead_code)]"," pub fn verify_restricted_loss_early_drop(","     full_losses: &[(usize, f64)],","     restricted_losses: &[(usize, f64)],"]}],"userModified":false,"replaceAll":false}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01Jy5f3CEyqhN8RcCHoGWGb2","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_0186ju3YtZvF75x6ixpxCumS","name":"Edit","input":{"replace_all":false,"file_path":"/Users/admin/MechInt/src/verify.rs","old_string":"pub fn verify_mlp_activation_waves<B: Backend>(","new_string":"/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_mlp_activation_waves<B: Backend>("}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":556,"cache_read_input_tokens":57545,"cache_creation":{"ephemeral_5m_input_tokens":556,"ephemeral_1h_input_tokens":0},"output_tokens":174,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"0ec9b971-321f-4001-8237-31e0783d57f4"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_0186ju3YtZvF75x6ixpxCumS","type":"tool_result","content":"The file /Users/admin/MechInt/src/verify.rs has been updated. Here's the result of running `cat -n` on a snippet of the edited file:\n   511â†’        baseline_loss,\n   512â†’    })\n   513â†’}\n   514â†’\n   515â†’/// NOTE: Verification function reserved for future integration or manual analysis\n   516â†’#[allow(dead_code)]\n   517â†’pub fn verify_mlp_activation_waves<B: Backend>(\n   518â†’    model: &Transformer<B>,\n   519â†’    device: &B::Device,\n   520â†’    fixed_y: usize,\n   521â†’    neuron_indices: &[usize],"}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"5b1e3e47-67e3-4ec6-b6cf-43b670d5085e","tool_use_result":{"filePath":"/Users/admin/MechInt/src/verify.rs","oldString":"pub fn verify_mlp_activation_waves<B: Backend>(","newString":"/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_mlp_activation_waves<B: Backend>(","originalFile":"use burn::{\n    data::dataset::Dataset,\n    tensor::{backend::Backend, Int, Tensor},\n};\nuse rustfft::{num_complex::Complex, FftPlanner};\n\nuse crate::analysis::{AccuracyHistory, LossHistory};\nuse crate::data::ModularAdditionDataset;\nuse crate::model::Transformer;\n\n#[derive(Debug, Clone)]\npub struct GrokkingVerificationConfig {\n    pub early_train_acc_threshold: f32,\n    pub early_step_max: usize,\n    pub plateau_min_step: usize,\n    pub generalization_window: usize,\n    pub chance_accuracy: f32,\n    pub chance_tolerance: f32,\n    pub target_val_acc_threshold: f32,\n    pub accuracy_window: usize,\n    pub loss_window: usize,\n    pub loss_drop_fraction: f64,\n}\n\nimpl GrokkingVerificationConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        Self {\n            early_train_acc_threshold: 0.99,\n            early_step_max: 1000,\n            plateau_min_step: 1000,\n            generalization_window: 10_000,\n            chance_accuracy: 1.0 / modulus as f32,\n            chance_tolerance: 0.02,\n            target_val_acc_threshold: 0.99,\n            accuracy_window: 5,\n            loss_window: 5,\n            loss_drop_fraction: 0.5,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct GrokkingPhaseReport {\n    pub early_train_step: usize,\n    pub transition_step: usize,\n    pub plateau_max_val_acc: f32,\n    pub loss_drop_ratio: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossVerificationConfig {\n    pub plateau_min_step: usize,\n    pub drop_window: usize,\n    pub drop_fraction: f64,\n    pub min_step_lead: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossReport {\n    pub restricted_drop_step: usize,\n    pub full_drop_step: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossDropConfig {\n    pub min_relative_decrease: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossDropReport {\n    pub drop_step: usize,\n    pub drop_loss: f64,\n    pub baseline_loss: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct ExcludedLossSpikeConfig {\n    pub min_relative_increase: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct ExcludedLossSpikeReport {\n    pub spike_step: usize,\n    pub spike_loss: f64,\n    pub baseline_loss: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct MlpWaveVerificationConfig {\n    pub max_negative_fraction: f64,\n    pub negative_tolerance: f64,\n    pub min_dominant_ratio: f64,\n    pub min_correlation: f64,\n    pub phase_steps: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct MlpWaveNeuronReport {\n    pub neuron_index: usize,\n    pub dominant_frequency: usize,\n    pub dominant_ratio: f64,\n    pub negative_fraction: f64,\n    pub best_correlation: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct MlpWaveReport {\n    pub neuron_reports: Vec<MlpWaveNeuronReport>,\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldConfig {\n    pub fixed_y: usize,\n    pub neuron_indices: Vec<usize>,\n    pub min_post_score: f64,\n    pub max_pre_score: f64,\n    pub max_axis_ratio: f64,\n    pub min_pairs_passing: usize,\n}\n\nimpl PairwiseManifoldConfig {\n    pub fn default_for_modulus(_modulus: usize) -> Self {\n        Self {\n            fixed_y: 0,\n            neuron_indices: (0..7).collect(),\n            min_post_score: 0.75,\n            max_pre_score: 0.55,\n            max_axis_ratio: 8.0,\n            min_pairs_passing: 10,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldPairReport {\n    pub neuron_a: usize,\n    pub neuron_b: usize,\n    pub circularity_score: f64,\n    pub radius_cv: f64,\n    pub axis_ratio: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldReport {\n    pub pair_reports: Vec<PairwiseManifoldPairReport>,\n    pub average_score: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldTransitionReport {\n    pub pre_average_score: f64,\n    pub post_average_score: f64,\n    pub passing_pairs: usize,\n    pub total_pairs: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct DiagonalRidgeConfig {\n    pub neuron_index: usize,\n    pub max_lag: usize,\n    pub min_diagonal_ratio: f64,\n    pub min_diagonal_score: f64,\n}\n\nimpl DiagonalRidgeConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        let max_lag = (modulus / 10).max(4).min(20);\n        Self {\n            neuron_index: 10,\n            max_lag,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct DiagonalRidgeReport {\n    pub neuron_index: usize,\n    pub diagonal_score: f64,\n    pub axis_score: f64,\n    pub ratio: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct SnakeCurveConfig {\n    pub early_step_max: usize,\n    pub train_convergence_threshold: f32,\n    pub val_plateau_min_step: usize,\n    pub val_plateau_max_acc: f32,\n    pub val_final_threshold: f32,\n}\n\nimpl SnakeCurveConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        Self {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 1000,\n            val_plateau_max_acc: 1.0 / modulus as f32 + 0.05,\n            val_final_threshold: 0.90,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct WeightNormDecayConfig {\n    pub min_relative_decrease: f64,\n    pub plateau_min_step: usize,\n    pub grok_step_window: usize,\n}\n\nimpl WeightNormDecayConfig {\n    pub fn default_for_grokking() -> Self {\n        Self {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct WeightNormDecayReport {\n    pub initial_norm: f64,\n    pub final_norm: f64,\n    pub relative_decrease: f64,\n    pub grok_step_norm: Option<f64>,\n}\n\n#[derive(Debug, Clone)]\npub struct SnakeCurveReport {\n    pub train_converged_by_step: usize,\n    pub val_plateau_max_acc: f32,\n    pub val_grok_step: usize,\n    pub curve_shape_valid: bool,\n}\n\npub fn verify_grokking_phase_transition(\n    loss_history: &LossHistory,\n    accuracy_history: &AccuracyHistory,\n    config: &GrokkingVerificationConfig,\n) -> Result<GrokkingPhaseReport, String> {\n    let (early_train_step, _) = accuracy_history\n        .train_snapshots\n        .iter()\n        .find(|(_, acc)| *acc >= config.early_train_acc_threshold)\n        .ok_or_else(|| \"train accuracy never reaches threshold\".to_string())?;\n\n    if *early_train_step > config.early_step_max {\n        return Err(format!(\n            \"train accuracy reaches {:.2} after step {}, expected before {}\",\n            config.early_train_acc_threshold, early_train_step, config.early_step_max\n        ));\n    }\n\n    let plateau_max_val_acc = accuracy_history\n        .val_snapshots\n        .iter()\n        .filter(|(step, _)| *step <= config.plateau_min_step)\n        .map(|(_, acc)| *acc)\n        .fold(None::<f32>, |accum, value| {\n            Some(accum.map_or(value, |max_val| max_val.max(value)))\n        })\n        .ok_or_else(|| \"no validation accuracy samples before plateau_min_step\".to_string())?;\n\n    if plateau_max_val_acc > config.chance_accuracy + config.chance_tolerance {\n        return Err(format!(\n            \"validation accuracy before step {} exceeds chance (max {:.4})\",\n            config.plateau_min_step, plateau_max_val_acc\n        ));\n    }\n\n    let (transition_step, _) = accuracy_history\n        .val_snapshots\n        .iter()\n        .find(|(step, acc)| {\n            *step >= config.plateau_min_step && *acc >= config.target_val_acc_threshold\n        })\n        .ok_or_else(|| \"validation accuracy never reaches target threshold\".to_string())?;\n\n    let latest_allowed = config.plateau_min_step + config.generalization_window;\n    if *transition_step > latest_allowed {\n        return Err(format!(\n            \"validation accuracy reaches {:.2} at step {}, expected before {}\",\n            config.target_val_acc_threshold, transition_step, latest_allowed\n        ));\n    }\n\n    verify_accuracy_jump(\n        &accuracy_history.val_snapshots,\n        *transition_step,\n        config.accuracy_window,\n        config.chance_accuracy,\n        config.chance_tolerance,\n        config.target_val_acc_threshold,\n    )?;\n\n    let loss_drop_ratio = verify_loss_drop(\n        &loss_history.val_snapshots,\n        *transition_step,\n        config.loss_window,\n        config.loss_drop_fraction,\n    )?;\n\n    Ok(GrokkingPhaseReport {\n        early_train_step: *early_train_step,\n        transition_step: *transition_step,\n        plateau_max_val_acc,\n        loss_drop_ratio,\n    })\n}\n\nfn verify_accuracy_jump(\n    val_accuracy: &[(usize, f32)],\n    transition_step: usize,\n    window: usize,\n    chance_accuracy: f32,\n    chance_tolerance: f32,\n    target_val_acc_threshold: f32,\n) -> Result<(), String> {\n    let before_values: Vec<f32> = val_accuracy\n        .iter()\n        .filter(|(step, _)| *step < transition_step)\n        .map(|(_, acc)| *acc)\n        .collect();\n\n    let after_values: Vec<f32> = val_accuracy\n        .iter()\n        .filter(|(step, _)| *step >= transition_step)\n        .map(|(_, acc)| *acc)\n        .collect();\n\n    if before_values.len() < window || after_values.len() < window {\n        return Err(\"insufficient accuracy samples around transition\".to_string());\n    }\n\n    let before_avg: f32 =\n        before_values[before_values.len() - window..].iter().sum::<f32>() / window as f32;\n    let after_avg: f32 = after_values[..window].iter().sum::<f32>() / window as f32;\n\n    if before_avg > chance_accuracy + chance_tolerance {\n        return Err(format!(\n            \"validation accuracy plateau too high before transition (avg {:.4})\",\n            before_avg\n        ));\n    }\n\n    if after_avg < target_val_acc_threshold {\n        return Err(format!(\n            \"validation accuracy after transition too low (avg {:.4})\",\n            after_avg\n        ));\n    }\n\n    Ok(())\n}\n\nfn verify_loss_drop(\n    val_losses: &[(usize, f64)],\n    transition_step: usize,\n    loss_window: usize,\n    loss_drop_fraction: f64,\n) -> Result<f64, String> {\n    let before_losses: Vec<f64> = val_losses\n        .iter()\n        .filter(|(step, _)| *step < transition_step)\n        .map(|(_, loss)| *loss)\n        .collect();\n\n    let after_losses: Vec<f64> = val_losses\n        .iter()\n        .filter(|(step, _)| *step >= transition_step)\n        .map(|(_, loss)| *loss)\n        .collect();\n\n    if before_losses.len() < loss_window || after_losses.len() < loss_window {\n        return Err(\"insufficient loss samples around transition\".to_string());\n    }\n\n    let before_avg: f64 =\n        before_losses[before_losses.len() - loss_window..].iter().sum::<f64>()\n            / loss_window as f64;\n    let after_avg: f64 = after_losses[..loss_window].iter().sum::<f64>() / loss_window as f64;\n\n    if after_avg > before_avg * (1.0 - loss_drop_fraction) {\n        return Err(format!(\n            \"validation loss drop too small (before {:.4}, after {:.4})\",\n            before_avg, after_avg\n        ));\n    }\n\n    Ok(after_avg / before_avg)\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_restricted_loss_early_drop(\n    full_losses: &[(usize, f64)],\n    restricted_losses: &[(usize, f64)],\n    config: &RestrictedLossVerificationConfig,\n) -> Result<RestrictedLossReport, String> {\n    let restricted_baseline =\n        baseline_loss(restricted_losses, config.plateau_min_step, config.drop_window)?;\n    let full_baseline = baseline_loss(full_losses, config.plateau_min_step, config.drop_window)?;\n\n    let restricted_drop_step = find_drop_step(\n        restricted_losses,\n        config.plateau_min_step,\n        restricted_baseline,\n        config.drop_fraction,\n    )?;\n    let full_drop_step = find_drop_step(\n        full_losses,\n        config.plateau_min_step,\n        full_baseline,\n        config.drop_fraction,\n    )?;\n\n    if restricted_drop_step + config.min_step_lead > full_drop_step {\n        return Err(format!(\n            \"restricted loss drops at step {}, expected at least {} steps before full loss (step {})\",\n            restricted_drop_step, config.min_step_lead, full_drop_step\n        ));\n    }\n\n    Ok(RestrictedLossReport {\n        restricted_drop_step,\n        full_drop_step,\n    })\n}\n\npub fn verify_restricted_loss_drop(\n    restricted_losses: &[(usize, f64)],\n    config: &RestrictedLossDropConfig,\n) -> Result<RestrictedLossDropReport, String> {\n    if restricted_losses.len() < 3 {\n        return Err(\"restricted loss history requires at least 3 points\".to_string());\n    }\n\n    let first = restricted_losses.first().ok_or_else(|| {\n        \"restricted loss history is empty\".to_string()\n    })?;\n    let baseline_loss = first.1;\n\n    let (min_idx, (min_step, min_loss)) = restricted_losses\n        .iter()\n        .enumerate()\n        .min_by(|(_, a), (_, b)| {\n            a.1.partial_cmp(&b.1).unwrap_or(std::cmp::Ordering::Equal)\n        })\n        .ok_or_else(|| \"restricted loss history is empty\".to_string())?;\n\n    if min_idx == 0 {\n        return Err(\"restricted loss minimum occurs at start (no drop detected)\".to_string());\n    }\n\n    let relative_decrease = (baseline_loss - min_loss) / baseline_loss;\n    if relative_decrease < config.min_relative_decrease {\n        return Err(format!(\n            \"restricted loss decrease too small (baseline {:.4}, min {:.4}, decrease {:.1}%)\",\n            baseline_loss, min_loss, relative_decrease * 100.0\n        ));\n    }\n\n    Ok(RestrictedLossDropReport {\n        drop_step: *min_step,\n        drop_loss: *min_loss,\n        baseline_loss,\n    })\n}\n\npub fn verify_excluded_loss_spike(\n    excluded_losses: &[(usize, f64)],\n    config: &ExcludedLossSpikeConfig,\n) -> Result<ExcludedLossSpikeReport, String> {\n    if excluded_losses.len() < 3 {\n        return Err(\"excluded loss history requires at least 3 points\".to_string());\n    }\n\n    let first = excluded_losses.first().ok_or_else(|| {\n        \"excluded loss history is empty\".to_string()\n    })?;\n    let last = excluded_losses.last().ok_or_else(|| {\n        \"excluded loss history is empty\".to_string()\n    })?;\n    let baseline_loss = (first.1 + last.1) / 2.0;\n\n    let (peak_idx, (peak_step, peak_loss)) = excluded_losses\n        .iter()\n        .enumerate()\n        .max_by(|(_, a), (_, b)| {\n            a.1.partial_cmp(&b.1).unwrap_or(std::cmp::Ordering::Equal)\n        })\n        .ok_or_else(|| \"excluded loss history is empty\".to_string())?;\n\n    if peak_idx == 0 || peak_idx + 1 == excluded_losses.len() {\n        return Err(\"excluded loss spike occurs at boundary\".to_string());\n    }\n\n    if peak_loss < &(baseline_loss * (1.0 + config.min_relative_increase)) {\n        return Err(format!(\n            \"excluded loss spike too small (baseline {:.4}, peak {:.4})\",\n            baseline_loss, peak_loss\n        ));\n    }\n\n    Ok(ExcludedLossSpikeReport {\n        spike_step: *peak_step,\n        spike_loss: *peak_loss,\n        baseline_loss,\n    })\n}\n\npub fn verify_mlp_activation_waves<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n    fixed_y: usize,\n    neuron_indices: &[usize],\n    config: &MlpWaveVerificationConfig,\n) -> Result<MlpWaveReport, String> {\n    let activations = collect_mlp_post_relu_activations(model, device, fixed_y)?;\n    if neuron_indices.is_empty() {\n        return Err(\"neuron indices list is empty\".to_string());\n    }\n\n    let d_ff = activations.len();\n    for &idx in neuron_indices {\n        if idx >= d_ff {\n            return Err(format!(\n                \"neuron index {} out of range (d_ff={})\",\n                idx, d_ff\n            ));\n        }\n    }\n\n    let signals: Vec<(usize, Vec<f64>)> = neuron_indices\n        .iter()\n        .map(|&idx| (idx, activations[idx].clone()))\n        .collect();\n    verify_mlp_activation_waves_from_signals(&signals, config)\n}\n\npub fn verify_mlp_activation_waves_from_signals(\n    signals: &[(usize, Vec<f64>)],\n    config: &MlpWaveVerificationConfig,\n) -> Result<MlpWaveReport, String> {\n    if signals.is_empty() {\n        return Err(\"no activation signals provided\".to_string());\n    }\n\n    let mut reports = Vec::with_capacity(signals.len());\n    for (neuron_index, signal) in signals {\n        let report = analyze_mlp_activation_wave(*neuron_index, signal, config)?;\n        reports.push(report);\n    }\n\n    Ok(MlpWaveReport {\n        neuron_reports: reports,\n    })\n}\n\npub fn verify_mlp_pairwise_manifold_transition<B: Backend>(\n    pre_model: &Transformer<B>,\n    post_model: &Transformer<B>,\n    device: &B::Device,\n    config: &PairwiseManifoldConfig,\n) -> Result<PairwiseManifoldTransitionReport, String> {\n    let modulus = ModularAdditionDataset::modulus();\n    if config.fixed_y >= modulus {\n        return Err(format!(\n            \"fixed_y {} out of range (modulus {})\",\n            config.fixed_y, modulus\n        ));\n    }\n\n    let pre_activations = collect_mlp_post_relu_activations(pre_model, device, config.fixed_y)?;\n    let post_activations = collect_mlp_post_relu_activations(post_model, device, config.fixed_y)?;\n\n    let d_ff = pre_activations.len();\n    if post_activations.len() != d_ff {\n        return Err(\"post activations shape mismatch\".to_string());\n    }\n\n    if config.neuron_indices.len() < 2 {\n        return Err(\"need at least two neuron indices\".to_string());\n    }\n\n    for &idx in &config.neuron_indices {\n        if idx >= d_ff {\n            return Err(format!(\n                \"neuron index {} out of range (d_ff={})\",\n                idx, d_ff\n            ));\n        }\n    }\n\n    let pre_signals: Vec<(usize, Vec<f64>)> = config\n        .neuron_indices\n        .iter()\n        .map(|&idx| (idx, pre_activations[idx].clone()))\n        .collect();\n    let post_signals: Vec<(usize, Vec<f64>)> = config\n        .neuron_indices\n        .iter()\n        .map(|&idx| (idx, post_activations[idx].clone()))\n        .collect();\n\n    verify_pairwise_manifold_transition_from_signals(&pre_signals, &post_signals, config)\n}\n\npub fn collect_mlp_activation_surface<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n    neuron_index: usize,\n) -> Result<Vec<Vec<f64>>, String> {\n    let modulus = ModularAdditionDataset::modulus();\n    let equals_token = ModularAdditionDataset::equals_token();\n\n    let mut inputs_vec = Vec::with_capacity(modulus * modulus * 3);\n    for x in 0..modulus {\n        for y in 0..modulus {\n            inputs_vec.push(x as i32);\n            inputs_vec.push(y as i32);\n            inputs_vec.push(equals_token as i32);\n        }\n    }\n\n    let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n        .reshape([modulus * modulus, 3]);\n    let (_, mlp_acts) = model.forward_with_mlp_activations(inputs);\n    let [batch_size, d_ff] = mlp_acts.dims();\n    if batch_size != modulus * modulus {\n        return Err(format!(\n            \"unexpected activation batch size {} (expected {})\",\n            batch_size,\n            modulus * modulus\n        ));\n    }\n    if neuron_index >= d_ff {\n        return Err(format!(\n            \"neuron index {} out of range (d_ff={})\",\n            neuron_index, d_ff\n        ));\n    }\n\n    let data: Vec<f32> = mlp_acts.into_data().to_vec().unwrap();\n    let mut surface = vec![vec![0.0f64; modulus]; modulus];\n    for x in 0..modulus {\n        for y in 0..modulus {\n            let batch = x * modulus + y;\n            let idx = batch * d_ff + neuron_index;\n            surface[x][y] = data[idx] as f64;\n        }\n    }\n\n    Ok(surface)\n}\n\npub fn verify_constructive_interference_surface_from_grid(\n    surface: &[Vec<f64>],\n    config: &DiagonalRidgeConfig,\n) -> Result<DiagonalRidgeReport, String> {\n    let (diag_main, diag_anti, axis_x, axis_y) =\n        surface_autocorrelation_scores(surface, config.max_lag)?;\n\n    let diagonal_score = diag_main.max(diag_anti);\n    let axis_score = axis_x.max(axis_y);\n    let ratio = if axis_score > 0.0 {\n        diagonal_score / axis_score\n    } else {\n        f64::INFINITY\n    };\n\n    if diagonal_score < config.min_diagonal_score {\n        return Err(format!(\n            \"diagonal autocorrelation {:.3} below threshold {:.3}\",\n            diagonal_score, config.min_diagonal_score\n        ));\n    }\n\n    if ratio < config.min_diagonal_ratio {\n        return Err(format!(\n            \"diagonal/axis ratio {:.3} below threshold {:.3}\",\n            ratio, config.min_diagonal_ratio\n        ));\n    }\n\n    Ok(DiagonalRidgeReport {\n        neuron_index: config.neuron_index,\n        diagonal_score,\n        axis_score,\n        ratio,\n    })\n}\n\npub fn verify_pairwise_manifold_transition_from_signals(\n    pre_signals: &[(usize, Vec<f64>)],\n    post_signals: &[(usize, Vec<f64>)],\n    config: &PairwiseManifoldConfig,\n) -> Result<PairwiseManifoldTransitionReport, String> {\n    let pre_report = evaluate_pairwise_manifolds_from_signals(pre_signals)?;\n    let post_report = evaluate_pairwise_manifolds_from_signals(post_signals)?;\n\n    if pre_report.average_score > config.max_pre_score {\n        return Err(format!(\n            \"pre-grok average circularity {:.3} exceeds threshold {:.3}\",\n            pre_report.average_score, config.max_pre_score\n        ));\n    }\n\n    let passing_pairs = post_report\n        .pair_reports\n        .iter()\n        .filter(|pair| {\n            pair.circularity_score >= config.min_post_score\n                && pair.axis_ratio <= config.max_axis_ratio\n        })\n        .count();\n    if passing_pairs < config.min_pairs_passing {\n        return Err(format!(\n            \"post-grok only {} pairs meet circularity threshold (need {})\",\n            passing_pairs, config.min_pairs_passing\n        ));\n    }\n\n    Ok(PairwiseManifoldTransitionReport {\n        pre_average_score: pre_report.average_score,\n        post_average_score: post_report.average_score,\n        passing_pairs,\n        total_pairs: post_report.pair_reports.len(),\n    })\n}\n\nfn evaluate_pairwise_manifolds_from_signals(\n    signals: &[(usize, Vec<f64>)],\n) -> Result<PairwiseManifoldReport, String> {\n    if signals.len() < 2 {\n        return Err(\"need at least two activation signals\".to_string());\n    }\n\n    let signal_len = signals[0].1.len();\n    if signal_len < 3 {\n        return Err(\"activation signal too short\".to_string());\n    }\n\n    for (idx, signal) in signals {\n        if signal.len() != signal_len {\n            return Err(format!(\n                \"signal length mismatch for neuron {}\",\n                idx\n            ));\n        }\n    }\n\n    let mut reports = Vec::new();\n    for i in 0..signals.len() {\n        for j in (i + 1)..signals.len() {\n            let (neuron_a, signal_a) = &signals[i];\n            let (neuron_b, signal_b) = &signals[j];\n            let points: Vec<(f64, f64)> = signal_a\n                .iter()\n                .zip(signal_b.iter())\n                .map(|(x, y)| (*x, *y))\n                .collect();\n            let (score, radius_cv, axis_ratio) =\n                match pairwise_circularity_metrics(&points) {\n                    Ok(metrics) => metrics,\n                    Err(_) => (0.0, 1.0, f64::INFINITY),\n                };\n            reports.push(PairwiseManifoldPairReport {\n                neuron_a: *neuron_a,\n                neuron_b: *neuron_b,\n                circularity_score: score,\n                radius_cv,\n                axis_ratio,\n            });\n        }\n    }\n\n    let average_score = reports\n        .iter()\n        .map(|pair| pair.circularity_score)\n        .sum::<f64>()\n        / reports.len() as f64;\n\n    Ok(PairwiseManifoldReport {\n        pair_reports: reports,\n        average_score,\n    })\n}\n\nfn pairwise_circularity_metrics(points: &[(f64, f64)]) -> Result<(f64, f64, f64), String> {\n    if points.len() < 3 {\n        return Err(\"not enough points for circularity\".to_string());\n    }\n\n    let n = points.len() as f64;\n    let mean_x = points.iter().map(|(x, _)| *x).sum::<f64>() / n;\n    let mean_y = points.iter().map(|(_, y)| *y).sum::<f64>() / n;\n\n    let mut var_x = 0.0;\n    let mut var_y = 0.0;\n    let mut cov_xy = 0.0;\n    for (x, y) in points {\n        let dx = x - mean_x;\n        let dy = y - mean_y;\n        var_x += dx * dx;\n        var_y += dy * dy;\n        cov_xy += dx * dy;\n    }\n    var_x /= n;\n    var_y /= n;\n    cov_xy /= n;\n\n    let trace = var_x + var_y;\n    let det = var_x * var_y - cov_xy * cov_xy;\n    if det <= 1.0e-12 || trace <= 0.0 {\n        return Err(\"degenerate covariance\".to_string());\n    }\n\n    let half_trace = trace * 0.5;\n    let disc = (half_trace * half_trace - det).max(0.0).sqrt();\n    let lambda1 = half_trace + disc;\n    let lambda2 = half_trace - disc;\n    if lambda2 <= 0.0 {\n        return Err(\"degenerate eigenvalues\".to_string());\n    }\n\n    let axis_ratio = (lambda1 / lambda2).sqrt();\n\n    let mut v1_x = cov_xy;\n    let mut v1_y = lambda1 - var_x;\n    if v1_x.abs() < 1.0e-12 && v1_y.abs() < 1.0e-12 {\n        v1_x = 1.0;\n        v1_y = 0.0;\n    }\n    let norm = (v1_x * v1_x + v1_y * v1_y).sqrt();\n    v1_x /= norm;\n    v1_y /= norm;\n    let v2_x = -v1_y;\n    let v2_y = v1_x;\n\n    let mut radii = Vec::with_capacity(points.len());\n    for (x, y) in points {\n        let dx = x - mean_x;\n        let dy = y - mean_y;\n        let p1 = dx * v1_x + dy * v1_y;\n        let p2 = dx * v2_x + dy * v2_y;\n        let w1 = p1 / lambda1.sqrt();\n        let w2 = p2 / lambda2.sqrt();\n        radii.push((w1 * w1 + w2 * w2).sqrt());\n    }\n\n    let mean_radius = radii.iter().sum::<f64>() / radii.len() as f64;\n    if mean_radius == 0.0 {\n        return Err(\"zero mean radius\".to_string());\n    }\n\n    let mut variance = 0.0;\n    for r in &radii {\n        let diff = r - mean_radius;\n        variance += diff * diff;\n    }\n    let std = (variance / radii.len() as f64).sqrt();\n    let radius_cv = std / mean_radius;\n    let score = 1.0 / (1.0 + radius_cv);\n\n    Ok((score, radius_cv, axis_ratio))\n}\n\nfn baseline_loss(\n    losses: &[(usize, f64)],\n    plateau_min_step: usize,\n    drop_window: usize,\n) -> Result<f64, String> {\n    let before: Vec<f64> = losses\n        .iter()\n        .filter(|(step, _)| *step <= plateau_min_step)\n        .map(|(_, loss)| *loss)\n        .collect();\n\n    if before.len() < drop_window || drop_window == 0 {\n        return Err(\"insufficient loss samples for baseline\".to_string());\n    }\n\n    let window = &before[before.len() - drop_window..];\n    Ok(window.iter().sum::<f64>() / drop_window as f64)\n}\n\nfn find_drop_step(\n    losses: &[(usize, f64)],\n    plateau_min_step: usize,\n    baseline: f64,\n    drop_fraction: f64,\n) -> Result<usize, String> {\n    losses\n        .iter()\n        .filter(|(step, _)| *step >= plateau_min_step)\n        .find(|(_, loss)| *loss <= baseline * (1.0 - drop_fraction))\n        .map(|(step, _)| *step)\n        .ok_or_else(|| \"loss never drops below threshold\".to_string())\n}\n\nfn collect_mlp_post_relu_activations<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n    fixed_y: usize,\n) -> Result<Vec<Vec<f64>>, String> {\n    let modulus = ModularAdditionDataset::modulus();\n    let equals_token = ModularAdditionDataset::equals_token();\n\n    let mut inputs_vec = Vec::with_capacity(modulus * 3);\n    for x in 0..modulus {\n        inputs_vec.push(x as i32);\n        inputs_vec.push(fixed_y as i32);\n        inputs_vec.push(equals_token as i32);\n    }\n\n    let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n        .reshape([modulus, 3]);\n    let (_, mlp_acts) = model.forward_with_mlp_activations(inputs);\n    let [batch_size, d_ff] = mlp_acts.dims();\n\n    if batch_size != modulus {\n        return Err(format!(\n            \"unexpected activation batch size {} (expected {})\",\n            batch_size, modulus\n        ));\n    }\n\n    let data: Vec<f32> = mlp_acts.into_data().to_vec().unwrap();\n    let mut per_neuron = vec![Vec::with_capacity(batch_size); d_ff];\n    for batch in 0..batch_size {\n        for neuron in 0..d_ff {\n            let idx = batch * d_ff + neuron;\n            per_neuron[neuron].push(data[idx] as f64);\n        }\n    }\n\n    Ok(per_neuron)\n}\n\nfn surface_autocorrelation_scores(\n    surface: &[Vec<f64>],\n    max_lag: usize,\n) -> Result<(f64, f64, f64, f64), String> {\n    let (height, width) = surface_dimensions(surface)?;\n    if height < 2 || width < 2 {\n        return Err(\"surface grid too small\".to_string());\n    }\n\n    let (mean, var) = surface_mean_variance(surface)?;\n    if var == 0.0 {\n        return Err(\"surface variance is zero\".to_string());\n    }\n\n    let max_lag = max_lag.min(height.saturating_sub(1)).min(width.saturating_sub(1));\n    if max_lag == 0 {\n        return Err(\"max_lag too small for surface\".to_string());\n    }\n\n    let diag_main = average_autocorrelation(surface, mean, var, max_lag, 1, 1)?;\n    let diag_anti = average_autocorrelation(surface, mean, var, max_lag, 1, -1)?;\n    let axis_x = average_autocorrelation(surface, mean, var, max_lag, 1, 0)?;\n    let axis_y = average_autocorrelation(surface, mean, var, max_lag, 0, 1)?;\n\n    Ok((diag_main, diag_anti, axis_x, axis_y))\n}\n\nfn surface_dimensions(surface: &[Vec<f64>]) -> Result<(usize, usize), String> {\n    let height = surface.len();\n    if height == 0 {\n        return Err(\"surface is empty\".to_string());\n    }\n    let width = surface[0].len();\n    if width == 0 {\n        return Err(\"surface has empty rows\".to_string());\n    }\n    for (idx, row) in surface.iter().enumerate() {\n        if row.len() != width {\n            return Err(format!(\n                \"surface row {} has length {}, expected {}\",\n                idx,\n                row.len(),\n                width\n            ));\n        }\n    }\n    Ok((height, width))\n}\n\nfn surface_mean_variance(surface: &[Vec<f64>]) -> Result<(f64, f64), String> {\n    let (height, width) = surface_dimensions(surface)?;\n    let mut sum = 0.0;\n    for row in surface {\n        for value in row {\n            sum += *value;\n        }\n    }\n    let count = (height * width) as f64;\n    let mean = sum / count;\n    let mut variance = 0.0;\n    for row in surface {\n        for value in row {\n            let diff = *value - mean;\n            variance += diff * diff;\n        }\n    }\n    variance /= count;\n    Ok((mean, variance))\n}\n\nfn average_autocorrelation(\n    surface: &[Vec<f64>],\n    mean: f64,\n    variance: f64,\n    max_lag: usize,\n    dx: i32,\n    dy: i32,\n) -> Result<f64, String> {\n    let (height, width) = surface_dimensions(surface)?;\n    let mut total = 0.0;\n    let mut lag_count = 0usize;\n\n    for lag in 1..=max_lag {\n        let mut sum = 0.0;\n        let mut count = 0usize;\n        let lag = lag as i32;\n        for x in 0..height {\n            for y in 0..width {\n                let x2 = x as i32 + dx * lag;\n                let y2 = y as i32 + dy * lag;\n                if x2 < 0 || y2 < 0 {\n                    continue;\n                }\n                let x2 = x2 as usize;\n                let y2 = y2 as usize;\n                if x2 >= height || y2 >= width {\n                    continue;\n                }\n                let a = surface[x][y] - mean;\n                let b = surface[x2][y2] - mean;\n                sum += a * b;\n                count += 1;\n            }\n        }\n        if count == 0 {\n            continue;\n        }\n        let corr = sum / (count as f64 * variance);\n        total += corr.abs();\n        lag_count += 1;\n    }\n\n    if lag_count == 0 {\n        return Err(\"no valid lags for autocorrelation\".to_string());\n    }\n\n    Ok(total / lag_count as f64)\n}\n\nfn analyze_mlp_activation_wave(\n    neuron_index: usize,\n    signal: &[f64],\n    config: &MlpWaveVerificationConfig,\n) -> Result<MlpWaveNeuronReport, String> {\n    if signal.len() < 3 {\n        return Err(format!(\n            \"activation signal for neuron {} is too short\",\n            neuron_index\n        ));\n    }\n\n    let negative_count = signal\n        .iter()\n        .filter(|value| **value < -config.negative_tolerance)\n        .count();\n    let negative_fraction = negative_count as f64 / signal.len() as f64;\n    if negative_fraction > config.max_negative_fraction {\n        return Err(format!(\n            \"neuron {} has {:.3} negative activations\",\n            neuron_index, negative_fraction\n        ));\n    }\n\n    let (dominant_frequency, dominant_ratio) = dominant_frequency_ratio(signal)?;\n    if dominant_ratio < config.min_dominant_ratio {\n        return Err(format!(\n            \"neuron {} dominant frequency ratio {:.3} below threshold {:.3}\",\n            neuron_index, dominant_ratio, config.min_dominant_ratio\n        ));\n    }\n\n    let best_correlation =\n        best_rectified_sine_correlation(signal, dominant_frequency, config.phase_steps);\n    if best_correlation < config.min_correlation {\n        return Err(format!(\n            \"neuron {} rectified sine correlation {:.3} below threshold {:.3}\",\n            neuron_index, best_correlation, config.min_correlation\n        ));\n    }\n\n    Ok(MlpWaveNeuronReport {\n        neuron_index,\n        dominant_frequency,\n        dominant_ratio,\n        negative_fraction,\n        best_correlation,\n    })\n}\n\nfn dominant_frequency_ratio(signal: &[f64]) -> Result<(usize, f64), String> {\n    if signal.len() < 2 {\n        return Err(\"signal too short for FFT\".to_string());\n    }\n\n    let mut planner = FftPlanner::new();\n    let fft = planner.plan_fft_forward(signal.len());\n    let mut buffer: Vec<Complex<f64>> = signal\n        .iter()\n        .map(|&x| Complex::new(x, 0.0))\n        .collect();\n    fft.process(&mut buffer);\n    let magnitudes: Vec<f64> = buffer.iter().map(|c| c.norm()).collect();\n\n    if magnitudes.len() < 2 {\n        return Err(\"insufficient FFT bins\".to_string());\n    }\n\n    let mut dominant_idx = 1usize;\n    let mut dominant_mag = magnitudes[1];\n    for (idx, &mag) in magnitudes.iter().enumerate().skip(2) {\n        if mag > dominant_mag {\n            dominant_mag = mag;\n            dominant_idx = idx;\n        }\n    }\n\n    if dominant_idx == 0 {\n        return Err(\"dominant frequency is zero\".to_string());\n    }\n\n    let mut nonzero = magnitudes[1..].to_vec();\n    nonzero.sort_by(|a, b| a.partial_cmp(b).unwrap_or(std::cmp::Ordering::Equal));\n    let median_mag = nonzero[nonzero.len() / 2];\n    let ratio = if median_mag == 0.0 {\n        dominant_mag\n    } else {\n        dominant_mag / median_mag\n    };\n\n    Ok((dominant_idx, ratio))\n}\n\nfn best_rectified_sine_correlation(signal: &[f64], frequency: usize, phase_steps: usize) -> f64 {\n    if signal.is_empty() || frequency == 0 || phase_steps == 0 {\n        return 0.0;\n    }\n\n    let tau = std::f64::consts::PI * 2.0;\n    let mut best = -1.0;\n    for step in 0..phase_steps {\n        let phase = tau * step as f64 / phase_steps as f64;\n        let wave: Vec<f64> = (0..signal.len())\n            .map(|idx| {\n                let angle = tau * frequency as f64 * idx as f64 / signal.len() as f64 + phase;\n                angle.sin().max(0.0)\n            })\n            .collect();\n        let corr = pearson_correlation(signal, &wave);\n        if corr > best {\n            best = corr;\n        }\n    }\n\n    best\n}\n\nfn pearson_correlation(a: &[f64], b: &[f64]) -> f64 {\n    if a.len() != b.len() || a.is_empty() {\n        return 0.0;\n    }\n\n    let mean_a = a.iter().sum::<f64>() / a.len() as f64;\n    let mean_b = b.iter().sum::<f64>() / b.len() as f64;\n\n    let mut num = 0.0;\n    let mut denom_a = 0.0;\n    let mut denom_b = 0.0;\n    for (x, y) in a.iter().zip(b.iter()) {\n        let da = x - mean_a;\n        let db = y - mean_b;\n        num += da * db;\n        denom_a += da * da;\n        denom_b += db * db;\n    }\n\n    if denom_a == 0.0 || denom_b == 0.0 {\n        return 0.0;\n    }\n\n    num / (denom_a.sqrt() * denom_b.sqrt())\n}\n\n/// Verify that the model actually learned modular addition\n/// by testing it on ALL possible examples systematically\npub fn verify_full_accuracy<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n) -> (f32, f32) {\n    println!(\"ðŸ” Verifying model on COMPLETE dataset (no sampling)...\");\n\n    let train_dataset = ModularAdditionDataset::new(true, 42);\n    let val_dataset = ModularAdditionDataset::new(false, 42);\n\n    let train_acc = test_all_examples(model, &train_dataset, device);\n    let val_acc = test_all_examples(model, &val_dataset, device);\n\n    println!(\"ðŸ“Š FULL Dataset Results:\");\n    println!(\"   Training: {}/{} = {:.2}%\",\n        (train_acc * train_dataset.len() as f32) as usize,\n        train_dataset.len(),\n        train_acc * 100.0);\n    println!(\"   Validation: {}/{} = {:.2}%\",\n        (val_acc * val_dataset.len() as f32) as usize,\n        val_dataset.len(),\n        val_acc * 100.0);\n\n    (train_acc, val_acc)\n}\n\n/// Test model on ALL examples in dataset (no random sampling)\nfn test_all_examples<B: Backend>(\n    model: &Transformer<B>,\n    dataset: &ModularAdditionDataset,\n    device: &B::Device,\n) -> f32 {\n    let mut total_correct = 0;\n    let total = dataset.len();\n\n    // Test in batches to avoid memory issues\n    let batch_size = 100;\n    for batch_start in (0..total).step_by(batch_size) {\n        let batch_end = (batch_start + batch_size).min(total);\n        let batch_len = batch_end - batch_start;\n\n        let mut inputs_vec = Vec::new();\n        let mut targets_vec = Vec::new();\n\n        for idx in batch_start..batch_end {\n            let (input, target) = dataset.get(idx).unwrap();\n            inputs_vec.extend(input);\n            targets_vec.push(target as i32);\n        }\n\n        let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n            .reshape([batch_len, 3]);\n\n        let logits = model.forward(inputs);\n        let predictions = logits.argmax(1).squeeze::<1>();\n        let predictions_vec: Vec<i32> = predictions.into_data().to_vec().unwrap();\n\n        let correct = predictions_vec\n            .iter()\n            .zip(targets_vec.iter())\n            .filter(|(p, t)| p == t)\n            .count();\n\n        total_correct += correct;\n    }\n\n    total_correct as f32 / total as f32\n}\n\n/// Verify that weight norms decrease over time with weight decay\n/// and show correlation with the grokking transition\npub fn verify_weight_norm_decay(\n    weight_norm_history: &[(usize, f64)],\n    grok_step: Option<usize>,\n    config: &WeightNormDecayConfig,\n) -> Result<WeightNormDecayReport, String> {\n    if weight_norm_history.len() < 2 {\n        return Err(\"weight norm history requires at least 2 snapshots\".to_string());\n    }\n\n    let initial_norm = weight_norm_history\n        .first()\n        .ok_or_else(|| \"weight norm history is empty\".to_string())?\n        .1;\n    let final_norm = weight_norm_history\n        .last()\n        .ok_or_else(|| \"weight norm history is empty\".to_string())?\n        .1;\n\n    if initial_norm <= 0.0 {\n        return Err(\"initial weight norm must be positive\".to_string());\n    }\n\n    // Check that norms are generally decreasing (with weight decay)\n    let relative_decrease = (initial_norm - final_norm) / initial_norm;\n    if relative_decrease < config.min_relative_decrease {\n        return Err(format!(\n            \"weight norm decrease {:.4} below threshold {:.4} (initial {:.4}, final {:.4})\",\n            relative_decrease, config.min_relative_decrease, initial_norm, final_norm\n        ));\n    }\n\n    // Check that norms are not increasing (weight decay should monotonically decrease norms)\n    for window in weight_norm_history.windows(2) {\n        let prev_norm = window[0].1;\n        let next_norm = window[1].1;\n        if next_norm > prev_norm * 1.01 {\n            // Allow 1% tolerance for numerical noise\n            return Err(format!(\n                \"weight norm increased from {:.4} to {:.4} at step {}\",\n                prev_norm, next_norm, window[1].0\n            ));\n        }\n    }\n\n    // If grok step is provided, check that norms show a shift around grokking\n    let grok_step_norm = if let Some(step) = grok_step {\n        weight_norm_history\n            .iter()\n            .filter(|(s, _)| {\n                *s >= step.saturating_sub(config.grok_step_window)\n                    && *s <= step + config.grok_step_window\n            })\n            .map(|(_, norm)| *norm)\n            .next()\n    } else {\n        None\n    };\n\n    Ok(WeightNormDecayReport {\n        initial_norm,\n        final_norm,\n        relative_decrease,\n        grok_step_norm,\n    })\n}\n\n/// Verify the characteristic \"snake curve\" shape of grokking phenomenon\n/// on log-scale plots: train accuracy rises early, validation stays flat then jumps\npub fn verify_snake_curve_shape(\n    train_acc: &[(usize, f32)],\n    val_acc: &[(usize, f32)],\n    config: &SnakeCurveConfig,\n) -> Result<SnakeCurveReport, String> {\n    if train_acc.is_empty() || val_acc.is_empty() {\n        return Err(\"empty accuracy history\".to_string());\n    }\n\n    // Phase 1: Verify early train convergence (immediate drop on log scale)\n    let train_converged_step = train_acc\n        .iter()\n        .find(|(_, acc)| *acc >= config.train_convergence_threshold)\n        .map(|(step, _)| *step)\n        .ok_or_else(|| {\n            format!(\n                \"train accuracy never reaches {} threshold\",\n                config.train_convergence_threshold\n            )\n        })?;\n\n    if train_converged_step > config.early_step_max {\n        return Err(format!(\n            \"train accuracy converges at step {}, expected before {}\",\n            train_converged_step, config.early_step_max\n        ));\n    }\n\n    // Phase 2: Verify validation plateau (flat on log scale)\n    let plateau_samples: Vec<f32> = val_acc\n        .iter()\n        .filter(|(step, _)| *step >= 1 && *step <= config.val_plateau_min_step)\n        .map(|(_, acc)| *acc)\n        .collect();\n\n    if plateau_samples.is_empty() {\n        return Err(\"no validation samples in plateau region\".to_string());\n    }\n\n    let val_plateau_max = plateau_samples\n        .iter()\n        .fold(0.0f32, |max_val, &acc| max_val.max(acc));\n\n    if val_plateau_max > config.val_plateau_max_acc {\n        return Err(format!(\n            \"validation accuracy in plateau region too high: max {:.4} exceeds {:.4}\",\n            val_plateau_max, config.val_plateau_max_acc\n        ));\n    }\n\n    // Phase 3: Verify delayed validation jump (crash up on log scale)\n    let val_grok_step = val_acc\n        .iter()\n        .filter(|(step, _)| *step > config.val_plateau_min_step)\n        .find(|(_, acc)| *acc >= config.val_final_threshold)\n        .map(|(step, _)| *step)\n        .ok_or_else(|| {\n            format!(\n                \"validation accuracy never reaches {} after plateau\",\n                config.val_final_threshold\n            )\n        })?;\n\n    // Phase 4: Verify phase ordering (snake shape: early train, late val)\n    let curve_shape_valid = train_converged_step < config.val_plateau_min_step\n        && val_grok_step > config.val_plateau_min_step\n        && train_converged_step < val_grok_step;\n\n    if !curve_shape_valid {\n        return Err(format!(\n            \"invalid curve shape: train converged at {}, val plateau until {}, val grokked at {}\",\n            train_converged_step, config.val_plateau_min_step, val_grok_step\n        ));\n    }\n\n    Ok(SnakeCurveReport {\n        train_converged_by_step: train_converged_step,\n        val_plateau_max_acc: val_plateau_max,\n        val_grok_step,\n        curve_shape_valid,\n    })\n}\n\n/// Test specific examples to see if model actually computes mod p\npub fn test_specific_examples<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n) {\n    println!();\n    println!(\"ðŸ§ª Testing specific examples:\");\n\n    let modulus = ModularAdditionDataset::modulus();\n    let equals_token = ModularAdditionDataset::equals_token();\n    let test_cases = vec![\n        (0, 0, 0),\n        (1, 1, 2),\n        (50, 50, (50 + 50) % modulus),\n        (112, 1, (112 + 1) % modulus),\n        (56, 57, (56 + 57) % modulus),\n        (112, 112, (112 + 112) % modulus),\n        (10, 20, 30),\n        (60, 60, (60 + 60) % modulus),\n    ];\n\n    for (a, b, expected) in test_cases {\n        let input_vec = vec![a as i32, b as i32, equals_token as i32];\n        let input = Tensor::<B, 1, Int>::from_ints(input_vec.as_slice(), device)\n            .reshape([1, 3]);\n\n        let logits = model.forward(input);\n        let prediction = logits.argmax(1).squeeze::<1>();\n        let pred_value: i32 = prediction.into_data().to_vec().unwrap()[0];\n\n        let correct = if pred_value == expected as i32 { \"âœ“\" } else { \"âœ—\" };\n        println!(\n            \"   {} + {} mod {} = {} | Model: {} {}\",\n            a, b, modulus, expected, pred_value, correct\n        );\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    fn make_histories(\n        train_acc: &[(usize, f32)],\n        val_acc: &[(usize, f32)],\n        val_loss: &[(usize, f64)],\n    ) -> (LossHistory, AccuracyHistory) {\n        let mut loss_history = LossHistory::new();\n        loss_history.val_snapshots = val_loss.to_vec();\n        let mut accuracy_history = AccuracyHistory::new();\n        accuracy_history.train_snapshots = train_acc.to_vec();\n        accuracy_history.val_snapshots = val_acc.to_vec();\n        (loss_history, accuracy_history)\n    }\n\n    #[test]\n    fn grokking_phase_verification_passes_on_synthetic_signal() {\n        let train_acc = vec![(0, 0.2), (500, 0.995), (1000, 0.999)];\n        let val_acc = vec![\n            (0, 0.01),\n            (500, 0.01),\n            (1500, 0.01),\n            (2200, 0.995),\n            (2500, 0.999),\n        ];\n        let val_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 2.0),\n            (2200, 0.2),\n            (2500, 0.15),\n        ];\n\n        let (loss_history, accuracy_history) = make_histories(&train_acc, &val_acc, &val_loss);\n        let config = GrokkingVerificationConfig {\n            early_step_max: 1000,\n            plateau_min_step: 1500,\n            generalization_window: 1500,\n            accuracy_window: 2,\n            loss_window: 2,\n            ..GrokkingVerificationConfig::default_for_modulus(ModularAdditionDataset::modulus())\n        };\n\n        let report = verify_grokking_phase_transition(&loss_history, &accuracy_history, &config)\n            .expect(\"expected grokking verification to pass\");\n        assert!(report.transition_step >= config.plateau_min_step);\n        assert!(report.early_train_step <= config.early_step_max);\n    }\n\n    #[test]\n    fn grokking_phase_verification_fails_without_transition() {\n        let train_acc = vec![(0, 0.2), (500, 0.995)];\n        let val_acc = vec![(0, 0.01), (500, 0.01), (1500, 0.02)];\n        let val_loss = vec![(0, 2.0), (500, 2.0), (1500, 1.9)];\n\n        let (loss_history, accuracy_history) = make_histories(&train_acc, &val_acc, &val_loss);\n        let config = GrokkingVerificationConfig {\n            early_step_max: 1000,\n            plateau_min_step: 1000,\n            generalization_window: 1000,\n            accuracy_window: 2,\n            loss_window: 2,\n            ..GrokkingVerificationConfig::default_for_modulus(ModularAdditionDataset::modulus())\n        };\n\n        let err = verify_grokking_phase_transition(&loss_history, &accuracy_history, &config)\n            .expect_err(\"expected grokking verification to fail\");\n        assert!(err.contains(\"validation accuracy never reaches target\"));\n    }\n\n    #[test]\n    fn grokking_phase_verification_fails_with_slow_accuracy_rise() {\n        let train_acc = vec![(0, 0.2), (500, 0.995), (1000, 0.999)];\n        let val_acc = vec![\n            (0, 0.01),\n            (500, 0.01),\n            (1500, 0.5),\n            (2000, 0.6),\n            (2200, 0.995),\n            (2500, 0.999),\n        ];\n        let val_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 2.0),\n            (2200, 0.2),\n            (2500, 0.15),\n        ];\n\n        let (loss_history, accuracy_history) = make_histories(&train_acc, &val_acc, &val_loss);\n        let config = GrokkingVerificationConfig {\n            early_step_max: 1000,\n            plateau_min_step: 1000,\n            generalization_window: 2000,\n            accuracy_window: 2,\n            loss_window: 2,\n            ..GrokkingVerificationConfig::default_for_modulus(ModularAdditionDataset::modulus())\n        };\n\n        let err = verify_grokking_phase_transition(&loss_history, &accuracy_history, &config)\n            .expect_err(\"expected grokking verification to fail\");\n        assert!(err.contains(\"plateau too high\"));\n    }\n\n    #[test]\n    fn restricted_loss_detection_passes_with_early_drop() {\n        let full_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 2.0),\n            (2500, 2.0),\n            (3500, 0.3),\n        ];\n        let restricted_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 0.8),\n            (2500, 0.4),\n            (3500, 0.3),\n        ];\n\n        let config = RestrictedLossVerificationConfig {\n            plateau_min_step: 1000,\n            drop_window: 2,\n            drop_fraction: 0.5,\n            min_step_lead: 500,\n        };\n\n        let report = verify_restricted_loss_early_drop(&full_loss, &restricted_loss, &config)\n            .expect(\"expected restricted loss verification to pass\");\n        assert!(report.restricted_drop_step < report.full_drop_step);\n    }\n\n    #[test]\n    fn restricted_loss_detection_fails_when_full_drops_first() {\n        let full_loss = vec![(0, 2.0), (1000, 1.0), (1500, 0.8)];\n        let restricted_loss = vec![(0, 2.0), (1000, 2.0), (1500, 0.8)];\n\n        let config = RestrictedLossVerificationConfig {\n            plateau_min_step: 500,\n            drop_window: 1,\n            drop_fraction: 0.3,\n            min_step_lead: 200,\n        };\n\n        let err = verify_restricted_loss_early_drop(&full_loss, &restricted_loss, &config)\n            .expect_err(\"expected restricted loss verification to fail\");\n        assert!(err.contains(\"restricted loss drops\"));\n    }\n\n    #[test]\n    fn excluded_loss_spike_verification_passes() {\n        let excluded_loss = vec![(0, 1.0), (1500, 2.2), (3000, 1.1)];\n        let config = ExcludedLossSpikeConfig {\n            min_relative_increase: 0.5,\n        };\n\n        let report = verify_excluded_loss_spike(&excluded_loss, &config)\n            .expect(\"expected excluded loss spike verification to pass\");\n        assert_eq!(report.spike_step, 1500);\n    }\n\n    #[test]\n    fn excluded_loss_spike_verification_fails_without_spike() {\n        let excluded_loss = vec![(0, 1.0), (1500, 1.1), (3000, 1.0)];\n        let config = ExcludedLossSpikeConfig {\n            min_relative_increase: 0.5,\n        };\n\n        let err = verify_excluded_loss_spike(&excluded_loss, &config)\n            .expect_err(\"expected excluded loss spike verification to fail\");\n        assert!(err.contains(\"excluded loss spike too small\"));\n    }\n\n    #[test]\n    fn restricted_loss_drop_verification_passes() {\n        let restricted_loss = vec![\n            (0, 2.0),\n            (1000, 1.8),\n            (5000, 1.2),\n            (7000, 0.8),\n            (10000, 0.9),\n        ];\n        let config = RestrictedLossDropConfig {\n            min_relative_decrease: 0.3,\n        };\n\n        let report = verify_restricted_loss_drop(&restricted_loss, &config)\n            .expect(\"expected restricted loss drop verification to pass\");\n        assert_eq!(report.drop_step, 7000);\n        assert!(report.baseline_loss > report.drop_loss);\n        assert!((report.baseline_loss - report.drop_loss) / report.baseline_loss >= 0.3);\n    }\n\n    #[test]\n    fn restricted_loss_drop_verification_fails_without_drop() {\n        let restricted_loss = vec![\n            (0, 2.0),\n            (1000, 2.1),\n            (5000, 1.9),\n            (7000, 2.0),\n            (10000, 2.05),\n        ];\n        let config = RestrictedLossDropConfig {\n            min_relative_decrease: 0.3,\n        };\n\n        let err = verify_restricted_loss_drop(&restricted_loss, &config)\n            .expect_err(\"expected restricted loss drop verification to fail\");\n        assert!(err.contains(\"restricted loss decrease too small\"));\n    }\n\n    #[test]\n    fn mlp_activation_wave_verification_passes_on_rectified_sine() {\n        let n = ModularAdditionDataset::modulus();\n        let freq = 5.0;\n        let mut signal = Vec::with_capacity(n);\n        for idx in 0..n {\n            let angle = 2.0 * std::f64::consts::PI * freq * idx as f64 / n as f64;\n            let rectified = angle.sin().max(0.0);\n            let wobble = (2.0 * std::f64::consts::PI * 2.0 * idx as f64 / n as f64)\n                .sin()\n                .abs()\n                * 0.02;\n            signal.push(rectified + wobble);\n        }\n\n        let config = MlpWaveVerificationConfig {\n            max_negative_fraction: 0.0,\n            negative_tolerance: 1e-6,\n            min_dominant_ratio: 3.0,\n            min_correlation: 0.7,\n            phase_steps: 32,\n        };\n\n        let report =\n            verify_mlp_activation_waves_from_signals(&[(0, signal)], &config)\n                .expect(\"expected rectified sine to pass\");\n        assert_eq!(report.neuron_reports.len(), 1);\n    }\n\n    #[test]\n    fn mlp_activation_wave_verification_fails_on_noise() {\n        let n = ModularAdditionDataset::modulus();\n        let signal: Vec<f64> = (0..n).map(|idx| idx as f64 / n as f64).collect();\n\n        let config = MlpWaveVerificationConfig {\n            max_negative_fraction: 0.0,\n            negative_tolerance: 1e-6,\n            min_dominant_ratio: 4.0,\n            min_correlation: 0.8,\n            phase_steps: 16,\n        };\n\n        let err =\n            verify_mlp_activation_waves_from_signals(&[(0, signal)], &config)\n                .expect_err(\"expected linear signal to fail\");\n        assert!(err.contains(\"dominant frequency ratio\") || err.contains(\"correlation\"));\n    }\n\n    #[test]\n    fn pairwise_manifold_transition_passes_on_elliptic_signals() {\n        let n = ModularAdditionDataset::modulus();\n        let freq = 5.0;\n        let mut post_signals = Vec::new();\n        for neuron_idx in 0..7usize {\n            let phase = 2.0 * std::f64::consts::PI * neuron_idx as f64 / 7.0;\n            let signal: Vec<f64> = (0..n)\n                .map(|idx| {\n                    let angle = 2.0 * std::f64::consts::PI * freq * idx as f64 / n as f64 + phase;\n                    angle.sin()\n                })\n                .collect();\n            post_signals.push((neuron_idx, signal));\n        }\n\n        let mut pre_signals = Vec::new();\n        for neuron_idx in 0..7usize {\n            let signal: Vec<f64> = (0..n)\n                .map(|idx| idx as f64 / n as f64 + neuron_idx as f64 * 0.01)\n                .collect();\n            pre_signals.push((neuron_idx, signal));\n        }\n\n        let config = PairwiseManifoldConfig {\n            fixed_y: 0,\n            neuron_indices: (0..7).collect(),\n            min_post_score: 0.75,\n            max_pre_score: 0.55,\n            max_axis_ratio: 8.0,\n            min_pairs_passing: 8,\n        };\n\n        let report = verify_pairwise_manifold_transition_from_signals(\n            &pre_signals,\n            &post_signals,\n            &config,\n        )\n        .expect(\"expected pairwise manifold transition to pass\");\n        assert!(report.passing_pairs >= config.min_pairs_passing);\n    }\n\n    #[test]\n    fn pairwise_manifold_transition_fails_without_rings() {\n        let n = ModularAdditionDataset::modulus();\n        let mut signals = Vec::new();\n        for neuron_idx in 0..7usize {\n            let signal: Vec<f64> = (0..n)\n                .map(|idx| idx as f64 / n as f64 + neuron_idx as f64 * 0.02)\n                .collect();\n            signals.push((neuron_idx, signal));\n        }\n\n        let config = PairwiseManifoldConfig {\n            fixed_y: 0,\n            neuron_indices: (0..7).collect(),\n            min_post_score: 0.8,\n            max_pre_score: 0.55,\n            max_axis_ratio: 4.0,\n            min_pairs_passing: 5,\n        };\n\n        let err = verify_pairwise_manifold_transition_from_signals(\n            &signals,\n            &signals,\n            &config,\n        )\n        .expect_err(\"expected pairwise manifold transition to fail\");\n        assert!(err.contains(\"post-grok\") || err.contains(\"pre-grok\"));\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_passes_on_diagonal_corrugation() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create diagonal ridges: f(x,y) = sin(2Ï€ * freq * (x+y) / n)\n        let freq = 3.0;\n        for x in 0..n {\n            for y in 0..n {\n                let angle = 2.0 * std::f64::consts::PI * freq * (x + y) as f64 / n as f64;\n                surface[x][y] = angle.sin();\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let report = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect(\"expected diagonal ridge verification to pass\");\n        assert!(report.diagonal_score > report.axis_score);\n        assert!(report.ratio >= config.min_diagonal_ratio);\n        assert!(report.diagonal_score >= config.min_diagonal_score);\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_passes_on_anti_diagonal_pattern() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create anti-diagonal ridges: f(x,y) = sin(2Ï€ * freq * (x-y) / n)\n        let freq = 3.0;\n        for x in 0..n {\n            for y in 0..n {\n                let angle = 2.0 * std::f64::consts::PI * freq * (x as i32 - y as i32) as f64 / n as f64;\n                surface[x][y] = angle.sin();\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let report = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect(\"expected anti-diagonal ridge verification to pass\");\n        assert!(report.diagonal_score > report.axis_score);\n        assert!(report.ratio >= config.min_diagonal_ratio);\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_fails_on_axis_aligned_pattern() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create axis-aligned ridges: f(x,y) = sin(2Ï€ * freq * x / n)\n        let freq = 3.0;\n        for x in 0..n {\n            for y in 0..n {\n                let angle = 2.0 * std::f64::consts::PI * freq * x as f64 / n as f64;\n                surface[x][y] = angle.sin();\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let err = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect_err(\"expected axis-aligned pattern to fail diagonal verification\");\n        assert!(err.contains(\"diagonal/axis ratio\") || err.contains(\"diagonal autocorrelation\"));\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_fails_on_flat_surface() {\n        let n = 50;\n        let surface = vec![vec![1.5; n]; n];\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let err = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect_err(\"expected flat surface to fail\");\n        assert!(err.contains(\"surface variance is zero\"));\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_fails_on_random_noise() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create random noise (low autocorrelation everywhere)\n        for x in 0..n {\n            for y in 0..n {\n                surface[x][y] = ((x * 17 + y * 31) % 100) as f64 / 100.0;\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let err = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect_err(\"expected random noise to fail diagonal verification\");\n        assert!(err.contains(\"diagonal/axis ratio\") || err.contains(\"diagonal autocorrelation\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_passes_on_valid_grokking_curve() {\n        // Simulate typical grokking: train converges early, val plateaus then jumps\n        let train_acc = vec![\n            (1, 0.10),\n            (10, 0.50),\n            (100, 0.80),\n            (500, 0.98),\n            (1000, 0.99),\n            (2000, 0.995),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (10, 0.009),\n            (100, 0.01),\n            (500, 0.011),\n            (1000, 0.009),\n            (1500, 0.01),\n            (2000, 0.92),\n            (3000, 0.98),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 1500,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let report = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect(\"expected snake curve verification to pass\");\n\n        assert_eq!(report.train_converged_by_step, 500);\n        assert!(report.val_plateau_max_acc < config.val_plateau_max_acc);\n        assert_eq!(report.val_grok_step, 2000);\n        assert!(report.curve_shape_valid);\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_on_simultaneous_convergence() {\n        // Both train and val converge at the same time (no grokking)\n        let train_acc = vec![\n            (1, 0.10),\n            (100, 0.98),\n            (500, 0.99),\n        ];\n        let val_acc = vec![\n            (1, 0.10),\n            (100, 0.97),\n            (500, 0.98),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 200,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let err = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect_err(\"expected snake curve verification to fail\");\n        assert!(err.contains(\"plateau region too high\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_on_slow_train_convergence() {\n        // Train takes too long to converge (not the typical snake curve)\n        let train_acc = vec![\n            (1, 0.10),\n            (500, 0.50),\n            (1000, 0.70),\n            (2000, 0.98),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (500, 0.01),\n            (1000, 0.01),\n            (2000, 0.92),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 1500,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let err = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect_err(\"expected snake curve verification to fail\");\n        assert!(err.contains(\"converges at step 2000\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_without_val_jump() {\n        // Validation never jumps up (no generalization)\n        let train_acc = vec![\n            (1, 0.10),\n            (100, 0.98),\n            (500, 0.99),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (100, 0.01),\n            (500, 0.01),\n            (2000, 0.02),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 200,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let err = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect_err(\"expected snake curve verification to fail\");\n        assert!(err.contains(\"validation accuracy never reaches\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_on_wrong_phase_ordering() {\n        // Val jumps before train converges (backwards snake)\n        let train_acc = vec![\n            (1, 0.10),\n            (500, 0.50),\n            (2000, 0.98),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (100, 0.95),\n            (500, 0.98),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 200,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let result = verify_snake_curve_shape(&train_acc, &val_acc, &config);\n        // Should fail because val jumps before train converges (not a proper snake curve)\n        // This can fail for multiple reasons, but should definitely fail\n        assert!(result.is_err(), \"Expected snake curve verification to fail but it passed\");\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_passes_on_decreasing_norms() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 85.0),\n            (1000, 72.0),\n            (1500, 63.0),\n            (2000, 58.0),\n            (2500, 55.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let report = verify_weight_norm_decay(&weight_norms, Some(2000), &config)\n            .expect(\"expected weight norm decay verification to pass\");\n\n        assert_eq!(report.initial_norm, 100.0);\n        assert_eq!(report.final_norm, 55.0);\n        assert!((report.relative_decrease - 0.45).abs() < 0.01);\n        assert!(report.grok_step_norm.is_some());\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_fails_on_flat_norms() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 99.0),\n            (1000, 98.5),\n            (1500, 98.0),\n            (2000, 97.5),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let err = verify_weight_norm_decay(&weight_norms, None, &config)\n            .expect_err(\"expected weight norm decay verification to fail\");\n        assert!(err.contains(\"weight norm decrease\"));\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_fails_on_increasing_norms() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 95.0),\n            (1000, 105.0), // Increase here\n            (1500, 90.0),\n            (2000, 85.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let err = verify_weight_norm_decay(&weight_norms, None, &config)\n            .expect_err(\"expected weight norm decay verification to fail\");\n        assert!(err.contains(\"weight norm increased\"));\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_allows_small_noise() {\n        // Weight decay with small numerical noise (within 1% tolerance)\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 95.0),\n            (1000, 90.5),  // Small increase within tolerance\n            (1500, 90.0),\n            (2000, 85.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let report = verify_weight_norm_decay(&weight_norms, None, &config)\n            .expect(\"expected weight norm decay with small noise to pass\");\n\n        assert_eq!(report.initial_norm, 100.0);\n        assert_eq!(report.final_norm, 85.0);\n        assert!((report.relative_decrease - 0.15).abs() < 0.01);\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_finds_grok_step_norm() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (1000, 80.0),\n            (1800, 68.0),  // Within window of grok step (2200 - 500 = 1700)\n            (2000, 65.0),\n            (2200, 62.0),  // Around grok step\n            (3000, 55.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let report = verify_weight_norm_decay(&weight_norms, Some(2200), &config)\n            .expect(\"expected weight norm decay verification to pass\");\n\n        assert!(report.grok_step_norm.is_some());\n        let grok_norm = report.grok_step_norm.unwrap();\n        // Should find the first norm in the window [1700, 2700], which is 68.0 at step 1800\n        assert!((grok_norm - 68.0).abs() < 0.01);\n    }\n}\n","structuredPatch":[{"oldStart":512,"oldLines":6,"newStart":512,"newLines":8,"lines":["     })"," }"," ","+/// NOTE: Verification function reserved for future integration or manual analysis","+#[allow(dead_code)]"," pub fn verify_mlp_activation_waves<B: Backend>(","     model: &Transformer<B>,","     device: &B::Device,"]}],"userModified":false,"replaceAll":false}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01PYE75qknFr6naoMddyXCg7","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01YLKVqAd2myvn8t9AuyWewz","name":"Edit","input":{"replace_all":false,"file_path":"/Users/admin/MechInt/src/verify.rs","old_string":"pub fn verify_mlp_activation_waves_from_signals(","new_string":"/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_mlp_activation_waves_from_signals("}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":831,"cache_read_input_tokens":58101,"cache_creation":{"ephemeral_5m_input_tokens":831,"ephemeral_1h_input_tokens":0},"output_tokens":174,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"54123283-2ff6-45a1-8729-a19732475de7"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01YLKVqAd2myvn8t9AuyWewz","type":"tool_result","content":"The file /Users/admin/MechInt/src/verify.rs has been updated. Here's the result of running `cat -n` on a snippet of the edited file:\n   542â†’        .collect();\n   543â†’    verify_mlp_activation_waves_from_signals(&signals, config)\n   544â†’}\n   545â†’\n   546â†’/// NOTE: Verification function reserved for future integration or manual analysis\n   547â†’#[allow(dead_code)]\n   548â†’pub fn verify_mlp_activation_waves_from_signals(\n   549â†’    signals: &[(usize, Vec<f64>)],\n   550â†’    config: &MlpWaveVerificationConfig,\n   551â†’) -> Result<MlpWaveReport, String> {\n   552â†’    if signals.is_empty() {"}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"12d10bc3-fdc2-4e8c-8e05-59bfad4f3be7","tool_use_result":{"filePath":"/Users/admin/MechInt/src/verify.rs","oldString":"pub fn verify_mlp_activation_waves_from_signals(","newString":"/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_mlp_activation_waves_from_signals(","originalFile":"use burn::{\n    data::dataset::Dataset,\n    tensor::{backend::Backend, Int, Tensor},\n};\nuse rustfft::{num_complex::Complex, FftPlanner};\n\nuse crate::analysis::{AccuracyHistory, LossHistory};\nuse crate::data::ModularAdditionDataset;\nuse crate::model::Transformer;\n\n#[derive(Debug, Clone)]\npub struct GrokkingVerificationConfig {\n    pub early_train_acc_threshold: f32,\n    pub early_step_max: usize,\n    pub plateau_min_step: usize,\n    pub generalization_window: usize,\n    pub chance_accuracy: f32,\n    pub chance_tolerance: f32,\n    pub target_val_acc_threshold: f32,\n    pub accuracy_window: usize,\n    pub loss_window: usize,\n    pub loss_drop_fraction: f64,\n}\n\nimpl GrokkingVerificationConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        Self {\n            early_train_acc_threshold: 0.99,\n            early_step_max: 1000,\n            plateau_min_step: 1000,\n            generalization_window: 10_000,\n            chance_accuracy: 1.0 / modulus as f32,\n            chance_tolerance: 0.02,\n            target_val_acc_threshold: 0.99,\n            accuracy_window: 5,\n            loss_window: 5,\n            loss_drop_fraction: 0.5,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct GrokkingPhaseReport {\n    pub early_train_step: usize,\n    pub transition_step: usize,\n    pub plateau_max_val_acc: f32,\n    pub loss_drop_ratio: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossVerificationConfig {\n    pub plateau_min_step: usize,\n    pub drop_window: usize,\n    pub drop_fraction: f64,\n    pub min_step_lead: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossReport {\n    pub restricted_drop_step: usize,\n    pub full_drop_step: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossDropConfig {\n    pub min_relative_decrease: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossDropReport {\n    pub drop_step: usize,\n    pub drop_loss: f64,\n    pub baseline_loss: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct ExcludedLossSpikeConfig {\n    pub min_relative_increase: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct ExcludedLossSpikeReport {\n    pub spike_step: usize,\n    pub spike_loss: f64,\n    pub baseline_loss: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct MlpWaveVerificationConfig {\n    pub max_negative_fraction: f64,\n    pub negative_tolerance: f64,\n    pub min_dominant_ratio: f64,\n    pub min_correlation: f64,\n    pub phase_steps: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct MlpWaveNeuronReport {\n    pub neuron_index: usize,\n    pub dominant_frequency: usize,\n    pub dominant_ratio: f64,\n    pub negative_fraction: f64,\n    pub best_correlation: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct MlpWaveReport {\n    pub neuron_reports: Vec<MlpWaveNeuronReport>,\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldConfig {\n    pub fixed_y: usize,\n    pub neuron_indices: Vec<usize>,\n    pub min_post_score: f64,\n    pub max_pre_score: f64,\n    pub max_axis_ratio: f64,\n    pub min_pairs_passing: usize,\n}\n\nimpl PairwiseManifoldConfig {\n    pub fn default_for_modulus(_modulus: usize) -> Self {\n        Self {\n            fixed_y: 0,\n            neuron_indices: (0..7).collect(),\n            min_post_score: 0.75,\n            max_pre_score: 0.55,\n            max_axis_ratio: 8.0,\n            min_pairs_passing: 10,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldPairReport {\n    pub neuron_a: usize,\n    pub neuron_b: usize,\n    pub circularity_score: f64,\n    pub radius_cv: f64,\n    pub axis_ratio: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldReport {\n    pub pair_reports: Vec<PairwiseManifoldPairReport>,\n    pub average_score: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldTransitionReport {\n    pub pre_average_score: f64,\n    pub post_average_score: f64,\n    pub passing_pairs: usize,\n    pub total_pairs: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct DiagonalRidgeConfig {\n    pub neuron_index: usize,\n    pub max_lag: usize,\n    pub min_diagonal_ratio: f64,\n    pub min_diagonal_score: f64,\n}\n\nimpl DiagonalRidgeConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        let max_lag = (modulus / 10).max(4).min(20);\n        Self {\n            neuron_index: 10,\n            max_lag,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct DiagonalRidgeReport {\n    pub neuron_index: usize,\n    pub diagonal_score: f64,\n    pub axis_score: f64,\n    pub ratio: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct SnakeCurveConfig {\n    pub early_step_max: usize,\n    pub train_convergence_threshold: f32,\n    pub val_plateau_min_step: usize,\n    pub val_plateau_max_acc: f32,\n    pub val_final_threshold: f32,\n}\n\nimpl SnakeCurveConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        Self {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 1000,\n            val_plateau_max_acc: 1.0 / modulus as f32 + 0.05,\n            val_final_threshold: 0.90,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct WeightNormDecayConfig {\n    pub min_relative_decrease: f64,\n    pub plateau_min_step: usize,\n    pub grok_step_window: usize,\n}\n\nimpl WeightNormDecayConfig {\n    pub fn default_for_grokking() -> Self {\n        Self {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct WeightNormDecayReport {\n    pub initial_norm: f64,\n    pub final_norm: f64,\n    pub relative_decrease: f64,\n    pub grok_step_norm: Option<f64>,\n}\n\n#[derive(Debug, Clone)]\npub struct SnakeCurveReport {\n    pub train_converged_by_step: usize,\n    pub val_plateau_max_acc: f32,\n    pub val_grok_step: usize,\n    pub curve_shape_valid: bool,\n}\n\npub fn verify_grokking_phase_transition(\n    loss_history: &LossHistory,\n    accuracy_history: &AccuracyHistory,\n    config: &GrokkingVerificationConfig,\n) -> Result<GrokkingPhaseReport, String> {\n    let (early_train_step, _) = accuracy_history\n        .train_snapshots\n        .iter()\n        .find(|(_, acc)| *acc >= config.early_train_acc_threshold)\n        .ok_or_else(|| \"train accuracy never reaches threshold\".to_string())?;\n\n    if *early_train_step > config.early_step_max {\n        return Err(format!(\n            \"train accuracy reaches {:.2} after step {}, expected before {}\",\n            config.early_train_acc_threshold, early_train_step, config.early_step_max\n        ));\n    }\n\n    let plateau_max_val_acc = accuracy_history\n        .val_snapshots\n        .iter()\n        .filter(|(step, _)| *step <= config.plateau_min_step)\n        .map(|(_, acc)| *acc)\n        .fold(None::<f32>, |accum, value| {\n            Some(accum.map_or(value, |max_val| max_val.max(value)))\n        })\n        .ok_or_else(|| \"no validation accuracy samples before plateau_min_step\".to_string())?;\n\n    if plateau_max_val_acc > config.chance_accuracy + config.chance_tolerance {\n        return Err(format!(\n            \"validation accuracy before step {} exceeds chance (max {:.4})\",\n            config.plateau_min_step, plateau_max_val_acc\n        ));\n    }\n\n    let (transition_step, _) = accuracy_history\n        .val_snapshots\n        .iter()\n        .find(|(step, acc)| {\n            *step >= config.plateau_min_step && *acc >= config.target_val_acc_threshold\n        })\n        .ok_or_else(|| \"validation accuracy never reaches target threshold\".to_string())?;\n\n    let latest_allowed = config.plateau_min_step + config.generalization_window;\n    if *transition_step > latest_allowed {\n        return Err(format!(\n            \"validation accuracy reaches {:.2} at step {}, expected before {}\",\n            config.target_val_acc_threshold, transition_step, latest_allowed\n        ));\n    }\n\n    verify_accuracy_jump(\n        &accuracy_history.val_snapshots,\n        *transition_step,\n        config.accuracy_window,\n        config.chance_accuracy,\n        config.chance_tolerance,\n        config.target_val_acc_threshold,\n    )?;\n\n    let loss_drop_ratio = verify_loss_drop(\n        &loss_history.val_snapshots,\n        *transition_step,\n        config.loss_window,\n        config.loss_drop_fraction,\n    )?;\n\n    Ok(GrokkingPhaseReport {\n        early_train_step: *early_train_step,\n        transition_step: *transition_step,\n        plateau_max_val_acc,\n        loss_drop_ratio,\n    })\n}\n\nfn verify_accuracy_jump(\n    val_accuracy: &[(usize, f32)],\n    transition_step: usize,\n    window: usize,\n    chance_accuracy: f32,\n    chance_tolerance: f32,\n    target_val_acc_threshold: f32,\n) -> Result<(), String> {\n    let before_values: Vec<f32> = val_accuracy\n        .iter()\n        .filter(|(step, _)| *step < transition_step)\n        .map(|(_, acc)| *acc)\n        .collect();\n\n    let after_values: Vec<f32> = val_accuracy\n        .iter()\n        .filter(|(step, _)| *step >= transition_step)\n        .map(|(_, acc)| *acc)\n        .collect();\n\n    if before_values.len() < window || after_values.len() < window {\n        return Err(\"insufficient accuracy samples around transition\".to_string());\n    }\n\n    let before_avg: f32 =\n        before_values[before_values.len() - window..].iter().sum::<f32>() / window as f32;\n    let after_avg: f32 = after_values[..window].iter().sum::<f32>() / window as f32;\n\n    if before_avg > chance_accuracy + chance_tolerance {\n        return Err(format!(\n            \"validation accuracy plateau too high before transition (avg {:.4})\",\n            before_avg\n        ));\n    }\n\n    if after_avg < target_val_acc_threshold {\n        return Err(format!(\n            \"validation accuracy after transition too low (avg {:.4})\",\n            after_avg\n        ));\n    }\n\n    Ok(())\n}\n\nfn verify_loss_drop(\n    val_losses: &[(usize, f64)],\n    transition_step: usize,\n    loss_window: usize,\n    loss_drop_fraction: f64,\n) -> Result<f64, String> {\n    let before_losses: Vec<f64> = val_losses\n        .iter()\n        .filter(|(step, _)| *step < transition_step)\n        .map(|(_, loss)| *loss)\n        .collect();\n\n    let after_losses: Vec<f64> = val_losses\n        .iter()\n        .filter(|(step, _)| *step >= transition_step)\n        .map(|(_, loss)| *loss)\n        .collect();\n\n    if before_losses.len() < loss_window || after_losses.len() < loss_window {\n        return Err(\"insufficient loss samples around transition\".to_string());\n    }\n\n    let before_avg: f64 =\n        before_losses[before_losses.len() - loss_window..].iter().sum::<f64>()\n            / loss_window as f64;\n    let after_avg: f64 = after_losses[..loss_window].iter().sum::<f64>() / loss_window as f64;\n\n    if after_avg > before_avg * (1.0 - loss_drop_fraction) {\n        return Err(format!(\n            \"validation loss drop too small (before {:.4}, after {:.4})\",\n            before_avg, after_avg\n        ));\n    }\n\n    Ok(after_avg / before_avg)\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_restricted_loss_early_drop(\n    full_losses: &[(usize, f64)],\n    restricted_losses: &[(usize, f64)],\n    config: &RestrictedLossVerificationConfig,\n) -> Result<RestrictedLossReport, String> {\n    let restricted_baseline =\n        baseline_loss(restricted_losses, config.plateau_min_step, config.drop_window)?;\n    let full_baseline = baseline_loss(full_losses, config.plateau_min_step, config.drop_window)?;\n\n    let restricted_drop_step = find_drop_step(\n        restricted_losses,\n        config.plateau_min_step,\n        restricted_baseline,\n        config.drop_fraction,\n    )?;\n    let full_drop_step = find_drop_step(\n        full_losses,\n        config.plateau_min_step,\n        full_baseline,\n        config.drop_fraction,\n    )?;\n\n    if restricted_drop_step + config.min_step_lead > full_drop_step {\n        return Err(format!(\n            \"restricted loss drops at step {}, expected at least {} steps before full loss (step {})\",\n            restricted_drop_step, config.min_step_lead, full_drop_step\n        ));\n    }\n\n    Ok(RestrictedLossReport {\n        restricted_drop_step,\n        full_drop_step,\n    })\n}\n\npub fn verify_restricted_loss_drop(\n    restricted_losses: &[(usize, f64)],\n    config: &RestrictedLossDropConfig,\n) -> Result<RestrictedLossDropReport, String> {\n    if restricted_losses.len() < 3 {\n        return Err(\"restricted loss history requires at least 3 points\".to_string());\n    }\n\n    let first = restricted_losses.first().ok_or_else(|| {\n        \"restricted loss history is empty\".to_string()\n    })?;\n    let baseline_loss = first.1;\n\n    let (min_idx, (min_step, min_loss)) = restricted_losses\n        .iter()\n        .enumerate()\n        .min_by(|(_, a), (_, b)| {\n            a.1.partial_cmp(&b.1).unwrap_or(std::cmp::Ordering::Equal)\n        })\n        .ok_or_else(|| \"restricted loss history is empty\".to_string())?;\n\n    if min_idx == 0 {\n        return Err(\"restricted loss minimum occurs at start (no drop detected)\".to_string());\n    }\n\n    let relative_decrease = (baseline_loss - min_loss) / baseline_loss;\n    if relative_decrease < config.min_relative_decrease {\n        return Err(format!(\n            \"restricted loss decrease too small (baseline {:.4}, min {:.4}, decrease {:.1}%)\",\n            baseline_loss, min_loss, relative_decrease * 100.0\n        ));\n    }\n\n    Ok(RestrictedLossDropReport {\n        drop_step: *min_step,\n        drop_loss: *min_loss,\n        baseline_loss,\n    })\n}\n\npub fn verify_excluded_loss_spike(\n    excluded_losses: &[(usize, f64)],\n    config: &ExcludedLossSpikeConfig,\n) -> Result<ExcludedLossSpikeReport, String> {\n    if excluded_losses.len() < 3 {\n        return Err(\"excluded loss history requires at least 3 points\".to_string());\n    }\n\n    let first = excluded_losses.first().ok_or_else(|| {\n        \"excluded loss history is empty\".to_string()\n    })?;\n    let last = excluded_losses.last().ok_or_else(|| {\n        \"excluded loss history is empty\".to_string()\n    })?;\n    let baseline_loss = (first.1 + last.1) / 2.0;\n\n    let (peak_idx, (peak_step, peak_loss)) = excluded_losses\n        .iter()\n        .enumerate()\n        .max_by(|(_, a), (_, b)| {\n            a.1.partial_cmp(&b.1).unwrap_or(std::cmp::Ordering::Equal)\n        })\n        .ok_or_else(|| \"excluded loss history is empty\".to_string())?;\n\n    if peak_idx == 0 || peak_idx + 1 == excluded_losses.len() {\n        return Err(\"excluded loss spike occurs at boundary\".to_string());\n    }\n\n    if peak_loss < &(baseline_loss * (1.0 + config.min_relative_increase)) {\n        return Err(format!(\n            \"excluded loss spike too small (baseline {:.4}, peak {:.4})\",\n            baseline_loss, peak_loss\n        ));\n    }\n\n    Ok(ExcludedLossSpikeReport {\n        spike_step: *peak_step,\n        spike_loss: *peak_loss,\n        baseline_loss,\n    })\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_mlp_activation_waves<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n    fixed_y: usize,\n    neuron_indices: &[usize],\n    config: &MlpWaveVerificationConfig,\n) -> Result<MlpWaveReport, String> {\n    let activations = collect_mlp_post_relu_activations(model, device, fixed_y)?;\n    if neuron_indices.is_empty() {\n        return Err(\"neuron indices list is empty\".to_string());\n    }\n\n    let d_ff = activations.len();\n    for &idx in neuron_indices {\n        if idx >= d_ff {\n            return Err(format!(\n                \"neuron index {} out of range (d_ff={})\",\n                idx, d_ff\n            ));\n        }\n    }\n\n    let signals: Vec<(usize, Vec<f64>)> = neuron_indices\n        .iter()\n        .map(|&idx| (idx, activations[idx].clone()))\n        .collect();\n    verify_mlp_activation_waves_from_signals(&signals, config)\n}\n\npub fn verify_mlp_activation_waves_from_signals(\n    signals: &[(usize, Vec<f64>)],\n    config: &MlpWaveVerificationConfig,\n) -> Result<MlpWaveReport, String> {\n    if signals.is_empty() {\n        return Err(\"no activation signals provided\".to_string());\n    }\n\n    let mut reports = Vec::with_capacity(signals.len());\n    for (neuron_index, signal) in signals {\n        let report = analyze_mlp_activation_wave(*neuron_index, signal, config)?;\n        reports.push(report);\n    }\n\n    Ok(MlpWaveReport {\n        neuron_reports: reports,\n    })\n}\n\npub fn verify_mlp_pairwise_manifold_transition<B: Backend>(\n    pre_model: &Transformer<B>,\n    post_model: &Transformer<B>,\n    device: &B::Device,\n    config: &PairwiseManifoldConfig,\n) -> Result<PairwiseManifoldTransitionReport, String> {\n    let modulus = ModularAdditionDataset::modulus();\n    if config.fixed_y >= modulus {\n        return Err(format!(\n            \"fixed_y {} out of range (modulus {})\",\n            config.fixed_y, modulus\n        ));\n    }\n\n    let pre_activations = collect_mlp_post_relu_activations(pre_model, device, config.fixed_y)?;\n    let post_activations = collect_mlp_post_relu_activations(post_model, device, config.fixed_y)?;\n\n    let d_ff = pre_activations.len();\n    if post_activations.len() != d_ff {\n        return Err(\"post activations shape mismatch\".to_string());\n    }\n\n    if config.neuron_indices.len() < 2 {\n        return Err(\"need at least two neuron indices\".to_string());\n    }\n\n    for &idx in &config.neuron_indices {\n        if idx >= d_ff {\n            return Err(format!(\n                \"neuron index {} out of range (d_ff={})\",\n                idx, d_ff\n            ));\n        }\n    }\n\n    let pre_signals: Vec<(usize, Vec<f64>)> = config\n        .neuron_indices\n        .iter()\n        .map(|&idx| (idx, pre_activations[idx].clone()))\n        .collect();\n    let post_signals: Vec<(usize, Vec<f64>)> = config\n        .neuron_indices\n        .iter()\n        .map(|&idx| (idx, post_activations[idx].clone()))\n        .collect();\n\n    verify_pairwise_manifold_transition_from_signals(&pre_signals, &post_signals, config)\n}\n\npub fn collect_mlp_activation_surface<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n    neuron_index: usize,\n) -> Result<Vec<Vec<f64>>, String> {\n    let modulus = ModularAdditionDataset::modulus();\n    let equals_token = ModularAdditionDataset::equals_token();\n\n    let mut inputs_vec = Vec::with_capacity(modulus * modulus * 3);\n    for x in 0..modulus {\n        for y in 0..modulus {\n            inputs_vec.push(x as i32);\n            inputs_vec.push(y as i32);\n            inputs_vec.push(equals_token as i32);\n        }\n    }\n\n    let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n        .reshape([modulus * modulus, 3]);\n    let (_, mlp_acts) = model.forward_with_mlp_activations(inputs);\n    let [batch_size, d_ff] = mlp_acts.dims();\n    if batch_size != modulus * modulus {\n        return Err(format!(\n            \"unexpected activation batch size {} (expected {})\",\n            batch_size,\n            modulus * modulus\n        ));\n    }\n    if neuron_index >= d_ff {\n        return Err(format!(\n            \"neuron index {} out of range (d_ff={})\",\n            neuron_index, d_ff\n        ));\n    }\n\n    let data: Vec<f32> = mlp_acts.into_data().to_vec().unwrap();\n    let mut surface = vec![vec![0.0f64; modulus]; modulus];\n    for x in 0..modulus {\n        for y in 0..modulus {\n            let batch = x * modulus + y;\n            let idx = batch * d_ff + neuron_index;\n            surface[x][y] = data[idx] as f64;\n        }\n    }\n\n    Ok(surface)\n}\n\npub fn verify_constructive_interference_surface_from_grid(\n    surface: &[Vec<f64>],\n    config: &DiagonalRidgeConfig,\n) -> Result<DiagonalRidgeReport, String> {\n    let (diag_main, diag_anti, axis_x, axis_y) =\n        surface_autocorrelation_scores(surface, config.max_lag)?;\n\n    let diagonal_score = diag_main.max(diag_anti);\n    let axis_score = axis_x.max(axis_y);\n    let ratio = if axis_score > 0.0 {\n        diagonal_score / axis_score\n    } else {\n        f64::INFINITY\n    };\n\n    if diagonal_score < config.min_diagonal_score {\n        return Err(format!(\n            \"diagonal autocorrelation {:.3} below threshold {:.3}\",\n            diagonal_score, config.min_diagonal_score\n        ));\n    }\n\n    if ratio < config.min_diagonal_ratio {\n        return Err(format!(\n            \"diagonal/axis ratio {:.3} below threshold {:.3}\",\n            ratio, config.min_diagonal_ratio\n        ));\n    }\n\n    Ok(DiagonalRidgeReport {\n        neuron_index: config.neuron_index,\n        diagonal_score,\n        axis_score,\n        ratio,\n    })\n}\n\npub fn verify_pairwise_manifold_transition_from_signals(\n    pre_signals: &[(usize, Vec<f64>)],\n    post_signals: &[(usize, Vec<f64>)],\n    config: &PairwiseManifoldConfig,\n) -> Result<PairwiseManifoldTransitionReport, String> {\n    let pre_report = evaluate_pairwise_manifolds_from_signals(pre_signals)?;\n    let post_report = evaluate_pairwise_manifolds_from_signals(post_signals)?;\n\n    if pre_report.average_score > config.max_pre_score {\n        return Err(format!(\n            \"pre-grok average circularity {:.3} exceeds threshold {:.3}\",\n            pre_report.average_score, config.max_pre_score\n        ));\n    }\n\n    let passing_pairs = post_report\n        .pair_reports\n        .iter()\n        .filter(|pair| {\n            pair.circularity_score >= config.min_post_score\n                && pair.axis_ratio <= config.max_axis_ratio\n        })\n        .count();\n    if passing_pairs < config.min_pairs_passing {\n        return Err(format!(\n            \"post-grok only {} pairs meet circularity threshold (need {})\",\n            passing_pairs, config.min_pairs_passing\n        ));\n    }\n\n    Ok(PairwiseManifoldTransitionReport {\n        pre_average_score: pre_report.average_score,\n        post_average_score: post_report.average_score,\n        passing_pairs,\n        total_pairs: post_report.pair_reports.len(),\n    })\n}\n\nfn evaluate_pairwise_manifolds_from_signals(\n    signals: &[(usize, Vec<f64>)],\n) -> Result<PairwiseManifoldReport, String> {\n    if signals.len() < 2 {\n        return Err(\"need at least two activation signals\".to_string());\n    }\n\n    let signal_len = signals[0].1.len();\n    if signal_len < 3 {\n        return Err(\"activation signal too short\".to_string());\n    }\n\n    for (idx, signal) in signals {\n        if signal.len() != signal_len {\n            return Err(format!(\n                \"signal length mismatch for neuron {}\",\n                idx\n            ));\n        }\n    }\n\n    let mut reports = Vec::new();\n    for i in 0..signals.len() {\n        for j in (i + 1)..signals.len() {\n            let (neuron_a, signal_a) = &signals[i];\n            let (neuron_b, signal_b) = &signals[j];\n            let points: Vec<(f64, f64)> = signal_a\n                .iter()\n                .zip(signal_b.iter())\n                .map(|(x, y)| (*x, *y))\n                .collect();\n            let (score, radius_cv, axis_ratio) =\n                match pairwise_circularity_metrics(&points) {\n                    Ok(metrics) => metrics,\n                    Err(_) => (0.0, 1.0, f64::INFINITY),\n                };\n            reports.push(PairwiseManifoldPairReport {\n                neuron_a: *neuron_a,\n                neuron_b: *neuron_b,\n                circularity_score: score,\n                radius_cv,\n                axis_ratio,\n            });\n        }\n    }\n\n    let average_score = reports\n        .iter()\n        .map(|pair| pair.circularity_score)\n        .sum::<f64>()\n        / reports.len() as f64;\n\n    Ok(PairwiseManifoldReport {\n        pair_reports: reports,\n        average_score,\n    })\n}\n\nfn pairwise_circularity_metrics(points: &[(f64, f64)]) -> Result<(f64, f64, f64), String> {\n    if points.len() < 3 {\n        return Err(\"not enough points for circularity\".to_string());\n    }\n\n    let n = points.len() as f64;\n    let mean_x = points.iter().map(|(x, _)| *x).sum::<f64>() / n;\n    let mean_y = points.iter().map(|(_, y)| *y).sum::<f64>() / n;\n\n    let mut var_x = 0.0;\n    let mut var_y = 0.0;\n    let mut cov_xy = 0.0;\n    for (x, y) in points {\n        let dx = x - mean_x;\n        let dy = y - mean_y;\n        var_x += dx * dx;\n        var_y += dy * dy;\n        cov_xy += dx * dy;\n    }\n    var_x /= n;\n    var_y /= n;\n    cov_xy /= n;\n\n    let trace = var_x + var_y;\n    let det = var_x * var_y - cov_xy * cov_xy;\n    if det <= 1.0e-12 || trace <= 0.0 {\n        return Err(\"degenerate covariance\".to_string());\n    }\n\n    let half_trace = trace * 0.5;\n    let disc = (half_trace * half_trace - det).max(0.0).sqrt();\n    let lambda1 = half_trace + disc;\n    let lambda2 = half_trace - disc;\n    if lambda2 <= 0.0 {\n        return Err(\"degenerate eigenvalues\".to_string());\n    }\n\n    let axis_ratio = (lambda1 / lambda2).sqrt();\n\n    let mut v1_x = cov_xy;\n    let mut v1_y = lambda1 - var_x;\n    if v1_x.abs() < 1.0e-12 && v1_y.abs() < 1.0e-12 {\n        v1_x = 1.0;\n        v1_y = 0.0;\n    }\n    let norm = (v1_x * v1_x + v1_y * v1_y).sqrt();\n    v1_x /= norm;\n    v1_y /= norm;\n    let v2_x = -v1_y;\n    let v2_y = v1_x;\n\n    let mut radii = Vec::with_capacity(points.len());\n    for (x, y) in points {\n        let dx = x - mean_x;\n        let dy = y - mean_y;\n        let p1 = dx * v1_x + dy * v1_y;\n        let p2 = dx * v2_x + dy * v2_y;\n        let w1 = p1 / lambda1.sqrt();\n        let w2 = p2 / lambda2.sqrt();\n        radii.push((w1 * w1 + w2 * w2).sqrt());\n    }\n\n    let mean_radius = radii.iter().sum::<f64>() / radii.len() as f64;\n    if mean_radius == 0.0 {\n        return Err(\"zero mean radius\".to_string());\n    }\n\n    let mut variance = 0.0;\n    for r in &radii {\n        let diff = r - mean_radius;\n        variance += diff * diff;\n    }\n    let std = (variance / radii.len() as f64).sqrt();\n    let radius_cv = std / mean_radius;\n    let score = 1.0 / (1.0 + radius_cv);\n\n    Ok((score, radius_cv, axis_ratio))\n}\n\nfn baseline_loss(\n    losses: &[(usize, f64)],\n    plateau_min_step: usize,\n    drop_window: usize,\n) -> Result<f64, String> {\n    let before: Vec<f64> = losses\n        .iter()\n        .filter(|(step, _)| *step <= plateau_min_step)\n        .map(|(_, loss)| *loss)\n        .collect();\n\n    if before.len() < drop_window || drop_window == 0 {\n        return Err(\"insufficient loss samples for baseline\".to_string());\n    }\n\n    let window = &before[before.len() - drop_window..];\n    Ok(window.iter().sum::<f64>() / drop_window as f64)\n}\n\nfn find_drop_step(\n    losses: &[(usize, f64)],\n    plateau_min_step: usize,\n    baseline: f64,\n    drop_fraction: f64,\n) -> Result<usize, String> {\n    losses\n        .iter()\n        .filter(|(step, _)| *step >= plateau_min_step)\n        .find(|(_, loss)| *loss <= baseline * (1.0 - drop_fraction))\n        .map(|(step, _)| *step)\n        .ok_or_else(|| \"loss never drops below threshold\".to_string())\n}\n\nfn collect_mlp_post_relu_activations<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n    fixed_y: usize,\n) -> Result<Vec<Vec<f64>>, String> {\n    let modulus = ModularAdditionDataset::modulus();\n    let equals_token = ModularAdditionDataset::equals_token();\n\n    let mut inputs_vec = Vec::with_capacity(modulus * 3);\n    for x in 0..modulus {\n        inputs_vec.push(x as i32);\n        inputs_vec.push(fixed_y as i32);\n        inputs_vec.push(equals_token as i32);\n    }\n\n    let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n        .reshape([modulus, 3]);\n    let (_, mlp_acts) = model.forward_with_mlp_activations(inputs);\n    let [batch_size, d_ff] = mlp_acts.dims();\n\n    if batch_size != modulus {\n        return Err(format!(\n            \"unexpected activation batch size {} (expected {})\",\n            batch_size, modulus\n        ));\n    }\n\n    let data: Vec<f32> = mlp_acts.into_data().to_vec().unwrap();\n    let mut per_neuron = vec![Vec::with_capacity(batch_size); d_ff];\n    for batch in 0..batch_size {\n        for neuron in 0..d_ff {\n            let idx = batch * d_ff + neuron;\n            per_neuron[neuron].push(data[idx] as f64);\n        }\n    }\n\n    Ok(per_neuron)\n}\n\nfn surface_autocorrelation_scores(\n    surface: &[Vec<f64>],\n    max_lag: usize,\n) -> Result<(f64, f64, f64, f64), String> {\n    let (height, width) = surface_dimensions(surface)?;\n    if height < 2 || width < 2 {\n        return Err(\"surface grid too small\".to_string());\n    }\n\n    let (mean, var) = surface_mean_variance(surface)?;\n    if var == 0.0 {\n        return Err(\"surface variance is zero\".to_string());\n    }\n\n    let max_lag = max_lag.min(height.saturating_sub(1)).min(width.saturating_sub(1));\n    if max_lag == 0 {\n        return Err(\"max_lag too small for surface\".to_string());\n    }\n\n    let diag_main = average_autocorrelation(surface, mean, var, max_lag, 1, 1)?;\n    let diag_anti = average_autocorrelation(surface, mean, var, max_lag, 1, -1)?;\n    let axis_x = average_autocorrelation(surface, mean, var, max_lag, 1, 0)?;\n    let axis_y = average_autocorrelation(surface, mean, var, max_lag, 0, 1)?;\n\n    Ok((diag_main, diag_anti, axis_x, axis_y))\n}\n\nfn surface_dimensions(surface: &[Vec<f64>]) -> Result<(usize, usize), String> {\n    let height = surface.len();\n    if height == 0 {\n        return Err(\"surface is empty\".to_string());\n    }\n    let width = surface[0].len();\n    if width == 0 {\n        return Err(\"surface has empty rows\".to_string());\n    }\n    for (idx, row) in surface.iter().enumerate() {\n        if row.len() != width {\n            return Err(format!(\n                \"surface row {} has length {}, expected {}\",\n                idx,\n                row.len(),\n                width\n            ));\n        }\n    }\n    Ok((height, width))\n}\n\nfn surface_mean_variance(surface: &[Vec<f64>]) -> Result<(f64, f64), String> {\n    let (height, width) = surface_dimensions(surface)?;\n    let mut sum = 0.0;\n    for row in surface {\n        for value in row {\n            sum += *value;\n        }\n    }\n    let count = (height * width) as f64;\n    let mean = sum / count;\n    let mut variance = 0.0;\n    for row in surface {\n        for value in row {\n            let diff = *value - mean;\n            variance += diff * diff;\n        }\n    }\n    variance /= count;\n    Ok((mean, variance))\n}\n\nfn average_autocorrelation(\n    surface: &[Vec<f64>],\n    mean: f64,\n    variance: f64,\n    max_lag: usize,\n    dx: i32,\n    dy: i32,\n) -> Result<f64, String> {\n    let (height, width) = surface_dimensions(surface)?;\n    let mut total = 0.0;\n    let mut lag_count = 0usize;\n\n    for lag in 1..=max_lag {\n        let mut sum = 0.0;\n        let mut count = 0usize;\n        let lag = lag as i32;\n        for x in 0..height {\n            for y in 0..width {\n                let x2 = x as i32 + dx * lag;\n                let y2 = y as i32 + dy * lag;\n                if x2 < 0 || y2 < 0 {\n                    continue;\n                }\n                let x2 = x2 as usize;\n                let y2 = y2 as usize;\n                if x2 >= height || y2 >= width {\n                    continue;\n                }\n                let a = surface[x][y] - mean;\n                let b = surface[x2][y2] - mean;\n                sum += a * b;\n                count += 1;\n            }\n        }\n        if count == 0 {\n            continue;\n        }\n        let corr = sum / (count as f64 * variance);\n        total += corr.abs();\n        lag_count += 1;\n    }\n\n    if lag_count == 0 {\n        return Err(\"no valid lags for autocorrelation\".to_string());\n    }\n\n    Ok(total / lag_count as f64)\n}\n\nfn analyze_mlp_activation_wave(\n    neuron_index: usize,\n    signal: &[f64],\n    config: &MlpWaveVerificationConfig,\n) -> Result<MlpWaveNeuronReport, String> {\n    if signal.len() < 3 {\n        return Err(format!(\n            \"activation signal for neuron {} is too short\",\n            neuron_index\n        ));\n    }\n\n    let negative_count = signal\n        .iter()\n        .filter(|value| **value < -config.negative_tolerance)\n        .count();\n    let negative_fraction = negative_count as f64 / signal.len() as f64;\n    if negative_fraction > config.max_negative_fraction {\n        return Err(format!(\n            \"neuron {} has {:.3} negative activations\",\n            neuron_index, negative_fraction\n        ));\n    }\n\n    let (dominant_frequency, dominant_ratio) = dominant_frequency_ratio(signal)?;\n    if dominant_ratio < config.min_dominant_ratio {\n        return Err(format!(\n            \"neuron {} dominant frequency ratio {:.3} below threshold {:.3}\",\n            neuron_index, dominant_ratio, config.min_dominant_ratio\n        ));\n    }\n\n    let best_correlation =\n        best_rectified_sine_correlation(signal, dominant_frequency, config.phase_steps);\n    if best_correlation < config.min_correlation {\n        return Err(format!(\n            \"neuron {} rectified sine correlation {:.3} below threshold {:.3}\",\n            neuron_index, best_correlation, config.min_correlation\n        ));\n    }\n\n    Ok(MlpWaveNeuronReport {\n        neuron_index,\n        dominant_frequency,\n        dominant_ratio,\n        negative_fraction,\n        best_correlation,\n    })\n}\n\nfn dominant_frequency_ratio(signal: &[f64]) -> Result<(usize, f64), String> {\n    if signal.len() < 2 {\n        return Err(\"signal too short for FFT\".to_string());\n    }\n\n    let mut planner = FftPlanner::new();\n    let fft = planner.plan_fft_forward(signal.len());\n    let mut buffer: Vec<Complex<f64>> = signal\n        .iter()\n        .map(|&x| Complex::new(x, 0.0))\n        .collect();\n    fft.process(&mut buffer);\n    let magnitudes: Vec<f64> = buffer.iter().map(|c| c.norm()).collect();\n\n    if magnitudes.len() < 2 {\n        return Err(\"insufficient FFT bins\".to_string());\n    }\n\n    let mut dominant_idx = 1usize;\n    let mut dominant_mag = magnitudes[1];\n    for (idx, &mag) in magnitudes.iter().enumerate().skip(2) {\n        if mag > dominant_mag {\n            dominant_mag = mag;\n            dominant_idx = idx;\n        }\n    }\n\n    if dominant_idx == 0 {\n        return Err(\"dominant frequency is zero\".to_string());\n    }\n\n    let mut nonzero = magnitudes[1..].to_vec();\n    nonzero.sort_by(|a, b| a.partial_cmp(b).unwrap_or(std::cmp::Ordering::Equal));\n    let median_mag = nonzero[nonzero.len() / 2];\n    let ratio = if median_mag == 0.0 {\n        dominant_mag\n    } else {\n        dominant_mag / median_mag\n    };\n\n    Ok((dominant_idx, ratio))\n}\n\nfn best_rectified_sine_correlation(signal: &[f64], frequency: usize, phase_steps: usize) -> f64 {\n    if signal.is_empty() || frequency == 0 || phase_steps == 0 {\n        return 0.0;\n    }\n\n    let tau = std::f64::consts::PI * 2.0;\n    let mut best = -1.0;\n    for step in 0..phase_steps {\n        let phase = tau * step as f64 / phase_steps as f64;\n        let wave: Vec<f64> = (0..signal.len())\n            .map(|idx| {\n                let angle = tau * frequency as f64 * idx as f64 / signal.len() as f64 + phase;\n                angle.sin().max(0.0)\n            })\n            .collect();\n        let corr = pearson_correlation(signal, &wave);\n        if corr > best {\n            best = corr;\n        }\n    }\n\n    best\n}\n\nfn pearson_correlation(a: &[f64], b: &[f64]) -> f64 {\n    if a.len() != b.len() || a.is_empty() {\n        return 0.0;\n    }\n\n    let mean_a = a.iter().sum::<f64>() / a.len() as f64;\n    let mean_b = b.iter().sum::<f64>() / b.len() as f64;\n\n    let mut num = 0.0;\n    let mut denom_a = 0.0;\n    let mut denom_b = 0.0;\n    for (x, y) in a.iter().zip(b.iter()) {\n        let da = x - mean_a;\n        let db = y - mean_b;\n        num += da * db;\n        denom_a += da * da;\n        denom_b += db * db;\n    }\n\n    if denom_a == 0.0 || denom_b == 0.0 {\n        return 0.0;\n    }\n\n    num / (denom_a.sqrt() * denom_b.sqrt())\n}\n\n/// Verify that the model actually learned modular addition\n/// by testing it on ALL possible examples systematically\npub fn verify_full_accuracy<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n) -> (f32, f32) {\n    println!(\"ðŸ” Verifying model on COMPLETE dataset (no sampling)...\");\n\n    let train_dataset = ModularAdditionDataset::new(true, 42);\n    let val_dataset = ModularAdditionDataset::new(false, 42);\n\n    let train_acc = test_all_examples(model, &train_dataset, device);\n    let val_acc = test_all_examples(model, &val_dataset, device);\n\n    println!(\"ðŸ“Š FULL Dataset Results:\");\n    println!(\"   Training: {}/{} = {:.2}%\",\n        (train_acc * train_dataset.len() as f32) as usize,\n        train_dataset.len(),\n        train_acc * 100.0);\n    println!(\"   Validation: {}/{} = {:.2}%\",\n        (val_acc * val_dataset.len() as f32) as usize,\n        val_dataset.len(),\n        val_acc * 100.0);\n\n    (train_acc, val_acc)\n}\n\n/// Test model on ALL examples in dataset (no random sampling)\nfn test_all_examples<B: Backend>(\n    model: &Transformer<B>,\n    dataset: &ModularAdditionDataset,\n    device: &B::Device,\n) -> f32 {\n    let mut total_correct = 0;\n    let total = dataset.len();\n\n    // Test in batches to avoid memory issues\n    let batch_size = 100;\n    for batch_start in (0..total).step_by(batch_size) {\n        let batch_end = (batch_start + batch_size).min(total);\n        let batch_len = batch_end - batch_start;\n\n        let mut inputs_vec = Vec::new();\n        let mut targets_vec = Vec::new();\n\n        for idx in batch_start..batch_end {\n            let (input, target) = dataset.get(idx).unwrap();\n            inputs_vec.extend(input);\n            targets_vec.push(target as i32);\n        }\n\n        let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n            .reshape([batch_len, 3]);\n\n        let logits = model.forward(inputs);\n        let predictions = logits.argmax(1).squeeze::<1>();\n        let predictions_vec: Vec<i32> = predictions.into_data().to_vec().unwrap();\n\n        let correct = predictions_vec\n            .iter()\n            .zip(targets_vec.iter())\n            .filter(|(p, t)| p == t)\n            .count();\n\n        total_correct += correct;\n    }\n\n    total_correct as f32 / total as f32\n}\n\n/// Verify that weight norms decrease over time with weight decay\n/// and show correlation with the grokking transition\npub fn verify_weight_norm_decay(\n    weight_norm_history: &[(usize, f64)],\n    grok_step: Option<usize>,\n    config: &WeightNormDecayConfig,\n) -> Result<WeightNormDecayReport, String> {\n    if weight_norm_history.len() < 2 {\n        return Err(\"weight norm history requires at least 2 snapshots\".to_string());\n    }\n\n    let initial_norm = weight_norm_history\n        .first()\n        .ok_or_else(|| \"weight norm history is empty\".to_string())?\n        .1;\n    let final_norm = weight_norm_history\n        .last()\n        .ok_or_else(|| \"weight norm history is empty\".to_string())?\n        .1;\n\n    if initial_norm <= 0.0 {\n        return Err(\"initial weight norm must be positive\".to_string());\n    }\n\n    // Check that norms are generally decreasing (with weight decay)\n    let relative_decrease = (initial_norm - final_norm) / initial_norm;\n    if relative_decrease < config.min_relative_decrease {\n        return Err(format!(\n            \"weight norm decrease {:.4} below threshold {:.4} (initial {:.4}, final {:.4})\",\n            relative_decrease, config.min_relative_decrease, initial_norm, final_norm\n        ));\n    }\n\n    // Check that norms are not increasing (weight decay should monotonically decrease norms)\n    for window in weight_norm_history.windows(2) {\n        let prev_norm = window[0].1;\n        let next_norm = window[1].1;\n        if next_norm > prev_norm * 1.01 {\n            // Allow 1% tolerance for numerical noise\n            return Err(format!(\n                \"weight norm increased from {:.4} to {:.4} at step {}\",\n                prev_norm, next_norm, window[1].0\n            ));\n        }\n    }\n\n    // If grok step is provided, check that norms show a shift around grokking\n    let grok_step_norm = if let Some(step) = grok_step {\n        weight_norm_history\n            .iter()\n            .filter(|(s, _)| {\n                *s >= step.saturating_sub(config.grok_step_window)\n                    && *s <= step + config.grok_step_window\n            })\n            .map(|(_, norm)| *norm)\n            .next()\n    } else {\n        None\n    };\n\n    Ok(WeightNormDecayReport {\n        initial_norm,\n        final_norm,\n        relative_decrease,\n        grok_step_norm,\n    })\n}\n\n/// Verify the characteristic \"snake curve\" shape of grokking phenomenon\n/// on log-scale plots: train accuracy rises early, validation stays flat then jumps\npub fn verify_snake_curve_shape(\n    train_acc: &[(usize, f32)],\n    val_acc: &[(usize, f32)],\n    config: &SnakeCurveConfig,\n) -> Result<SnakeCurveReport, String> {\n    if train_acc.is_empty() || val_acc.is_empty() {\n        return Err(\"empty accuracy history\".to_string());\n    }\n\n    // Phase 1: Verify early train convergence (immediate drop on log scale)\n    let train_converged_step = train_acc\n        .iter()\n        .find(|(_, acc)| *acc >= config.train_convergence_threshold)\n        .map(|(step, _)| *step)\n        .ok_or_else(|| {\n            format!(\n                \"train accuracy never reaches {} threshold\",\n                config.train_convergence_threshold\n            )\n        })?;\n\n    if train_converged_step > config.early_step_max {\n        return Err(format!(\n            \"train accuracy converges at step {}, expected before {}\",\n            train_converged_step, config.early_step_max\n        ));\n    }\n\n    // Phase 2: Verify validation plateau (flat on log scale)\n    let plateau_samples: Vec<f32> = val_acc\n        .iter()\n        .filter(|(step, _)| *step >= 1 && *step <= config.val_plateau_min_step)\n        .map(|(_, acc)| *acc)\n        .collect();\n\n    if plateau_samples.is_empty() {\n        return Err(\"no validation samples in plateau region\".to_string());\n    }\n\n    let val_plateau_max = plateau_samples\n        .iter()\n        .fold(0.0f32, |max_val, &acc| max_val.max(acc));\n\n    if val_plateau_max > config.val_plateau_max_acc {\n        return Err(format!(\n            \"validation accuracy in plateau region too high: max {:.4} exceeds {:.4}\",\n            val_plateau_max, config.val_plateau_max_acc\n        ));\n    }\n\n    // Phase 3: Verify delayed validation jump (crash up on log scale)\n    let val_grok_step = val_acc\n        .iter()\n        .filter(|(step, _)| *step > config.val_plateau_min_step)\n        .find(|(_, acc)| *acc >= config.val_final_threshold)\n        .map(|(step, _)| *step)\n        .ok_or_else(|| {\n            format!(\n                \"validation accuracy never reaches {} after plateau\",\n                config.val_final_threshold\n            )\n        })?;\n\n    // Phase 4: Verify phase ordering (snake shape: early train, late val)\n    let curve_shape_valid = train_converged_step < config.val_plateau_min_step\n        && val_grok_step > config.val_plateau_min_step\n        && train_converged_step < val_grok_step;\n\n    if !curve_shape_valid {\n        return Err(format!(\n            \"invalid curve shape: train converged at {}, val plateau until {}, val grokked at {}\",\n            train_converged_step, config.val_plateau_min_step, val_grok_step\n        ));\n    }\n\n    Ok(SnakeCurveReport {\n        train_converged_by_step: train_converged_step,\n        val_plateau_max_acc: val_plateau_max,\n        val_grok_step,\n        curve_shape_valid,\n    })\n}\n\n/// Test specific examples to see if model actually computes mod p\npub fn test_specific_examples<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n) {\n    println!();\n    println!(\"ðŸ§ª Testing specific examples:\");\n\n    let modulus = ModularAdditionDataset::modulus();\n    let equals_token = ModularAdditionDataset::equals_token();\n    let test_cases = vec![\n        (0, 0, 0),\n        (1, 1, 2),\n        (50, 50, (50 + 50) % modulus),\n        (112, 1, (112 + 1) % modulus),\n        (56, 57, (56 + 57) % modulus),\n        (112, 112, (112 + 112) % modulus),\n        (10, 20, 30),\n        (60, 60, (60 + 60) % modulus),\n    ];\n\n    for (a, b, expected) in test_cases {\n        let input_vec = vec![a as i32, b as i32, equals_token as i32];\n        let input = Tensor::<B, 1, Int>::from_ints(input_vec.as_slice(), device)\n            .reshape([1, 3]);\n\n        let logits = model.forward(input);\n        let prediction = logits.argmax(1).squeeze::<1>();\n        let pred_value: i32 = prediction.into_data().to_vec().unwrap()[0];\n\n        let correct = if pred_value == expected as i32 { \"âœ“\" } else { \"âœ—\" };\n        println!(\n            \"   {} + {} mod {} = {} | Model: {} {}\",\n            a, b, modulus, expected, pred_value, correct\n        );\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    fn make_histories(\n        train_acc: &[(usize, f32)],\n        val_acc: &[(usize, f32)],\n        val_loss: &[(usize, f64)],\n    ) -> (LossHistory, AccuracyHistory) {\n        let mut loss_history = LossHistory::new();\n        loss_history.val_snapshots = val_loss.to_vec();\n        let mut accuracy_history = AccuracyHistory::new();\n        accuracy_history.train_snapshots = train_acc.to_vec();\n        accuracy_history.val_snapshots = val_acc.to_vec();\n        (loss_history, accuracy_history)\n    }\n\n    #[test]\n    fn grokking_phase_verification_passes_on_synthetic_signal() {\n        let train_acc = vec![(0, 0.2), (500, 0.995), (1000, 0.999)];\n        let val_acc = vec![\n            (0, 0.01),\n            (500, 0.01),\n            (1500, 0.01),\n            (2200, 0.995),\n            (2500, 0.999),\n        ];\n        let val_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 2.0),\n            (2200, 0.2),\n            (2500, 0.15),\n        ];\n\n        let (loss_history, accuracy_history) = make_histories(&train_acc, &val_acc, &val_loss);\n        let config = GrokkingVerificationConfig {\n            early_step_max: 1000,\n            plateau_min_step: 1500,\n            generalization_window: 1500,\n            accuracy_window: 2,\n            loss_window: 2,\n            ..GrokkingVerificationConfig::default_for_modulus(ModularAdditionDataset::modulus())\n        };\n\n        let report = verify_grokking_phase_transition(&loss_history, &accuracy_history, &config)\n            .expect(\"expected grokking verification to pass\");\n        assert!(report.transition_step >= config.plateau_min_step);\n        assert!(report.early_train_step <= config.early_step_max);\n    }\n\n    #[test]\n    fn grokking_phase_verification_fails_without_transition() {\n        let train_acc = vec![(0, 0.2), (500, 0.995)];\n        let val_acc = vec![(0, 0.01), (500, 0.01), (1500, 0.02)];\n        let val_loss = vec![(0, 2.0), (500, 2.0), (1500, 1.9)];\n\n        let (loss_history, accuracy_history) = make_histories(&train_acc, &val_acc, &val_loss);\n        let config = GrokkingVerificationConfig {\n            early_step_max: 1000,\n            plateau_min_step: 1000,\n            generalization_window: 1000,\n            accuracy_window: 2,\n            loss_window: 2,\n            ..GrokkingVerificationConfig::default_for_modulus(ModularAdditionDataset::modulus())\n        };\n\n        let err = verify_grokking_phase_transition(&loss_history, &accuracy_history, &config)\n            .expect_err(\"expected grokking verification to fail\");\n        assert!(err.contains(\"validation accuracy never reaches target\"));\n    }\n\n    #[test]\n    fn grokking_phase_verification_fails_with_slow_accuracy_rise() {\n        let train_acc = vec![(0, 0.2), (500, 0.995), (1000, 0.999)];\n        let val_acc = vec![\n            (0, 0.01),\n            (500, 0.01),\n            (1500, 0.5),\n            (2000, 0.6),\n            (2200, 0.995),\n            (2500, 0.999),\n        ];\n        let val_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 2.0),\n            (2200, 0.2),\n            (2500, 0.15),\n        ];\n\n        let (loss_history, accuracy_history) = make_histories(&train_acc, &val_acc, &val_loss);\n        let config = GrokkingVerificationConfig {\n            early_step_max: 1000,\n            plateau_min_step: 1000,\n            generalization_window: 2000,\n            accuracy_window: 2,\n            loss_window: 2,\n            ..GrokkingVerificationConfig::default_for_modulus(ModularAdditionDataset::modulus())\n        };\n\n        let err = verify_grokking_phase_transition(&loss_history, &accuracy_history, &config)\n            .expect_err(\"expected grokking verification to fail\");\n        assert!(err.contains(\"plateau too high\"));\n    }\n\n    #[test]\n    fn restricted_loss_detection_passes_with_early_drop() {\n        let full_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 2.0),\n            (2500, 2.0),\n            (3500, 0.3),\n        ];\n        let restricted_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 0.8),\n            (2500, 0.4),\n            (3500, 0.3),\n        ];\n\n        let config = RestrictedLossVerificationConfig {\n            plateau_min_step: 1000,\n            drop_window: 2,\n            drop_fraction: 0.5,\n            min_step_lead: 500,\n        };\n\n        let report = verify_restricted_loss_early_drop(&full_loss, &restricted_loss, &config)\n            .expect(\"expected restricted loss verification to pass\");\n        assert!(report.restricted_drop_step < report.full_drop_step);\n    }\n\n    #[test]\n    fn restricted_loss_detection_fails_when_full_drops_first() {\n        let full_loss = vec![(0, 2.0), (1000, 1.0), (1500, 0.8)];\n        let restricted_loss = vec![(0, 2.0), (1000, 2.0), (1500, 0.8)];\n\n        let config = RestrictedLossVerificationConfig {\n            plateau_min_step: 500,\n            drop_window: 1,\n            drop_fraction: 0.3,\n            min_step_lead: 200,\n        };\n\n        let err = verify_restricted_loss_early_drop(&full_loss, &restricted_loss, &config)\n            .expect_err(\"expected restricted loss verification to fail\");\n        assert!(err.contains(\"restricted loss drops\"));\n    }\n\n    #[test]\n    fn excluded_loss_spike_verification_passes() {\n        let excluded_loss = vec![(0, 1.0), (1500, 2.2), (3000, 1.1)];\n        let config = ExcludedLossSpikeConfig {\n            min_relative_increase: 0.5,\n        };\n\n        let report = verify_excluded_loss_spike(&excluded_loss, &config)\n            .expect(\"expected excluded loss spike verification to pass\");\n        assert_eq!(report.spike_step, 1500);\n    }\n\n    #[test]\n    fn excluded_loss_spike_verification_fails_without_spike() {\n        let excluded_loss = vec![(0, 1.0), (1500, 1.1), (3000, 1.0)];\n        let config = ExcludedLossSpikeConfig {\n            min_relative_increase: 0.5,\n        };\n\n        let err = verify_excluded_loss_spike(&excluded_loss, &config)\n            .expect_err(\"expected excluded loss spike verification to fail\");\n        assert!(err.contains(\"excluded loss spike too small\"));\n    }\n\n    #[test]\n    fn restricted_loss_drop_verification_passes() {\n        let restricted_loss = vec![\n            (0, 2.0),\n            (1000, 1.8),\n            (5000, 1.2),\n            (7000, 0.8),\n            (10000, 0.9),\n        ];\n        let config = RestrictedLossDropConfig {\n            min_relative_decrease: 0.3,\n        };\n\n        let report = verify_restricted_loss_drop(&restricted_loss, &config)\n            .expect(\"expected restricted loss drop verification to pass\");\n        assert_eq!(report.drop_step, 7000);\n        assert!(report.baseline_loss > report.drop_loss);\n        assert!((report.baseline_loss - report.drop_loss) / report.baseline_loss >= 0.3);\n    }\n\n    #[test]\n    fn restricted_loss_drop_verification_fails_without_drop() {\n        let restricted_loss = vec![\n            (0, 2.0),\n            (1000, 2.1),\n            (5000, 1.9),\n            (7000, 2.0),\n            (10000, 2.05),\n        ];\n        let config = RestrictedLossDropConfig {\n            min_relative_decrease: 0.3,\n        };\n\n        let err = verify_restricted_loss_drop(&restricted_loss, &config)\n            .expect_err(\"expected restricted loss drop verification to fail\");\n        assert!(err.contains(\"restricted loss decrease too small\"));\n    }\n\n    #[test]\n    fn mlp_activation_wave_verification_passes_on_rectified_sine() {\n        let n = ModularAdditionDataset::modulus();\n        let freq = 5.0;\n        let mut signal = Vec::with_capacity(n);\n        for idx in 0..n {\n            let angle = 2.0 * std::f64::consts::PI * freq * idx as f64 / n as f64;\n            let rectified = angle.sin().max(0.0);\n            let wobble = (2.0 * std::f64::consts::PI * 2.0 * idx as f64 / n as f64)\n                .sin()\n                .abs()\n                * 0.02;\n            signal.push(rectified + wobble);\n        }\n\n        let config = MlpWaveVerificationConfig {\n            max_negative_fraction: 0.0,\n            negative_tolerance: 1e-6,\n            min_dominant_ratio: 3.0,\n            min_correlation: 0.7,\n            phase_steps: 32,\n        };\n\n        let report =\n            verify_mlp_activation_waves_from_signals(&[(0, signal)], &config)\n                .expect(\"expected rectified sine to pass\");\n        assert_eq!(report.neuron_reports.len(), 1);\n    }\n\n    #[test]\n    fn mlp_activation_wave_verification_fails_on_noise() {\n        let n = ModularAdditionDataset::modulus();\n        let signal: Vec<f64> = (0..n).map(|idx| idx as f64 / n as f64).collect();\n\n        let config = MlpWaveVerificationConfig {\n            max_negative_fraction: 0.0,\n            negative_tolerance: 1e-6,\n            min_dominant_ratio: 4.0,\n            min_correlation: 0.8,\n            phase_steps: 16,\n        };\n\n        let err =\n            verify_mlp_activation_waves_from_signals(&[(0, signal)], &config)\n                .expect_err(\"expected linear signal to fail\");\n        assert!(err.contains(\"dominant frequency ratio\") || err.contains(\"correlation\"));\n    }\n\n    #[test]\n    fn pairwise_manifold_transition_passes_on_elliptic_signals() {\n        let n = ModularAdditionDataset::modulus();\n        let freq = 5.0;\n        let mut post_signals = Vec::new();\n        for neuron_idx in 0..7usize {\n            let phase = 2.0 * std::f64::consts::PI * neuron_idx as f64 / 7.0;\n            let signal: Vec<f64> = (0..n)\n                .map(|idx| {\n                    let angle = 2.0 * std::f64::consts::PI * freq * idx as f64 / n as f64 + phase;\n                    angle.sin()\n                })\n                .collect();\n            post_signals.push((neuron_idx, signal));\n        }\n\n        let mut pre_signals = Vec::new();\n        for neuron_idx in 0..7usize {\n            let signal: Vec<f64> = (0..n)\n                .map(|idx| idx as f64 / n as f64 + neuron_idx as f64 * 0.01)\n                .collect();\n            pre_signals.push((neuron_idx, signal));\n        }\n\n        let config = PairwiseManifoldConfig {\n            fixed_y: 0,\n            neuron_indices: (0..7).collect(),\n            min_post_score: 0.75,\n            max_pre_score: 0.55,\n            max_axis_ratio: 8.0,\n            min_pairs_passing: 8,\n        };\n\n        let report = verify_pairwise_manifold_transition_from_signals(\n            &pre_signals,\n            &post_signals,\n            &config,\n        )\n        .expect(\"expected pairwise manifold transition to pass\");\n        assert!(report.passing_pairs >= config.min_pairs_passing);\n    }\n\n    #[test]\n    fn pairwise_manifold_transition_fails_without_rings() {\n        let n = ModularAdditionDataset::modulus();\n        let mut signals = Vec::new();\n        for neuron_idx in 0..7usize {\n            let signal: Vec<f64> = (0..n)\n                .map(|idx| idx as f64 / n as f64 + neuron_idx as f64 * 0.02)\n                .collect();\n            signals.push((neuron_idx, signal));\n        }\n\n        let config = PairwiseManifoldConfig {\n            fixed_y: 0,\n            neuron_indices: (0..7).collect(),\n            min_post_score: 0.8,\n            max_pre_score: 0.55,\n            max_axis_ratio: 4.0,\n            min_pairs_passing: 5,\n        };\n\n        let err = verify_pairwise_manifold_transition_from_signals(\n            &signals,\n            &signals,\n            &config,\n        )\n        .expect_err(\"expected pairwise manifold transition to fail\");\n        assert!(err.contains(\"post-grok\") || err.contains(\"pre-grok\"));\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_passes_on_diagonal_corrugation() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create diagonal ridges: f(x,y) = sin(2Ï€ * freq * (x+y) / n)\n        let freq = 3.0;\n        for x in 0..n {\n            for y in 0..n {\n                let angle = 2.0 * std::f64::consts::PI * freq * (x + y) as f64 / n as f64;\n                surface[x][y] = angle.sin();\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let report = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect(\"expected diagonal ridge verification to pass\");\n        assert!(report.diagonal_score > report.axis_score);\n        assert!(report.ratio >= config.min_diagonal_ratio);\n        assert!(report.diagonal_score >= config.min_diagonal_score);\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_passes_on_anti_diagonal_pattern() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create anti-diagonal ridges: f(x,y) = sin(2Ï€ * freq * (x-y) / n)\n        let freq = 3.0;\n        for x in 0..n {\n            for y in 0..n {\n                let angle = 2.0 * std::f64::consts::PI * freq * (x as i32 - y as i32) as f64 / n as f64;\n                surface[x][y] = angle.sin();\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let report = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect(\"expected anti-diagonal ridge verification to pass\");\n        assert!(report.diagonal_score > report.axis_score);\n        assert!(report.ratio >= config.min_diagonal_ratio);\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_fails_on_axis_aligned_pattern() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create axis-aligned ridges: f(x,y) = sin(2Ï€ * freq * x / n)\n        let freq = 3.0;\n        for x in 0..n {\n            for y in 0..n {\n                let angle = 2.0 * std::f64::consts::PI * freq * x as f64 / n as f64;\n                surface[x][y] = angle.sin();\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let err = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect_err(\"expected axis-aligned pattern to fail diagonal verification\");\n        assert!(err.contains(\"diagonal/axis ratio\") || err.contains(\"diagonal autocorrelation\"));\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_fails_on_flat_surface() {\n        let n = 50;\n        let surface = vec![vec![1.5; n]; n];\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let err = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect_err(\"expected flat surface to fail\");\n        assert!(err.contains(\"surface variance is zero\"));\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_fails_on_random_noise() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create random noise (low autocorrelation everywhere)\n        for x in 0..n {\n            for y in 0..n {\n                surface[x][y] = ((x * 17 + y * 31) % 100) as f64 / 100.0;\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let err = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect_err(\"expected random noise to fail diagonal verification\");\n        assert!(err.contains(\"diagonal/axis ratio\") || err.contains(\"diagonal autocorrelation\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_passes_on_valid_grokking_curve() {\n        // Simulate typical grokking: train converges early, val plateaus then jumps\n        let train_acc = vec![\n            (1, 0.10),\n            (10, 0.50),\n            (100, 0.80),\n            (500, 0.98),\n            (1000, 0.99),\n            (2000, 0.995),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (10, 0.009),\n            (100, 0.01),\n            (500, 0.011),\n            (1000, 0.009),\n            (1500, 0.01),\n            (2000, 0.92),\n            (3000, 0.98),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 1500,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let report = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect(\"expected snake curve verification to pass\");\n\n        assert_eq!(report.train_converged_by_step, 500);\n        assert!(report.val_plateau_max_acc < config.val_plateau_max_acc);\n        assert_eq!(report.val_grok_step, 2000);\n        assert!(report.curve_shape_valid);\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_on_simultaneous_convergence() {\n        // Both train and val converge at the same time (no grokking)\n        let train_acc = vec![\n            (1, 0.10),\n            (100, 0.98),\n            (500, 0.99),\n        ];\n        let val_acc = vec![\n            (1, 0.10),\n            (100, 0.97),\n            (500, 0.98),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 200,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let err = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect_err(\"expected snake curve verification to fail\");\n        assert!(err.contains(\"plateau region too high\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_on_slow_train_convergence() {\n        // Train takes too long to converge (not the typical snake curve)\n        let train_acc = vec![\n            (1, 0.10),\n            (500, 0.50),\n            (1000, 0.70),\n            (2000, 0.98),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (500, 0.01),\n            (1000, 0.01),\n            (2000, 0.92),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 1500,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let err = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect_err(\"expected snake curve verification to fail\");\n        assert!(err.contains(\"converges at step 2000\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_without_val_jump() {\n        // Validation never jumps up (no generalization)\n        let train_acc = vec![\n            (1, 0.10),\n            (100, 0.98),\n            (500, 0.99),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (100, 0.01),\n            (500, 0.01),\n            (2000, 0.02),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 200,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let err = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect_err(\"expected snake curve verification to fail\");\n        assert!(err.contains(\"validation accuracy never reaches\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_on_wrong_phase_ordering() {\n        // Val jumps before train converges (backwards snake)\n        let train_acc = vec![\n            (1, 0.10),\n            (500, 0.50),\n            (2000, 0.98),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (100, 0.95),\n            (500, 0.98),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 200,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let result = verify_snake_curve_shape(&train_acc, &val_acc, &config);\n        // Should fail because val jumps before train converges (not a proper snake curve)\n        // This can fail for multiple reasons, but should definitely fail\n        assert!(result.is_err(), \"Expected snake curve verification to fail but it passed\");\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_passes_on_decreasing_norms() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 85.0),\n            (1000, 72.0),\n            (1500, 63.0),\n            (2000, 58.0),\n            (2500, 55.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let report = verify_weight_norm_decay(&weight_norms, Some(2000), &config)\n            .expect(\"expected weight norm decay verification to pass\");\n\n        assert_eq!(report.initial_norm, 100.0);\n        assert_eq!(report.final_norm, 55.0);\n        assert!((report.relative_decrease - 0.45).abs() < 0.01);\n        assert!(report.grok_step_norm.is_some());\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_fails_on_flat_norms() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 99.0),\n            (1000, 98.5),\n            (1500, 98.0),\n            (2000, 97.5),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let err = verify_weight_norm_decay(&weight_norms, None, &config)\n            .expect_err(\"expected weight norm decay verification to fail\");\n        assert!(err.contains(\"weight norm decrease\"));\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_fails_on_increasing_norms() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 95.0),\n            (1000, 105.0), // Increase here\n            (1500, 90.0),\n            (2000, 85.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let err = verify_weight_norm_decay(&weight_norms, None, &config)\n            .expect_err(\"expected weight norm decay verification to fail\");\n        assert!(err.contains(\"weight norm increased\"));\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_allows_small_noise() {\n        // Weight decay with small numerical noise (within 1% tolerance)\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 95.0),\n            (1000, 90.5),  // Small increase within tolerance\n            (1500, 90.0),\n            (2000, 85.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let report = verify_weight_norm_decay(&weight_norms, None, &config)\n            .expect(\"expected weight norm decay with small noise to pass\");\n\n        assert_eq!(report.initial_norm, 100.0);\n        assert_eq!(report.final_norm, 85.0);\n        assert!((report.relative_decrease - 0.15).abs() < 0.01);\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_finds_grok_step_norm() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (1000, 80.0),\n            (1800, 68.0),  // Within window of grok step (2200 - 500 = 1700)\n            (2000, 65.0),\n            (2200, 62.0),  // Around grok step\n            (3000, 55.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let report = verify_weight_norm_decay(&weight_norms, Some(2200), &config)\n            .expect(\"expected weight norm decay verification to pass\");\n\n        assert!(report.grok_step_norm.is_some());\n        let grok_norm = report.grok_step_norm.unwrap();\n        // Should find the first norm in the window [1700, 2700], which is 68.0 at step 1800\n        assert!((grok_norm - 68.0).abs() < 0.01);\n    }\n}\n","structuredPatch":[{"oldStart":543,"oldLines":6,"newStart":543,"newLines":8,"lines":["     verify_mlp_activation_waves_from_signals(&signals, config)"," }"," ","+/// NOTE: Verification function reserved for future integration or manual analysis","+#[allow(dead_code)]"," pub fn verify_mlp_activation_waves_from_signals(","     signals: &[(usize, Vec<f64>)],","     config: &MlpWaveVerificationConfig,"]}],"userModified":false,"replaceAll":false}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01AGZtEpRtkfQcqz4BZnZidR","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01Twj3gJy3bgCEXE2gjE9gRB","name":"Edit","input":{"replace_all":false,"file_path":"/Users/admin/MechInt/src/verify.rs","old_string":"pub fn verify_constructive_interference_surface_from_grid(","new_string":"/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_constructive_interference_surface_from_grid("}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":748,"cache_read_input_tokens":58932,"cache_creation":{"ephemeral_5m_input_tokens":748,"ephemeral_1h_input_tokens":0},"output_tokens":174,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"93a8a221-9fdd-499f-9bab-5bd96a2fa9e0"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01Twj3gJy3bgCEXE2gjE9gRB","type":"tool_result","content":"The file /Users/admin/MechInt/src/verify.rs has been updated. Here's the result of running `cat -n` on a snippet of the edited file:\n   660â†’\n   661â†’    Ok(surface)\n   662â†’}\n   663â†’\n   664â†’/// NOTE: Verification function reserved for future integration or manual analysis\n   665â†’#[allow(dead_code)]\n   666â†’pub fn verify_constructive_interference_surface_from_grid(\n   667â†’    surface: &[Vec<f64>],\n   668â†’    config: &DiagonalRidgeConfig,\n   669â†’) -> Result<DiagonalRidgeReport, String> {\n   670â†’    let (diag_main, diag_anti, axis_x, axis_y) ="}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"ac29d8a1-713e-49ac-b637-9a1d776dd02f","tool_use_result":{"filePath":"/Users/admin/MechInt/src/verify.rs","oldString":"pub fn verify_constructive_interference_surface_from_grid(","newString":"/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_constructive_interference_surface_from_grid(","originalFile":"use burn::{\n    data::dataset::Dataset,\n    tensor::{backend::Backend, Int, Tensor},\n};\nuse rustfft::{num_complex::Complex, FftPlanner};\n\nuse crate::analysis::{AccuracyHistory, LossHistory};\nuse crate::data::ModularAdditionDataset;\nuse crate::model::Transformer;\n\n#[derive(Debug, Clone)]\npub struct GrokkingVerificationConfig {\n    pub early_train_acc_threshold: f32,\n    pub early_step_max: usize,\n    pub plateau_min_step: usize,\n    pub generalization_window: usize,\n    pub chance_accuracy: f32,\n    pub chance_tolerance: f32,\n    pub target_val_acc_threshold: f32,\n    pub accuracy_window: usize,\n    pub loss_window: usize,\n    pub loss_drop_fraction: f64,\n}\n\nimpl GrokkingVerificationConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        Self {\n            early_train_acc_threshold: 0.99,\n            early_step_max: 1000,\n            plateau_min_step: 1000,\n            generalization_window: 10_000,\n            chance_accuracy: 1.0 / modulus as f32,\n            chance_tolerance: 0.02,\n            target_val_acc_threshold: 0.99,\n            accuracy_window: 5,\n            loss_window: 5,\n            loss_drop_fraction: 0.5,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct GrokkingPhaseReport {\n    pub early_train_step: usize,\n    pub transition_step: usize,\n    pub plateau_max_val_acc: f32,\n    pub loss_drop_ratio: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossVerificationConfig {\n    pub plateau_min_step: usize,\n    pub drop_window: usize,\n    pub drop_fraction: f64,\n    pub min_step_lead: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossReport {\n    pub restricted_drop_step: usize,\n    pub full_drop_step: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossDropConfig {\n    pub min_relative_decrease: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossDropReport {\n    pub drop_step: usize,\n    pub drop_loss: f64,\n    pub baseline_loss: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct ExcludedLossSpikeConfig {\n    pub min_relative_increase: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct ExcludedLossSpikeReport {\n    pub spike_step: usize,\n    pub spike_loss: f64,\n    pub baseline_loss: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct MlpWaveVerificationConfig {\n    pub max_negative_fraction: f64,\n    pub negative_tolerance: f64,\n    pub min_dominant_ratio: f64,\n    pub min_correlation: f64,\n    pub phase_steps: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct MlpWaveNeuronReport {\n    pub neuron_index: usize,\n    pub dominant_frequency: usize,\n    pub dominant_ratio: f64,\n    pub negative_fraction: f64,\n    pub best_correlation: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct MlpWaveReport {\n    pub neuron_reports: Vec<MlpWaveNeuronReport>,\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldConfig {\n    pub fixed_y: usize,\n    pub neuron_indices: Vec<usize>,\n    pub min_post_score: f64,\n    pub max_pre_score: f64,\n    pub max_axis_ratio: f64,\n    pub min_pairs_passing: usize,\n}\n\nimpl PairwiseManifoldConfig {\n    pub fn default_for_modulus(_modulus: usize) -> Self {\n        Self {\n            fixed_y: 0,\n            neuron_indices: (0..7).collect(),\n            min_post_score: 0.75,\n            max_pre_score: 0.55,\n            max_axis_ratio: 8.0,\n            min_pairs_passing: 10,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldPairReport {\n    pub neuron_a: usize,\n    pub neuron_b: usize,\n    pub circularity_score: f64,\n    pub radius_cv: f64,\n    pub axis_ratio: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldReport {\n    pub pair_reports: Vec<PairwiseManifoldPairReport>,\n    pub average_score: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldTransitionReport {\n    pub pre_average_score: f64,\n    pub post_average_score: f64,\n    pub passing_pairs: usize,\n    pub total_pairs: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct DiagonalRidgeConfig {\n    pub neuron_index: usize,\n    pub max_lag: usize,\n    pub min_diagonal_ratio: f64,\n    pub min_diagonal_score: f64,\n}\n\nimpl DiagonalRidgeConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        let max_lag = (modulus / 10).max(4).min(20);\n        Self {\n            neuron_index: 10,\n            max_lag,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct DiagonalRidgeReport {\n    pub neuron_index: usize,\n    pub diagonal_score: f64,\n    pub axis_score: f64,\n    pub ratio: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct SnakeCurveConfig {\n    pub early_step_max: usize,\n    pub train_convergence_threshold: f32,\n    pub val_plateau_min_step: usize,\n    pub val_plateau_max_acc: f32,\n    pub val_final_threshold: f32,\n}\n\nimpl SnakeCurveConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        Self {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 1000,\n            val_plateau_max_acc: 1.0 / modulus as f32 + 0.05,\n            val_final_threshold: 0.90,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct WeightNormDecayConfig {\n    pub min_relative_decrease: f64,\n    pub plateau_min_step: usize,\n    pub grok_step_window: usize,\n}\n\nimpl WeightNormDecayConfig {\n    pub fn default_for_grokking() -> Self {\n        Self {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct WeightNormDecayReport {\n    pub initial_norm: f64,\n    pub final_norm: f64,\n    pub relative_decrease: f64,\n    pub grok_step_norm: Option<f64>,\n}\n\n#[derive(Debug, Clone)]\npub struct SnakeCurveReport {\n    pub train_converged_by_step: usize,\n    pub val_plateau_max_acc: f32,\n    pub val_grok_step: usize,\n    pub curve_shape_valid: bool,\n}\n\npub fn verify_grokking_phase_transition(\n    loss_history: &LossHistory,\n    accuracy_history: &AccuracyHistory,\n    config: &GrokkingVerificationConfig,\n) -> Result<GrokkingPhaseReport, String> {\n    let (early_train_step, _) = accuracy_history\n        .train_snapshots\n        .iter()\n        .find(|(_, acc)| *acc >= config.early_train_acc_threshold)\n        .ok_or_else(|| \"train accuracy never reaches threshold\".to_string())?;\n\n    if *early_train_step > config.early_step_max {\n        return Err(format!(\n            \"train accuracy reaches {:.2} after step {}, expected before {}\",\n            config.early_train_acc_threshold, early_train_step, config.early_step_max\n        ));\n    }\n\n    let plateau_max_val_acc = accuracy_history\n        .val_snapshots\n        .iter()\n        .filter(|(step, _)| *step <= config.plateau_min_step)\n        .map(|(_, acc)| *acc)\n        .fold(None::<f32>, |accum, value| {\n            Some(accum.map_or(value, |max_val| max_val.max(value)))\n        })\n        .ok_or_else(|| \"no validation accuracy samples before plateau_min_step\".to_string())?;\n\n    if plateau_max_val_acc > config.chance_accuracy + config.chance_tolerance {\n        return Err(format!(\n            \"validation accuracy before step {} exceeds chance (max {:.4})\",\n            config.plateau_min_step, plateau_max_val_acc\n        ));\n    }\n\n    let (transition_step, _) = accuracy_history\n        .val_snapshots\n        .iter()\n        .find(|(step, acc)| {\n            *step >= config.plateau_min_step && *acc >= config.target_val_acc_threshold\n        })\n        .ok_or_else(|| \"validation accuracy never reaches target threshold\".to_string())?;\n\n    let latest_allowed = config.plateau_min_step + config.generalization_window;\n    if *transition_step > latest_allowed {\n        return Err(format!(\n            \"validation accuracy reaches {:.2} at step {}, expected before {}\",\n            config.target_val_acc_threshold, transition_step, latest_allowed\n        ));\n    }\n\n    verify_accuracy_jump(\n        &accuracy_history.val_snapshots,\n        *transition_step,\n        config.accuracy_window,\n        config.chance_accuracy,\n        config.chance_tolerance,\n        config.target_val_acc_threshold,\n    )?;\n\n    let loss_drop_ratio = verify_loss_drop(\n        &loss_history.val_snapshots,\n        *transition_step,\n        config.loss_window,\n        config.loss_drop_fraction,\n    )?;\n\n    Ok(GrokkingPhaseReport {\n        early_train_step: *early_train_step,\n        transition_step: *transition_step,\n        plateau_max_val_acc,\n        loss_drop_ratio,\n    })\n}\n\nfn verify_accuracy_jump(\n    val_accuracy: &[(usize, f32)],\n    transition_step: usize,\n    window: usize,\n    chance_accuracy: f32,\n    chance_tolerance: f32,\n    target_val_acc_threshold: f32,\n) -> Result<(), String> {\n    let before_values: Vec<f32> = val_accuracy\n        .iter()\n        .filter(|(step, _)| *step < transition_step)\n        .map(|(_, acc)| *acc)\n        .collect();\n\n    let after_values: Vec<f32> = val_accuracy\n        .iter()\n        .filter(|(step, _)| *step >= transition_step)\n        .map(|(_, acc)| *acc)\n        .collect();\n\n    if before_values.len() < window || after_values.len() < window {\n        return Err(\"insufficient accuracy samples around transition\".to_string());\n    }\n\n    let before_avg: f32 =\n        before_values[before_values.len() - window..].iter().sum::<f32>() / window as f32;\n    let after_avg: f32 = after_values[..window].iter().sum::<f32>() / window as f32;\n\n    if before_avg > chance_accuracy + chance_tolerance {\n        return Err(format!(\n            \"validation accuracy plateau too high before transition (avg {:.4})\",\n            before_avg\n        ));\n    }\n\n    if after_avg < target_val_acc_threshold {\n        return Err(format!(\n            \"validation accuracy after transition too low (avg {:.4})\",\n            after_avg\n        ));\n    }\n\n    Ok(())\n}\n\nfn verify_loss_drop(\n    val_losses: &[(usize, f64)],\n    transition_step: usize,\n    loss_window: usize,\n    loss_drop_fraction: f64,\n) -> Result<f64, String> {\n    let before_losses: Vec<f64> = val_losses\n        .iter()\n        .filter(|(step, _)| *step < transition_step)\n        .map(|(_, loss)| *loss)\n        .collect();\n\n    let after_losses: Vec<f64> = val_losses\n        .iter()\n        .filter(|(step, _)| *step >= transition_step)\n        .map(|(_, loss)| *loss)\n        .collect();\n\n    if before_losses.len() < loss_window || after_losses.len() < loss_window {\n        return Err(\"insufficient loss samples around transition\".to_string());\n    }\n\n    let before_avg: f64 =\n        before_losses[before_losses.len() - loss_window..].iter().sum::<f64>()\n            / loss_window as f64;\n    let after_avg: f64 = after_losses[..loss_window].iter().sum::<f64>() / loss_window as f64;\n\n    if after_avg > before_avg * (1.0 - loss_drop_fraction) {\n        return Err(format!(\n            \"validation loss drop too small (before {:.4}, after {:.4})\",\n            before_avg, after_avg\n        ));\n    }\n\n    Ok(after_avg / before_avg)\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_restricted_loss_early_drop(\n    full_losses: &[(usize, f64)],\n    restricted_losses: &[(usize, f64)],\n    config: &RestrictedLossVerificationConfig,\n) -> Result<RestrictedLossReport, String> {\n    let restricted_baseline =\n        baseline_loss(restricted_losses, config.plateau_min_step, config.drop_window)?;\n    let full_baseline = baseline_loss(full_losses, config.plateau_min_step, config.drop_window)?;\n\n    let restricted_drop_step = find_drop_step(\n        restricted_losses,\n        config.plateau_min_step,\n        restricted_baseline,\n        config.drop_fraction,\n    )?;\n    let full_drop_step = find_drop_step(\n        full_losses,\n        config.plateau_min_step,\n        full_baseline,\n        config.drop_fraction,\n    )?;\n\n    if restricted_drop_step + config.min_step_lead > full_drop_step {\n        return Err(format!(\n            \"restricted loss drops at step {}, expected at least {} steps before full loss (step {})\",\n            restricted_drop_step, config.min_step_lead, full_drop_step\n        ));\n    }\n\n    Ok(RestrictedLossReport {\n        restricted_drop_step,\n        full_drop_step,\n    })\n}\n\npub fn verify_restricted_loss_drop(\n    restricted_losses: &[(usize, f64)],\n    config: &RestrictedLossDropConfig,\n) -> Result<RestrictedLossDropReport, String> {\n    if restricted_losses.len() < 3 {\n        return Err(\"restricted loss history requires at least 3 points\".to_string());\n    }\n\n    let first = restricted_losses.first().ok_or_else(|| {\n        \"restricted loss history is empty\".to_string()\n    })?;\n    let baseline_loss = first.1;\n\n    let (min_idx, (min_step, min_loss)) = restricted_losses\n        .iter()\n        .enumerate()\n        .min_by(|(_, a), (_, b)| {\n            a.1.partial_cmp(&b.1).unwrap_or(std::cmp::Ordering::Equal)\n        })\n        .ok_or_else(|| \"restricted loss history is empty\".to_string())?;\n\n    if min_idx == 0 {\n        return Err(\"restricted loss minimum occurs at start (no drop detected)\".to_string());\n    }\n\n    let relative_decrease = (baseline_loss - min_loss) / baseline_loss;\n    if relative_decrease < config.min_relative_decrease {\n        return Err(format!(\n            \"restricted loss decrease too small (baseline {:.4}, min {:.4}, decrease {:.1}%)\",\n            baseline_loss, min_loss, relative_decrease * 100.0\n        ));\n    }\n\n    Ok(RestrictedLossDropReport {\n        drop_step: *min_step,\n        drop_loss: *min_loss,\n        baseline_loss,\n    })\n}\n\npub fn verify_excluded_loss_spike(\n    excluded_losses: &[(usize, f64)],\n    config: &ExcludedLossSpikeConfig,\n) -> Result<ExcludedLossSpikeReport, String> {\n    if excluded_losses.len() < 3 {\n        return Err(\"excluded loss history requires at least 3 points\".to_string());\n    }\n\n    let first = excluded_losses.first().ok_or_else(|| {\n        \"excluded loss history is empty\".to_string()\n    })?;\n    let last = excluded_losses.last().ok_or_else(|| {\n        \"excluded loss history is empty\".to_string()\n    })?;\n    let baseline_loss = (first.1 + last.1) / 2.0;\n\n    let (peak_idx, (peak_step, peak_loss)) = excluded_losses\n        .iter()\n        .enumerate()\n        .max_by(|(_, a), (_, b)| {\n            a.1.partial_cmp(&b.1).unwrap_or(std::cmp::Ordering::Equal)\n        })\n        .ok_or_else(|| \"excluded loss history is empty\".to_string())?;\n\n    if peak_idx == 0 || peak_idx + 1 == excluded_losses.len() {\n        return Err(\"excluded loss spike occurs at boundary\".to_string());\n    }\n\n    if peak_loss < &(baseline_loss * (1.0 + config.min_relative_increase)) {\n        return Err(format!(\n            \"excluded loss spike too small (baseline {:.4}, peak {:.4})\",\n            baseline_loss, peak_loss\n        ));\n    }\n\n    Ok(ExcludedLossSpikeReport {\n        spike_step: *peak_step,\n        spike_loss: *peak_loss,\n        baseline_loss,\n    })\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_mlp_activation_waves<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n    fixed_y: usize,\n    neuron_indices: &[usize],\n    config: &MlpWaveVerificationConfig,\n) -> Result<MlpWaveReport, String> {\n    let activations = collect_mlp_post_relu_activations(model, device, fixed_y)?;\n    if neuron_indices.is_empty() {\n        return Err(\"neuron indices list is empty\".to_string());\n    }\n\n    let d_ff = activations.len();\n    for &idx in neuron_indices {\n        if idx >= d_ff {\n            return Err(format!(\n                \"neuron index {} out of range (d_ff={})\",\n                idx, d_ff\n            ));\n        }\n    }\n\n    let signals: Vec<(usize, Vec<f64>)> = neuron_indices\n        .iter()\n        .map(|&idx| (idx, activations[idx].clone()))\n        .collect();\n    verify_mlp_activation_waves_from_signals(&signals, config)\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_mlp_activation_waves_from_signals(\n    signals: &[(usize, Vec<f64>)],\n    config: &MlpWaveVerificationConfig,\n) -> Result<MlpWaveReport, String> {\n    if signals.is_empty() {\n        return Err(\"no activation signals provided\".to_string());\n    }\n\n    let mut reports = Vec::with_capacity(signals.len());\n    for (neuron_index, signal) in signals {\n        let report = analyze_mlp_activation_wave(*neuron_index, signal, config)?;\n        reports.push(report);\n    }\n\n    Ok(MlpWaveReport {\n        neuron_reports: reports,\n    })\n}\n\npub fn verify_mlp_pairwise_manifold_transition<B: Backend>(\n    pre_model: &Transformer<B>,\n    post_model: &Transformer<B>,\n    device: &B::Device,\n    config: &PairwiseManifoldConfig,\n) -> Result<PairwiseManifoldTransitionReport, String> {\n    let modulus = ModularAdditionDataset::modulus();\n    if config.fixed_y >= modulus {\n        return Err(format!(\n            \"fixed_y {} out of range (modulus {})\",\n            config.fixed_y, modulus\n        ));\n    }\n\n    let pre_activations = collect_mlp_post_relu_activations(pre_model, device, config.fixed_y)?;\n    let post_activations = collect_mlp_post_relu_activations(post_model, device, config.fixed_y)?;\n\n    let d_ff = pre_activations.len();\n    if post_activations.len() != d_ff {\n        return Err(\"post activations shape mismatch\".to_string());\n    }\n\n    if config.neuron_indices.len() < 2 {\n        return Err(\"need at least two neuron indices\".to_string());\n    }\n\n    for &idx in &config.neuron_indices {\n        if idx >= d_ff {\n            return Err(format!(\n                \"neuron index {} out of range (d_ff={})\",\n                idx, d_ff\n            ));\n        }\n    }\n\n    let pre_signals: Vec<(usize, Vec<f64>)> = config\n        .neuron_indices\n        .iter()\n        .map(|&idx| (idx, pre_activations[idx].clone()))\n        .collect();\n    let post_signals: Vec<(usize, Vec<f64>)> = config\n        .neuron_indices\n        .iter()\n        .map(|&idx| (idx, post_activations[idx].clone()))\n        .collect();\n\n    verify_pairwise_manifold_transition_from_signals(&pre_signals, &post_signals, config)\n}\n\npub fn collect_mlp_activation_surface<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n    neuron_index: usize,\n) -> Result<Vec<Vec<f64>>, String> {\n    let modulus = ModularAdditionDataset::modulus();\n    let equals_token = ModularAdditionDataset::equals_token();\n\n    let mut inputs_vec = Vec::with_capacity(modulus * modulus * 3);\n    for x in 0..modulus {\n        for y in 0..modulus {\n            inputs_vec.push(x as i32);\n            inputs_vec.push(y as i32);\n            inputs_vec.push(equals_token as i32);\n        }\n    }\n\n    let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n        .reshape([modulus * modulus, 3]);\n    let (_, mlp_acts) = model.forward_with_mlp_activations(inputs);\n    let [batch_size, d_ff] = mlp_acts.dims();\n    if batch_size != modulus * modulus {\n        return Err(format!(\n            \"unexpected activation batch size {} (expected {})\",\n            batch_size,\n            modulus * modulus\n        ));\n    }\n    if neuron_index >= d_ff {\n        return Err(format!(\n            \"neuron index {} out of range (d_ff={})\",\n            neuron_index, d_ff\n        ));\n    }\n\n    let data: Vec<f32> = mlp_acts.into_data().to_vec().unwrap();\n    let mut surface = vec![vec![0.0f64; modulus]; modulus];\n    for x in 0..modulus {\n        for y in 0..modulus {\n            let batch = x * modulus + y;\n            let idx = batch * d_ff + neuron_index;\n            surface[x][y] = data[idx] as f64;\n        }\n    }\n\n    Ok(surface)\n}\n\npub fn verify_constructive_interference_surface_from_grid(\n    surface: &[Vec<f64>],\n    config: &DiagonalRidgeConfig,\n) -> Result<DiagonalRidgeReport, String> {\n    let (diag_main, diag_anti, axis_x, axis_y) =\n        surface_autocorrelation_scores(surface, config.max_lag)?;\n\n    let diagonal_score = diag_main.max(diag_anti);\n    let axis_score = axis_x.max(axis_y);\n    let ratio = if axis_score > 0.0 {\n        diagonal_score / axis_score\n    } else {\n        f64::INFINITY\n    };\n\n    if diagonal_score < config.min_diagonal_score {\n        return Err(format!(\n            \"diagonal autocorrelation {:.3} below threshold {:.3}\",\n            diagonal_score, config.min_diagonal_score\n        ));\n    }\n\n    if ratio < config.min_diagonal_ratio {\n        return Err(format!(\n            \"diagonal/axis ratio {:.3} below threshold {:.3}\",\n            ratio, config.min_diagonal_ratio\n        ));\n    }\n\n    Ok(DiagonalRidgeReport {\n        neuron_index: config.neuron_index,\n        diagonal_score,\n        axis_score,\n        ratio,\n    })\n}\n\npub fn verify_pairwise_manifold_transition_from_signals(\n    pre_signals: &[(usize, Vec<f64>)],\n    post_signals: &[(usize, Vec<f64>)],\n    config: &PairwiseManifoldConfig,\n) -> Result<PairwiseManifoldTransitionReport, String> {\n    let pre_report = evaluate_pairwise_manifolds_from_signals(pre_signals)?;\n    let post_report = evaluate_pairwise_manifolds_from_signals(post_signals)?;\n\n    if pre_report.average_score > config.max_pre_score {\n        return Err(format!(\n            \"pre-grok average circularity {:.3} exceeds threshold {:.3}\",\n            pre_report.average_score, config.max_pre_score\n        ));\n    }\n\n    let passing_pairs = post_report\n        .pair_reports\n        .iter()\n        .filter(|pair| {\n            pair.circularity_score >= config.min_post_score\n                && pair.axis_ratio <= config.max_axis_ratio\n        })\n        .count();\n    if passing_pairs < config.min_pairs_passing {\n        return Err(format!(\n            \"post-grok only {} pairs meet circularity threshold (need {})\",\n            passing_pairs, config.min_pairs_passing\n        ));\n    }\n\n    Ok(PairwiseManifoldTransitionReport {\n        pre_average_score: pre_report.average_score,\n        post_average_score: post_report.average_score,\n        passing_pairs,\n        total_pairs: post_report.pair_reports.len(),\n    })\n}\n\nfn evaluate_pairwise_manifolds_from_signals(\n    signals: &[(usize, Vec<f64>)],\n) -> Result<PairwiseManifoldReport, String> {\n    if signals.len() < 2 {\n        return Err(\"need at least two activation signals\".to_string());\n    }\n\n    let signal_len = signals[0].1.len();\n    if signal_len < 3 {\n        return Err(\"activation signal too short\".to_string());\n    }\n\n    for (idx, signal) in signals {\n        if signal.len() != signal_len {\n            return Err(format!(\n                \"signal length mismatch for neuron {}\",\n                idx\n            ));\n        }\n    }\n\n    let mut reports = Vec::new();\n    for i in 0..signals.len() {\n        for j in (i + 1)..signals.len() {\n            let (neuron_a, signal_a) = &signals[i];\n            let (neuron_b, signal_b) = &signals[j];\n            let points: Vec<(f64, f64)> = signal_a\n                .iter()\n                .zip(signal_b.iter())\n                .map(|(x, y)| (*x, *y))\n                .collect();\n            let (score, radius_cv, axis_ratio) =\n                match pairwise_circularity_metrics(&points) {\n                    Ok(metrics) => metrics,\n                    Err(_) => (0.0, 1.0, f64::INFINITY),\n                };\n            reports.push(PairwiseManifoldPairReport {\n                neuron_a: *neuron_a,\n                neuron_b: *neuron_b,\n                circularity_score: score,\n                radius_cv,\n                axis_ratio,\n            });\n        }\n    }\n\n    let average_score = reports\n        .iter()\n        .map(|pair| pair.circularity_score)\n        .sum::<f64>()\n        / reports.len() as f64;\n\n    Ok(PairwiseManifoldReport {\n        pair_reports: reports,\n        average_score,\n    })\n}\n\nfn pairwise_circularity_metrics(points: &[(f64, f64)]) -> Result<(f64, f64, f64), String> {\n    if points.len() < 3 {\n        return Err(\"not enough points for circularity\".to_string());\n    }\n\n    let n = points.len() as f64;\n    let mean_x = points.iter().map(|(x, _)| *x).sum::<f64>() / n;\n    let mean_y = points.iter().map(|(_, y)| *y).sum::<f64>() / n;\n\n    let mut var_x = 0.0;\n    let mut var_y = 0.0;\n    let mut cov_xy = 0.0;\n    for (x, y) in points {\n        let dx = x - mean_x;\n        let dy = y - mean_y;\n        var_x += dx * dx;\n        var_y += dy * dy;\n        cov_xy += dx * dy;\n    }\n    var_x /= n;\n    var_y /= n;\n    cov_xy /= n;\n\n    let trace = var_x + var_y;\n    let det = var_x * var_y - cov_xy * cov_xy;\n    if det <= 1.0e-12 || trace <= 0.0 {\n        return Err(\"degenerate covariance\".to_string());\n    }\n\n    let half_trace = trace * 0.5;\n    let disc = (half_trace * half_trace - det).max(0.0).sqrt();\n    let lambda1 = half_trace + disc;\n    let lambda2 = half_trace - disc;\n    if lambda2 <= 0.0 {\n        return Err(\"degenerate eigenvalues\".to_string());\n    }\n\n    let axis_ratio = (lambda1 / lambda2).sqrt();\n\n    let mut v1_x = cov_xy;\n    let mut v1_y = lambda1 - var_x;\n    if v1_x.abs() < 1.0e-12 && v1_y.abs() < 1.0e-12 {\n        v1_x = 1.0;\n        v1_y = 0.0;\n    }\n    let norm = (v1_x * v1_x + v1_y * v1_y).sqrt();\n    v1_x /= norm;\n    v1_y /= norm;\n    let v2_x = -v1_y;\n    let v2_y = v1_x;\n\n    let mut radii = Vec::with_capacity(points.len());\n    for (x, y) in points {\n        let dx = x - mean_x;\n        let dy = y - mean_y;\n        let p1 = dx * v1_x + dy * v1_y;\n        let p2 = dx * v2_x + dy * v2_y;\n        let w1 = p1 / lambda1.sqrt();\n        let w2 = p2 / lambda2.sqrt();\n        radii.push((w1 * w1 + w2 * w2).sqrt());\n    }\n\n    let mean_radius = radii.iter().sum::<f64>() / radii.len() as f64;\n    if mean_radius == 0.0 {\n        return Err(\"zero mean radius\".to_string());\n    }\n\n    let mut variance = 0.0;\n    for r in &radii {\n        let diff = r - mean_radius;\n        variance += diff * diff;\n    }\n    let std = (variance / radii.len() as f64).sqrt();\n    let radius_cv = std / mean_radius;\n    let score = 1.0 / (1.0 + radius_cv);\n\n    Ok((score, radius_cv, axis_ratio))\n}\n\nfn baseline_loss(\n    losses: &[(usize, f64)],\n    plateau_min_step: usize,\n    drop_window: usize,\n) -> Result<f64, String> {\n    let before: Vec<f64> = losses\n        .iter()\n        .filter(|(step, _)| *step <= plateau_min_step)\n        .map(|(_, loss)| *loss)\n        .collect();\n\n    if before.len() < drop_window || drop_window == 0 {\n        return Err(\"insufficient loss samples for baseline\".to_string());\n    }\n\n    let window = &before[before.len() - drop_window..];\n    Ok(window.iter().sum::<f64>() / drop_window as f64)\n}\n\nfn find_drop_step(\n    losses: &[(usize, f64)],\n    plateau_min_step: usize,\n    baseline: f64,\n    drop_fraction: f64,\n) -> Result<usize, String> {\n    losses\n        .iter()\n        .filter(|(step, _)| *step >= plateau_min_step)\n        .find(|(_, loss)| *loss <= baseline * (1.0 - drop_fraction))\n        .map(|(step, _)| *step)\n        .ok_or_else(|| \"loss never drops below threshold\".to_string())\n}\n\nfn collect_mlp_post_relu_activations<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n    fixed_y: usize,\n) -> Result<Vec<Vec<f64>>, String> {\n    let modulus = ModularAdditionDataset::modulus();\n    let equals_token = ModularAdditionDataset::equals_token();\n\n    let mut inputs_vec = Vec::with_capacity(modulus * 3);\n    for x in 0..modulus {\n        inputs_vec.push(x as i32);\n        inputs_vec.push(fixed_y as i32);\n        inputs_vec.push(equals_token as i32);\n    }\n\n    let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n        .reshape([modulus, 3]);\n    let (_, mlp_acts) = model.forward_with_mlp_activations(inputs);\n    let [batch_size, d_ff] = mlp_acts.dims();\n\n    if batch_size != modulus {\n        return Err(format!(\n            \"unexpected activation batch size {} (expected {})\",\n            batch_size, modulus\n        ));\n    }\n\n    let data: Vec<f32> = mlp_acts.into_data().to_vec().unwrap();\n    let mut per_neuron = vec![Vec::with_capacity(batch_size); d_ff];\n    for batch in 0..batch_size {\n        for neuron in 0..d_ff {\n            let idx = batch * d_ff + neuron;\n            per_neuron[neuron].push(data[idx] as f64);\n        }\n    }\n\n    Ok(per_neuron)\n}\n\nfn surface_autocorrelation_scores(\n    surface: &[Vec<f64>],\n    max_lag: usize,\n) -> Result<(f64, f64, f64, f64), String> {\n    let (height, width) = surface_dimensions(surface)?;\n    if height < 2 || width < 2 {\n        return Err(\"surface grid too small\".to_string());\n    }\n\n    let (mean, var) = surface_mean_variance(surface)?;\n    if var == 0.0 {\n        return Err(\"surface variance is zero\".to_string());\n    }\n\n    let max_lag = max_lag.min(height.saturating_sub(1)).min(width.saturating_sub(1));\n    if max_lag == 0 {\n        return Err(\"max_lag too small for surface\".to_string());\n    }\n\n    let diag_main = average_autocorrelation(surface, mean, var, max_lag, 1, 1)?;\n    let diag_anti = average_autocorrelation(surface, mean, var, max_lag, 1, -1)?;\n    let axis_x = average_autocorrelation(surface, mean, var, max_lag, 1, 0)?;\n    let axis_y = average_autocorrelation(surface, mean, var, max_lag, 0, 1)?;\n\n    Ok((diag_main, diag_anti, axis_x, axis_y))\n}\n\nfn surface_dimensions(surface: &[Vec<f64>]) -> Result<(usize, usize), String> {\n    let height = surface.len();\n    if height == 0 {\n        return Err(\"surface is empty\".to_string());\n    }\n    let width = surface[0].len();\n    if width == 0 {\n        return Err(\"surface has empty rows\".to_string());\n    }\n    for (idx, row) in surface.iter().enumerate() {\n        if row.len() != width {\n            return Err(format!(\n                \"surface row {} has length {}, expected {}\",\n                idx,\n                row.len(),\n                width\n            ));\n        }\n    }\n    Ok((height, width))\n}\n\nfn surface_mean_variance(surface: &[Vec<f64>]) -> Result<(f64, f64), String> {\n    let (height, width) = surface_dimensions(surface)?;\n    let mut sum = 0.0;\n    for row in surface {\n        for value in row {\n            sum += *value;\n        }\n    }\n    let count = (height * width) as f64;\n    let mean = sum / count;\n    let mut variance = 0.0;\n    for row in surface {\n        for value in row {\n            let diff = *value - mean;\n            variance += diff * diff;\n        }\n    }\n    variance /= count;\n    Ok((mean, variance))\n}\n\nfn average_autocorrelation(\n    surface: &[Vec<f64>],\n    mean: f64,\n    variance: f64,\n    max_lag: usize,\n    dx: i32,\n    dy: i32,\n) -> Result<f64, String> {\n    let (height, width) = surface_dimensions(surface)?;\n    let mut total = 0.0;\n    let mut lag_count = 0usize;\n\n    for lag in 1..=max_lag {\n        let mut sum = 0.0;\n        let mut count = 0usize;\n        let lag = lag as i32;\n        for x in 0..height {\n            for y in 0..width {\n                let x2 = x as i32 + dx * lag;\n                let y2 = y as i32 + dy * lag;\n                if x2 < 0 || y2 < 0 {\n                    continue;\n                }\n                let x2 = x2 as usize;\n                let y2 = y2 as usize;\n                if x2 >= height || y2 >= width {\n                    continue;\n                }\n                let a = surface[x][y] - mean;\n                let b = surface[x2][y2] - mean;\n                sum += a * b;\n                count += 1;\n            }\n        }\n        if count == 0 {\n            continue;\n        }\n        let corr = sum / (count as f64 * variance);\n        total += corr.abs();\n        lag_count += 1;\n    }\n\n    if lag_count == 0 {\n        return Err(\"no valid lags for autocorrelation\".to_string());\n    }\n\n    Ok(total / lag_count as f64)\n}\n\nfn analyze_mlp_activation_wave(\n    neuron_index: usize,\n    signal: &[f64],\n    config: &MlpWaveVerificationConfig,\n) -> Result<MlpWaveNeuronReport, String> {\n    if signal.len() < 3 {\n        return Err(format!(\n            \"activation signal for neuron {} is too short\",\n            neuron_index\n        ));\n    }\n\n    let negative_count = signal\n        .iter()\n        .filter(|value| **value < -config.negative_tolerance)\n        .count();\n    let negative_fraction = negative_count as f64 / signal.len() as f64;\n    if negative_fraction > config.max_negative_fraction {\n        return Err(format!(\n            \"neuron {} has {:.3} negative activations\",\n            neuron_index, negative_fraction\n        ));\n    }\n\n    let (dominant_frequency, dominant_ratio) = dominant_frequency_ratio(signal)?;\n    if dominant_ratio < config.min_dominant_ratio {\n        return Err(format!(\n            \"neuron {} dominant frequency ratio {:.3} below threshold {:.3}\",\n            neuron_index, dominant_ratio, config.min_dominant_ratio\n        ));\n    }\n\n    let best_correlation =\n        best_rectified_sine_correlation(signal, dominant_frequency, config.phase_steps);\n    if best_correlation < config.min_correlation {\n        return Err(format!(\n            \"neuron {} rectified sine correlation {:.3} below threshold {:.3}\",\n            neuron_index, best_correlation, config.min_correlation\n        ));\n    }\n\n    Ok(MlpWaveNeuronReport {\n        neuron_index,\n        dominant_frequency,\n        dominant_ratio,\n        negative_fraction,\n        best_correlation,\n    })\n}\n\nfn dominant_frequency_ratio(signal: &[f64]) -> Result<(usize, f64), String> {\n    if signal.len() < 2 {\n        return Err(\"signal too short for FFT\".to_string());\n    }\n\n    let mut planner = FftPlanner::new();\n    let fft = planner.plan_fft_forward(signal.len());\n    let mut buffer: Vec<Complex<f64>> = signal\n        .iter()\n        .map(|&x| Complex::new(x, 0.0))\n        .collect();\n    fft.process(&mut buffer);\n    let magnitudes: Vec<f64> = buffer.iter().map(|c| c.norm()).collect();\n\n    if magnitudes.len() < 2 {\n        return Err(\"insufficient FFT bins\".to_string());\n    }\n\n    let mut dominant_idx = 1usize;\n    let mut dominant_mag = magnitudes[1];\n    for (idx, &mag) in magnitudes.iter().enumerate().skip(2) {\n        if mag > dominant_mag {\n            dominant_mag = mag;\n            dominant_idx = idx;\n        }\n    }\n\n    if dominant_idx == 0 {\n        return Err(\"dominant frequency is zero\".to_string());\n    }\n\n    let mut nonzero = magnitudes[1..].to_vec();\n    nonzero.sort_by(|a, b| a.partial_cmp(b).unwrap_or(std::cmp::Ordering::Equal));\n    let median_mag = nonzero[nonzero.len() / 2];\n    let ratio = if median_mag == 0.0 {\n        dominant_mag\n    } else {\n        dominant_mag / median_mag\n    };\n\n    Ok((dominant_idx, ratio))\n}\n\nfn best_rectified_sine_correlation(signal: &[f64], frequency: usize, phase_steps: usize) -> f64 {\n    if signal.is_empty() || frequency == 0 || phase_steps == 0 {\n        return 0.0;\n    }\n\n    let tau = std::f64::consts::PI * 2.0;\n    let mut best = -1.0;\n    for step in 0..phase_steps {\n        let phase = tau * step as f64 / phase_steps as f64;\n        let wave: Vec<f64> = (0..signal.len())\n            .map(|idx| {\n                let angle = tau * frequency as f64 * idx as f64 / signal.len() as f64 + phase;\n                angle.sin().max(0.0)\n            })\n            .collect();\n        let corr = pearson_correlation(signal, &wave);\n        if corr > best {\n            best = corr;\n        }\n    }\n\n    best\n}\n\nfn pearson_correlation(a: &[f64], b: &[f64]) -> f64 {\n    if a.len() != b.len() || a.is_empty() {\n        return 0.0;\n    }\n\n    let mean_a = a.iter().sum::<f64>() / a.len() as f64;\n    let mean_b = b.iter().sum::<f64>() / b.len() as f64;\n\n    let mut num = 0.0;\n    let mut denom_a = 0.0;\n    let mut denom_b = 0.0;\n    for (x, y) in a.iter().zip(b.iter()) {\n        let da = x - mean_a;\n        let db = y - mean_b;\n        num += da * db;\n        denom_a += da * da;\n        denom_b += db * db;\n    }\n\n    if denom_a == 0.0 || denom_b == 0.0 {\n        return 0.0;\n    }\n\n    num / (denom_a.sqrt() * denom_b.sqrt())\n}\n\n/// Verify that the model actually learned modular addition\n/// by testing it on ALL possible examples systematically\npub fn verify_full_accuracy<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n) -> (f32, f32) {\n    println!(\"ðŸ” Verifying model on COMPLETE dataset (no sampling)...\");\n\n    let train_dataset = ModularAdditionDataset::new(true, 42);\n    let val_dataset = ModularAdditionDataset::new(false, 42);\n\n    let train_acc = test_all_examples(model, &train_dataset, device);\n    let val_acc = test_all_examples(model, &val_dataset, device);\n\n    println!(\"ðŸ“Š FULL Dataset Results:\");\n    println!(\"   Training: {}/{} = {:.2}%\",\n        (train_acc * train_dataset.len() as f32) as usize,\n        train_dataset.len(),\n        train_acc * 100.0);\n    println!(\"   Validation: {}/{} = {:.2}%\",\n        (val_acc * val_dataset.len() as f32) as usize,\n        val_dataset.len(),\n        val_acc * 100.0);\n\n    (train_acc, val_acc)\n}\n\n/// Test model on ALL examples in dataset (no random sampling)\nfn test_all_examples<B: Backend>(\n    model: &Transformer<B>,\n    dataset: &ModularAdditionDataset,\n    device: &B::Device,\n) -> f32 {\n    let mut total_correct = 0;\n    let total = dataset.len();\n\n    // Test in batches to avoid memory issues\n    let batch_size = 100;\n    for batch_start in (0..total).step_by(batch_size) {\n        let batch_end = (batch_start + batch_size).min(total);\n        let batch_len = batch_end - batch_start;\n\n        let mut inputs_vec = Vec::new();\n        let mut targets_vec = Vec::new();\n\n        for idx in batch_start..batch_end {\n            let (input, target) = dataset.get(idx).unwrap();\n            inputs_vec.extend(input);\n            targets_vec.push(target as i32);\n        }\n\n        let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n            .reshape([batch_len, 3]);\n\n        let logits = model.forward(inputs);\n        let predictions = logits.argmax(1).squeeze::<1>();\n        let predictions_vec: Vec<i32> = predictions.into_data().to_vec().unwrap();\n\n        let correct = predictions_vec\n            .iter()\n            .zip(targets_vec.iter())\n            .filter(|(p, t)| p == t)\n            .count();\n\n        total_correct += correct;\n    }\n\n    total_correct as f32 / total as f32\n}\n\n/// Verify that weight norms decrease over time with weight decay\n/// and show correlation with the grokking transition\npub fn verify_weight_norm_decay(\n    weight_norm_history: &[(usize, f64)],\n    grok_step: Option<usize>,\n    config: &WeightNormDecayConfig,\n) -> Result<WeightNormDecayReport, String> {\n    if weight_norm_history.len() < 2 {\n        return Err(\"weight norm history requires at least 2 snapshots\".to_string());\n    }\n\n    let initial_norm = weight_norm_history\n        .first()\n        .ok_or_else(|| \"weight norm history is empty\".to_string())?\n        .1;\n    let final_norm = weight_norm_history\n        .last()\n        .ok_or_else(|| \"weight norm history is empty\".to_string())?\n        .1;\n\n    if initial_norm <= 0.0 {\n        return Err(\"initial weight norm must be positive\".to_string());\n    }\n\n    // Check that norms are generally decreasing (with weight decay)\n    let relative_decrease = (initial_norm - final_norm) / initial_norm;\n    if relative_decrease < config.min_relative_decrease {\n        return Err(format!(\n            \"weight norm decrease {:.4} below threshold {:.4} (initial {:.4}, final {:.4})\",\n            relative_decrease, config.min_relative_decrease, initial_norm, final_norm\n        ));\n    }\n\n    // Check that norms are not increasing (weight decay should monotonically decrease norms)\n    for window in weight_norm_history.windows(2) {\n        let prev_norm = window[0].1;\n        let next_norm = window[1].1;\n        if next_norm > prev_norm * 1.01 {\n            // Allow 1% tolerance for numerical noise\n            return Err(format!(\n                \"weight norm increased from {:.4} to {:.4} at step {}\",\n                prev_norm, next_norm, window[1].0\n            ));\n        }\n    }\n\n    // If grok step is provided, check that norms show a shift around grokking\n    let grok_step_norm = if let Some(step) = grok_step {\n        weight_norm_history\n            .iter()\n            .filter(|(s, _)| {\n                *s >= step.saturating_sub(config.grok_step_window)\n                    && *s <= step + config.grok_step_window\n            })\n            .map(|(_, norm)| *norm)\n            .next()\n    } else {\n        None\n    };\n\n    Ok(WeightNormDecayReport {\n        initial_norm,\n        final_norm,\n        relative_decrease,\n        grok_step_norm,\n    })\n}\n\n/// Verify the characteristic \"snake curve\" shape of grokking phenomenon\n/// on log-scale plots: train accuracy rises early, validation stays flat then jumps\npub fn verify_snake_curve_shape(\n    train_acc: &[(usize, f32)],\n    val_acc: &[(usize, f32)],\n    config: &SnakeCurveConfig,\n) -> Result<SnakeCurveReport, String> {\n    if train_acc.is_empty() || val_acc.is_empty() {\n        return Err(\"empty accuracy history\".to_string());\n    }\n\n    // Phase 1: Verify early train convergence (immediate drop on log scale)\n    let train_converged_step = train_acc\n        .iter()\n        .find(|(_, acc)| *acc >= config.train_convergence_threshold)\n        .map(|(step, _)| *step)\n        .ok_or_else(|| {\n            format!(\n                \"train accuracy never reaches {} threshold\",\n                config.train_convergence_threshold\n            )\n        })?;\n\n    if train_converged_step > config.early_step_max {\n        return Err(format!(\n            \"train accuracy converges at step {}, expected before {}\",\n            train_converged_step, config.early_step_max\n        ));\n    }\n\n    // Phase 2: Verify validation plateau (flat on log scale)\n    let plateau_samples: Vec<f32> = val_acc\n        .iter()\n        .filter(|(step, _)| *step >= 1 && *step <= config.val_plateau_min_step)\n        .map(|(_, acc)| *acc)\n        .collect();\n\n    if plateau_samples.is_empty() {\n        return Err(\"no validation samples in plateau region\".to_string());\n    }\n\n    let val_plateau_max = plateau_samples\n        .iter()\n        .fold(0.0f32, |max_val, &acc| max_val.max(acc));\n\n    if val_plateau_max > config.val_plateau_max_acc {\n        return Err(format!(\n            \"validation accuracy in plateau region too high: max {:.4} exceeds {:.4}\",\n            val_plateau_max, config.val_plateau_max_acc\n        ));\n    }\n\n    // Phase 3: Verify delayed validation jump (crash up on log scale)\n    let val_grok_step = val_acc\n        .iter()\n        .filter(|(step, _)| *step > config.val_plateau_min_step)\n        .find(|(_, acc)| *acc >= config.val_final_threshold)\n        .map(|(step, _)| *step)\n        .ok_or_else(|| {\n            format!(\n                \"validation accuracy never reaches {} after plateau\",\n                config.val_final_threshold\n            )\n        })?;\n\n    // Phase 4: Verify phase ordering (snake shape: early train, late val)\n    let curve_shape_valid = train_converged_step < config.val_plateau_min_step\n        && val_grok_step > config.val_plateau_min_step\n        && train_converged_step < val_grok_step;\n\n    if !curve_shape_valid {\n        return Err(format!(\n            \"invalid curve shape: train converged at {}, val plateau until {}, val grokked at {}\",\n            train_converged_step, config.val_plateau_min_step, val_grok_step\n        ));\n    }\n\n    Ok(SnakeCurveReport {\n        train_converged_by_step: train_converged_step,\n        val_plateau_max_acc: val_plateau_max,\n        val_grok_step,\n        curve_shape_valid,\n    })\n}\n\n/// Test specific examples to see if model actually computes mod p\npub fn test_specific_examples<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n) {\n    println!();\n    println!(\"ðŸ§ª Testing specific examples:\");\n\n    let modulus = ModularAdditionDataset::modulus();\n    let equals_token = ModularAdditionDataset::equals_token();\n    let test_cases = vec![\n        (0, 0, 0),\n        (1, 1, 2),\n        (50, 50, (50 + 50) % modulus),\n        (112, 1, (112 + 1) % modulus),\n        (56, 57, (56 + 57) % modulus),\n        (112, 112, (112 + 112) % modulus),\n        (10, 20, 30),\n        (60, 60, (60 + 60) % modulus),\n    ];\n\n    for (a, b, expected) in test_cases {\n        let input_vec = vec![a as i32, b as i32, equals_token as i32];\n        let input = Tensor::<B, 1, Int>::from_ints(input_vec.as_slice(), device)\n            .reshape([1, 3]);\n\n        let logits = model.forward(input);\n        let prediction = logits.argmax(1).squeeze::<1>();\n        let pred_value: i32 = prediction.into_data().to_vec().unwrap()[0];\n\n        let correct = if pred_value == expected as i32 { \"âœ“\" } else { \"âœ—\" };\n        println!(\n            \"   {} + {} mod {} = {} | Model: {} {}\",\n            a, b, modulus, expected, pred_value, correct\n        );\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    fn make_histories(\n        train_acc: &[(usize, f32)],\n        val_acc: &[(usize, f32)],\n        val_loss: &[(usize, f64)],\n    ) -> (LossHistory, AccuracyHistory) {\n        let mut loss_history = LossHistory::new();\n        loss_history.val_snapshots = val_loss.to_vec();\n        let mut accuracy_history = AccuracyHistory::new();\n        accuracy_history.train_snapshots = train_acc.to_vec();\n        accuracy_history.val_snapshots = val_acc.to_vec();\n        (loss_history, accuracy_history)\n    }\n\n    #[test]\n    fn grokking_phase_verification_passes_on_synthetic_signal() {\n        let train_acc = vec![(0, 0.2), (500, 0.995), (1000, 0.999)];\n        let val_acc = vec![\n            (0, 0.01),\n            (500, 0.01),\n            (1500, 0.01),\n            (2200, 0.995),\n            (2500, 0.999),\n        ];\n        let val_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 2.0),\n            (2200, 0.2),\n            (2500, 0.15),\n        ];\n\n        let (loss_history, accuracy_history) = make_histories(&train_acc, &val_acc, &val_loss);\n        let config = GrokkingVerificationConfig {\n            early_step_max: 1000,\n            plateau_min_step: 1500,\n            generalization_window: 1500,\n            accuracy_window: 2,\n            loss_window: 2,\n            ..GrokkingVerificationConfig::default_for_modulus(ModularAdditionDataset::modulus())\n        };\n\n        let report = verify_grokking_phase_transition(&loss_history, &accuracy_history, &config)\n            .expect(\"expected grokking verification to pass\");\n        assert!(report.transition_step >= config.plateau_min_step);\n        assert!(report.early_train_step <= config.early_step_max);\n    }\n\n    #[test]\n    fn grokking_phase_verification_fails_without_transition() {\n        let train_acc = vec![(0, 0.2), (500, 0.995)];\n        let val_acc = vec![(0, 0.01), (500, 0.01), (1500, 0.02)];\n        let val_loss = vec![(0, 2.0), (500, 2.0), (1500, 1.9)];\n\n        let (loss_history, accuracy_history) = make_histories(&train_acc, &val_acc, &val_loss);\n        let config = GrokkingVerificationConfig {\n            early_step_max: 1000,\n            plateau_min_step: 1000,\n            generalization_window: 1000,\n            accuracy_window: 2,\n            loss_window: 2,\n            ..GrokkingVerificationConfig::default_for_modulus(ModularAdditionDataset::modulus())\n        };\n\n        let err = verify_grokking_phase_transition(&loss_history, &accuracy_history, &config)\n            .expect_err(\"expected grokking verification to fail\");\n        assert!(err.contains(\"validation accuracy never reaches target\"));\n    }\n\n    #[test]\n    fn grokking_phase_verification_fails_with_slow_accuracy_rise() {\n        let train_acc = vec![(0, 0.2), (500, 0.995), (1000, 0.999)];\n        let val_acc = vec![\n            (0, 0.01),\n            (500, 0.01),\n            (1500, 0.5),\n            (2000, 0.6),\n            (2200, 0.995),\n            (2500, 0.999),\n        ];\n        let val_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 2.0),\n            (2200, 0.2),\n            (2500, 0.15),\n        ];\n\n        let (loss_history, accuracy_history) = make_histories(&train_acc, &val_acc, &val_loss);\n        let config = GrokkingVerificationConfig {\n            early_step_max: 1000,\n            plateau_min_step: 1000,\n            generalization_window: 2000,\n            accuracy_window: 2,\n            loss_window: 2,\n            ..GrokkingVerificationConfig::default_for_modulus(ModularAdditionDataset::modulus())\n        };\n\n        let err = verify_grokking_phase_transition(&loss_history, &accuracy_history, &config)\n            .expect_err(\"expected grokking verification to fail\");\n        assert!(err.contains(\"plateau too high\"));\n    }\n\n    #[test]\n    fn restricted_loss_detection_passes_with_early_drop() {\n        let full_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 2.0),\n            (2500, 2.0),\n            (3500, 0.3),\n        ];\n        let restricted_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 0.8),\n            (2500, 0.4),\n            (3500, 0.3),\n        ];\n\n        let config = RestrictedLossVerificationConfig {\n            plateau_min_step: 1000,\n            drop_window: 2,\n            drop_fraction: 0.5,\n            min_step_lead: 500,\n        };\n\n        let report = verify_restricted_loss_early_drop(&full_loss, &restricted_loss, &config)\n            .expect(\"expected restricted loss verification to pass\");\n        assert!(report.restricted_drop_step < report.full_drop_step);\n    }\n\n    #[test]\n    fn restricted_loss_detection_fails_when_full_drops_first() {\n        let full_loss = vec![(0, 2.0), (1000, 1.0), (1500, 0.8)];\n        let restricted_loss = vec![(0, 2.0), (1000, 2.0), (1500, 0.8)];\n\n        let config = RestrictedLossVerificationConfig {\n            plateau_min_step: 500,\n            drop_window: 1,\n            drop_fraction: 0.3,\n            min_step_lead: 200,\n        };\n\n        let err = verify_restricted_loss_early_drop(&full_loss, &restricted_loss, &config)\n            .expect_err(\"expected restricted loss verification to fail\");\n        assert!(err.contains(\"restricted loss drops\"));\n    }\n\n    #[test]\n    fn excluded_loss_spike_verification_passes() {\n        let excluded_loss = vec![(0, 1.0), (1500, 2.2), (3000, 1.1)];\n        let config = ExcludedLossSpikeConfig {\n            min_relative_increase: 0.5,\n        };\n\n        let report = verify_excluded_loss_spike(&excluded_loss, &config)\n            .expect(\"expected excluded loss spike verification to pass\");\n        assert_eq!(report.spike_step, 1500);\n    }\n\n    #[test]\n    fn excluded_loss_spike_verification_fails_without_spike() {\n        let excluded_loss = vec![(0, 1.0), (1500, 1.1), (3000, 1.0)];\n        let config = ExcludedLossSpikeConfig {\n            min_relative_increase: 0.5,\n        };\n\n        let err = verify_excluded_loss_spike(&excluded_loss, &config)\n            .expect_err(\"expected excluded loss spike verification to fail\");\n        assert!(err.contains(\"excluded loss spike too small\"));\n    }\n\n    #[test]\n    fn restricted_loss_drop_verification_passes() {\n        let restricted_loss = vec![\n            (0, 2.0),\n            (1000, 1.8),\n            (5000, 1.2),\n            (7000, 0.8),\n            (10000, 0.9),\n        ];\n        let config = RestrictedLossDropConfig {\n            min_relative_decrease: 0.3,\n        };\n\n        let report = verify_restricted_loss_drop(&restricted_loss, &config)\n            .expect(\"expected restricted loss drop verification to pass\");\n        assert_eq!(report.drop_step, 7000);\n        assert!(report.baseline_loss > report.drop_loss);\n        assert!((report.baseline_loss - report.drop_loss) / report.baseline_loss >= 0.3);\n    }\n\n    #[test]\n    fn restricted_loss_drop_verification_fails_without_drop() {\n        let restricted_loss = vec![\n            (0, 2.0),\n            (1000, 2.1),\n            (5000, 1.9),\n            (7000, 2.0),\n            (10000, 2.05),\n        ];\n        let config = RestrictedLossDropConfig {\n            min_relative_decrease: 0.3,\n        };\n\n        let err = verify_restricted_loss_drop(&restricted_loss, &config)\n            .expect_err(\"expected restricted loss drop verification to fail\");\n        assert!(err.contains(\"restricted loss decrease too small\"));\n    }\n\n    #[test]\n    fn mlp_activation_wave_verification_passes_on_rectified_sine() {\n        let n = ModularAdditionDataset::modulus();\n        let freq = 5.0;\n        let mut signal = Vec::with_capacity(n);\n        for idx in 0..n {\n            let angle = 2.0 * std::f64::consts::PI * freq * idx as f64 / n as f64;\n            let rectified = angle.sin().max(0.0);\n            let wobble = (2.0 * std::f64::consts::PI * 2.0 * idx as f64 / n as f64)\n                .sin()\n                .abs()\n                * 0.02;\n            signal.push(rectified + wobble);\n        }\n\n        let config = MlpWaveVerificationConfig {\n            max_negative_fraction: 0.0,\n            negative_tolerance: 1e-6,\n            min_dominant_ratio: 3.0,\n            min_correlation: 0.7,\n            phase_steps: 32,\n        };\n\n        let report =\n            verify_mlp_activation_waves_from_signals(&[(0, signal)], &config)\n                .expect(\"expected rectified sine to pass\");\n        assert_eq!(report.neuron_reports.len(), 1);\n    }\n\n    #[test]\n    fn mlp_activation_wave_verification_fails_on_noise() {\n        let n = ModularAdditionDataset::modulus();\n        let signal: Vec<f64> = (0..n).map(|idx| idx as f64 / n as f64).collect();\n\n        let config = MlpWaveVerificationConfig {\n            max_negative_fraction: 0.0,\n            negative_tolerance: 1e-6,\n            min_dominant_ratio: 4.0,\n            min_correlation: 0.8,\n            phase_steps: 16,\n        };\n\n        let err =\n            verify_mlp_activation_waves_from_signals(&[(0, signal)], &config)\n                .expect_err(\"expected linear signal to fail\");\n        assert!(err.contains(\"dominant frequency ratio\") || err.contains(\"correlation\"));\n    }\n\n    #[test]\n    fn pairwise_manifold_transition_passes_on_elliptic_signals() {\n        let n = ModularAdditionDataset::modulus();\n        let freq = 5.0;\n        let mut post_signals = Vec::new();\n        for neuron_idx in 0..7usize {\n            let phase = 2.0 * std::f64::consts::PI * neuron_idx as f64 / 7.0;\n            let signal: Vec<f64> = (0..n)\n                .map(|idx| {\n                    let angle = 2.0 * std::f64::consts::PI * freq * idx as f64 / n as f64 + phase;\n                    angle.sin()\n                })\n                .collect();\n            post_signals.push((neuron_idx, signal));\n        }\n\n        let mut pre_signals = Vec::new();\n        for neuron_idx in 0..7usize {\n            let signal: Vec<f64> = (0..n)\n                .map(|idx| idx as f64 / n as f64 + neuron_idx as f64 * 0.01)\n                .collect();\n            pre_signals.push((neuron_idx, signal));\n        }\n\n        let config = PairwiseManifoldConfig {\n            fixed_y: 0,\n            neuron_indices: (0..7).collect(),\n            min_post_score: 0.75,\n            max_pre_score: 0.55,\n            max_axis_ratio: 8.0,\n            min_pairs_passing: 8,\n        };\n\n        let report = verify_pairwise_manifold_transition_from_signals(\n            &pre_signals,\n            &post_signals,\n            &config,\n        )\n        .expect(\"expected pairwise manifold transition to pass\");\n        assert!(report.passing_pairs >= config.min_pairs_passing);\n    }\n\n    #[test]\n    fn pairwise_manifold_transition_fails_without_rings() {\n        let n = ModularAdditionDataset::modulus();\n        let mut signals = Vec::new();\n        for neuron_idx in 0..7usize {\n            let signal: Vec<f64> = (0..n)\n                .map(|idx| idx as f64 / n as f64 + neuron_idx as f64 * 0.02)\n                .collect();\n            signals.push((neuron_idx, signal));\n        }\n\n        let config = PairwiseManifoldConfig {\n            fixed_y: 0,\n            neuron_indices: (0..7).collect(),\n            min_post_score: 0.8,\n            max_pre_score: 0.55,\n            max_axis_ratio: 4.0,\n            min_pairs_passing: 5,\n        };\n\n        let err = verify_pairwise_manifold_transition_from_signals(\n            &signals,\n            &signals,\n            &config,\n        )\n        .expect_err(\"expected pairwise manifold transition to fail\");\n        assert!(err.contains(\"post-grok\") || err.contains(\"pre-grok\"));\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_passes_on_diagonal_corrugation() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create diagonal ridges: f(x,y) = sin(2Ï€ * freq * (x+y) / n)\n        let freq = 3.0;\n        for x in 0..n {\n            for y in 0..n {\n                let angle = 2.0 * std::f64::consts::PI * freq * (x + y) as f64 / n as f64;\n                surface[x][y] = angle.sin();\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let report = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect(\"expected diagonal ridge verification to pass\");\n        assert!(report.diagonal_score > report.axis_score);\n        assert!(report.ratio >= config.min_diagonal_ratio);\n        assert!(report.diagonal_score >= config.min_diagonal_score);\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_passes_on_anti_diagonal_pattern() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create anti-diagonal ridges: f(x,y) = sin(2Ï€ * freq * (x-y) / n)\n        let freq = 3.0;\n        for x in 0..n {\n            for y in 0..n {\n                let angle = 2.0 * std::f64::consts::PI * freq * (x as i32 - y as i32) as f64 / n as f64;\n                surface[x][y] = angle.sin();\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let report = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect(\"expected anti-diagonal ridge verification to pass\");\n        assert!(report.diagonal_score > report.axis_score);\n        assert!(report.ratio >= config.min_diagonal_ratio);\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_fails_on_axis_aligned_pattern() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create axis-aligned ridges: f(x,y) = sin(2Ï€ * freq * x / n)\n        let freq = 3.0;\n        for x in 0..n {\n            for y in 0..n {\n                let angle = 2.0 * std::f64::consts::PI * freq * x as f64 / n as f64;\n                surface[x][y] = angle.sin();\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let err = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect_err(\"expected axis-aligned pattern to fail diagonal verification\");\n        assert!(err.contains(\"diagonal/axis ratio\") || err.contains(\"diagonal autocorrelation\"));\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_fails_on_flat_surface() {\n        let n = 50;\n        let surface = vec![vec![1.5; n]; n];\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let err = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect_err(\"expected flat surface to fail\");\n        assert!(err.contains(\"surface variance is zero\"));\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_fails_on_random_noise() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create random noise (low autocorrelation everywhere)\n        for x in 0..n {\n            for y in 0..n {\n                surface[x][y] = ((x * 17 + y * 31) % 100) as f64 / 100.0;\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let err = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect_err(\"expected random noise to fail diagonal verification\");\n        assert!(err.contains(\"diagonal/axis ratio\") || err.contains(\"diagonal autocorrelation\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_passes_on_valid_grokking_curve() {\n        // Simulate typical grokking: train converges early, val plateaus then jumps\n        let train_acc = vec![\n            (1, 0.10),\n            (10, 0.50),\n            (100, 0.80),\n            (500, 0.98),\n            (1000, 0.99),\n            (2000, 0.995),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (10, 0.009),\n            (100, 0.01),\n            (500, 0.011),\n            (1000, 0.009),\n            (1500, 0.01),\n            (2000, 0.92),\n            (3000, 0.98),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 1500,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let report = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect(\"expected snake curve verification to pass\");\n\n        assert_eq!(report.train_converged_by_step, 500);\n        assert!(report.val_plateau_max_acc < config.val_plateau_max_acc);\n        assert_eq!(report.val_grok_step, 2000);\n        assert!(report.curve_shape_valid);\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_on_simultaneous_convergence() {\n        // Both train and val converge at the same time (no grokking)\n        let train_acc = vec![\n            (1, 0.10),\n            (100, 0.98),\n            (500, 0.99),\n        ];\n        let val_acc = vec![\n            (1, 0.10),\n            (100, 0.97),\n            (500, 0.98),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 200,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let err = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect_err(\"expected snake curve verification to fail\");\n        assert!(err.contains(\"plateau region too high\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_on_slow_train_convergence() {\n        // Train takes too long to converge (not the typical snake curve)\n        let train_acc = vec![\n            (1, 0.10),\n            (500, 0.50),\n            (1000, 0.70),\n            (2000, 0.98),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (500, 0.01),\n            (1000, 0.01),\n            (2000, 0.92),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 1500,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let err = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect_err(\"expected snake curve verification to fail\");\n        assert!(err.contains(\"converges at step 2000\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_without_val_jump() {\n        // Validation never jumps up (no generalization)\n        let train_acc = vec![\n            (1, 0.10),\n            (100, 0.98),\n            (500, 0.99),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (100, 0.01),\n            (500, 0.01),\n            (2000, 0.02),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 200,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let err = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect_err(\"expected snake curve verification to fail\");\n        assert!(err.contains(\"validation accuracy never reaches\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_on_wrong_phase_ordering() {\n        // Val jumps before train converges (backwards snake)\n        let train_acc = vec![\n            (1, 0.10),\n            (500, 0.50),\n            (2000, 0.98),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (100, 0.95),\n            (500, 0.98),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 200,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let result = verify_snake_curve_shape(&train_acc, &val_acc, &config);\n        // Should fail because val jumps before train converges (not a proper snake curve)\n        // This can fail for multiple reasons, but should definitely fail\n        assert!(result.is_err(), \"Expected snake curve verification to fail but it passed\");\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_passes_on_decreasing_norms() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 85.0),\n            (1000, 72.0),\n            (1500, 63.0),\n            (2000, 58.0),\n            (2500, 55.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let report = verify_weight_norm_decay(&weight_norms, Some(2000), &config)\n            .expect(\"expected weight norm decay verification to pass\");\n\n        assert_eq!(report.initial_norm, 100.0);\n        assert_eq!(report.final_norm, 55.0);\n        assert!((report.relative_decrease - 0.45).abs() < 0.01);\n        assert!(report.grok_step_norm.is_some());\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_fails_on_flat_norms() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 99.0),\n            (1000, 98.5),\n            (1500, 98.0),\n            (2000, 97.5),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let err = verify_weight_norm_decay(&weight_norms, None, &config)\n            .expect_err(\"expected weight norm decay verification to fail\");\n        assert!(err.contains(\"weight norm decrease\"));\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_fails_on_increasing_norms() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 95.0),\n            (1000, 105.0), // Increase here\n            (1500, 90.0),\n            (2000, 85.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let err = verify_weight_norm_decay(&weight_norms, None, &config)\n            .expect_err(\"expected weight norm decay verification to fail\");\n        assert!(err.contains(\"weight norm increased\"));\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_allows_small_noise() {\n        // Weight decay with small numerical noise (within 1% tolerance)\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 95.0),\n            (1000, 90.5),  // Small increase within tolerance\n            (1500, 90.0),\n            (2000, 85.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let report = verify_weight_norm_decay(&weight_norms, None, &config)\n            .expect(\"expected weight norm decay with small noise to pass\");\n\n        assert_eq!(report.initial_norm, 100.0);\n        assert_eq!(report.final_norm, 85.0);\n        assert!((report.relative_decrease - 0.15).abs() < 0.01);\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_finds_grok_step_norm() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (1000, 80.0),\n            (1800, 68.0),  // Within window of grok step (2200 - 500 = 1700)\n            (2000, 65.0),\n            (2200, 62.0),  // Around grok step\n            (3000, 55.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let report = verify_weight_norm_decay(&weight_norms, Some(2200), &config)\n            .expect(\"expected weight norm decay verification to pass\");\n\n        assert!(report.grok_step_norm.is_some());\n        let grok_norm = report.grok_step_norm.unwrap();\n        // Should find the first norm in the window [1700, 2700], which is 68.0 at step 1800\n        assert!((grok_norm - 68.0).abs() < 0.01);\n    }\n}\n","structuredPatch":[{"oldStart":661,"oldLines":6,"newStart":661,"newLines":8,"lines":["     Ok(surface)"," }"," ","+/// NOTE: Verification function reserved for future integration or manual analysis","+#[allow(dead_code)]"," pub fn verify_constructive_interference_surface_from_grid(","     surface: &[Vec<f64>],","     config: &DiagonalRidgeConfig,"]}],"userModified":false,"replaceAll":false}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01AMpXfsiHghhRk8M6W21sEh","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_015QF3kGcPprSCAiFb2dKzii","name":"Edit","input":{"replace_all":false,"file_path":"/Users/admin/MechInt/src/verify.rs","old_string":"pub fn verify_snake_curve_shape(","new_string":"/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_snake_curve_shape("}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":624,"cache_read_input_tokens":59680,"cache_creation":{"ephemeral_5m_input_tokens":624,"ephemeral_1h_input_tokens":0},"output_tokens":164,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"c006e0d0-79b6-4a73-9fb7-5fd7031fa1eb"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_015QF3kGcPprSCAiFb2dKzii","type":"tool_result","content":"The file /Users/admin/MechInt/src/verify.rs has been updated. Here's the result of running `cat -n` on a snippet of the edited file:\n  1348â†’}\n  1349â†’\n  1350â†’/// Verify the characteristic \"snake curve\" shape of grokking phenomenon\n  1351â†’/// on log-scale plots: train accuracy rises early, validation stays flat then jumps\n  1352â†’/// NOTE: Verification function reserved for future integration or manual analysis\n  1353â†’#[allow(dead_code)]\n  1354â†’pub fn verify_snake_curve_shape(\n  1355â†’    train_acc: &[(usize, f32)],\n  1356â†’    val_acc: &[(usize, f32)],\n  1357â†’    config: &SnakeCurveConfig,\n  1358â†’) -> Result<SnakeCurveReport, String> {"}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"d381e1a9-47bd-480f-a157-347227646fbf","tool_use_result":{"filePath":"/Users/admin/MechInt/src/verify.rs","oldString":"pub fn verify_snake_curve_shape(","newString":"/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_snake_curve_shape(","originalFile":"use burn::{\n    data::dataset::Dataset,\n    tensor::{backend::Backend, Int, Tensor},\n};\nuse rustfft::{num_complex::Complex, FftPlanner};\n\nuse crate::analysis::{AccuracyHistory, LossHistory};\nuse crate::data::ModularAdditionDataset;\nuse crate::model::Transformer;\n\n#[derive(Debug, Clone)]\npub struct GrokkingVerificationConfig {\n    pub early_train_acc_threshold: f32,\n    pub early_step_max: usize,\n    pub plateau_min_step: usize,\n    pub generalization_window: usize,\n    pub chance_accuracy: f32,\n    pub chance_tolerance: f32,\n    pub target_val_acc_threshold: f32,\n    pub accuracy_window: usize,\n    pub loss_window: usize,\n    pub loss_drop_fraction: f64,\n}\n\nimpl GrokkingVerificationConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        Self {\n            early_train_acc_threshold: 0.99,\n            early_step_max: 1000,\n            plateau_min_step: 1000,\n            generalization_window: 10_000,\n            chance_accuracy: 1.0 / modulus as f32,\n            chance_tolerance: 0.02,\n            target_val_acc_threshold: 0.99,\n            accuracy_window: 5,\n            loss_window: 5,\n            loss_drop_fraction: 0.5,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct GrokkingPhaseReport {\n    pub early_train_step: usize,\n    pub transition_step: usize,\n    pub plateau_max_val_acc: f32,\n    pub loss_drop_ratio: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossVerificationConfig {\n    pub plateau_min_step: usize,\n    pub drop_window: usize,\n    pub drop_fraction: f64,\n    pub min_step_lead: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossReport {\n    pub restricted_drop_step: usize,\n    pub full_drop_step: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossDropConfig {\n    pub min_relative_decrease: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossDropReport {\n    pub drop_step: usize,\n    pub drop_loss: f64,\n    pub baseline_loss: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct ExcludedLossSpikeConfig {\n    pub min_relative_increase: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct ExcludedLossSpikeReport {\n    pub spike_step: usize,\n    pub spike_loss: f64,\n    pub baseline_loss: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct MlpWaveVerificationConfig {\n    pub max_negative_fraction: f64,\n    pub negative_tolerance: f64,\n    pub min_dominant_ratio: f64,\n    pub min_correlation: f64,\n    pub phase_steps: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct MlpWaveNeuronReport {\n    pub neuron_index: usize,\n    pub dominant_frequency: usize,\n    pub dominant_ratio: f64,\n    pub negative_fraction: f64,\n    pub best_correlation: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct MlpWaveReport {\n    pub neuron_reports: Vec<MlpWaveNeuronReport>,\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldConfig {\n    pub fixed_y: usize,\n    pub neuron_indices: Vec<usize>,\n    pub min_post_score: f64,\n    pub max_pre_score: f64,\n    pub max_axis_ratio: f64,\n    pub min_pairs_passing: usize,\n}\n\nimpl PairwiseManifoldConfig {\n    pub fn default_for_modulus(_modulus: usize) -> Self {\n        Self {\n            fixed_y: 0,\n            neuron_indices: (0..7).collect(),\n            min_post_score: 0.75,\n            max_pre_score: 0.55,\n            max_axis_ratio: 8.0,\n            min_pairs_passing: 10,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldPairReport {\n    pub neuron_a: usize,\n    pub neuron_b: usize,\n    pub circularity_score: f64,\n    pub radius_cv: f64,\n    pub axis_ratio: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldReport {\n    pub pair_reports: Vec<PairwiseManifoldPairReport>,\n    pub average_score: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldTransitionReport {\n    pub pre_average_score: f64,\n    pub post_average_score: f64,\n    pub passing_pairs: usize,\n    pub total_pairs: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct DiagonalRidgeConfig {\n    pub neuron_index: usize,\n    pub max_lag: usize,\n    pub min_diagonal_ratio: f64,\n    pub min_diagonal_score: f64,\n}\n\nimpl DiagonalRidgeConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        let max_lag = (modulus / 10).max(4).min(20);\n        Self {\n            neuron_index: 10,\n            max_lag,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct DiagonalRidgeReport {\n    pub neuron_index: usize,\n    pub diagonal_score: f64,\n    pub axis_score: f64,\n    pub ratio: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct SnakeCurveConfig {\n    pub early_step_max: usize,\n    pub train_convergence_threshold: f32,\n    pub val_plateau_min_step: usize,\n    pub val_plateau_max_acc: f32,\n    pub val_final_threshold: f32,\n}\n\nimpl SnakeCurveConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        Self {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 1000,\n            val_plateau_max_acc: 1.0 / modulus as f32 + 0.05,\n            val_final_threshold: 0.90,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct WeightNormDecayConfig {\n    pub min_relative_decrease: f64,\n    pub plateau_min_step: usize,\n    pub grok_step_window: usize,\n}\n\nimpl WeightNormDecayConfig {\n    pub fn default_for_grokking() -> Self {\n        Self {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct WeightNormDecayReport {\n    pub initial_norm: f64,\n    pub final_norm: f64,\n    pub relative_decrease: f64,\n    pub grok_step_norm: Option<f64>,\n}\n\n#[derive(Debug, Clone)]\npub struct SnakeCurveReport {\n    pub train_converged_by_step: usize,\n    pub val_plateau_max_acc: f32,\n    pub val_grok_step: usize,\n    pub curve_shape_valid: bool,\n}\n\npub fn verify_grokking_phase_transition(\n    loss_history: &LossHistory,\n    accuracy_history: &AccuracyHistory,\n    config: &GrokkingVerificationConfig,\n) -> Result<GrokkingPhaseReport, String> {\n    let (early_train_step, _) = accuracy_history\n        .train_snapshots\n        .iter()\n        .find(|(_, acc)| *acc >= config.early_train_acc_threshold)\n        .ok_or_else(|| \"train accuracy never reaches threshold\".to_string())?;\n\n    if *early_train_step > config.early_step_max {\n        return Err(format!(\n            \"train accuracy reaches {:.2} after step {}, expected before {}\",\n            config.early_train_acc_threshold, early_train_step, config.early_step_max\n        ));\n    }\n\n    let plateau_max_val_acc = accuracy_history\n        .val_snapshots\n        .iter()\n        .filter(|(step, _)| *step <= config.plateau_min_step)\n        .map(|(_, acc)| *acc)\n        .fold(None::<f32>, |accum, value| {\n            Some(accum.map_or(value, |max_val| max_val.max(value)))\n        })\n        .ok_or_else(|| \"no validation accuracy samples before plateau_min_step\".to_string())?;\n\n    if plateau_max_val_acc > config.chance_accuracy + config.chance_tolerance {\n        return Err(format!(\n            \"validation accuracy before step {} exceeds chance (max {:.4})\",\n            config.plateau_min_step, plateau_max_val_acc\n        ));\n    }\n\n    let (transition_step, _) = accuracy_history\n        .val_snapshots\n        .iter()\n        .find(|(step, acc)| {\n            *step >= config.plateau_min_step && *acc >= config.target_val_acc_threshold\n        })\n        .ok_or_else(|| \"validation accuracy never reaches target threshold\".to_string())?;\n\n    let latest_allowed = config.plateau_min_step + config.generalization_window;\n    if *transition_step > latest_allowed {\n        return Err(format!(\n            \"validation accuracy reaches {:.2} at step {}, expected before {}\",\n            config.target_val_acc_threshold, transition_step, latest_allowed\n        ));\n    }\n\n    verify_accuracy_jump(\n        &accuracy_history.val_snapshots,\n        *transition_step,\n        config.accuracy_window,\n        config.chance_accuracy,\n        config.chance_tolerance,\n        config.target_val_acc_threshold,\n    )?;\n\n    let loss_drop_ratio = verify_loss_drop(\n        &loss_history.val_snapshots,\n        *transition_step,\n        config.loss_window,\n        config.loss_drop_fraction,\n    )?;\n\n    Ok(GrokkingPhaseReport {\n        early_train_step: *early_train_step,\n        transition_step: *transition_step,\n        plateau_max_val_acc,\n        loss_drop_ratio,\n    })\n}\n\nfn verify_accuracy_jump(\n    val_accuracy: &[(usize, f32)],\n    transition_step: usize,\n    window: usize,\n    chance_accuracy: f32,\n    chance_tolerance: f32,\n    target_val_acc_threshold: f32,\n) -> Result<(), String> {\n    let before_values: Vec<f32> = val_accuracy\n        .iter()\n        .filter(|(step, _)| *step < transition_step)\n        .map(|(_, acc)| *acc)\n        .collect();\n\n    let after_values: Vec<f32> = val_accuracy\n        .iter()\n        .filter(|(step, _)| *step >= transition_step)\n        .map(|(_, acc)| *acc)\n        .collect();\n\n    if before_values.len() < window || after_values.len() < window {\n        return Err(\"insufficient accuracy samples around transition\".to_string());\n    }\n\n    let before_avg: f32 =\n        before_values[before_values.len() - window..].iter().sum::<f32>() / window as f32;\n    let after_avg: f32 = after_values[..window].iter().sum::<f32>() / window as f32;\n\n    if before_avg > chance_accuracy + chance_tolerance {\n        return Err(format!(\n            \"validation accuracy plateau too high before transition (avg {:.4})\",\n            before_avg\n        ));\n    }\n\n    if after_avg < target_val_acc_threshold {\n        return Err(format!(\n            \"validation accuracy after transition too low (avg {:.4})\",\n            after_avg\n        ));\n    }\n\n    Ok(())\n}\n\nfn verify_loss_drop(\n    val_losses: &[(usize, f64)],\n    transition_step: usize,\n    loss_window: usize,\n    loss_drop_fraction: f64,\n) -> Result<f64, String> {\n    let before_losses: Vec<f64> = val_losses\n        .iter()\n        .filter(|(step, _)| *step < transition_step)\n        .map(|(_, loss)| *loss)\n        .collect();\n\n    let after_losses: Vec<f64> = val_losses\n        .iter()\n        .filter(|(step, _)| *step >= transition_step)\n        .map(|(_, loss)| *loss)\n        .collect();\n\n    if before_losses.len() < loss_window || after_losses.len() < loss_window {\n        return Err(\"insufficient loss samples around transition\".to_string());\n    }\n\n    let before_avg: f64 =\n        before_losses[before_losses.len() - loss_window..].iter().sum::<f64>()\n            / loss_window as f64;\n    let after_avg: f64 = after_losses[..loss_window].iter().sum::<f64>() / loss_window as f64;\n\n    if after_avg > before_avg * (1.0 - loss_drop_fraction) {\n        return Err(format!(\n            \"validation loss drop too small (before {:.4}, after {:.4})\",\n            before_avg, after_avg\n        ));\n    }\n\n    Ok(after_avg / before_avg)\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_restricted_loss_early_drop(\n    full_losses: &[(usize, f64)],\n    restricted_losses: &[(usize, f64)],\n    config: &RestrictedLossVerificationConfig,\n) -> Result<RestrictedLossReport, String> {\n    let restricted_baseline =\n        baseline_loss(restricted_losses, config.plateau_min_step, config.drop_window)?;\n    let full_baseline = baseline_loss(full_losses, config.plateau_min_step, config.drop_window)?;\n\n    let restricted_drop_step = find_drop_step(\n        restricted_losses,\n        config.plateau_min_step,\n        restricted_baseline,\n        config.drop_fraction,\n    )?;\n    let full_drop_step = find_drop_step(\n        full_losses,\n        config.plateau_min_step,\n        full_baseline,\n        config.drop_fraction,\n    )?;\n\n    if restricted_drop_step + config.min_step_lead > full_drop_step {\n        return Err(format!(\n            \"restricted loss drops at step {}, expected at least {} steps before full loss (step {})\",\n            restricted_drop_step, config.min_step_lead, full_drop_step\n        ));\n    }\n\n    Ok(RestrictedLossReport {\n        restricted_drop_step,\n        full_drop_step,\n    })\n}\n\npub fn verify_restricted_loss_drop(\n    restricted_losses: &[(usize, f64)],\n    config: &RestrictedLossDropConfig,\n) -> Result<RestrictedLossDropReport, String> {\n    if restricted_losses.len() < 3 {\n        return Err(\"restricted loss history requires at least 3 points\".to_string());\n    }\n\n    let first = restricted_losses.first().ok_or_else(|| {\n        \"restricted loss history is empty\".to_string()\n    })?;\n    let baseline_loss = first.1;\n\n    let (min_idx, (min_step, min_loss)) = restricted_losses\n        .iter()\n        .enumerate()\n        .min_by(|(_, a), (_, b)| {\n            a.1.partial_cmp(&b.1).unwrap_or(std::cmp::Ordering::Equal)\n        })\n        .ok_or_else(|| \"restricted loss history is empty\".to_string())?;\n\n    if min_idx == 0 {\n        return Err(\"restricted loss minimum occurs at start (no drop detected)\".to_string());\n    }\n\n    let relative_decrease = (baseline_loss - min_loss) / baseline_loss;\n    if relative_decrease < config.min_relative_decrease {\n        return Err(format!(\n            \"restricted loss decrease too small (baseline {:.4}, min {:.4}, decrease {:.1}%)\",\n            baseline_loss, min_loss, relative_decrease * 100.0\n        ));\n    }\n\n    Ok(RestrictedLossDropReport {\n        drop_step: *min_step,\n        drop_loss: *min_loss,\n        baseline_loss,\n    })\n}\n\npub fn verify_excluded_loss_spike(\n    excluded_losses: &[(usize, f64)],\n    config: &ExcludedLossSpikeConfig,\n) -> Result<ExcludedLossSpikeReport, String> {\n    if excluded_losses.len() < 3 {\n        return Err(\"excluded loss history requires at least 3 points\".to_string());\n    }\n\n    let first = excluded_losses.first().ok_or_else(|| {\n        \"excluded loss history is empty\".to_string()\n    })?;\n    let last = excluded_losses.last().ok_or_else(|| {\n        \"excluded loss history is empty\".to_string()\n    })?;\n    let baseline_loss = (first.1 + last.1) / 2.0;\n\n    let (peak_idx, (peak_step, peak_loss)) = excluded_losses\n        .iter()\n        .enumerate()\n        .max_by(|(_, a), (_, b)| {\n            a.1.partial_cmp(&b.1).unwrap_or(std::cmp::Ordering::Equal)\n        })\n        .ok_or_else(|| \"excluded loss history is empty\".to_string())?;\n\n    if peak_idx == 0 || peak_idx + 1 == excluded_losses.len() {\n        return Err(\"excluded loss spike occurs at boundary\".to_string());\n    }\n\n    if peak_loss < &(baseline_loss * (1.0 + config.min_relative_increase)) {\n        return Err(format!(\n            \"excluded loss spike too small (baseline {:.4}, peak {:.4})\",\n            baseline_loss, peak_loss\n        ));\n    }\n\n    Ok(ExcludedLossSpikeReport {\n        spike_step: *peak_step,\n        spike_loss: *peak_loss,\n        baseline_loss,\n    })\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_mlp_activation_waves<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n    fixed_y: usize,\n    neuron_indices: &[usize],\n    config: &MlpWaveVerificationConfig,\n) -> Result<MlpWaveReport, String> {\n    let activations = collect_mlp_post_relu_activations(model, device, fixed_y)?;\n    if neuron_indices.is_empty() {\n        return Err(\"neuron indices list is empty\".to_string());\n    }\n\n    let d_ff = activations.len();\n    for &idx in neuron_indices {\n        if idx >= d_ff {\n            return Err(format!(\n                \"neuron index {} out of range (d_ff={})\",\n                idx, d_ff\n            ));\n        }\n    }\n\n    let signals: Vec<(usize, Vec<f64>)> = neuron_indices\n        .iter()\n        .map(|&idx| (idx, activations[idx].clone()))\n        .collect();\n    verify_mlp_activation_waves_from_signals(&signals, config)\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_mlp_activation_waves_from_signals(\n    signals: &[(usize, Vec<f64>)],\n    config: &MlpWaveVerificationConfig,\n) -> Result<MlpWaveReport, String> {\n    if signals.is_empty() {\n        return Err(\"no activation signals provided\".to_string());\n    }\n\n    let mut reports = Vec::with_capacity(signals.len());\n    for (neuron_index, signal) in signals {\n        let report = analyze_mlp_activation_wave(*neuron_index, signal, config)?;\n        reports.push(report);\n    }\n\n    Ok(MlpWaveReport {\n        neuron_reports: reports,\n    })\n}\n\npub fn verify_mlp_pairwise_manifold_transition<B: Backend>(\n    pre_model: &Transformer<B>,\n    post_model: &Transformer<B>,\n    device: &B::Device,\n    config: &PairwiseManifoldConfig,\n) -> Result<PairwiseManifoldTransitionReport, String> {\n    let modulus = ModularAdditionDataset::modulus();\n    if config.fixed_y >= modulus {\n        return Err(format!(\n            \"fixed_y {} out of range (modulus {})\",\n            config.fixed_y, modulus\n        ));\n    }\n\n    let pre_activations = collect_mlp_post_relu_activations(pre_model, device, config.fixed_y)?;\n    let post_activations = collect_mlp_post_relu_activations(post_model, device, config.fixed_y)?;\n\n    let d_ff = pre_activations.len();\n    if post_activations.len() != d_ff {\n        return Err(\"post activations shape mismatch\".to_string());\n    }\n\n    if config.neuron_indices.len() < 2 {\n        return Err(\"need at least two neuron indices\".to_string());\n    }\n\n    for &idx in &config.neuron_indices {\n        if idx >= d_ff {\n            return Err(format!(\n                \"neuron index {} out of range (d_ff={})\",\n                idx, d_ff\n            ));\n        }\n    }\n\n    let pre_signals: Vec<(usize, Vec<f64>)> = config\n        .neuron_indices\n        .iter()\n        .map(|&idx| (idx, pre_activations[idx].clone()))\n        .collect();\n    let post_signals: Vec<(usize, Vec<f64>)> = config\n        .neuron_indices\n        .iter()\n        .map(|&idx| (idx, post_activations[idx].clone()))\n        .collect();\n\n    verify_pairwise_manifold_transition_from_signals(&pre_signals, &post_signals, config)\n}\n\npub fn collect_mlp_activation_surface<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n    neuron_index: usize,\n) -> Result<Vec<Vec<f64>>, String> {\n    let modulus = ModularAdditionDataset::modulus();\n    let equals_token = ModularAdditionDataset::equals_token();\n\n    let mut inputs_vec = Vec::with_capacity(modulus * modulus * 3);\n    for x in 0..modulus {\n        for y in 0..modulus {\n            inputs_vec.push(x as i32);\n            inputs_vec.push(y as i32);\n            inputs_vec.push(equals_token as i32);\n        }\n    }\n\n    let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n        .reshape([modulus * modulus, 3]);\n    let (_, mlp_acts) = model.forward_with_mlp_activations(inputs);\n    let [batch_size, d_ff] = mlp_acts.dims();\n    if batch_size != modulus * modulus {\n        return Err(format!(\n            \"unexpected activation batch size {} (expected {})\",\n            batch_size,\n            modulus * modulus\n        ));\n    }\n    if neuron_index >= d_ff {\n        return Err(format!(\n            \"neuron index {} out of range (d_ff={})\",\n            neuron_index, d_ff\n        ));\n    }\n\n    let data: Vec<f32> = mlp_acts.into_data().to_vec().unwrap();\n    let mut surface = vec![vec![0.0f64; modulus]; modulus];\n    for x in 0..modulus {\n        for y in 0..modulus {\n            let batch = x * modulus + y;\n            let idx = batch * d_ff + neuron_index;\n            surface[x][y] = data[idx] as f64;\n        }\n    }\n\n    Ok(surface)\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_constructive_interference_surface_from_grid(\n    surface: &[Vec<f64>],\n    config: &DiagonalRidgeConfig,\n) -> Result<DiagonalRidgeReport, String> {\n    let (diag_main, diag_anti, axis_x, axis_y) =\n        surface_autocorrelation_scores(surface, config.max_lag)?;\n\n    let diagonal_score = diag_main.max(diag_anti);\n    let axis_score = axis_x.max(axis_y);\n    let ratio = if axis_score > 0.0 {\n        diagonal_score / axis_score\n    } else {\n        f64::INFINITY\n    };\n\n    if diagonal_score < config.min_diagonal_score {\n        return Err(format!(\n            \"diagonal autocorrelation {:.3} below threshold {:.3}\",\n            diagonal_score, config.min_diagonal_score\n        ));\n    }\n\n    if ratio < config.min_diagonal_ratio {\n        return Err(format!(\n            \"diagonal/axis ratio {:.3} below threshold {:.3}\",\n            ratio, config.min_diagonal_ratio\n        ));\n    }\n\n    Ok(DiagonalRidgeReport {\n        neuron_index: config.neuron_index,\n        diagonal_score,\n        axis_score,\n        ratio,\n    })\n}\n\npub fn verify_pairwise_manifold_transition_from_signals(\n    pre_signals: &[(usize, Vec<f64>)],\n    post_signals: &[(usize, Vec<f64>)],\n    config: &PairwiseManifoldConfig,\n) -> Result<PairwiseManifoldTransitionReport, String> {\n    let pre_report = evaluate_pairwise_manifolds_from_signals(pre_signals)?;\n    let post_report = evaluate_pairwise_manifolds_from_signals(post_signals)?;\n\n    if pre_report.average_score > config.max_pre_score {\n        return Err(format!(\n            \"pre-grok average circularity {:.3} exceeds threshold {:.3}\",\n            pre_report.average_score, config.max_pre_score\n        ));\n    }\n\n    let passing_pairs = post_report\n        .pair_reports\n        .iter()\n        .filter(|pair| {\n            pair.circularity_score >= config.min_post_score\n                && pair.axis_ratio <= config.max_axis_ratio\n        })\n        .count();\n    if passing_pairs < config.min_pairs_passing {\n        return Err(format!(\n            \"post-grok only {} pairs meet circularity threshold (need {})\",\n            passing_pairs, config.min_pairs_passing\n        ));\n    }\n\n    Ok(PairwiseManifoldTransitionReport {\n        pre_average_score: pre_report.average_score,\n        post_average_score: post_report.average_score,\n        passing_pairs,\n        total_pairs: post_report.pair_reports.len(),\n    })\n}\n\nfn evaluate_pairwise_manifolds_from_signals(\n    signals: &[(usize, Vec<f64>)],\n) -> Result<PairwiseManifoldReport, String> {\n    if signals.len() < 2 {\n        return Err(\"need at least two activation signals\".to_string());\n    }\n\n    let signal_len = signals[0].1.len();\n    if signal_len < 3 {\n        return Err(\"activation signal too short\".to_string());\n    }\n\n    for (idx, signal) in signals {\n        if signal.len() != signal_len {\n            return Err(format!(\n                \"signal length mismatch for neuron {}\",\n                idx\n            ));\n        }\n    }\n\n    let mut reports = Vec::new();\n    for i in 0..signals.len() {\n        for j in (i + 1)..signals.len() {\n            let (neuron_a, signal_a) = &signals[i];\n            let (neuron_b, signal_b) = &signals[j];\n            let points: Vec<(f64, f64)> = signal_a\n                .iter()\n                .zip(signal_b.iter())\n                .map(|(x, y)| (*x, *y))\n                .collect();\n            let (score, radius_cv, axis_ratio) =\n                match pairwise_circularity_metrics(&points) {\n                    Ok(metrics) => metrics,\n                    Err(_) => (0.0, 1.0, f64::INFINITY),\n                };\n            reports.push(PairwiseManifoldPairReport {\n                neuron_a: *neuron_a,\n                neuron_b: *neuron_b,\n                circularity_score: score,\n                radius_cv,\n                axis_ratio,\n            });\n        }\n    }\n\n    let average_score = reports\n        .iter()\n        .map(|pair| pair.circularity_score)\n        .sum::<f64>()\n        / reports.len() as f64;\n\n    Ok(PairwiseManifoldReport {\n        pair_reports: reports,\n        average_score,\n    })\n}\n\nfn pairwise_circularity_metrics(points: &[(f64, f64)]) -> Result<(f64, f64, f64), String> {\n    if points.len() < 3 {\n        return Err(\"not enough points for circularity\".to_string());\n    }\n\n    let n = points.len() as f64;\n    let mean_x = points.iter().map(|(x, _)| *x).sum::<f64>() / n;\n    let mean_y = points.iter().map(|(_, y)| *y).sum::<f64>() / n;\n\n    let mut var_x = 0.0;\n    let mut var_y = 0.0;\n    let mut cov_xy = 0.0;\n    for (x, y) in points {\n        let dx = x - mean_x;\n        let dy = y - mean_y;\n        var_x += dx * dx;\n        var_y += dy * dy;\n        cov_xy += dx * dy;\n    }\n    var_x /= n;\n    var_y /= n;\n    cov_xy /= n;\n\n    let trace = var_x + var_y;\n    let det = var_x * var_y - cov_xy * cov_xy;\n    if det <= 1.0e-12 || trace <= 0.0 {\n        return Err(\"degenerate covariance\".to_string());\n    }\n\n    let half_trace = trace * 0.5;\n    let disc = (half_trace * half_trace - det).max(0.0).sqrt();\n    let lambda1 = half_trace + disc;\n    let lambda2 = half_trace - disc;\n    if lambda2 <= 0.0 {\n        return Err(\"degenerate eigenvalues\".to_string());\n    }\n\n    let axis_ratio = (lambda1 / lambda2).sqrt();\n\n    let mut v1_x = cov_xy;\n    let mut v1_y = lambda1 - var_x;\n    if v1_x.abs() < 1.0e-12 && v1_y.abs() < 1.0e-12 {\n        v1_x = 1.0;\n        v1_y = 0.0;\n    }\n    let norm = (v1_x * v1_x + v1_y * v1_y).sqrt();\n    v1_x /= norm;\n    v1_y /= norm;\n    let v2_x = -v1_y;\n    let v2_y = v1_x;\n\n    let mut radii = Vec::with_capacity(points.len());\n    for (x, y) in points {\n        let dx = x - mean_x;\n        let dy = y - mean_y;\n        let p1 = dx * v1_x + dy * v1_y;\n        let p2 = dx * v2_x + dy * v2_y;\n        let w1 = p1 / lambda1.sqrt();\n        let w2 = p2 / lambda2.sqrt();\n        radii.push((w1 * w1 + w2 * w2).sqrt());\n    }\n\n    let mean_radius = radii.iter().sum::<f64>() / radii.len() as f64;\n    if mean_radius == 0.0 {\n        return Err(\"zero mean radius\".to_string());\n    }\n\n    let mut variance = 0.0;\n    for r in &radii {\n        let diff = r - mean_radius;\n        variance += diff * diff;\n    }\n    let std = (variance / radii.len() as f64).sqrt();\n    let radius_cv = std / mean_radius;\n    let score = 1.0 / (1.0 + radius_cv);\n\n    Ok((score, radius_cv, axis_ratio))\n}\n\nfn baseline_loss(\n    losses: &[(usize, f64)],\n    plateau_min_step: usize,\n    drop_window: usize,\n) -> Result<f64, String> {\n    let before: Vec<f64> = losses\n        .iter()\n        .filter(|(step, _)| *step <= plateau_min_step)\n        .map(|(_, loss)| *loss)\n        .collect();\n\n    if before.len() < drop_window || drop_window == 0 {\n        return Err(\"insufficient loss samples for baseline\".to_string());\n    }\n\n    let window = &before[before.len() - drop_window..];\n    Ok(window.iter().sum::<f64>() / drop_window as f64)\n}\n\nfn find_drop_step(\n    losses: &[(usize, f64)],\n    plateau_min_step: usize,\n    baseline: f64,\n    drop_fraction: f64,\n) -> Result<usize, String> {\n    losses\n        .iter()\n        .filter(|(step, _)| *step >= plateau_min_step)\n        .find(|(_, loss)| *loss <= baseline * (1.0 - drop_fraction))\n        .map(|(step, _)| *step)\n        .ok_or_else(|| \"loss never drops below threshold\".to_string())\n}\n\nfn collect_mlp_post_relu_activations<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n    fixed_y: usize,\n) -> Result<Vec<Vec<f64>>, String> {\n    let modulus = ModularAdditionDataset::modulus();\n    let equals_token = ModularAdditionDataset::equals_token();\n\n    let mut inputs_vec = Vec::with_capacity(modulus * 3);\n    for x in 0..modulus {\n        inputs_vec.push(x as i32);\n        inputs_vec.push(fixed_y as i32);\n        inputs_vec.push(equals_token as i32);\n    }\n\n    let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n        .reshape([modulus, 3]);\n    let (_, mlp_acts) = model.forward_with_mlp_activations(inputs);\n    let [batch_size, d_ff] = mlp_acts.dims();\n\n    if batch_size != modulus {\n        return Err(format!(\n            \"unexpected activation batch size {} (expected {})\",\n            batch_size, modulus\n        ));\n    }\n\n    let data: Vec<f32> = mlp_acts.into_data().to_vec().unwrap();\n    let mut per_neuron = vec![Vec::with_capacity(batch_size); d_ff];\n    for batch in 0..batch_size {\n        for neuron in 0..d_ff {\n            let idx = batch * d_ff + neuron;\n            per_neuron[neuron].push(data[idx] as f64);\n        }\n    }\n\n    Ok(per_neuron)\n}\n\nfn surface_autocorrelation_scores(\n    surface: &[Vec<f64>],\n    max_lag: usize,\n) -> Result<(f64, f64, f64, f64), String> {\n    let (height, width) = surface_dimensions(surface)?;\n    if height < 2 || width < 2 {\n        return Err(\"surface grid too small\".to_string());\n    }\n\n    let (mean, var) = surface_mean_variance(surface)?;\n    if var == 0.0 {\n        return Err(\"surface variance is zero\".to_string());\n    }\n\n    let max_lag = max_lag.min(height.saturating_sub(1)).min(width.saturating_sub(1));\n    if max_lag == 0 {\n        return Err(\"max_lag too small for surface\".to_string());\n    }\n\n    let diag_main = average_autocorrelation(surface, mean, var, max_lag, 1, 1)?;\n    let diag_anti = average_autocorrelation(surface, mean, var, max_lag, 1, -1)?;\n    let axis_x = average_autocorrelation(surface, mean, var, max_lag, 1, 0)?;\n    let axis_y = average_autocorrelation(surface, mean, var, max_lag, 0, 1)?;\n\n    Ok((diag_main, diag_anti, axis_x, axis_y))\n}\n\nfn surface_dimensions(surface: &[Vec<f64>]) -> Result<(usize, usize), String> {\n    let height = surface.len();\n    if height == 0 {\n        return Err(\"surface is empty\".to_string());\n    }\n    let width = surface[0].len();\n    if width == 0 {\n        return Err(\"surface has empty rows\".to_string());\n    }\n    for (idx, row) in surface.iter().enumerate() {\n        if row.len() != width {\n            return Err(format!(\n                \"surface row {} has length {}, expected {}\",\n                idx,\n                row.len(),\n                width\n            ));\n        }\n    }\n    Ok((height, width))\n}\n\nfn surface_mean_variance(surface: &[Vec<f64>]) -> Result<(f64, f64), String> {\n    let (height, width) = surface_dimensions(surface)?;\n    let mut sum = 0.0;\n    for row in surface {\n        for value in row {\n            sum += *value;\n        }\n    }\n    let count = (height * width) as f64;\n    let mean = sum / count;\n    let mut variance = 0.0;\n    for row in surface {\n        for value in row {\n            let diff = *value - mean;\n            variance += diff * diff;\n        }\n    }\n    variance /= count;\n    Ok((mean, variance))\n}\n\nfn average_autocorrelation(\n    surface: &[Vec<f64>],\n    mean: f64,\n    variance: f64,\n    max_lag: usize,\n    dx: i32,\n    dy: i32,\n) -> Result<f64, String> {\n    let (height, width) = surface_dimensions(surface)?;\n    let mut total = 0.0;\n    let mut lag_count = 0usize;\n\n    for lag in 1..=max_lag {\n        let mut sum = 0.0;\n        let mut count = 0usize;\n        let lag = lag as i32;\n        for x in 0..height {\n            for y in 0..width {\n                let x2 = x as i32 + dx * lag;\n                let y2 = y as i32 + dy * lag;\n                if x2 < 0 || y2 < 0 {\n                    continue;\n                }\n                let x2 = x2 as usize;\n                let y2 = y2 as usize;\n                if x2 >= height || y2 >= width {\n                    continue;\n                }\n                let a = surface[x][y] - mean;\n                let b = surface[x2][y2] - mean;\n                sum += a * b;\n                count += 1;\n            }\n        }\n        if count == 0 {\n            continue;\n        }\n        let corr = sum / (count as f64 * variance);\n        total += corr.abs();\n        lag_count += 1;\n    }\n\n    if lag_count == 0 {\n        return Err(\"no valid lags for autocorrelation\".to_string());\n    }\n\n    Ok(total / lag_count as f64)\n}\n\nfn analyze_mlp_activation_wave(\n    neuron_index: usize,\n    signal: &[f64],\n    config: &MlpWaveVerificationConfig,\n) -> Result<MlpWaveNeuronReport, String> {\n    if signal.len() < 3 {\n        return Err(format!(\n            \"activation signal for neuron {} is too short\",\n            neuron_index\n        ));\n    }\n\n    let negative_count = signal\n        .iter()\n        .filter(|value| **value < -config.negative_tolerance)\n        .count();\n    let negative_fraction = negative_count as f64 / signal.len() as f64;\n    if negative_fraction > config.max_negative_fraction {\n        return Err(format!(\n            \"neuron {} has {:.3} negative activations\",\n            neuron_index, negative_fraction\n        ));\n    }\n\n    let (dominant_frequency, dominant_ratio) = dominant_frequency_ratio(signal)?;\n    if dominant_ratio < config.min_dominant_ratio {\n        return Err(format!(\n            \"neuron {} dominant frequency ratio {:.3} below threshold {:.3}\",\n            neuron_index, dominant_ratio, config.min_dominant_ratio\n        ));\n    }\n\n    let best_correlation =\n        best_rectified_sine_correlation(signal, dominant_frequency, config.phase_steps);\n    if best_correlation < config.min_correlation {\n        return Err(format!(\n            \"neuron {} rectified sine correlation {:.3} below threshold {:.3}\",\n            neuron_index, best_correlation, config.min_correlation\n        ));\n    }\n\n    Ok(MlpWaveNeuronReport {\n        neuron_index,\n        dominant_frequency,\n        dominant_ratio,\n        negative_fraction,\n        best_correlation,\n    })\n}\n\nfn dominant_frequency_ratio(signal: &[f64]) -> Result<(usize, f64), String> {\n    if signal.len() < 2 {\n        return Err(\"signal too short for FFT\".to_string());\n    }\n\n    let mut planner = FftPlanner::new();\n    let fft = planner.plan_fft_forward(signal.len());\n    let mut buffer: Vec<Complex<f64>> = signal\n        .iter()\n        .map(|&x| Complex::new(x, 0.0))\n        .collect();\n    fft.process(&mut buffer);\n    let magnitudes: Vec<f64> = buffer.iter().map(|c| c.norm()).collect();\n\n    if magnitudes.len() < 2 {\n        return Err(\"insufficient FFT bins\".to_string());\n    }\n\n    let mut dominant_idx = 1usize;\n    let mut dominant_mag = magnitudes[1];\n    for (idx, &mag) in magnitudes.iter().enumerate().skip(2) {\n        if mag > dominant_mag {\n            dominant_mag = mag;\n            dominant_idx = idx;\n        }\n    }\n\n    if dominant_idx == 0 {\n        return Err(\"dominant frequency is zero\".to_string());\n    }\n\n    let mut nonzero = magnitudes[1..].to_vec();\n    nonzero.sort_by(|a, b| a.partial_cmp(b).unwrap_or(std::cmp::Ordering::Equal));\n    let median_mag = nonzero[nonzero.len() / 2];\n    let ratio = if median_mag == 0.0 {\n        dominant_mag\n    } else {\n        dominant_mag / median_mag\n    };\n\n    Ok((dominant_idx, ratio))\n}\n\nfn best_rectified_sine_correlation(signal: &[f64], frequency: usize, phase_steps: usize) -> f64 {\n    if signal.is_empty() || frequency == 0 || phase_steps == 0 {\n        return 0.0;\n    }\n\n    let tau = std::f64::consts::PI * 2.0;\n    let mut best = -1.0;\n    for step in 0..phase_steps {\n        let phase = tau * step as f64 / phase_steps as f64;\n        let wave: Vec<f64> = (0..signal.len())\n            .map(|idx| {\n                let angle = tau * frequency as f64 * idx as f64 / signal.len() as f64 + phase;\n                angle.sin().max(0.0)\n            })\n            .collect();\n        let corr = pearson_correlation(signal, &wave);\n        if corr > best {\n            best = corr;\n        }\n    }\n\n    best\n}\n\nfn pearson_correlation(a: &[f64], b: &[f64]) -> f64 {\n    if a.len() != b.len() || a.is_empty() {\n        return 0.0;\n    }\n\n    let mean_a = a.iter().sum::<f64>() / a.len() as f64;\n    let mean_b = b.iter().sum::<f64>() / b.len() as f64;\n\n    let mut num = 0.0;\n    let mut denom_a = 0.0;\n    let mut denom_b = 0.0;\n    for (x, y) in a.iter().zip(b.iter()) {\n        let da = x - mean_a;\n        let db = y - mean_b;\n        num += da * db;\n        denom_a += da * da;\n        denom_b += db * db;\n    }\n\n    if denom_a == 0.0 || denom_b == 0.0 {\n        return 0.0;\n    }\n\n    num / (denom_a.sqrt() * denom_b.sqrt())\n}\n\n/// Verify that the model actually learned modular addition\n/// by testing it on ALL possible examples systematically\npub fn verify_full_accuracy<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n) -> (f32, f32) {\n    println!(\"ðŸ” Verifying model on COMPLETE dataset (no sampling)...\");\n\n    let train_dataset = ModularAdditionDataset::new(true, 42);\n    let val_dataset = ModularAdditionDataset::new(false, 42);\n\n    let train_acc = test_all_examples(model, &train_dataset, device);\n    let val_acc = test_all_examples(model, &val_dataset, device);\n\n    println!(\"ðŸ“Š FULL Dataset Results:\");\n    println!(\"   Training: {}/{} = {:.2}%\",\n        (train_acc * train_dataset.len() as f32) as usize,\n        train_dataset.len(),\n        train_acc * 100.0);\n    println!(\"   Validation: {}/{} = {:.2}%\",\n        (val_acc * val_dataset.len() as f32) as usize,\n        val_dataset.len(),\n        val_acc * 100.0);\n\n    (train_acc, val_acc)\n}\n\n/// Test model on ALL examples in dataset (no random sampling)\nfn test_all_examples<B: Backend>(\n    model: &Transformer<B>,\n    dataset: &ModularAdditionDataset,\n    device: &B::Device,\n) -> f32 {\n    let mut total_correct = 0;\n    let total = dataset.len();\n\n    // Test in batches to avoid memory issues\n    let batch_size = 100;\n    for batch_start in (0..total).step_by(batch_size) {\n        let batch_end = (batch_start + batch_size).min(total);\n        let batch_len = batch_end - batch_start;\n\n        let mut inputs_vec = Vec::new();\n        let mut targets_vec = Vec::new();\n\n        for idx in batch_start..batch_end {\n            let (input, target) = dataset.get(idx).unwrap();\n            inputs_vec.extend(input);\n            targets_vec.push(target as i32);\n        }\n\n        let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n            .reshape([batch_len, 3]);\n\n        let logits = model.forward(inputs);\n        let predictions = logits.argmax(1).squeeze::<1>();\n        let predictions_vec: Vec<i32> = predictions.into_data().to_vec().unwrap();\n\n        let correct = predictions_vec\n            .iter()\n            .zip(targets_vec.iter())\n            .filter(|(p, t)| p == t)\n            .count();\n\n        total_correct += correct;\n    }\n\n    total_correct as f32 / total as f32\n}\n\n/// Verify that weight norms decrease over time with weight decay\n/// and show correlation with the grokking transition\npub fn verify_weight_norm_decay(\n    weight_norm_history: &[(usize, f64)],\n    grok_step: Option<usize>,\n    config: &WeightNormDecayConfig,\n) -> Result<WeightNormDecayReport, String> {\n    if weight_norm_history.len() < 2 {\n        return Err(\"weight norm history requires at least 2 snapshots\".to_string());\n    }\n\n    let initial_norm = weight_norm_history\n        .first()\n        .ok_or_else(|| \"weight norm history is empty\".to_string())?\n        .1;\n    let final_norm = weight_norm_history\n        .last()\n        .ok_or_else(|| \"weight norm history is empty\".to_string())?\n        .1;\n\n    if initial_norm <= 0.0 {\n        return Err(\"initial weight norm must be positive\".to_string());\n    }\n\n    // Check that norms are generally decreasing (with weight decay)\n    let relative_decrease = (initial_norm - final_norm) / initial_norm;\n    if relative_decrease < config.min_relative_decrease {\n        return Err(format!(\n            \"weight norm decrease {:.4} below threshold {:.4} (initial {:.4}, final {:.4})\",\n            relative_decrease, config.min_relative_decrease, initial_norm, final_norm\n        ));\n    }\n\n    // Check that norms are not increasing (weight decay should monotonically decrease norms)\n    for window in weight_norm_history.windows(2) {\n        let prev_norm = window[0].1;\n        let next_norm = window[1].1;\n        if next_norm > prev_norm * 1.01 {\n            // Allow 1% tolerance for numerical noise\n            return Err(format!(\n                \"weight norm increased from {:.4} to {:.4} at step {}\",\n                prev_norm, next_norm, window[1].0\n            ));\n        }\n    }\n\n    // If grok step is provided, check that norms show a shift around grokking\n    let grok_step_norm = if let Some(step) = grok_step {\n        weight_norm_history\n            .iter()\n            .filter(|(s, _)| {\n                *s >= step.saturating_sub(config.grok_step_window)\n                    && *s <= step + config.grok_step_window\n            })\n            .map(|(_, norm)| *norm)\n            .next()\n    } else {\n        None\n    };\n\n    Ok(WeightNormDecayReport {\n        initial_norm,\n        final_norm,\n        relative_decrease,\n        grok_step_norm,\n    })\n}\n\n/// Verify the characteristic \"snake curve\" shape of grokking phenomenon\n/// on log-scale plots: train accuracy rises early, validation stays flat then jumps\npub fn verify_snake_curve_shape(\n    train_acc: &[(usize, f32)],\n    val_acc: &[(usize, f32)],\n    config: &SnakeCurveConfig,\n) -> Result<SnakeCurveReport, String> {\n    if train_acc.is_empty() || val_acc.is_empty() {\n        return Err(\"empty accuracy history\".to_string());\n    }\n\n    // Phase 1: Verify early train convergence (immediate drop on log scale)\n    let train_converged_step = train_acc\n        .iter()\n        .find(|(_, acc)| *acc >= config.train_convergence_threshold)\n        .map(|(step, _)| *step)\n        .ok_or_else(|| {\n            format!(\n                \"train accuracy never reaches {} threshold\",\n                config.train_convergence_threshold\n            )\n        })?;\n\n    if train_converged_step > config.early_step_max {\n        return Err(format!(\n            \"train accuracy converges at step {}, expected before {}\",\n            train_converged_step, config.early_step_max\n        ));\n    }\n\n    // Phase 2: Verify validation plateau (flat on log scale)\n    let plateau_samples: Vec<f32> = val_acc\n        .iter()\n        .filter(|(step, _)| *step >= 1 && *step <= config.val_plateau_min_step)\n        .map(|(_, acc)| *acc)\n        .collect();\n\n    if plateau_samples.is_empty() {\n        return Err(\"no validation samples in plateau region\".to_string());\n    }\n\n    let val_plateau_max = plateau_samples\n        .iter()\n        .fold(0.0f32, |max_val, &acc| max_val.max(acc));\n\n    if val_plateau_max > config.val_plateau_max_acc {\n        return Err(format!(\n            \"validation accuracy in plateau region too high: max {:.4} exceeds {:.4}\",\n            val_plateau_max, config.val_plateau_max_acc\n        ));\n    }\n\n    // Phase 3: Verify delayed validation jump (crash up on log scale)\n    let val_grok_step = val_acc\n        .iter()\n        .filter(|(step, _)| *step > config.val_plateau_min_step)\n        .find(|(_, acc)| *acc >= config.val_final_threshold)\n        .map(|(step, _)| *step)\n        .ok_or_else(|| {\n            format!(\n                \"validation accuracy never reaches {} after plateau\",\n                config.val_final_threshold\n            )\n        })?;\n\n    // Phase 4: Verify phase ordering (snake shape: early train, late val)\n    let curve_shape_valid = train_converged_step < config.val_plateau_min_step\n        && val_grok_step > config.val_plateau_min_step\n        && train_converged_step < val_grok_step;\n\n    if !curve_shape_valid {\n        return Err(format!(\n            \"invalid curve shape: train converged at {}, val plateau until {}, val grokked at {}\",\n            train_converged_step, config.val_plateau_min_step, val_grok_step\n        ));\n    }\n\n    Ok(SnakeCurveReport {\n        train_converged_by_step: train_converged_step,\n        val_plateau_max_acc: val_plateau_max,\n        val_grok_step,\n        curve_shape_valid,\n    })\n}\n\n/// Test specific examples to see if model actually computes mod p\npub fn test_specific_examples<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n) {\n    println!();\n    println!(\"ðŸ§ª Testing specific examples:\");\n\n    let modulus = ModularAdditionDataset::modulus();\n    let equals_token = ModularAdditionDataset::equals_token();\n    let test_cases = vec![\n        (0, 0, 0),\n        (1, 1, 2),\n        (50, 50, (50 + 50) % modulus),\n        (112, 1, (112 + 1) % modulus),\n        (56, 57, (56 + 57) % modulus),\n        (112, 112, (112 + 112) % modulus),\n        (10, 20, 30),\n        (60, 60, (60 + 60) % modulus),\n    ];\n\n    for (a, b, expected) in test_cases {\n        let input_vec = vec![a as i32, b as i32, equals_token as i32];\n        let input = Tensor::<B, 1, Int>::from_ints(input_vec.as_slice(), device)\n            .reshape([1, 3]);\n\n        let logits = model.forward(input);\n        let prediction = logits.argmax(1).squeeze::<1>();\n        let pred_value: i32 = prediction.into_data().to_vec().unwrap()[0];\n\n        let correct = if pred_value == expected as i32 { \"âœ“\" } else { \"âœ—\" };\n        println!(\n            \"   {} + {} mod {} = {} | Model: {} {}\",\n            a, b, modulus, expected, pred_value, correct\n        );\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    fn make_histories(\n        train_acc: &[(usize, f32)],\n        val_acc: &[(usize, f32)],\n        val_loss: &[(usize, f64)],\n    ) -> (LossHistory, AccuracyHistory) {\n        let mut loss_history = LossHistory::new();\n        loss_history.val_snapshots = val_loss.to_vec();\n        let mut accuracy_history = AccuracyHistory::new();\n        accuracy_history.train_snapshots = train_acc.to_vec();\n        accuracy_history.val_snapshots = val_acc.to_vec();\n        (loss_history, accuracy_history)\n    }\n\n    #[test]\n    fn grokking_phase_verification_passes_on_synthetic_signal() {\n        let train_acc = vec![(0, 0.2), (500, 0.995), (1000, 0.999)];\n        let val_acc = vec![\n            (0, 0.01),\n            (500, 0.01),\n            (1500, 0.01),\n            (2200, 0.995),\n            (2500, 0.999),\n        ];\n        let val_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 2.0),\n            (2200, 0.2),\n            (2500, 0.15),\n        ];\n\n        let (loss_history, accuracy_history) = make_histories(&train_acc, &val_acc, &val_loss);\n        let config = GrokkingVerificationConfig {\n            early_step_max: 1000,\n            plateau_min_step: 1500,\n            generalization_window: 1500,\n            accuracy_window: 2,\n            loss_window: 2,\n            ..GrokkingVerificationConfig::default_for_modulus(ModularAdditionDataset::modulus())\n        };\n\n        let report = verify_grokking_phase_transition(&loss_history, &accuracy_history, &config)\n            .expect(\"expected grokking verification to pass\");\n        assert!(report.transition_step >= config.plateau_min_step);\n        assert!(report.early_train_step <= config.early_step_max);\n    }\n\n    #[test]\n    fn grokking_phase_verification_fails_without_transition() {\n        let train_acc = vec![(0, 0.2), (500, 0.995)];\n        let val_acc = vec![(0, 0.01), (500, 0.01), (1500, 0.02)];\n        let val_loss = vec![(0, 2.0), (500, 2.0), (1500, 1.9)];\n\n        let (loss_history, accuracy_history) = make_histories(&train_acc, &val_acc, &val_loss);\n        let config = GrokkingVerificationConfig {\n            early_step_max: 1000,\n            plateau_min_step: 1000,\n            generalization_window: 1000,\n            accuracy_window: 2,\n            loss_window: 2,\n            ..GrokkingVerificationConfig::default_for_modulus(ModularAdditionDataset::modulus())\n        };\n\n        let err = verify_grokking_phase_transition(&loss_history, &accuracy_history, &config)\n            .expect_err(\"expected grokking verification to fail\");\n        assert!(err.contains(\"validation accuracy never reaches target\"));\n    }\n\n    #[test]\n    fn grokking_phase_verification_fails_with_slow_accuracy_rise() {\n        let train_acc = vec![(0, 0.2), (500, 0.995), (1000, 0.999)];\n        let val_acc = vec![\n            (0, 0.01),\n            (500, 0.01),\n            (1500, 0.5),\n            (2000, 0.6),\n            (2200, 0.995),\n            (2500, 0.999),\n        ];\n        let val_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 2.0),\n            (2200, 0.2),\n            (2500, 0.15),\n        ];\n\n        let (loss_history, accuracy_history) = make_histories(&train_acc, &val_acc, &val_loss);\n        let config = GrokkingVerificationConfig {\n            early_step_max: 1000,\n            plateau_min_step: 1000,\n            generalization_window: 2000,\n            accuracy_window: 2,\n            loss_window: 2,\n            ..GrokkingVerificationConfig::default_for_modulus(ModularAdditionDataset::modulus())\n        };\n\n        let err = verify_grokking_phase_transition(&loss_history, &accuracy_history, &config)\n            .expect_err(\"expected grokking verification to fail\");\n        assert!(err.contains(\"plateau too high\"));\n    }\n\n    #[test]\n    fn restricted_loss_detection_passes_with_early_drop() {\n        let full_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 2.0),\n            (2500, 2.0),\n            (3500, 0.3),\n        ];\n        let restricted_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 0.8),\n            (2500, 0.4),\n            (3500, 0.3),\n        ];\n\n        let config = RestrictedLossVerificationConfig {\n            plateau_min_step: 1000,\n            drop_window: 2,\n            drop_fraction: 0.5,\n            min_step_lead: 500,\n        };\n\n        let report = verify_restricted_loss_early_drop(&full_loss, &restricted_loss, &config)\n            .expect(\"expected restricted loss verification to pass\");\n        assert!(report.restricted_drop_step < report.full_drop_step);\n    }\n\n    #[test]\n    fn restricted_loss_detection_fails_when_full_drops_first() {\n        let full_loss = vec![(0, 2.0), (1000, 1.0), (1500, 0.8)];\n        let restricted_loss = vec![(0, 2.0), (1000, 2.0), (1500, 0.8)];\n\n        let config = RestrictedLossVerificationConfig {\n            plateau_min_step: 500,\n            drop_window: 1,\n            drop_fraction: 0.3,\n            min_step_lead: 200,\n        };\n\n        let err = verify_restricted_loss_early_drop(&full_loss, &restricted_loss, &config)\n            .expect_err(\"expected restricted loss verification to fail\");\n        assert!(err.contains(\"restricted loss drops\"));\n    }\n\n    #[test]\n    fn excluded_loss_spike_verification_passes() {\n        let excluded_loss = vec![(0, 1.0), (1500, 2.2), (3000, 1.1)];\n        let config = ExcludedLossSpikeConfig {\n            min_relative_increase: 0.5,\n        };\n\n        let report = verify_excluded_loss_spike(&excluded_loss, &config)\n            .expect(\"expected excluded loss spike verification to pass\");\n        assert_eq!(report.spike_step, 1500);\n    }\n\n    #[test]\n    fn excluded_loss_spike_verification_fails_without_spike() {\n        let excluded_loss = vec![(0, 1.0), (1500, 1.1), (3000, 1.0)];\n        let config = ExcludedLossSpikeConfig {\n            min_relative_increase: 0.5,\n        };\n\n        let err = verify_excluded_loss_spike(&excluded_loss, &config)\n            .expect_err(\"expected excluded loss spike verification to fail\");\n        assert!(err.contains(\"excluded loss spike too small\"));\n    }\n\n    #[test]\n    fn restricted_loss_drop_verification_passes() {\n        let restricted_loss = vec![\n            (0, 2.0),\n            (1000, 1.8),\n            (5000, 1.2),\n            (7000, 0.8),\n            (10000, 0.9),\n        ];\n        let config = RestrictedLossDropConfig {\n            min_relative_decrease: 0.3,\n        };\n\n        let report = verify_restricted_loss_drop(&restricted_loss, &config)\n            .expect(\"expected restricted loss drop verification to pass\");\n        assert_eq!(report.drop_step, 7000);\n        assert!(report.baseline_loss > report.drop_loss);\n        assert!((report.baseline_loss - report.drop_loss) / report.baseline_loss >= 0.3);\n    }\n\n    #[test]\n    fn restricted_loss_drop_verification_fails_without_drop() {\n        let restricted_loss = vec![\n            (0, 2.0),\n            (1000, 2.1),\n            (5000, 1.9),\n            (7000, 2.0),\n            (10000, 2.05),\n        ];\n        let config = RestrictedLossDropConfig {\n            min_relative_decrease: 0.3,\n        };\n\n        let err = verify_restricted_loss_drop(&restricted_loss, &config)\n            .expect_err(\"expected restricted loss drop verification to fail\");\n        assert!(err.contains(\"restricted loss decrease too small\"));\n    }\n\n    #[test]\n    fn mlp_activation_wave_verification_passes_on_rectified_sine() {\n        let n = ModularAdditionDataset::modulus();\n        let freq = 5.0;\n        let mut signal = Vec::with_capacity(n);\n        for idx in 0..n {\n            let angle = 2.0 * std::f64::consts::PI * freq * idx as f64 / n as f64;\n            let rectified = angle.sin().max(0.0);\n            let wobble = (2.0 * std::f64::consts::PI * 2.0 * idx as f64 / n as f64)\n                .sin()\n                .abs()\n                * 0.02;\n            signal.push(rectified + wobble);\n        }\n\n        let config = MlpWaveVerificationConfig {\n            max_negative_fraction: 0.0,\n            negative_tolerance: 1e-6,\n            min_dominant_ratio: 3.0,\n            min_correlation: 0.7,\n            phase_steps: 32,\n        };\n\n        let report =\n            verify_mlp_activation_waves_from_signals(&[(0, signal)], &config)\n                .expect(\"expected rectified sine to pass\");\n        assert_eq!(report.neuron_reports.len(), 1);\n    }\n\n    #[test]\n    fn mlp_activation_wave_verification_fails_on_noise() {\n        let n = ModularAdditionDataset::modulus();\n        let signal: Vec<f64> = (0..n).map(|idx| idx as f64 / n as f64).collect();\n\n        let config = MlpWaveVerificationConfig {\n            max_negative_fraction: 0.0,\n            negative_tolerance: 1e-6,\n            min_dominant_ratio: 4.0,\n            min_correlation: 0.8,\n            phase_steps: 16,\n        };\n\n        let err =\n            verify_mlp_activation_waves_from_signals(&[(0, signal)], &config)\n                .expect_err(\"expected linear signal to fail\");\n        assert!(err.contains(\"dominant frequency ratio\") || err.contains(\"correlation\"));\n    }\n\n    #[test]\n    fn pairwise_manifold_transition_passes_on_elliptic_signals() {\n        let n = ModularAdditionDataset::modulus();\n        let freq = 5.0;\n        let mut post_signals = Vec::new();\n        for neuron_idx in 0..7usize {\n            let phase = 2.0 * std::f64::consts::PI * neuron_idx as f64 / 7.0;\n            let signal: Vec<f64> = (0..n)\n                .map(|idx| {\n                    let angle = 2.0 * std::f64::consts::PI * freq * idx as f64 / n as f64 + phase;\n                    angle.sin()\n                })\n                .collect();\n            post_signals.push((neuron_idx, signal));\n        }\n\n        let mut pre_signals = Vec::new();\n        for neuron_idx in 0..7usize {\n            let signal: Vec<f64> = (0..n)\n                .map(|idx| idx as f64 / n as f64 + neuron_idx as f64 * 0.01)\n                .collect();\n            pre_signals.push((neuron_idx, signal));\n        }\n\n        let config = PairwiseManifoldConfig {\n            fixed_y: 0,\n            neuron_indices: (0..7).collect(),\n            min_post_score: 0.75,\n            max_pre_score: 0.55,\n            max_axis_ratio: 8.0,\n            min_pairs_passing: 8,\n        };\n\n        let report = verify_pairwise_manifold_transition_from_signals(\n            &pre_signals,\n            &post_signals,\n            &config,\n        )\n        .expect(\"expected pairwise manifold transition to pass\");\n        assert!(report.passing_pairs >= config.min_pairs_passing);\n    }\n\n    #[test]\n    fn pairwise_manifold_transition_fails_without_rings() {\n        let n = ModularAdditionDataset::modulus();\n        let mut signals = Vec::new();\n        for neuron_idx in 0..7usize {\n            let signal: Vec<f64> = (0..n)\n                .map(|idx| idx as f64 / n as f64 + neuron_idx as f64 * 0.02)\n                .collect();\n            signals.push((neuron_idx, signal));\n        }\n\n        let config = PairwiseManifoldConfig {\n            fixed_y: 0,\n            neuron_indices: (0..7).collect(),\n            min_post_score: 0.8,\n            max_pre_score: 0.55,\n            max_axis_ratio: 4.0,\n            min_pairs_passing: 5,\n        };\n\n        let err = verify_pairwise_manifold_transition_from_signals(\n            &signals,\n            &signals,\n            &config,\n        )\n        .expect_err(\"expected pairwise manifold transition to fail\");\n        assert!(err.contains(\"post-grok\") || err.contains(\"pre-grok\"));\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_passes_on_diagonal_corrugation() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create diagonal ridges: f(x,y) = sin(2Ï€ * freq * (x+y) / n)\n        let freq = 3.0;\n        for x in 0..n {\n            for y in 0..n {\n                let angle = 2.0 * std::f64::consts::PI * freq * (x + y) as f64 / n as f64;\n                surface[x][y] = angle.sin();\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let report = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect(\"expected diagonal ridge verification to pass\");\n        assert!(report.diagonal_score > report.axis_score);\n        assert!(report.ratio >= config.min_diagonal_ratio);\n        assert!(report.diagonal_score >= config.min_diagonal_score);\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_passes_on_anti_diagonal_pattern() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create anti-diagonal ridges: f(x,y) = sin(2Ï€ * freq * (x-y) / n)\n        let freq = 3.0;\n        for x in 0..n {\n            for y in 0..n {\n                let angle = 2.0 * std::f64::consts::PI * freq * (x as i32 - y as i32) as f64 / n as f64;\n                surface[x][y] = angle.sin();\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let report = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect(\"expected anti-diagonal ridge verification to pass\");\n        assert!(report.diagonal_score > report.axis_score);\n        assert!(report.ratio >= config.min_diagonal_ratio);\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_fails_on_axis_aligned_pattern() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create axis-aligned ridges: f(x,y) = sin(2Ï€ * freq * x / n)\n        let freq = 3.0;\n        for x in 0..n {\n            for y in 0..n {\n                let angle = 2.0 * std::f64::consts::PI * freq * x as f64 / n as f64;\n                surface[x][y] = angle.sin();\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let err = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect_err(\"expected axis-aligned pattern to fail diagonal verification\");\n        assert!(err.contains(\"diagonal/axis ratio\") || err.contains(\"diagonal autocorrelation\"));\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_fails_on_flat_surface() {\n        let n = 50;\n        let surface = vec![vec![1.5; n]; n];\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let err = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect_err(\"expected flat surface to fail\");\n        assert!(err.contains(\"surface variance is zero\"));\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_fails_on_random_noise() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create random noise (low autocorrelation everywhere)\n        for x in 0..n {\n            for y in 0..n {\n                surface[x][y] = ((x * 17 + y * 31) % 100) as f64 / 100.0;\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let err = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect_err(\"expected random noise to fail diagonal verification\");\n        assert!(err.contains(\"diagonal/axis ratio\") || err.contains(\"diagonal autocorrelation\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_passes_on_valid_grokking_curve() {\n        // Simulate typical grokking: train converges early, val plateaus then jumps\n        let train_acc = vec![\n            (1, 0.10),\n            (10, 0.50),\n            (100, 0.80),\n            (500, 0.98),\n            (1000, 0.99),\n            (2000, 0.995),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (10, 0.009),\n            (100, 0.01),\n            (500, 0.011),\n            (1000, 0.009),\n            (1500, 0.01),\n            (2000, 0.92),\n            (3000, 0.98),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 1500,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let report = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect(\"expected snake curve verification to pass\");\n\n        assert_eq!(report.train_converged_by_step, 500);\n        assert!(report.val_plateau_max_acc < config.val_plateau_max_acc);\n        assert_eq!(report.val_grok_step, 2000);\n        assert!(report.curve_shape_valid);\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_on_simultaneous_convergence() {\n        // Both train and val converge at the same time (no grokking)\n        let train_acc = vec![\n            (1, 0.10),\n            (100, 0.98),\n            (500, 0.99),\n        ];\n        let val_acc = vec![\n            (1, 0.10),\n            (100, 0.97),\n            (500, 0.98),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 200,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let err = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect_err(\"expected snake curve verification to fail\");\n        assert!(err.contains(\"plateau region too high\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_on_slow_train_convergence() {\n        // Train takes too long to converge (not the typical snake curve)\n        let train_acc = vec![\n            (1, 0.10),\n            (500, 0.50),\n            (1000, 0.70),\n            (2000, 0.98),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (500, 0.01),\n            (1000, 0.01),\n            (2000, 0.92),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 1500,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let err = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect_err(\"expected snake curve verification to fail\");\n        assert!(err.contains(\"converges at step 2000\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_without_val_jump() {\n        // Validation never jumps up (no generalization)\n        let train_acc = vec![\n            (1, 0.10),\n            (100, 0.98),\n            (500, 0.99),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (100, 0.01),\n            (500, 0.01),\n            (2000, 0.02),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 200,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let err = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect_err(\"expected snake curve verification to fail\");\n        assert!(err.contains(\"validation accuracy never reaches\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_on_wrong_phase_ordering() {\n        // Val jumps before train converges (backwards snake)\n        let train_acc = vec![\n            (1, 0.10),\n            (500, 0.50),\n            (2000, 0.98),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (100, 0.95),\n            (500, 0.98),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 200,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let result = verify_snake_curve_shape(&train_acc, &val_acc, &config);\n        // Should fail because val jumps before train converges (not a proper snake curve)\n        // This can fail for multiple reasons, but should definitely fail\n        assert!(result.is_err(), \"Expected snake curve verification to fail but it passed\");\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_passes_on_decreasing_norms() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 85.0),\n            (1000, 72.0),\n            (1500, 63.0),\n            (2000, 58.0),\n            (2500, 55.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let report = verify_weight_norm_decay(&weight_norms, Some(2000), &config)\n            .expect(\"expected weight norm decay verification to pass\");\n\n        assert_eq!(report.initial_norm, 100.0);\n        assert_eq!(report.final_norm, 55.0);\n        assert!((report.relative_decrease - 0.45).abs() < 0.01);\n        assert!(report.grok_step_norm.is_some());\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_fails_on_flat_norms() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 99.0),\n            (1000, 98.5),\n            (1500, 98.0),\n            (2000, 97.5),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let err = verify_weight_norm_decay(&weight_norms, None, &config)\n            .expect_err(\"expected weight norm decay verification to fail\");\n        assert!(err.contains(\"weight norm decrease\"));\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_fails_on_increasing_norms() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 95.0),\n            (1000, 105.0), // Increase here\n            (1500, 90.0),\n            (2000, 85.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let err = verify_weight_norm_decay(&weight_norms, None, &config)\n            .expect_err(\"expected weight norm decay verification to fail\");\n        assert!(err.contains(\"weight norm increased\"));\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_allows_small_noise() {\n        // Weight decay with small numerical noise (within 1% tolerance)\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 95.0),\n            (1000, 90.5),  // Small increase within tolerance\n            (1500, 90.0),\n            (2000, 85.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let report = verify_weight_norm_decay(&weight_norms, None, &config)\n            .expect(\"expected weight norm decay with small noise to pass\");\n\n        assert_eq!(report.initial_norm, 100.0);\n        assert_eq!(report.final_norm, 85.0);\n        assert!((report.relative_decrease - 0.15).abs() < 0.01);\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_finds_grok_step_norm() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (1000, 80.0),\n            (1800, 68.0),  // Within window of grok step (2200 - 500 = 1700)\n            (2000, 65.0),\n            (2200, 62.0),  // Around grok step\n            (3000, 55.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let report = verify_weight_norm_decay(&weight_norms, Some(2200), &config)\n            .expect(\"expected weight norm decay verification to pass\");\n\n        assert!(report.grok_step_norm.is_some());\n        let grok_norm = report.grok_step_norm.unwrap();\n        // Should find the first norm in the window [1700, 2700], which is 68.0 at step 1800\n        assert!((grok_norm - 68.0).abs() < 0.01);\n    }\n}\n","structuredPatch":[{"oldStart":1349,"oldLines":6,"newStart":1349,"newLines":8,"lines":[" "," /// Verify the characteristic \"snake curve\" shape of grokking phenomenon"," /// on log-scale plots: train accuracy rises early, validation stays flat then jumps","+/// NOTE: Verification function reserved for future integration or manual analysis","+#[allow(dead_code)]"," pub fn verify_snake_curve_shape(","     train_acc: &[(usize, f32)],","     val_acc: &[(usize, f32)],"]}],"userModified":false,"replaceAll":false}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01HFY9LHSw9HfzDuxbBj5dXd","type":"message","role":"assistant","content":[{"type":"text","text":"Now let me mark the helper functions used by these verification functions:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":812,"cache_read_input_tokens":60304,"cache_creation":{"ephemeral_5m_input_tokens":812,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"4e589f27-fc31-4584-839f-435f05d84366"}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01HFY9LHSw9HfzDuxbBj5dXd","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01GtLVssHZsAzeR58pHf3ybz","name":"Bash","input":{"command":"grep -n \"^fn baseline_loss\\|^fn find_drop_step\\|^fn surface_autocorrelation_scores\\|^fn surface_dimensions\\|^fn surface_mean_variance\\|^fn average_autocorrelation\\|^fn analyze_mlp_activation_wave\\|^fn dominant_frequency_ratio\\|^fn best_rectified_sine_correlation\\|^fn pearson_correlation\" src/verify.rs","description":"Find helper functions"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":812,"cache_read_input_tokens":60304,"cache_creation":{"ephemeral_5m_input_tokens":812,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"70ef5bda-ddae-4cba-8f4a-a70bae7a2944"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01GtLVssHZsAzeR58pHf3ybz","type":"tool_result","content":"878:fn baseline_loss(\n897:fn find_drop_step(\n950:fn surface_autocorrelation_scores(\n977:fn surface_dimensions(surface: &[Vec<f64>]) -> Result<(usize, usize), String> {\n999:fn surface_mean_variance(surface: &[Vec<f64>]) -> Result<(f64, f64), String> {\n1020:fn average_autocorrelation(\n1069:fn analyze_mlp_activation_wave(\n1119:fn dominant_frequency_ratio(signal: &[f64]) -> Result<(usize, f64), String> {\n1162:fn best_rectified_sine_correlation(signal: &[f64], frequency: usize, phase_steps: usize) -> f64 {\n1186:fn pearson_correlation(a: &[f64], b: &[f64]) -> f64 {","is_error":false}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"df12e804-299f-4669-bc5d-8a0f3cae6352","tool_use_result":{"stdout":"878:fn baseline_loss(\n897:fn find_drop_step(\n950:fn surface_autocorrelation_scores(\n977:fn surface_dimensions(surface: &[Vec<f64>]) -> Result<(usize, usize), String> {\n999:fn surface_mean_variance(surface: &[Vec<f64>]) -> Result<(f64, f64), String> {\n1020:fn average_autocorrelation(\n1069:fn analyze_mlp_activation_wave(\n1119:fn dominant_frequency_ratio(signal: &[f64]) -> Result<(usize, f64), String> {\n1162:fn best_rectified_sine_correlation(signal: &[f64], frequency: usize, phase_steps: usize) -> f64 {\n1186:fn pearson_correlation(a: &[f64], b: &[f64]) -> f64 {","stderr":"","interrupted":false,"isImage":false}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01JYDmkwV293LHqrxDYFWNp1","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01BfzZeqCkWV8C8r1vSu8mdy","name":"Edit","input":{"replace_all":false,"file_path":"/Users/admin/MechInt/src/verify.rs","old_string":"fn baseline_loss(","new_string":"// Helper functions for verification - reserved for future integration\n#[allow(dead_code)]\nfn baseline_loss("}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":862,"cache_read_input_tokens":61116,"cache_creation":{"ephemeral_5m_input_tokens":862,"ephemeral_1h_input_tokens":0},"output_tokens":151,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"d87ab2b8-52d2-4414-965e-23662a57adb5"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01BfzZeqCkWV8C8r1vSu8mdy","type":"tool_result","content":"The file /Users/admin/MechInt/src/verify.rs has been updated. Here's the result of running `cat -n` on a snippet of the edited file:\n   874â†’\n   875â†’    Ok((score, radius_cv, axis_ratio))\n   876â†’}\n   877â†’\n   878â†’// Helper functions for verification - reserved for future integration\n   879â†’#[allow(dead_code)]\n   880â†’fn baseline_loss(\n   881â†’    losses: &[(usize, f64)],\n   882â†’    plateau_min_step: usize,\n   883â†’    drop_window: usize,\n   884â†’) -> Result<f64, String> {"}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"59ce24bb-b4b3-4364-a5b5-488d7675c322","tool_use_result":{"filePath":"/Users/admin/MechInt/src/verify.rs","oldString":"fn baseline_loss(","newString":"// Helper functions for verification - reserved for future integration\n#[allow(dead_code)]\nfn baseline_loss(","originalFile":"use burn::{\n    data::dataset::Dataset,\n    tensor::{backend::Backend, Int, Tensor},\n};\nuse rustfft::{num_complex::Complex, FftPlanner};\n\nuse crate::analysis::{AccuracyHistory, LossHistory};\nuse crate::data::ModularAdditionDataset;\nuse crate::model::Transformer;\n\n#[derive(Debug, Clone)]\npub struct GrokkingVerificationConfig {\n    pub early_train_acc_threshold: f32,\n    pub early_step_max: usize,\n    pub plateau_min_step: usize,\n    pub generalization_window: usize,\n    pub chance_accuracy: f32,\n    pub chance_tolerance: f32,\n    pub target_val_acc_threshold: f32,\n    pub accuracy_window: usize,\n    pub loss_window: usize,\n    pub loss_drop_fraction: f64,\n}\n\nimpl GrokkingVerificationConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        Self {\n            early_train_acc_threshold: 0.99,\n            early_step_max: 1000,\n            plateau_min_step: 1000,\n            generalization_window: 10_000,\n            chance_accuracy: 1.0 / modulus as f32,\n            chance_tolerance: 0.02,\n            target_val_acc_threshold: 0.99,\n            accuracy_window: 5,\n            loss_window: 5,\n            loss_drop_fraction: 0.5,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct GrokkingPhaseReport {\n    pub early_train_step: usize,\n    pub transition_step: usize,\n    pub plateau_max_val_acc: f32,\n    pub loss_drop_ratio: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossVerificationConfig {\n    pub plateau_min_step: usize,\n    pub drop_window: usize,\n    pub drop_fraction: f64,\n    pub min_step_lead: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossReport {\n    pub restricted_drop_step: usize,\n    pub full_drop_step: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossDropConfig {\n    pub min_relative_decrease: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossDropReport {\n    pub drop_step: usize,\n    pub drop_loss: f64,\n    pub baseline_loss: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct ExcludedLossSpikeConfig {\n    pub min_relative_increase: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct ExcludedLossSpikeReport {\n    pub spike_step: usize,\n    pub spike_loss: f64,\n    pub baseline_loss: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct MlpWaveVerificationConfig {\n    pub max_negative_fraction: f64,\n    pub negative_tolerance: f64,\n    pub min_dominant_ratio: f64,\n    pub min_correlation: f64,\n    pub phase_steps: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct MlpWaveNeuronReport {\n    pub neuron_index: usize,\n    pub dominant_frequency: usize,\n    pub dominant_ratio: f64,\n    pub negative_fraction: f64,\n    pub best_correlation: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct MlpWaveReport {\n    pub neuron_reports: Vec<MlpWaveNeuronReport>,\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldConfig {\n    pub fixed_y: usize,\n    pub neuron_indices: Vec<usize>,\n    pub min_post_score: f64,\n    pub max_pre_score: f64,\n    pub max_axis_ratio: f64,\n    pub min_pairs_passing: usize,\n}\n\nimpl PairwiseManifoldConfig {\n    pub fn default_for_modulus(_modulus: usize) -> Self {\n        Self {\n            fixed_y: 0,\n            neuron_indices: (0..7).collect(),\n            min_post_score: 0.75,\n            max_pre_score: 0.55,\n            max_axis_ratio: 8.0,\n            min_pairs_passing: 10,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldPairReport {\n    pub neuron_a: usize,\n    pub neuron_b: usize,\n    pub circularity_score: f64,\n    pub radius_cv: f64,\n    pub axis_ratio: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldReport {\n    pub pair_reports: Vec<PairwiseManifoldPairReport>,\n    pub average_score: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldTransitionReport {\n    pub pre_average_score: f64,\n    pub post_average_score: f64,\n    pub passing_pairs: usize,\n    pub total_pairs: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct DiagonalRidgeConfig {\n    pub neuron_index: usize,\n    pub max_lag: usize,\n    pub min_diagonal_ratio: f64,\n    pub min_diagonal_score: f64,\n}\n\nimpl DiagonalRidgeConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        let max_lag = (modulus / 10).max(4).min(20);\n        Self {\n            neuron_index: 10,\n            max_lag,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct DiagonalRidgeReport {\n    pub neuron_index: usize,\n    pub diagonal_score: f64,\n    pub axis_score: f64,\n    pub ratio: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct SnakeCurveConfig {\n    pub early_step_max: usize,\n    pub train_convergence_threshold: f32,\n    pub val_plateau_min_step: usize,\n    pub val_plateau_max_acc: f32,\n    pub val_final_threshold: f32,\n}\n\nimpl SnakeCurveConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        Self {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 1000,\n            val_plateau_max_acc: 1.0 / modulus as f32 + 0.05,\n            val_final_threshold: 0.90,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct WeightNormDecayConfig {\n    pub min_relative_decrease: f64,\n    pub plateau_min_step: usize,\n    pub grok_step_window: usize,\n}\n\nimpl WeightNormDecayConfig {\n    pub fn default_for_grokking() -> Self {\n        Self {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct WeightNormDecayReport {\n    pub initial_norm: f64,\n    pub final_norm: f64,\n    pub relative_decrease: f64,\n    pub grok_step_norm: Option<f64>,\n}\n\n#[derive(Debug, Clone)]\npub struct SnakeCurveReport {\n    pub train_converged_by_step: usize,\n    pub val_plateau_max_acc: f32,\n    pub val_grok_step: usize,\n    pub curve_shape_valid: bool,\n}\n\npub fn verify_grokking_phase_transition(\n    loss_history: &LossHistory,\n    accuracy_history: &AccuracyHistory,\n    config: &GrokkingVerificationConfig,\n) -> Result<GrokkingPhaseReport, String> {\n    let (early_train_step, _) = accuracy_history\n        .train_snapshots\n        .iter()\n        .find(|(_, acc)| *acc >= config.early_train_acc_threshold)\n        .ok_or_else(|| \"train accuracy never reaches threshold\".to_string())?;\n\n    if *early_train_step > config.early_step_max {\n        return Err(format!(\n            \"train accuracy reaches {:.2} after step {}, expected before {}\",\n            config.early_train_acc_threshold, early_train_step, config.early_step_max\n        ));\n    }\n\n    let plateau_max_val_acc = accuracy_history\n        .val_snapshots\n        .iter()\n        .filter(|(step, _)| *step <= config.plateau_min_step)\n        .map(|(_, acc)| *acc)\n        .fold(None::<f32>, |accum, value| {\n            Some(accum.map_or(value, |max_val| max_val.max(value)))\n        })\n        .ok_or_else(|| \"no validation accuracy samples before plateau_min_step\".to_string())?;\n\n    if plateau_max_val_acc > config.chance_accuracy + config.chance_tolerance {\n        return Err(format!(\n            \"validation accuracy before step {} exceeds chance (max {:.4})\",\n            config.plateau_min_step, plateau_max_val_acc\n        ));\n    }\n\n    let (transition_step, _) = accuracy_history\n        .val_snapshots\n        .iter()\n        .find(|(step, acc)| {\n            *step >= config.plateau_min_step && *acc >= config.target_val_acc_threshold\n        })\n        .ok_or_else(|| \"validation accuracy never reaches target threshold\".to_string())?;\n\n    let latest_allowed = config.plateau_min_step + config.generalization_window;\n    if *transition_step > latest_allowed {\n        return Err(format!(\n            \"validation accuracy reaches {:.2} at step {}, expected before {}\",\n            config.target_val_acc_threshold, transition_step, latest_allowed\n        ));\n    }\n\n    verify_accuracy_jump(\n        &accuracy_history.val_snapshots,\n        *transition_step,\n        config.accuracy_window,\n        config.chance_accuracy,\n        config.chance_tolerance,\n        config.target_val_acc_threshold,\n    )?;\n\n    let loss_drop_ratio = verify_loss_drop(\n        &loss_history.val_snapshots,\n        *transition_step,\n        config.loss_window,\n        config.loss_drop_fraction,\n    )?;\n\n    Ok(GrokkingPhaseReport {\n        early_train_step: *early_train_step,\n        transition_step: *transition_step,\n        plateau_max_val_acc,\n        loss_drop_ratio,\n    })\n}\n\nfn verify_accuracy_jump(\n    val_accuracy: &[(usize, f32)],\n    transition_step: usize,\n    window: usize,\n    chance_accuracy: f32,\n    chance_tolerance: f32,\n    target_val_acc_threshold: f32,\n) -> Result<(), String> {\n    let before_values: Vec<f32> = val_accuracy\n        .iter()\n        .filter(|(step, _)| *step < transition_step)\n        .map(|(_, acc)| *acc)\n        .collect();\n\n    let after_values: Vec<f32> = val_accuracy\n        .iter()\n        .filter(|(step, _)| *step >= transition_step)\n        .map(|(_, acc)| *acc)\n        .collect();\n\n    if before_values.len() < window || after_values.len() < window {\n        return Err(\"insufficient accuracy samples around transition\".to_string());\n    }\n\n    let before_avg: f32 =\n        before_values[before_values.len() - window..].iter().sum::<f32>() / window as f32;\n    let after_avg: f32 = after_values[..window].iter().sum::<f32>() / window as f32;\n\n    if before_avg > chance_accuracy + chance_tolerance {\n        return Err(format!(\n            \"validation accuracy plateau too high before transition (avg {:.4})\",\n            before_avg\n        ));\n    }\n\n    if after_avg < target_val_acc_threshold {\n        return Err(format!(\n            \"validation accuracy after transition too low (avg {:.4})\",\n            after_avg\n        ));\n    }\n\n    Ok(())\n}\n\nfn verify_loss_drop(\n    val_losses: &[(usize, f64)],\n    transition_step: usize,\n    loss_window: usize,\n    loss_drop_fraction: f64,\n) -> Result<f64, String> {\n    let before_losses: Vec<f64> = val_losses\n        .iter()\n        .filter(|(step, _)| *step < transition_step)\n        .map(|(_, loss)| *loss)\n        .collect();\n\n    let after_losses: Vec<f64> = val_losses\n        .iter()\n        .filter(|(step, _)| *step >= transition_step)\n        .map(|(_, loss)| *loss)\n        .collect();\n\n    if before_losses.len() < loss_window || after_losses.len() < loss_window {\n        return Err(\"insufficient loss samples around transition\".to_string());\n    }\n\n    let before_avg: f64 =\n        before_losses[before_losses.len() - loss_window..].iter().sum::<f64>()\n            / loss_window as f64;\n    let after_avg: f64 = after_losses[..loss_window].iter().sum::<f64>() / loss_window as f64;\n\n    if after_avg > before_avg * (1.0 - loss_drop_fraction) {\n        return Err(format!(\n            \"validation loss drop too small (before {:.4}, after {:.4})\",\n            before_avg, after_avg\n        ));\n    }\n\n    Ok(after_avg / before_avg)\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_restricted_loss_early_drop(\n    full_losses: &[(usize, f64)],\n    restricted_losses: &[(usize, f64)],\n    config: &RestrictedLossVerificationConfig,\n) -> Result<RestrictedLossReport, String> {\n    let restricted_baseline =\n        baseline_loss(restricted_losses, config.plateau_min_step, config.drop_window)?;\n    let full_baseline = baseline_loss(full_losses, config.plateau_min_step, config.drop_window)?;\n\n    let restricted_drop_step = find_drop_step(\n        restricted_losses,\n        config.plateau_min_step,\n        restricted_baseline,\n        config.drop_fraction,\n    )?;\n    let full_drop_step = find_drop_step(\n        full_losses,\n        config.plateau_min_step,\n        full_baseline,\n        config.drop_fraction,\n    )?;\n\n    if restricted_drop_step + config.min_step_lead > full_drop_step {\n        return Err(format!(\n            \"restricted loss drops at step {}, expected at least {} steps before full loss (step {})\",\n            restricted_drop_step, config.min_step_lead, full_drop_step\n        ));\n    }\n\n    Ok(RestrictedLossReport {\n        restricted_drop_step,\n        full_drop_step,\n    })\n}\n\npub fn verify_restricted_loss_drop(\n    restricted_losses: &[(usize, f64)],\n    config: &RestrictedLossDropConfig,\n) -> Result<RestrictedLossDropReport, String> {\n    if restricted_losses.len() < 3 {\n        return Err(\"restricted loss history requires at least 3 points\".to_string());\n    }\n\n    let first = restricted_losses.first().ok_or_else(|| {\n        \"restricted loss history is empty\".to_string()\n    })?;\n    let baseline_loss = first.1;\n\n    let (min_idx, (min_step, min_loss)) = restricted_losses\n        .iter()\n        .enumerate()\n        .min_by(|(_, a), (_, b)| {\n            a.1.partial_cmp(&b.1).unwrap_or(std::cmp::Ordering::Equal)\n        })\n        .ok_or_else(|| \"restricted loss history is empty\".to_string())?;\n\n    if min_idx == 0 {\n        return Err(\"restricted loss minimum occurs at start (no drop detected)\".to_string());\n    }\n\n    let relative_decrease = (baseline_loss - min_loss) / baseline_loss;\n    if relative_decrease < config.min_relative_decrease {\n        return Err(format!(\n            \"restricted loss decrease too small (baseline {:.4}, min {:.4}, decrease {:.1}%)\",\n            baseline_loss, min_loss, relative_decrease * 100.0\n        ));\n    }\n\n    Ok(RestrictedLossDropReport {\n        drop_step: *min_step,\n        drop_loss: *min_loss,\n        baseline_loss,\n    })\n}\n\npub fn verify_excluded_loss_spike(\n    excluded_losses: &[(usize, f64)],\n    config: &ExcludedLossSpikeConfig,\n) -> Result<ExcludedLossSpikeReport, String> {\n    if excluded_losses.len() < 3 {\n        return Err(\"excluded loss history requires at least 3 points\".to_string());\n    }\n\n    let first = excluded_losses.first().ok_or_else(|| {\n        \"excluded loss history is empty\".to_string()\n    })?;\n    let last = excluded_losses.last().ok_or_else(|| {\n        \"excluded loss history is empty\".to_string()\n    })?;\n    let baseline_loss = (first.1 + last.1) / 2.0;\n\n    let (peak_idx, (peak_step, peak_loss)) = excluded_losses\n        .iter()\n        .enumerate()\n        .max_by(|(_, a), (_, b)| {\n            a.1.partial_cmp(&b.1).unwrap_or(std::cmp::Ordering::Equal)\n        })\n        .ok_or_else(|| \"excluded loss history is empty\".to_string())?;\n\n    if peak_idx == 0 || peak_idx + 1 == excluded_losses.len() {\n        return Err(\"excluded loss spike occurs at boundary\".to_string());\n    }\n\n    if peak_loss < &(baseline_loss * (1.0 + config.min_relative_increase)) {\n        return Err(format!(\n            \"excluded loss spike too small (baseline {:.4}, peak {:.4})\",\n            baseline_loss, peak_loss\n        ));\n    }\n\n    Ok(ExcludedLossSpikeReport {\n        spike_step: *peak_step,\n        spike_loss: *peak_loss,\n        baseline_loss,\n    })\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_mlp_activation_waves<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n    fixed_y: usize,\n    neuron_indices: &[usize],\n    config: &MlpWaveVerificationConfig,\n) -> Result<MlpWaveReport, String> {\n    let activations = collect_mlp_post_relu_activations(model, device, fixed_y)?;\n    if neuron_indices.is_empty() {\n        return Err(\"neuron indices list is empty\".to_string());\n    }\n\n    let d_ff = activations.len();\n    for &idx in neuron_indices {\n        if idx >= d_ff {\n            return Err(format!(\n                \"neuron index {} out of range (d_ff={})\",\n                idx, d_ff\n            ));\n        }\n    }\n\n    let signals: Vec<(usize, Vec<f64>)> = neuron_indices\n        .iter()\n        .map(|&idx| (idx, activations[idx].clone()))\n        .collect();\n    verify_mlp_activation_waves_from_signals(&signals, config)\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_mlp_activation_waves_from_signals(\n    signals: &[(usize, Vec<f64>)],\n    config: &MlpWaveVerificationConfig,\n) -> Result<MlpWaveReport, String> {\n    if signals.is_empty() {\n        return Err(\"no activation signals provided\".to_string());\n    }\n\n    let mut reports = Vec::with_capacity(signals.len());\n    for (neuron_index, signal) in signals {\n        let report = analyze_mlp_activation_wave(*neuron_index, signal, config)?;\n        reports.push(report);\n    }\n\n    Ok(MlpWaveReport {\n        neuron_reports: reports,\n    })\n}\n\npub fn verify_mlp_pairwise_manifold_transition<B: Backend>(\n    pre_model: &Transformer<B>,\n    post_model: &Transformer<B>,\n    device: &B::Device,\n    config: &PairwiseManifoldConfig,\n) -> Result<PairwiseManifoldTransitionReport, String> {\n    let modulus = ModularAdditionDataset::modulus();\n    if config.fixed_y >= modulus {\n        return Err(format!(\n            \"fixed_y {} out of range (modulus {})\",\n            config.fixed_y, modulus\n        ));\n    }\n\n    let pre_activations = collect_mlp_post_relu_activations(pre_model, device, config.fixed_y)?;\n    let post_activations = collect_mlp_post_relu_activations(post_model, device, config.fixed_y)?;\n\n    let d_ff = pre_activations.len();\n    if post_activations.len() != d_ff {\n        return Err(\"post activations shape mismatch\".to_string());\n    }\n\n    if config.neuron_indices.len() < 2 {\n        return Err(\"need at least two neuron indices\".to_string());\n    }\n\n    for &idx in &config.neuron_indices {\n        if idx >= d_ff {\n            return Err(format!(\n                \"neuron index {} out of range (d_ff={})\",\n                idx, d_ff\n            ));\n        }\n    }\n\n    let pre_signals: Vec<(usize, Vec<f64>)> = config\n        .neuron_indices\n        .iter()\n        .map(|&idx| (idx, pre_activations[idx].clone()))\n        .collect();\n    let post_signals: Vec<(usize, Vec<f64>)> = config\n        .neuron_indices\n        .iter()\n        .map(|&idx| (idx, post_activations[idx].clone()))\n        .collect();\n\n    verify_pairwise_manifold_transition_from_signals(&pre_signals, &post_signals, config)\n}\n\npub fn collect_mlp_activation_surface<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n    neuron_index: usize,\n) -> Result<Vec<Vec<f64>>, String> {\n    let modulus = ModularAdditionDataset::modulus();\n    let equals_token = ModularAdditionDataset::equals_token();\n\n    let mut inputs_vec = Vec::with_capacity(modulus * modulus * 3);\n    for x in 0..modulus {\n        for y in 0..modulus {\n            inputs_vec.push(x as i32);\n            inputs_vec.push(y as i32);\n            inputs_vec.push(equals_token as i32);\n        }\n    }\n\n    let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n        .reshape([modulus * modulus, 3]);\n    let (_, mlp_acts) = model.forward_with_mlp_activations(inputs);\n    let [batch_size, d_ff] = mlp_acts.dims();\n    if batch_size != modulus * modulus {\n        return Err(format!(\n            \"unexpected activation batch size {} (expected {})\",\n            batch_size,\n            modulus * modulus\n        ));\n    }\n    if neuron_index >= d_ff {\n        return Err(format!(\n            \"neuron index {} out of range (d_ff={})\",\n            neuron_index, d_ff\n        ));\n    }\n\n    let data: Vec<f32> = mlp_acts.into_data().to_vec().unwrap();\n    let mut surface = vec![vec![0.0f64; modulus]; modulus];\n    for x in 0..modulus {\n        for y in 0..modulus {\n            let batch = x * modulus + y;\n            let idx = batch * d_ff + neuron_index;\n            surface[x][y] = data[idx] as f64;\n        }\n    }\n\n    Ok(surface)\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_constructive_interference_surface_from_grid(\n    surface: &[Vec<f64>],\n    config: &DiagonalRidgeConfig,\n) -> Result<DiagonalRidgeReport, String> {\n    let (diag_main, diag_anti, axis_x, axis_y) =\n        surface_autocorrelation_scores(surface, config.max_lag)?;\n\n    let diagonal_score = diag_main.max(diag_anti);\n    let axis_score = axis_x.max(axis_y);\n    let ratio = if axis_score > 0.0 {\n        diagonal_score / axis_score\n    } else {\n        f64::INFINITY\n    };\n\n    if diagonal_score < config.min_diagonal_score {\n        return Err(format!(\n            \"diagonal autocorrelation {:.3} below threshold {:.3}\",\n            diagonal_score, config.min_diagonal_score\n        ));\n    }\n\n    if ratio < config.min_diagonal_ratio {\n        return Err(format!(\n            \"diagonal/axis ratio {:.3} below threshold {:.3}\",\n            ratio, config.min_diagonal_ratio\n        ));\n    }\n\n    Ok(DiagonalRidgeReport {\n        neuron_index: config.neuron_index,\n        diagonal_score,\n        axis_score,\n        ratio,\n    })\n}\n\npub fn verify_pairwise_manifold_transition_from_signals(\n    pre_signals: &[(usize, Vec<f64>)],\n    post_signals: &[(usize, Vec<f64>)],\n    config: &PairwiseManifoldConfig,\n) -> Result<PairwiseManifoldTransitionReport, String> {\n    let pre_report = evaluate_pairwise_manifolds_from_signals(pre_signals)?;\n    let post_report = evaluate_pairwise_manifolds_from_signals(post_signals)?;\n\n    if pre_report.average_score > config.max_pre_score {\n        return Err(format!(\n            \"pre-grok average circularity {:.3} exceeds threshold {:.3}\",\n            pre_report.average_score, config.max_pre_score\n        ));\n    }\n\n    let passing_pairs = post_report\n        .pair_reports\n        .iter()\n        .filter(|pair| {\n            pair.circularity_score >= config.min_post_score\n                && pair.axis_ratio <= config.max_axis_ratio\n        })\n        .count();\n    if passing_pairs < config.min_pairs_passing {\n        return Err(format!(\n            \"post-grok only {} pairs meet circularity threshold (need {})\",\n            passing_pairs, config.min_pairs_passing\n        ));\n    }\n\n    Ok(PairwiseManifoldTransitionReport {\n        pre_average_score: pre_report.average_score,\n        post_average_score: post_report.average_score,\n        passing_pairs,\n        total_pairs: post_report.pair_reports.len(),\n    })\n}\n\nfn evaluate_pairwise_manifolds_from_signals(\n    signals: &[(usize, Vec<f64>)],\n) -> Result<PairwiseManifoldReport, String> {\n    if signals.len() < 2 {\n        return Err(\"need at least two activation signals\".to_string());\n    }\n\n    let signal_len = signals[0].1.len();\n    if signal_len < 3 {\n        return Err(\"activation signal too short\".to_string());\n    }\n\n    for (idx, signal) in signals {\n        if signal.len() != signal_len {\n            return Err(format!(\n                \"signal length mismatch for neuron {}\",\n                idx\n            ));\n        }\n    }\n\n    let mut reports = Vec::new();\n    for i in 0..signals.len() {\n        for j in (i + 1)..signals.len() {\n            let (neuron_a, signal_a) = &signals[i];\n            let (neuron_b, signal_b) = &signals[j];\n            let points: Vec<(f64, f64)> = signal_a\n                .iter()\n                .zip(signal_b.iter())\n                .map(|(x, y)| (*x, *y))\n                .collect();\n            let (score, radius_cv, axis_ratio) =\n                match pairwise_circularity_metrics(&points) {\n                    Ok(metrics) => metrics,\n                    Err(_) => (0.0, 1.0, f64::INFINITY),\n                };\n            reports.push(PairwiseManifoldPairReport {\n                neuron_a: *neuron_a,\n                neuron_b: *neuron_b,\n                circularity_score: score,\n                radius_cv,\n                axis_ratio,\n            });\n        }\n    }\n\n    let average_score = reports\n        .iter()\n        .map(|pair| pair.circularity_score)\n        .sum::<f64>()\n        / reports.len() as f64;\n\n    Ok(PairwiseManifoldReport {\n        pair_reports: reports,\n        average_score,\n    })\n}\n\nfn pairwise_circularity_metrics(points: &[(f64, f64)]) -> Result<(f64, f64, f64), String> {\n    if points.len() < 3 {\n        return Err(\"not enough points for circularity\".to_string());\n    }\n\n    let n = points.len() as f64;\n    let mean_x = points.iter().map(|(x, _)| *x).sum::<f64>() / n;\n    let mean_y = points.iter().map(|(_, y)| *y).sum::<f64>() / n;\n\n    let mut var_x = 0.0;\n    let mut var_y = 0.0;\n    let mut cov_xy = 0.0;\n    for (x, y) in points {\n        let dx = x - mean_x;\n        let dy = y - mean_y;\n        var_x += dx * dx;\n        var_y += dy * dy;\n        cov_xy += dx * dy;\n    }\n    var_x /= n;\n    var_y /= n;\n    cov_xy /= n;\n\n    let trace = var_x + var_y;\n    let det = var_x * var_y - cov_xy * cov_xy;\n    if det <= 1.0e-12 || trace <= 0.0 {\n        return Err(\"degenerate covariance\".to_string());\n    }\n\n    let half_trace = trace * 0.5;\n    let disc = (half_trace * half_trace - det).max(0.0).sqrt();\n    let lambda1 = half_trace + disc;\n    let lambda2 = half_trace - disc;\n    if lambda2 <= 0.0 {\n        return Err(\"degenerate eigenvalues\".to_string());\n    }\n\n    let axis_ratio = (lambda1 / lambda2).sqrt();\n\n    let mut v1_x = cov_xy;\n    let mut v1_y = lambda1 - var_x;\n    if v1_x.abs() < 1.0e-12 && v1_y.abs() < 1.0e-12 {\n        v1_x = 1.0;\n        v1_y = 0.0;\n    }\n    let norm = (v1_x * v1_x + v1_y * v1_y).sqrt();\n    v1_x /= norm;\n    v1_y /= norm;\n    let v2_x = -v1_y;\n    let v2_y = v1_x;\n\n    let mut radii = Vec::with_capacity(points.len());\n    for (x, y) in points {\n        let dx = x - mean_x;\n        let dy = y - mean_y;\n        let p1 = dx * v1_x + dy * v1_y;\n        let p2 = dx * v2_x + dy * v2_y;\n        let w1 = p1 / lambda1.sqrt();\n        let w2 = p2 / lambda2.sqrt();\n        radii.push((w1 * w1 + w2 * w2).sqrt());\n    }\n\n    let mean_radius = radii.iter().sum::<f64>() / radii.len() as f64;\n    if mean_radius == 0.0 {\n        return Err(\"zero mean radius\".to_string());\n    }\n\n    let mut variance = 0.0;\n    for r in &radii {\n        let diff = r - mean_radius;\n        variance += diff * diff;\n    }\n    let std = (variance / radii.len() as f64).sqrt();\n    let radius_cv = std / mean_radius;\n    let score = 1.0 / (1.0 + radius_cv);\n\n    Ok((score, radius_cv, axis_ratio))\n}\n\nfn baseline_loss(\n    losses: &[(usize, f64)],\n    plateau_min_step: usize,\n    drop_window: usize,\n) -> Result<f64, String> {\n    let before: Vec<f64> = losses\n        .iter()\n        .filter(|(step, _)| *step <= plateau_min_step)\n        .map(|(_, loss)| *loss)\n        .collect();\n\n    if before.len() < drop_window || drop_window == 0 {\n        return Err(\"insufficient loss samples for baseline\".to_string());\n    }\n\n    let window = &before[before.len() - drop_window..];\n    Ok(window.iter().sum::<f64>() / drop_window as f64)\n}\n\nfn find_drop_step(\n    losses: &[(usize, f64)],\n    plateau_min_step: usize,\n    baseline: f64,\n    drop_fraction: f64,\n) -> Result<usize, String> {\n    losses\n        .iter()\n        .filter(|(step, _)| *step >= plateau_min_step)\n        .find(|(_, loss)| *loss <= baseline * (1.0 - drop_fraction))\n        .map(|(step, _)| *step)\n        .ok_or_else(|| \"loss never drops below threshold\".to_string())\n}\n\nfn collect_mlp_post_relu_activations<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n    fixed_y: usize,\n) -> Result<Vec<Vec<f64>>, String> {\n    let modulus = ModularAdditionDataset::modulus();\n    let equals_token = ModularAdditionDataset::equals_token();\n\n    let mut inputs_vec = Vec::with_capacity(modulus * 3);\n    for x in 0..modulus {\n        inputs_vec.push(x as i32);\n        inputs_vec.push(fixed_y as i32);\n        inputs_vec.push(equals_token as i32);\n    }\n\n    let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n        .reshape([modulus, 3]);\n    let (_, mlp_acts) = model.forward_with_mlp_activations(inputs);\n    let [batch_size, d_ff] = mlp_acts.dims();\n\n    if batch_size != modulus {\n        return Err(format!(\n            \"unexpected activation batch size {} (expected {})\",\n            batch_size, modulus\n        ));\n    }\n\n    let data: Vec<f32> = mlp_acts.into_data().to_vec().unwrap();\n    let mut per_neuron = vec![Vec::with_capacity(batch_size); d_ff];\n    for batch in 0..batch_size {\n        for neuron in 0..d_ff {\n            let idx = batch * d_ff + neuron;\n            per_neuron[neuron].push(data[idx] as f64);\n        }\n    }\n\n    Ok(per_neuron)\n}\n\nfn surface_autocorrelation_scores(\n    surface: &[Vec<f64>],\n    max_lag: usize,\n) -> Result<(f64, f64, f64, f64), String> {\n    let (height, width) = surface_dimensions(surface)?;\n    if height < 2 || width < 2 {\n        return Err(\"surface grid too small\".to_string());\n    }\n\n    let (mean, var) = surface_mean_variance(surface)?;\n    if var == 0.0 {\n        return Err(\"surface variance is zero\".to_string());\n    }\n\n    let max_lag = max_lag.min(height.saturating_sub(1)).min(width.saturating_sub(1));\n    if max_lag == 0 {\n        return Err(\"max_lag too small for surface\".to_string());\n    }\n\n    let diag_main = average_autocorrelation(surface, mean, var, max_lag, 1, 1)?;\n    let diag_anti = average_autocorrelation(surface, mean, var, max_lag, 1, -1)?;\n    let axis_x = average_autocorrelation(surface, mean, var, max_lag, 1, 0)?;\n    let axis_y = average_autocorrelation(surface, mean, var, max_lag, 0, 1)?;\n\n    Ok((diag_main, diag_anti, axis_x, axis_y))\n}\n\nfn surface_dimensions(surface: &[Vec<f64>]) -> Result<(usize, usize), String> {\n    let height = surface.len();\n    if height == 0 {\n        return Err(\"surface is empty\".to_string());\n    }\n    let width = surface[0].len();\n    if width == 0 {\n        return Err(\"surface has empty rows\".to_string());\n    }\n    for (idx, row) in surface.iter().enumerate() {\n        if row.len() != width {\n            return Err(format!(\n                \"surface row {} has length {}, expected {}\",\n                idx,\n                row.len(),\n                width\n            ));\n        }\n    }\n    Ok((height, width))\n}\n\nfn surface_mean_variance(surface: &[Vec<f64>]) -> Result<(f64, f64), String> {\n    let (height, width) = surface_dimensions(surface)?;\n    let mut sum = 0.0;\n    for row in surface {\n        for value in row {\n            sum += *value;\n        }\n    }\n    let count = (height * width) as f64;\n    let mean = sum / count;\n    let mut variance = 0.0;\n    for row in surface {\n        for value in row {\n            let diff = *value - mean;\n            variance += diff * diff;\n        }\n    }\n    variance /= count;\n    Ok((mean, variance))\n}\n\nfn average_autocorrelation(\n    surface: &[Vec<f64>],\n    mean: f64,\n    variance: f64,\n    max_lag: usize,\n    dx: i32,\n    dy: i32,\n) -> Result<f64, String> {\n    let (height, width) = surface_dimensions(surface)?;\n    let mut total = 0.0;\n    let mut lag_count = 0usize;\n\n    for lag in 1..=max_lag {\n        let mut sum = 0.0;\n        let mut count = 0usize;\n        let lag = lag as i32;\n        for x in 0..height {\n            for y in 0..width {\n                let x2 = x as i32 + dx * lag;\n                let y2 = y as i32 + dy * lag;\n                if x2 < 0 || y2 < 0 {\n                    continue;\n                }\n                let x2 = x2 as usize;\n                let y2 = y2 as usize;\n                if x2 >= height || y2 >= width {\n                    continue;\n                }\n                let a = surface[x][y] - mean;\n                let b = surface[x2][y2] - mean;\n                sum += a * b;\n                count += 1;\n            }\n        }\n        if count == 0 {\n            continue;\n        }\n        let corr = sum / (count as f64 * variance);\n        total += corr.abs();\n        lag_count += 1;\n    }\n\n    if lag_count == 0 {\n        return Err(\"no valid lags for autocorrelation\".to_string());\n    }\n\n    Ok(total / lag_count as f64)\n}\n\nfn analyze_mlp_activation_wave(\n    neuron_index: usize,\n    signal: &[f64],\n    config: &MlpWaveVerificationConfig,\n) -> Result<MlpWaveNeuronReport, String> {\n    if signal.len() < 3 {\n        return Err(format!(\n            \"activation signal for neuron {} is too short\",\n            neuron_index\n        ));\n    }\n\n    let negative_count = signal\n        .iter()\n        .filter(|value| **value < -config.negative_tolerance)\n        .count();\n    let negative_fraction = negative_count as f64 / signal.len() as f64;\n    if negative_fraction > config.max_negative_fraction {\n        return Err(format!(\n            \"neuron {} has {:.3} negative activations\",\n            neuron_index, negative_fraction\n        ));\n    }\n\n    let (dominant_frequency, dominant_ratio) = dominant_frequency_ratio(signal)?;\n    if dominant_ratio < config.min_dominant_ratio {\n        return Err(format!(\n            \"neuron {} dominant frequency ratio {:.3} below threshold {:.3}\",\n            neuron_index, dominant_ratio, config.min_dominant_ratio\n        ));\n    }\n\n    let best_correlation =\n        best_rectified_sine_correlation(signal, dominant_frequency, config.phase_steps);\n    if best_correlation < config.min_correlation {\n        return Err(format!(\n            \"neuron {} rectified sine correlation {:.3} below threshold {:.3}\",\n            neuron_index, best_correlation, config.min_correlation\n        ));\n    }\n\n    Ok(MlpWaveNeuronReport {\n        neuron_index,\n        dominant_frequency,\n        dominant_ratio,\n        negative_fraction,\n        best_correlation,\n    })\n}\n\nfn dominant_frequency_ratio(signal: &[f64]) -> Result<(usize, f64), String> {\n    if signal.len() < 2 {\n        return Err(\"signal too short for FFT\".to_string());\n    }\n\n    let mut planner = FftPlanner::new();\n    let fft = planner.plan_fft_forward(signal.len());\n    let mut buffer: Vec<Complex<f64>> = signal\n        .iter()\n        .map(|&x| Complex::new(x, 0.0))\n        .collect();\n    fft.process(&mut buffer);\n    let magnitudes: Vec<f64> = buffer.iter().map(|c| c.norm()).collect();\n\n    if magnitudes.len() < 2 {\n        return Err(\"insufficient FFT bins\".to_string());\n    }\n\n    let mut dominant_idx = 1usize;\n    let mut dominant_mag = magnitudes[1];\n    for (idx, &mag) in magnitudes.iter().enumerate().skip(2) {\n        if mag > dominant_mag {\n            dominant_mag = mag;\n            dominant_idx = idx;\n        }\n    }\n\n    if dominant_idx == 0 {\n        return Err(\"dominant frequency is zero\".to_string());\n    }\n\n    let mut nonzero = magnitudes[1..].to_vec();\n    nonzero.sort_by(|a, b| a.partial_cmp(b).unwrap_or(std::cmp::Ordering::Equal));\n    let median_mag = nonzero[nonzero.len() / 2];\n    let ratio = if median_mag == 0.0 {\n        dominant_mag\n    } else {\n        dominant_mag / median_mag\n    };\n\n    Ok((dominant_idx, ratio))\n}\n\nfn best_rectified_sine_correlation(signal: &[f64], frequency: usize, phase_steps: usize) -> f64 {\n    if signal.is_empty() || frequency == 0 || phase_steps == 0 {\n        return 0.0;\n    }\n\n    let tau = std::f64::consts::PI * 2.0;\n    let mut best = -1.0;\n    for step in 0..phase_steps {\n        let phase = tau * step as f64 / phase_steps as f64;\n        let wave: Vec<f64> = (0..signal.len())\n            .map(|idx| {\n                let angle = tau * frequency as f64 * idx as f64 / signal.len() as f64 + phase;\n                angle.sin().max(0.0)\n            })\n            .collect();\n        let corr = pearson_correlation(signal, &wave);\n        if corr > best {\n            best = corr;\n        }\n    }\n\n    best\n}\n\nfn pearson_correlation(a: &[f64], b: &[f64]) -> f64 {\n    if a.len() != b.len() || a.is_empty() {\n        return 0.0;\n    }\n\n    let mean_a = a.iter().sum::<f64>() / a.len() as f64;\n    let mean_b = b.iter().sum::<f64>() / b.len() as f64;\n\n    let mut num = 0.0;\n    let mut denom_a = 0.0;\n    let mut denom_b = 0.0;\n    for (x, y) in a.iter().zip(b.iter()) {\n        let da = x - mean_a;\n        let db = y - mean_b;\n        num += da * db;\n        denom_a += da * da;\n        denom_b += db * db;\n    }\n\n    if denom_a == 0.0 || denom_b == 0.0 {\n        return 0.0;\n    }\n\n    num / (denom_a.sqrt() * denom_b.sqrt())\n}\n\n/// Verify that the model actually learned modular addition\n/// by testing it on ALL possible examples systematically\npub fn verify_full_accuracy<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n) -> (f32, f32) {\n    println!(\"ðŸ” Verifying model on COMPLETE dataset (no sampling)...\");\n\n    let train_dataset = ModularAdditionDataset::new(true, 42);\n    let val_dataset = ModularAdditionDataset::new(false, 42);\n\n    let train_acc = test_all_examples(model, &train_dataset, device);\n    let val_acc = test_all_examples(model, &val_dataset, device);\n\n    println!(\"ðŸ“Š FULL Dataset Results:\");\n    println!(\"   Training: {}/{} = {:.2}%\",\n        (train_acc * train_dataset.len() as f32) as usize,\n        train_dataset.len(),\n        train_acc * 100.0);\n    println!(\"   Validation: {}/{} = {:.2}%\",\n        (val_acc * val_dataset.len() as f32) as usize,\n        val_dataset.len(),\n        val_acc * 100.0);\n\n    (train_acc, val_acc)\n}\n\n/// Test model on ALL examples in dataset (no random sampling)\nfn test_all_examples<B: Backend>(\n    model: &Transformer<B>,\n    dataset: &ModularAdditionDataset,\n    device: &B::Device,\n) -> f32 {\n    let mut total_correct = 0;\n    let total = dataset.len();\n\n    // Test in batches to avoid memory issues\n    let batch_size = 100;\n    for batch_start in (0..total).step_by(batch_size) {\n        let batch_end = (batch_start + batch_size).min(total);\n        let batch_len = batch_end - batch_start;\n\n        let mut inputs_vec = Vec::new();\n        let mut targets_vec = Vec::new();\n\n        for idx in batch_start..batch_end {\n            let (input, target) = dataset.get(idx).unwrap();\n            inputs_vec.extend(input);\n            targets_vec.push(target as i32);\n        }\n\n        let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n            .reshape([batch_len, 3]);\n\n        let logits = model.forward(inputs);\n        let predictions = logits.argmax(1).squeeze::<1>();\n        let predictions_vec: Vec<i32> = predictions.into_data().to_vec().unwrap();\n\n        let correct = predictions_vec\n            .iter()\n            .zip(targets_vec.iter())\n            .filter(|(p, t)| p == t)\n            .count();\n\n        total_correct += correct;\n    }\n\n    total_correct as f32 / total as f32\n}\n\n/// Verify that weight norms decrease over time with weight decay\n/// and show correlation with the grokking transition\npub fn verify_weight_norm_decay(\n    weight_norm_history: &[(usize, f64)],\n    grok_step: Option<usize>,\n    config: &WeightNormDecayConfig,\n) -> Result<WeightNormDecayReport, String> {\n    if weight_norm_history.len() < 2 {\n        return Err(\"weight norm history requires at least 2 snapshots\".to_string());\n    }\n\n    let initial_norm = weight_norm_history\n        .first()\n        .ok_or_else(|| \"weight norm history is empty\".to_string())?\n        .1;\n    let final_norm = weight_norm_history\n        .last()\n        .ok_or_else(|| \"weight norm history is empty\".to_string())?\n        .1;\n\n    if initial_norm <= 0.0 {\n        return Err(\"initial weight norm must be positive\".to_string());\n    }\n\n    // Check that norms are generally decreasing (with weight decay)\n    let relative_decrease = (initial_norm - final_norm) / initial_norm;\n    if relative_decrease < config.min_relative_decrease {\n        return Err(format!(\n            \"weight norm decrease {:.4} below threshold {:.4} (initial {:.4}, final {:.4})\",\n            relative_decrease, config.min_relative_decrease, initial_norm, final_norm\n        ));\n    }\n\n    // Check that norms are not increasing (weight decay should monotonically decrease norms)\n    for window in weight_norm_history.windows(2) {\n        let prev_norm = window[0].1;\n        let next_norm = window[1].1;\n        if next_norm > prev_norm * 1.01 {\n            // Allow 1% tolerance for numerical noise\n            return Err(format!(\n                \"weight norm increased from {:.4} to {:.4} at step {}\",\n                prev_norm, next_norm, window[1].0\n            ));\n        }\n    }\n\n    // If grok step is provided, check that norms show a shift around grokking\n    let grok_step_norm = if let Some(step) = grok_step {\n        weight_norm_history\n            .iter()\n            .filter(|(s, _)| {\n                *s >= step.saturating_sub(config.grok_step_window)\n                    && *s <= step + config.grok_step_window\n            })\n            .map(|(_, norm)| *norm)\n            .next()\n    } else {\n        None\n    };\n\n    Ok(WeightNormDecayReport {\n        initial_norm,\n        final_norm,\n        relative_decrease,\n        grok_step_norm,\n    })\n}\n\n/// Verify the characteristic \"snake curve\" shape of grokking phenomenon\n/// on log-scale plots: train accuracy rises early, validation stays flat then jumps\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_snake_curve_shape(\n    train_acc: &[(usize, f32)],\n    val_acc: &[(usize, f32)],\n    config: &SnakeCurveConfig,\n) -> Result<SnakeCurveReport, String> {\n    if train_acc.is_empty() || val_acc.is_empty() {\n        return Err(\"empty accuracy history\".to_string());\n    }\n\n    // Phase 1: Verify early train convergence (immediate drop on log scale)\n    let train_converged_step = train_acc\n        .iter()\n        .find(|(_, acc)| *acc >= config.train_convergence_threshold)\n        .map(|(step, _)| *step)\n        .ok_or_else(|| {\n            format!(\n                \"train accuracy never reaches {} threshold\",\n                config.train_convergence_threshold\n            )\n        })?;\n\n    if train_converged_step > config.early_step_max {\n        return Err(format!(\n            \"train accuracy converges at step {}, expected before {}\",\n            train_converged_step, config.early_step_max\n        ));\n    }\n\n    // Phase 2: Verify validation plateau (flat on log scale)\n    let plateau_samples: Vec<f32> = val_acc\n        .iter()\n        .filter(|(step, _)| *step >= 1 && *step <= config.val_plateau_min_step)\n        .map(|(_, acc)| *acc)\n        .collect();\n\n    if plateau_samples.is_empty() {\n        return Err(\"no validation samples in plateau region\".to_string());\n    }\n\n    let val_plateau_max = plateau_samples\n        .iter()\n        .fold(0.0f32, |max_val, &acc| max_val.max(acc));\n\n    if val_plateau_max > config.val_plateau_max_acc {\n        return Err(format!(\n            \"validation accuracy in plateau region too high: max {:.4} exceeds {:.4}\",\n            val_plateau_max, config.val_plateau_max_acc\n        ));\n    }\n\n    // Phase 3: Verify delayed validation jump (crash up on log scale)\n    let val_grok_step = val_acc\n        .iter()\n        .filter(|(step, _)| *step > config.val_plateau_min_step)\n        .find(|(_, acc)| *acc >= config.val_final_threshold)\n        .map(|(step, _)| *step)\n        .ok_or_else(|| {\n            format!(\n                \"validation accuracy never reaches {} after plateau\",\n                config.val_final_threshold\n            )\n        })?;\n\n    // Phase 4: Verify phase ordering (snake shape: early train, late val)\n    let curve_shape_valid = train_converged_step < config.val_plateau_min_step\n        && val_grok_step > config.val_plateau_min_step\n        && train_converged_step < val_grok_step;\n\n    if !curve_shape_valid {\n        return Err(format!(\n            \"invalid curve shape: train converged at {}, val plateau until {}, val grokked at {}\",\n            train_converged_step, config.val_plateau_min_step, val_grok_step\n        ));\n    }\n\n    Ok(SnakeCurveReport {\n        train_converged_by_step: train_converged_step,\n        val_plateau_max_acc: val_plateau_max,\n        val_grok_step,\n        curve_shape_valid,\n    })\n}\n\n/// Test specific examples to see if model actually computes mod p\npub fn test_specific_examples<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n) {\n    println!();\n    println!(\"ðŸ§ª Testing specific examples:\");\n\n    let modulus = ModularAdditionDataset::modulus();\n    let equals_token = ModularAdditionDataset::equals_token();\n    let test_cases = vec![\n        (0, 0, 0),\n        (1, 1, 2),\n        (50, 50, (50 + 50) % modulus),\n        (112, 1, (112 + 1) % modulus),\n        (56, 57, (56 + 57) % modulus),\n        (112, 112, (112 + 112) % modulus),\n        (10, 20, 30),\n        (60, 60, (60 + 60) % modulus),\n    ];\n\n    for (a, b, expected) in test_cases {\n        let input_vec = vec![a as i32, b as i32, equals_token as i32];\n        let input = Tensor::<B, 1, Int>::from_ints(input_vec.as_slice(), device)\n            .reshape([1, 3]);\n\n        let logits = model.forward(input);\n        let prediction = logits.argmax(1).squeeze::<1>();\n        let pred_value: i32 = prediction.into_data().to_vec().unwrap()[0];\n\n        let correct = if pred_value == expected as i32 { \"âœ“\" } else { \"âœ—\" };\n        println!(\n            \"   {} + {} mod {} = {} | Model: {} {}\",\n            a, b, modulus, expected, pred_value, correct\n        );\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    fn make_histories(\n        train_acc: &[(usize, f32)],\n        val_acc: &[(usize, f32)],\n        val_loss: &[(usize, f64)],\n    ) -> (LossHistory, AccuracyHistory) {\n        let mut loss_history = LossHistory::new();\n        loss_history.val_snapshots = val_loss.to_vec();\n        let mut accuracy_history = AccuracyHistory::new();\n        accuracy_history.train_snapshots = train_acc.to_vec();\n        accuracy_history.val_snapshots = val_acc.to_vec();\n        (loss_history, accuracy_history)\n    }\n\n    #[test]\n    fn grokking_phase_verification_passes_on_synthetic_signal() {\n        let train_acc = vec![(0, 0.2), (500, 0.995), (1000, 0.999)];\n        let val_acc = vec![\n            (0, 0.01),\n            (500, 0.01),\n            (1500, 0.01),\n            (2200, 0.995),\n            (2500, 0.999),\n        ];\n        let val_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 2.0),\n            (2200, 0.2),\n            (2500, 0.15),\n        ];\n\n        let (loss_history, accuracy_history) = make_histories(&train_acc, &val_acc, &val_loss);\n        let config = GrokkingVerificationConfig {\n            early_step_max: 1000,\n            plateau_min_step: 1500,\n            generalization_window: 1500,\n            accuracy_window: 2,\n            loss_window: 2,\n            ..GrokkingVerificationConfig::default_for_modulus(ModularAdditionDataset::modulus())\n        };\n\n        let report = verify_grokking_phase_transition(&loss_history, &accuracy_history, &config)\n            .expect(\"expected grokking verification to pass\");\n        assert!(report.transition_step >= config.plateau_min_step);\n        assert!(report.early_train_step <= config.early_step_max);\n    }\n\n    #[test]\n    fn grokking_phase_verification_fails_without_transition() {\n        let train_acc = vec![(0, 0.2), (500, 0.995)];\n        let val_acc = vec![(0, 0.01), (500, 0.01), (1500, 0.02)];\n        let val_loss = vec![(0, 2.0), (500, 2.0), (1500, 1.9)];\n\n        let (loss_history, accuracy_history) = make_histories(&train_acc, &val_acc, &val_loss);\n        let config = GrokkingVerificationConfig {\n            early_step_max: 1000,\n            plateau_min_step: 1000,\n            generalization_window: 1000,\n            accuracy_window: 2,\n            loss_window: 2,\n            ..GrokkingVerificationConfig::default_for_modulus(ModularAdditionDataset::modulus())\n        };\n\n        let err = verify_grokking_phase_transition(&loss_history, &accuracy_history, &config)\n            .expect_err(\"expected grokking verification to fail\");\n        assert!(err.contains(\"validation accuracy never reaches target\"));\n    }\n\n    #[test]\n    fn grokking_phase_verification_fails_with_slow_accuracy_rise() {\n        let train_acc = vec![(0, 0.2), (500, 0.995), (1000, 0.999)];\n        let val_acc = vec![\n            (0, 0.01),\n            (500, 0.01),\n            (1500, 0.5),\n            (2000, 0.6),\n            (2200, 0.995),\n            (2500, 0.999),\n        ];\n        let val_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 2.0),\n            (2200, 0.2),\n            (2500, 0.15),\n        ];\n\n        let (loss_history, accuracy_history) = make_histories(&train_acc, &val_acc, &val_loss);\n        let config = GrokkingVerificationConfig {\n            early_step_max: 1000,\n            plateau_min_step: 1000,\n            generalization_window: 2000,\n            accuracy_window: 2,\n            loss_window: 2,\n            ..GrokkingVerificationConfig::default_for_modulus(ModularAdditionDataset::modulus())\n        };\n\n        let err = verify_grokking_phase_transition(&loss_history, &accuracy_history, &config)\n            .expect_err(\"expected grokking verification to fail\");\n        assert!(err.contains(\"plateau too high\"));\n    }\n\n    #[test]\n    fn restricted_loss_detection_passes_with_early_drop() {\n        let full_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 2.0),\n            (2500, 2.0),\n            (3500, 0.3),\n        ];\n        let restricted_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 0.8),\n            (2500, 0.4),\n            (3500, 0.3),\n        ];\n\n        let config = RestrictedLossVerificationConfig {\n            plateau_min_step: 1000,\n            drop_window: 2,\n            drop_fraction: 0.5,\n            min_step_lead: 500,\n        };\n\n        let report = verify_restricted_loss_early_drop(&full_loss, &restricted_loss, &config)\n            .expect(\"expected restricted loss verification to pass\");\n        assert!(report.restricted_drop_step < report.full_drop_step);\n    }\n\n    #[test]\n    fn restricted_loss_detection_fails_when_full_drops_first() {\n        let full_loss = vec![(0, 2.0), (1000, 1.0), (1500, 0.8)];\n        let restricted_loss = vec![(0, 2.0), (1000, 2.0), (1500, 0.8)];\n\n        let config = RestrictedLossVerificationConfig {\n            plateau_min_step: 500,\n            drop_window: 1,\n            drop_fraction: 0.3,\n            min_step_lead: 200,\n        };\n\n        let err = verify_restricted_loss_early_drop(&full_loss, &restricted_loss, &config)\n            .expect_err(\"expected restricted loss verification to fail\");\n        assert!(err.contains(\"restricted loss drops\"));\n    }\n\n    #[test]\n    fn excluded_loss_spike_verification_passes() {\n        let excluded_loss = vec![(0, 1.0), (1500, 2.2), (3000, 1.1)];\n        let config = ExcludedLossSpikeConfig {\n            min_relative_increase: 0.5,\n        };\n\n        let report = verify_excluded_loss_spike(&excluded_loss, &config)\n            .expect(\"expected excluded loss spike verification to pass\");\n        assert_eq!(report.spike_step, 1500);\n    }\n\n    #[test]\n    fn excluded_loss_spike_verification_fails_without_spike() {\n        let excluded_loss = vec![(0, 1.0), (1500, 1.1), (3000, 1.0)];\n        let config = ExcludedLossSpikeConfig {\n            min_relative_increase: 0.5,\n        };\n\n        let err = verify_excluded_loss_spike(&excluded_loss, &config)\n            .expect_err(\"expected excluded loss spike verification to fail\");\n        assert!(err.contains(\"excluded loss spike too small\"));\n    }\n\n    #[test]\n    fn restricted_loss_drop_verification_passes() {\n        let restricted_loss = vec![\n            (0, 2.0),\n            (1000, 1.8),\n            (5000, 1.2),\n            (7000, 0.8),\n            (10000, 0.9),\n        ];\n        let config = RestrictedLossDropConfig {\n            min_relative_decrease: 0.3,\n        };\n\n        let report = verify_restricted_loss_drop(&restricted_loss, &config)\n            .expect(\"expected restricted loss drop verification to pass\");\n        assert_eq!(report.drop_step, 7000);\n        assert!(report.baseline_loss > report.drop_loss);\n        assert!((report.baseline_loss - report.drop_loss) / report.baseline_loss >= 0.3);\n    }\n\n    #[test]\n    fn restricted_loss_drop_verification_fails_without_drop() {\n        let restricted_loss = vec![\n            (0, 2.0),\n            (1000, 2.1),\n            (5000, 1.9),\n            (7000, 2.0),\n            (10000, 2.05),\n        ];\n        let config = RestrictedLossDropConfig {\n            min_relative_decrease: 0.3,\n        };\n\n        let err = verify_restricted_loss_drop(&restricted_loss, &config)\n            .expect_err(\"expected restricted loss drop verification to fail\");\n        assert!(err.contains(\"restricted loss decrease too small\"));\n    }\n\n    #[test]\n    fn mlp_activation_wave_verification_passes_on_rectified_sine() {\n        let n = ModularAdditionDataset::modulus();\n        let freq = 5.0;\n        let mut signal = Vec::with_capacity(n);\n        for idx in 0..n {\n            let angle = 2.0 * std::f64::consts::PI * freq * idx as f64 / n as f64;\n            let rectified = angle.sin().max(0.0);\n            let wobble = (2.0 * std::f64::consts::PI * 2.0 * idx as f64 / n as f64)\n                .sin()\n                .abs()\n                * 0.02;\n            signal.push(rectified + wobble);\n        }\n\n        let config = MlpWaveVerificationConfig {\n            max_negative_fraction: 0.0,\n            negative_tolerance: 1e-6,\n            min_dominant_ratio: 3.0,\n            min_correlation: 0.7,\n            phase_steps: 32,\n        };\n\n        let report =\n            verify_mlp_activation_waves_from_signals(&[(0, signal)], &config)\n                .expect(\"expected rectified sine to pass\");\n        assert_eq!(report.neuron_reports.len(), 1);\n    }\n\n    #[test]\n    fn mlp_activation_wave_verification_fails_on_noise() {\n        let n = ModularAdditionDataset::modulus();\n        let signal: Vec<f64> = (0..n).map(|idx| idx as f64 / n as f64).collect();\n\n        let config = MlpWaveVerificationConfig {\n            max_negative_fraction: 0.0,\n            negative_tolerance: 1e-6,\n            min_dominant_ratio: 4.0,\n            min_correlation: 0.8,\n            phase_steps: 16,\n        };\n\n        let err =\n            verify_mlp_activation_waves_from_signals(&[(0, signal)], &config)\n                .expect_err(\"expected linear signal to fail\");\n        assert!(err.contains(\"dominant frequency ratio\") || err.contains(\"correlation\"));\n    }\n\n    #[test]\n    fn pairwise_manifold_transition_passes_on_elliptic_signals() {\n        let n = ModularAdditionDataset::modulus();\n        let freq = 5.0;\n        let mut post_signals = Vec::new();\n        for neuron_idx in 0..7usize {\n            let phase = 2.0 * std::f64::consts::PI * neuron_idx as f64 / 7.0;\n            let signal: Vec<f64> = (0..n)\n                .map(|idx| {\n                    let angle = 2.0 * std::f64::consts::PI * freq * idx as f64 / n as f64 + phase;\n                    angle.sin()\n                })\n                .collect();\n            post_signals.push((neuron_idx, signal));\n        }\n\n        let mut pre_signals = Vec::new();\n        for neuron_idx in 0..7usize {\n            let signal: Vec<f64> = (0..n)\n                .map(|idx| idx as f64 / n as f64 + neuron_idx as f64 * 0.01)\n                .collect();\n            pre_signals.push((neuron_idx, signal));\n        }\n\n        let config = PairwiseManifoldConfig {\n            fixed_y: 0,\n            neuron_indices: (0..7).collect(),\n            min_post_score: 0.75,\n            max_pre_score: 0.55,\n            max_axis_ratio: 8.0,\n            min_pairs_passing: 8,\n        };\n\n        let report = verify_pairwise_manifold_transition_from_signals(\n            &pre_signals,\n            &post_signals,\n            &config,\n        )\n        .expect(\"expected pairwise manifold transition to pass\");\n        assert!(report.passing_pairs >= config.min_pairs_passing);\n    }\n\n    #[test]\n    fn pairwise_manifold_transition_fails_without_rings() {\n        let n = ModularAdditionDataset::modulus();\n        let mut signals = Vec::new();\n        for neuron_idx in 0..7usize {\n            let signal: Vec<f64> = (0..n)\n                .map(|idx| idx as f64 / n as f64 + neuron_idx as f64 * 0.02)\n                .collect();\n            signals.push((neuron_idx, signal));\n        }\n\n        let config = PairwiseManifoldConfig {\n            fixed_y: 0,\n            neuron_indices: (0..7).collect(),\n            min_post_score: 0.8,\n            max_pre_score: 0.55,\n            max_axis_ratio: 4.0,\n            min_pairs_passing: 5,\n        };\n\n        let err = verify_pairwise_manifold_transition_from_signals(\n            &signals,\n            &signals,\n            &config,\n        )\n        .expect_err(\"expected pairwise manifold transition to fail\");\n        assert!(err.contains(\"post-grok\") || err.contains(\"pre-grok\"));\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_passes_on_diagonal_corrugation() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create diagonal ridges: f(x,y) = sin(2Ï€ * freq * (x+y) / n)\n        let freq = 3.0;\n        for x in 0..n {\n            for y in 0..n {\n                let angle = 2.0 * std::f64::consts::PI * freq * (x + y) as f64 / n as f64;\n                surface[x][y] = angle.sin();\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let report = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect(\"expected diagonal ridge verification to pass\");\n        assert!(report.diagonal_score > report.axis_score);\n        assert!(report.ratio >= config.min_diagonal_ratio);\n        assert!(report.diagonal_score >= config.min_diagonal_score);\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_passes_on_anti_diagonal_pattern() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create anti-diagonal ridges: f(x,y) = sin(2Ï€ * freq * (x-y) / n)\n        let freq = 3.0;\n        for x in 0..n {\n            for y in 0..n {\n                let angle = 2.0 * std::f64::consts::PI * freq * (x as i32 - y as i32) as f64 / n as f64;\n                surface[x][y] = angle.sin();\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let report = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect(\"expected anti-diagonal ridge verification to pass\");\n        assert!(report.diagonal_score > report.axis_score);\n        assert!(report.ratio >= config.min_diagonal_ratio);\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_fails_on_axis_aligned_pattern() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create axis-aligned ridges: f(x,y) = sin(2Ï€ * freq * x / n)\n        let freq = 3.0;\n        for x in 0..n {\n            for y in 0..n {\n                let angle = 2.0 * std::f64::consts::PI * freq * x as f64 / n as f64;\n                surface[x][y] = angle.sin();\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let err = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect_err(\"expected axis-aligned pattern to fail diagonal verification\");\n        assert!(err.contains(\"diagonal/axis ratio\") || err.contains(\"diagonal autocorrelation\"));\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_fails_on_flat_surface() {\n        let n = 50;\n        let surface = vec![vec![1.5; n]; n];\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let err = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect_err(\"expected flat surface to fail\");\n        assert!(err.contains(\"surface variance is zero\"));\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_fails_on_random_noise() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create random noise (low autocorrelation everywhere)\n        for x in 0..n {\n            for y in 0..n {\n                surface[x][y] = ((x * 17 + y * 31) % 100) as f64 / 100.0;\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let err = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect_err(\"expected random noise to fail diagonal verification\");\n        assert!(err.contains(\"diagonal/axis ratio\") || err.contains(\"diagonal autocorrelation\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_passes_on_valid_grokking_curve() {\n        // Simulate typical grokking: train converges early, val plateaus then jumps\n        let train_acc = vec![\n            (1, 0.10),\n            (10, 0.50),\n            (100, 0.80),\n            (500, 0.98),\n            (1000, 0.99),\n            (2000, 0.995),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (10, 0.009),\n            (100, 0.01),\n            (500, 0.011),\n            (1000, 0.009),\n            (1500, 0.01),\n            (2000, 0.92),\n            (3000, 0.98),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 1500,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let report = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect(\"expected snake curve verification to pass\");\n\n        assert_eq!(report.train_converged_by_step, 500);\n        assert!(report.val_plateau_max_acc < config.val_plateau_max_acc);\n        assert_eq!(report.val_grok_step, 2000);\n        assert!(report.curve_shape_valid);\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_on_simultaneous_convergence() {\n        // Both train and val converge at the same time (no grokking)\n        let train_acc = vec![\n            (1, 0.10),\n            (100, 0.98),\n            (500, 0.99),\n        ];\n        let val_acc = vec![\n            (1, 0.10),\n            (100, 0.97),\n            (500, 0.98),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 200,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let err = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect_err(\"expected snake curve verification to fail\");\n        assert!(err.contains(\"plateau region too high\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_on_slow_train_convergence() {\n        // Train takes too long to converge (not the typical snake curve)\n        let train_acc = vec![\n            (1, 0.10),\n            (500, 0.50),\n            (1000, 0.70),\n            (2000, 0.98),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (500, 0.01),\n            (1000, 0.01),\n            (2000, 0.92),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 1500,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let err = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect_err(\"expected snake curve verification to fail\");\n        assert!(err.contains(\"converges at step 2000\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_without_val_jump() {\n        // Validation never jumps up (no generalization)\n        let train_acc = vec![\n            (1, 0.10),\n            (100, 0.98),\n            (500, 0.99),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (100, 0.01),\n            (500, 0.01),\n            (2000, 0.02),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 200,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let err = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect_err(\"expected snake curve verification to fail\");\n        assert!(err.contains(\"validation accuracy never reaches\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_on_wrong_phase_ordering() {\n        // Val jumps before train converges (backwards snake)\n        let train_acc = vec![\n            (1, 0.10),\n            (500, 0.50),\n            (2000, 0.98),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (100, 0.95),\n            (500, 0.98),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 200,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let result = verify_snake_curve_shape(&train_acc, &val_acc, &config);\n        // Should fail because val jumps before train converges (not a proper snake curve)\n        // This can fail for multiple reasons, but should definitely fail\n        assert!(result.is_err(), \"Expected snake curve verification to fail but it passed\");\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_passes_on_decreasing_norms() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 85.0),\n            (1000, 72.0),\n            (1500, 63.0),\n            (2000, 58.0),\n            (2500, 55.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let report = verify_weight_norm_decay(&weight_norms, Some(2000), &config)\n            .expect(\"expected weight norm decay verification to pass\");\n\n        assert_eq!(report.initial_norm, 100.0);\n        assert_eq!(report.final_norm, 55.0);\n        assert!((report.relative_decrease - 0.45).abs() < 0.01);\n        assert!(report.grok_step_norm.is_some());\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_fails_on_flat_norms() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 99.0),\n            (1000, 98.5),\n            (1500, 98.0),\n            (2000, 97.5),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let err = verify_weight_norm_decay(&weight_norms, None, &config)\n            .expect_err(\"expected weight norm decay verification to fail\");\n        assert!(err.contains(\"weight norm decrease\"));\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_fails_on_increasing_norms() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 95.0),\n            (1000, 105.0), // Increase here\n            (1500, 90.0),\n            (2000, 85.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let err = verify_weight_norm_decay(&weight_norms, None, &config)\n            .expect_err(\"expected weight norm decay verification to fail\");\n        assert!(err.contains(\"weight norm increased\"));\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_allows_small_noise() {\n        // Weight decay with small numerical noise (within 1% tolerance)\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 95.0),\n            (1000, 90.5),  // Small increase within tolerance\n            (1500, 90.0),\n            (2000, 85.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let report = verify_weight_norm_decay(&weight_norms, None, &config)\n            .expect(\"expected weight norm decay with small noise to pass\");\n\n        assert_eq!(report.initial_norm, 100.0);\n        assert_eq!(report.final_norm, 85.0);\n        assert!((report.relative_decrease - 0.15).abs() < 0.01);\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_finds_grok_step_norm() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (1000, 80.0),\n            (1800, 68.0),  // Within window of grok step (2200 - 500 = 1700)\n            (2000, 65.0),\n            (2200, 62.0),  // Around grok step\n            (3000, 55.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let report = verify_weight_norm_decay(&weight_norms, Some(2200), &config)\n            .expect(\"expected weight norm decay verification to pass\");\n\n        assert!(report.grok_step_norm.is_some());\n        let grok_norm = report.grok_step_norm.unwrap();\n        // Should find the first norm in the window [1700, 2700], which is 68.0 at step 1800\n        assert!((grok_norm - 68.0).abs() < 0.01);\n    }\n}\n","structuredPatch":[{"oldStart":875,"oldLines":6,"newStart":875,"newLines":8,"lines":["     Ok((score, radius_cv, axis_ratio))"," }"," ","+// Helper functions for verification - reserved for future integration","+#[allow(dead_code)]"," fn baseline_loss(","     losses: &[(usize, f64)],","     plateau_min_step: usize,"]}],"userModified":false,"replaceAll":false}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01FHeAivAxVjU7z6CtCEfx49","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01XdqPtUXPbbSDei9NQFxD79","name":"Edit","input":{"replace_all":false,"file_path":"/Users/admin/MechInt/src/verify.rs","old_string":"fn find_drop_step(","new_string":"#[allow(dead_code)]\nfn find_drop_step("}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":323,"cache_read_input_tokens":61978,"cache_creation":{"ephemeral_5m_input_tokens":323,"ephemeral_1h_input_tokens":0},"output_tokens":24,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"3f1c52e0-8790-4185-b98d-d2acd000ee43"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01XdqPtUXPbbSDei9NQFxD79","type":"tool_result","content":"The file /Users/admin/MechInt/src/verify.rs has been updated. Here's the result of running `cat -n` on a snippet of the edited file:\n   895â†’    let window = &before[before.len() - drop_window..];\n   896â†’    Ok(window.iter().sum::<f64>() / drop_window as f64)\n   897â†’}\n   898â†’\n   899â†’#[allow(dead_code)]\n   900â†’fn find_drop_step(\n   901â†’    losses: &[(usize, f64)],\n   902â†’    plateau_min_step: usize,\n   903â†’    baseline: f64,\n   904â†’    drop_fraction: f64,"}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"eff39634-2ccc-4a82-b78e-1054ddaf04dd","tool_use_result":{"filePath":"/Users/admin/MechInt/src/verify.rs","oldString":"fn find_drop_step(","newString":"#[allow(dead_code)]\nfn find_drop_step(","originalFile":"use burn::{\n    data::dataset::Dataset,\n    tensor::{backend::Backend, Int, Tensor},\n};\nuse rustfft::{num_complex::Complex, FftPlanner};\n\nuse crate::analysis::{AccuracyHistory, LossHistory};\nuse crate::data::ModularAdditionDataset;\nuse crate::model::Transformer;\n\n#[derive(Debug, Clone)]\npub struct GrokkingVerificationConfig {\n    pub early_train_acc_threshold: f32,\n    pub early_step_max: usize,\n    pub plateau_min_step: usize,\n    pub generalization_window: usize,\n    pub chance_accuracy: f32,\n    pub chance_tolerance: f32,\n    pub target_val_acc_threshold: f32,\n    pub accuracy_window: usize,\n    pub loss_window: usize,\n    pub loss_drop_fraction: f64,\n}\n\nimpl GrokkingVerificationConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        Self {\n            early_train_acc_threshold: 0.99,\n            early_step_max: 1000,\n            plateau_min_step: 1000,\n            generalization_window: 10_000,\n            chance_accuracy: 1.0 / modulus as f32,\n            chance_tolerance: 0.02,\n            target_val_acc_threshold: 0.99,\n            accuracy_window: 5,\n            loss_window: 5,\n            loss_drop_fraction: 0.5,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct GrokkingPhaseReport {\n    pub early_train_step: usize,\n    pub transition_step: usize,\n    pub plateau_max_val_acc: f32,\n    pub loss_drop_ratio: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossVerificationConfig {\n    pub plateau_min_step: usize,\n    pub drop_window: usize,\n    pub drop_fraction: f64,\n    pub min_step_lead: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossReport {\n    pub restricted_drop_step: usize,\n    pub full_drop_step: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossDropConfig {\n    pub min_relative_decrease: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossDropReport {\n    pub drop_step: usize,\n    pub drop_loss: f64,\n    pub baseline_loss: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct ExcludedLossSpikeConfig {\n    pub min_relative_increase: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct ExcludedLossSpikeReport {\n    pub spike_step: usize,\n    pub spike_loss: f64,\n    pub baseline_loss: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct MlpWaveVerificationConfig {\n    pub max_negative_fraction: f64,\n    pub negative_tolerance: f64,\n    pub min_dominant_ratio: f64,\n    pub min_correlation: f64,\n    pub phase_steps: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct MlpWaveNeuronReport {\n    pub neuron_index: usize,\n    pub dominant_frequency: usize,\n    pub dominant_ratio: f64,\n    pub negative_fraction: f64,\n    pub best_correlation: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct MlpWaveReport {\n    pub neuron_reports: Vec<MlpWaveNeuronReport>,\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldConfig {\n    pub fixed_y: usize,\n    pub neuron_indices: Vec<usize>,\n    pub min_post_score: f64,\n    pub max_pre_score: f64,\n    pub max_axis_ratio: f64,\n    pub min_pairs_passing: usize,\n}\n\nimpl PairwiseManifoldConfig {\n    pub fn default_for_modulus(_modulus: usize) -> Self {\n        Self {\n            fixed_y: 0,\n            neuron_indices: (0..7).collect(),\n            min_post_score: 0.75,\n            max_pre_score: 0.55,\n            max_axis_ratio: 8.0,\n            min_pairs_passing: 10,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldPairReport {\n    pub neuron_a: usize,\n    pub neuron_b: usize,\n    pub circularity_score: f64,\n    pub radius_cv: f64,\n    pub axis_ratio: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldReport {\n    pub pair_reports: Vec<PairwiseManifoldPairReport>,\n    pub average_score: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldTransitionReport {\n    pub pre_average_score: f64,\n    pub post_average_score: f64,\n    pub passing_pairs: usize,\n    pub total_pairs: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct DiagonalRidgeConfig {\n    pub neuron_index: usize,\n    pub max_lag: usize,\n    pub min_diagonal_ratio: f64,\n    pub min_diagonal_score: f64,\n}\n\nimpl DiagonalRidgeConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        let max_lag = (modulus / 10).max(4).min(20);\n        Self {\n            neuron_index: 10,\n            max_lag,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct DiagonalRidgeReport {\n    pub neuron_index: usize,\n    pub diagonal_score: f64,\n    pub axis_score: f64,\n    pub ratio: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct SnakeCurveConfig {\n    pub early_step_max: usize,\n    pub train_convergence_threshold: f32,\n    pub val_plateau_min_step: usize,\n    pub val_plateau_max_acc: f32,\n    pub val_final_threshold: f32,\n}\n\nimpl SnakeCurveConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        Self {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 1000,\n            val_plateau_max_acc: 1.0 / modulus as f32 + 0.05,\n            val_final_threshold: 0.90,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct WeightNormDecayConfig {\n    pub min_relative_decrease: f64,\n    pub plateau_min_step: usize,\n    pub grok_step_window: usize,\n}\n\nimpl WeightNormDecayConfig {\n    pub fn default_for_grokking() -> Self {\n        Self {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct WeightNormDecayReport {\n    pub initial_norm: f64,\n    pub final_norm: f64,\n    pub relative_decrease: f64,\n    pub grok_step_norm: Option<f64>,\n}\n\n#[derive(Debug, Clone)]\npub struct SnakeCurveReport {\n    pub train_converged_by_step: usize,\n    pub val_plateau_max_acc: f32,\n    pub val_grok_step: usize,\n    pub curve_shape_valid: bool,\n}\n\npub fn verify_grokking_phase_transition(\n    loss_history: &LossHistory,\n    accuracy_history: &AccuracyHistory,\n    config: &GrokkingVerificationConfig,\n) -> Result<GrokkingPhaseReport, String> {\n    let (early_train_step, _) = accuracy_history\n        .train_snapshots\n        .iter()\n        .find(|(_, acc)| *acc >= config.early_train_acc_threshold)\n        .ok_or_else(|| \"train accuracy never reaches threshold\".to_string())?;\n\n    if *early_train_step > config.early_step_max {\n        return Err(format!(\n            \"train accuracy reaches {:.2} after step {}, expected before {}\",\n            config.early_train_acc_threshold, early_train_step, config.early_step_max\n        ));\n    }\n\n    let plateau_max_val_acc = accuracy_history\n        .val_snapshots\n        .iter()\n        .filter(|(step, _)| *step <= config.plateau_min_step)\n        .map(|(_, acc)| *acc)\n        .fold(None::<f32>, |accum, value| {\n            Some(accum.map_or(value, |max_val| max_val.max(value)))\n        })\n        .ok_or_else(|| \"no validation accuracy samples before plateau_min_step\".to_string())?;\n\n    if plateau_max_val_acc > config.chance_accuracy + config.chance_tolerance {\n        return Err(format!(\n            \"validation accuracy before step {} exceeds chance (max {:.4})\",\n            config.plateau_min_step, plateau_max_val_acc\n        ));\n    }\n\n    let (transition_step, _) = accuracy_history\n        .val_snapshots\n        .iter()\n        .find(|(step, acc)| {\n            *step >= config.plateau_min_step && *acc >= config.target_val_acc_threshold\n        })\n        .ok_or_else(|| \"validation accuracy never reaches target threshold\".to_string())?;\n\n    let latest_allowed = config.plateau_min_step + config.generalization_window;\n    if *transition_step > latest_allowed {\n        return Err(format!(\n            \"validation accuracy reaches {:.2} at step {}, expected before {}\",\n            config.target_val_acc_threshold, transition_step, latest_allowed\n        ));\n    }\n\n    verify_accuracy_jump(\n        &accuracy_history.val_snapshots,\n        *transition_step,\n        config.accuracy_window,\n        config.chance_accuracy,\n        config.chance_tolerance,\n        config.target_val_acc_threshold,\n    )?;\n\n    let loss_drop_ratio = verify_loss_drop(\n        &loss_history.val_snapshots,\n        *transition_step,\n        config.loss_window,\n        config.loss_drop_fraction,\n    )?;\n\n    Ok(GrokkingPhaseReport {\n        early_train_step: *early_train_step,\n        transition_step: *transition_step,\n        plateau_max_val_acc,\n        loss_drop_ratio,\n    })\n}\n\nfn verify_accuracy_jump(\n    val_accuracy: &[(usize, f32)],\n    transition_step: usize,\n    window: usize,\n    chance_accuracy: f32,\n    chance_tolerance: f32,\n    target_val_acc_threshold: f32,\n) -> Result<(), String> {\n    let before_values: Vec<f32> = val_accuracy\n        .iter()\n        .filter(|(step, _)| *step < transition_step)\n        .map(|(_, acc)| *acc)\n        .collect();\n\n    let after_values: Vec<f32> = val_accuracy\n        .iter()\n        .filter(|(step, _)| *step >= transition_step)\n        .map(|(_, acc)| *acc)\n        .collect();\n\n    if before_values.len() < window || after_values.len() < window {\n        return Err(\"insufficient accuracy samples around transition\".to_string());\n    }\n\n    let before_avg: f32 =\n        before_values[before_values.len() - window..].iter().sum::<f32>() / window as f32;\n    let after_avg: f32 = after_values[..window].iter().sum::<f32>() / window as f32;\n\n    if before_avg > chance_accuracy + chance_tolerance {\n        return Err(format!(\n            \"validation accuracy plateau too high before transition (avg {:.4})\",\n            before_avg\n        ));\n    }\n\n    if after_avg < target_val_acc_threshold {\n        return Err(format!(\n            \"validation accuracy after transition too low (avg {:.4})\",\n            after_avg\n        ));\n    }\n\n    Ok(())\n}\n\nfn verify_loss_drop(\n    val_losses: &[(usize, f64)],\n    transition_step: usize,\n    loss_window: usize,\n    loss_drop_fraction: f64,\n) -> Result<f64, String> {\n    let before_losses: Vec<f64> = val_losses\n        .iter()\n        .filter(|(step, _)| *step < transition_step)\n        .map(|(_, loss)| *loss)\n        .collect();\n\n    let after_losses: Vec<f64> = val_losses\n        .iter()\n        .filter(|(step, _)| *step >= transition_step)\n        .map(|(_, loss)| *loss)\n        .collect();\n\n    if before_losses.len() < loss_window || after_losses.len() < loss_window {\n        return Err(\"insufficient loss samples around transition\".to_string());\n    }\n\n    let before_avg: f64 =\n        before_losses[before_losses.len() - loss_window..].iter().sum::<f64>()\n            / loss_window as f64;\n    let after_avg: f64 = after_losses[..loss_window].iter().sum::<f64>() / loss_window as f64;\n\n    if after_avg > before_avg * (1.0 - loss_drop_fraction) {\n        return Err(format!(\n            \"validation loss drop too small (before {:.4}, after {:.4})\",\n            before_avg, after_avg\n        ));\n    }\n\n    Ok(after_avg / before_avg)\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_restricted_loss_early_drop(\n    full_losses: &[(usize, f64)],\n    restricted_losses: &[(usize, f64)],\n    config: &RestrictedLossVerificationConfig,\n) -> Result<RestrictedLossReport, String> {\n    let restricted_baseline =\n        baseline_loss(restricted_losses, config.plateau_min_step, config.drop_window)?;\n    let full_baseline = baseline_loss(full_losses, config.plateau_min_step, config.drop_window)?;\n\n    let restricted_drop_step = find_drop_step(\n        restricted_losses,\n        config.plateau_min_step,\n        restricted_baseline,\n        config.drop_fraction,\n    )?;\n    let full_drop_step = find_drop_step(\n        full_losses,\n        config.plateau_min_step,\n        full_baseline,\n        config.drop_fraction,\n    )?;\n\n    if restricted_drop_step + config.min_step_lead > full_drop_step {\n        return Err(format!(\n            \"restricted loss drops at step {}, expected at least {} steps before full loss (step {})\",\n            restricted_drop_step, config.min_step_lead, full_drop_step\n        ));\n    }\n\n    Ok(RestrictedLossReport {\n        restricted_drop_step,\n        full_drop_step,\n    })\n}\n\npub fn verify_restricted_loss_drop(\n    restricted_losses: &[(usize, f64)],\n    config: &RestrictedLossDropConfig,\n) -> Result<RestrictedLossDropReport, String> {\n    if restricted_losses.len() < 3 {\n        return Err(\"restricted loss history requires at least 3 points\".to_string());\n    }\n\n    let first = restricted_losses.first().ok_or_else(|| {\n        \"restricted loss history is empty\".to_string()\n    })?;\n    let baseline_loss = first.1;\n\n    let (min_idx, (min_step, min_loss)) = restricted_losses\n        .iter()\n        .enumerate()\n        .min_by(|(_, a), (_, b)| {\n            a.1.partial_cmp(&b.1).unwrap_or(std::cmp::Ordering::Equal)\n        })\n        .ok_or_else(|| \"restricted loss history is empty\".to_string())?;\n\n    if min_idx == 0 {\n        return Err(\"restricted loss minimum occurs at start (no drop detected)\".to_string());\n    }\n\n    let relative_decrease = (baseline_loss - min_loss) / baseline_loss;\n    if relative_decrease < config.min_relative_decrease {\n        return Err(format!(\n            \"restricted loss decrease too small (baseline {:.4}, min {:.4}, decrease {:.1}%)\",\n            baseline_loss, min_loss, relative_decrease * 100.0\n        ));\n    }\n\n    Ok(RestrictedLossDropReport {\n        drop_step: *min_step,\n        drop_loss: *min_loss,\n        baseline_loss,\n    })\n}\n\npub fn verify_excluded_loss_spike(\n    excluded_losses: &[(usize, f64)],\n    config: &ExcludedLossSpikeConfig,\n) -> Result<ExcludedLossSpikeReport, String> {\n    if excluded_losses.len() < 3 {\n        return Err(\"excluded loss history requires at least 3 points\".to_string());\n    }\n\n    let first = excluded_losses.first().ok_or_else(|| {\n        \"excluded loss history is empty\".to_string()\n    })?;\n    let last = excluded_losses.last().ok_or_else(|| {\n        \"excluded loss history is empty\".to_string()\n    })?;\n    let baseline_loss = (first.1 + last.1) / 2.0;\n\n    let (peak_idx, (peak_step, peak_loss)) = excluded_losses\n        .iter()\n        .enumerate()\n        .max_by(|(_, a), (_, b)| {\n            a.1.partial_cmp(&b.1).unwrap_or(std::cmp::Ordering::Equal)\n        })\n        .ok_or_else(|| \"excluded loss history is empty\".to_string())?;\n\n    if peak_idx == 0 || peak_idx + 1 == excluded_losses.len() {\n        return Err(\"excluded loss spike occurs at boundary\".to_string());\n    }\n\n    if peak_loss < &(baseline_loss * (1.0 + config.min_relative_increase)) {\n        return Err(format!(\n            \"excluded loss spike too small (baseline {:.4}, peak {:.4})\",\n            baseline_loss, peak_loss\n        ));\n    }\n\n    Ok(ExcludedLossSpikeReport {\n        spike_step: *peak_step,\n        spike_loss: *peak_loss,\n        baseline_loss,\n    })\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_mlp_activation_waves<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n    fixed_y: usize,\n    neuron_indices: &[usize],\n    config: &MlpWaveVerificationConfig,\n) -> Result<MlpWaveReport, String> {\n    let activations = collect_mlp_post_relu_activations(model, device, fixed_y)?;\n    if neuron_indices.is_empty() {\n        return Err(\"neuron indices list is empty\".to_string());\n    }\n\n    let d_ff = activations.len();\n    for &idx in neuron_indices {\n        if idx >= d_ff {\n            return Err(format!(\n                \"neuron index {} out of range (d_ff={})\",\n                idx, d_ff\n            ));\n        }\n    }\n\n    let signals: Vec<(usize, Vec<f64>)> = neuron_indices\n        .iter()\n        .map(|&idx| (idx, activations[idx].clone()))\n        .collect();\n    verify_mlp_activation_waves_from_signals(&signals, config)\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_mlp_activation_waves_from_signals(\n    signals: &[(usize, Vec<f64>)],\n    config: &MlpWaveVerificationConfig,\n) -> Result<MlpWaveReport, String> {\n    if signals.is_empty() {\n        return Err(\"no activation signals provided\".to_string());\n    }\n\n    let mut reports = Vec::with_capacity(signals.len());\n    for (neuron_index, signal) in signals {\n        let report = analyze_mlp_activation_wave(*neuron_index, signal, config)?;\n        reports.push(report);\n    }\n\n    Ok(MlpWaveReport {\n        neuron_reports: reports,\n    })\n}\n\npub fn verify_mlp_pairwise_manifold_transition<B: Backend>(\n    pre_model: &Transformer<B>,\n    post_model: &Transformer<B>,\n    device: &B::Device,\n    config: &PairwiseManifoldConfig,\n) -> Result<PairwiseManifoldTransitionReport, String> {\n    let modulus = ModularAdditionDataset::modulus();\n    if config.fixed_y >= modulus {\n        return Err(format!(\n            \"fixed_y {} out of range (modulus {})\",\n            config.fixed_y, modulus\n        ));\n    }\n\n    let pre_activations = collect_mlp_post_relu_activations(pre_model, device, config.fixed_y)?;\n    let post_activations = collect_mlp_post_relu_activations(post_model, device, config.fixed_y)?;\n\n    let d_ff = pre_activations.len();\n    if post_activations.len() != d_ff {\n        return Err(\"post activations shape mismatch\".to_string());\n    }\n\n    if config.neuron_indices.len() < 2 {\n        return Err(\"need at least two neuron indices\".to_string());\n    }\n\n    for &idx in &config.neuron_indices {\n        if idx >= d_ff {\n            return Err(format!(\n                \"neuron index {} out of range (d_ff={})\",\n                idx, d_ff\n            ));\n        }\n    }\n\n    let pre_signals: Vec<(usize, Vec<f64>)> = config\n        .neuron_indices\n        .iter()\n        .map(|&idx| (idx, pre_activations[idx].clone()))\n        .collect();\n    let post_signals: Vec<(usize, Vec<f64>)> = config\n        .neuron_indices\n        .iter()\n        .map(|&idx| (idx, post_activations[idx].clone()))\n        .collect();\n\n    verify_pairwise_manifold_transition_from_signals(&pre_signals, &post_signals, config)\n}\n\npub fn collect_mlp_activation_surface<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n    neuron_index: usize,\n) -> Result<Vec<Vec<f64>>, String> {\n    let modulus = ModularAdditionDataset::modulus();\n    let equals_token = ModularAdditionDataset::equals_token();\n\n    let mut inputs_vec = Vec::with_capacity(modulus * modulus * 3);\n    for x in 0..modulus {\n        for y in 0..modulus {\n            inputs_vec.push(x as i32);\n            inputs_vec.push(y as i32);\n            inputs_vec.push(equals_token as i32);\n        }\n    }\n\n    let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n        .reshape([modulus * modulus, 3]);\n    let (_, mlp_acts) = model.forward_with_mlp_activations(inputs);\n    let [batch_size, d_ff] = mlp_acts.dims();\n    if batch_size != modulus * modulus {\n        return Err(format!(\n            \"unexpected activation batch size {} (expected {})\",\n            batch_size,\n            modulus * modulus\n        ));\n    }\n    if neuron_index >= d_ff {\n        return Err(format!(\n            \"neuron index {} out of range (d_ff={})\",\n            neuron_index, d_ff\n        ));\n    }\n\n    let data: Vec<f32> = mlp_acts.into_data().to_vec().unwrap();\n    let mut surface = vec![vec![0.0f64; modulus]; modulus];\n    for x in 0..modulus {\n        for y in 0..modulus {\n            let batch = x * modulus + y;\n            let idx = batch * d_ff + neuron_index;\n            surface[x][y] = data[idx] as f64;\n        }\n    }\n\n    Ok(surface)\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_constructive_interference_surface_from_grid(\n    surface: &[Vec<f64>],\n    config: &DiagonalRidgeConfig,\n) -> Result<DiagonalRidgeReport, String> {\n    let (diag_main, diag_anti, axis_x, axis_y) =\n        surface_autocorrelation_scores(surface, config.max_lag)?;\n\n    let diagonal_score = diag_main.max(diag_anti);\n    let axis_score = axis_x.max(axis_y);\n    let ratio = if axis_score > 0.0 {\n        diagonal_score / axis_score\n    } else {\n        f64::INFINITY\n    };\n\n    if diagonal_score < config.min_diagonal_score {\n        return Err(format!(\n            \"diagonal autocorrelation {:.3} below threshold {:.3}\",\n            diagonal_score, config.min_diagonal_score\n        ));\n    }\n\n    if ratio < config.min_diagonal_ratio {\n        return Err(format!(\n            \"diagonal/axis ratio {:.3} below threshold {:.3}\",\n            ratio, config.min_diagonal_ratio\n        ));\n    }\n\n    Ok(DiagonalRidgeReport {\n        neuron_index: config.neuron_index,\n        diagonal_score,\n        axis_score,\n        ratio,\n    })\n}\n\npub fn verify_pairwise_manifold_transition_from_signals(\n    pre_signals: &[(usize, Vec<f64>)],\n    post_signals: &[(usize, Vec<f64>)],\n    config: &PairwiseManifoldConfig,\n) -> Result<PairwiseManifoldTransitionReport, String> {\n    let pre_report = evaluate_pairwise_manifolds_from_signals(pre_signals)?;\n    let post_report = evaluate_pairwise_manifolds_from_signals(post_signals)?;\n\n    if pre_report.average_score > config.max_pre_score {\n        return Err(format!(\n            \"pre-grok average circularity {:.3} exceeds threshold {:.3}\",\n            pre_report.average_score, config.max_pre_score\n        ));\n    }\n\n    let passing_pairs = post_report\n        .pair_reports\n        .iter()\n        .filter(|pair| {\n            pair.circularity_score >= config.min_post_score\n                && pair.axis_ratio <= config.max_axis_ratio\n        })\n        .count();\n    if passing_pairs < config.min_pairs_passing {\n        return Err(format!(\n            \"post-grok only {} pairs meet circularity threshold (need {})\",\n            passing_pairs, config.min_pairs_passing\n        ));\n    }\n\n    Ok(PairwiseManifoldTransitionReport {\n        pre_average_score: pre_report.average_score,\n        post_average_score: post_report.average_score,\n        passing_pairs,\n        total_pairs: post_report.pair_reports.len(),\n    })\n}\n\nfn evaluate_pairwise_manifolds_from_signals(\n    signals: &[(usize, Vec<f64>)],\n) -> Result<PairwiseManifoldReport, String> {\n    if signals.len() < 2 {\n        return Err(\"need at least two activation signals\".to_string());\n    }\n\n    let signal_len = signals[0].1.len();\n    if signal_len < 3 {\n        return Err(\"activation signal too short\".to_string());\n    }\n\n    for (idx, signal) in signals {\n        if signal.len() != signal_len {\n            return Err(format!(\n                \"signal length mismatch for neuron {}\",\n                idx\n            ));\n        }\n    }\n\n    let mut reports = Vec::new();\n    for i in 0..signals.len() {\n        for j in (i + 1)..signals.len() {\n            let (neuron_a, signal_a) = &signals[i];\n            let (neuron_b, signal_b) = &signals[j];\n            let points: Vec<(f64, f64)> = signal_a\n                .iter()\n                .zip(signal_b.iter())\n                .map(|(x, y)| (*x, *y))\n                .collect();\n            let (score, radius_cv, axis_ratio) =\n                match pairwise_circularity_metrics(&points) {\n                    Ok(metrics) => metrics,\n                    Err(_) => (0.0, 1.0, f64::INFINITY),\n                };\n            reports.push(PairwiseManifoldPairReport {\n                neuron_a: *neuron_a,\n                neuron_b: *neuron_b,\n                circularity_score: score,\n                radius_cv,\n                axis_ratio,\n            });\n        }\n    }\n\n    let average_score = reports\n        .iter()\n        .map(|pair| pair.circularity_score)\n        .sum::<f64>()\n        / reports.len() as f64;\n\n    Ok(PairwiseManifoldReport {\n        pair_reports: reports,\n        average_score,\n    })\n}\n\nfn pairwise_circularity_metrics(points: &[(f64, f64)]) -> Result<(f64, f64, f64), String> {\n    if points.len() < 3 {\n        return Err(\"not enough points for circularity\".to_string());\n    }\n\n    let n = points.len() as f64;\n    let mean_x = points.iter().map(|(x, _)| *x).sum::<f64>() / n;\n    let mean_y = points.iter().map(|(_, y)| *y).sum::<f64>() / n;\n\n    let mut var_x = 0.0;\n    let mut var_y = 0.0;\n    let mut cov_xy = 0.0;\n    for (x, y) in points {\n        let dx = x - mean_x;\n        let dy = y - mean_y;\n        var_x += dx * dx;\n        var_y += dy * dy;\n        cov_xy += dx * dy;\n    }\n    var_x /= n;\n    var_y /= n;\n    cov_xy /= n;\n\n    let trace = var_x + var_y;\n    let det = var_x * var_y - cov_xy * cov_xy;\n    if det <= 1.0e-12 || trace <= 0.0 {\n        return Err(\"degenerate covariance\".to_string());\n    }\n\n    let half_trace = trace * 0.5;\n    let disc = (half_trace * half_trace - det).max(0.0).sqrt();\n    let lambda1 = half_trace + disc;\n    let lambda2 = half_trace - disc;\n    if lambda2 <= 0.0 {\n        return Err(\"degenerate eigenvalues\".to_string());\n    }\n\n    let axis_ratio = (lambda1 / lambda2).sqrt();\n\n    let mut v1_x = cov_xy;\n    let mut v1_y = lambda1 - var_x;\n    if v1_x.abs() < 1.0e-12 && v1_y.abs() < 1.0e-12 {\n        v1_x = 1.0;\n        v1_y = 0.0;\n    }\n    let norm = (v1_x * v1_x + v1_y * v1_y).sqrt();\n    v1_x /= norm;\n    v1_y /= norm;\n    let v2_x = -v1_y;\n    let v2_y = v1_x;\n\n    let mut radii = Vec::with_capacity(points.len());\n    for (x, y) in points {\n        let dx = x - mean_x;\n        let dy = y - mean_y;\n        let p1 = dx * v1_x + dy * v1_y;\n        let p2 = dx * v2_x + dy * v2_y;\n        let w1 = p1 / lambda1.sqrt();\n        let w2 = p2 / lambda2.sqrt();\n        radii.push((w1 * w1 + w2 * w2).sqrt());\n    }\n\n    let mean_radius = radii.iter().sum::<f64>() / radii.len() as f64;\n    if mean_radius == 0.0 {\n        return Err(\"zero mean radius\".to_string());\n    }\n\n    let mut variance = 0.0;\n    for r in &radii {\n        let diff = r - mean_radius;\n        variance += diff * diff;\n    }\n    let std = (variance / radii.len() as f64).sqrt();\n    let radius_cv = std / mean_radius;\n    let score = 1.0 / (1.0 + radius_cv);\n\n    Ok((score, radius_cv, axis_ratio))\n}\n\n// Helper functions for verification - reserved for future integration\n#[allow(dead_code)]\nfn baseline_loss(\n    losses: &[(usize, f64)],\n    plateau_min_step: usize,\n    drop_window: usize,\n) -> Result<f64, String> {\n    let before: Vec<f64> = losses\n        .iter()\n        .filter(|(step, _)| *step <= plateau_min_step)\n        .map(|(_, loss)| *loss)\n        .collect();\n\n    if before.len() < drop_window || drop_window == 0 {\n        return Err(\"insufficient loss samples for baseline\".to_string());\n    }\n\n    let window = &before[before.len() - drop_window..];\n    Ok(window.iter().sum::<f64>() / drop_window as f64)\n}\n\nfn find_drop_step(\n    losses: &[(usize, f64)],\n    plateau_min_step: usize,\n    baseline: f64,\n    drop_fraction: f64,\n) -> Result<usize, String> {\n    losses\n        .iter()\n        .filter(|(step, _)| *step >= plateau_min_step)\n        .find(|(_, loss)| *loss <= baseline * (1.0 - drop_fraction))\n        .map(|(step, _)| *step)\n        .ok_or_else(|| \"loss never drops below threshold\".to_string())\n}\n\nfn collect_mlp_post_relu_activations<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n    fixed_y: usize,\n) -> Result<Vec<Vec<f64>>, String> {\n    let modulus = ModularAdditionDataset::modulus();\n    let equals_token = ModularAdditionDataset::equals_token();\n\n    let mut inputs_vec = Vec::with_capacity(modulus * 3);\n    for x in 0..modulus {\n        inputs_vec.push(x as i32);\n        inputs_vec.push(fixed_y as i32);\n        inputs_vec.push(equals_token as i32);\n    }\n\n    let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n        .reshape([modulus, 3]);\n    let (_, mlp_acts) = model.forward_with_mlp_activations(inputs);\n    let [batch_size, d_ff] = mlp_acts.dims();\n\n    if batch_size != modulus {\n        return Err(format!(\n            \"unexpected activation batch size {} (expected {})\",\n            batch_size, modulus\n        ));\n    }\n\n    let data: Vec<f32> = mlp_acts.into_data().to_vec().unwrap();\n    let mut per_neuron = vec![Vec::with_capacity(batch_size); d_ff];\n    for batch in 0..batch_size {\n        for neuron in 0..d_ff {\n            let idx = batch * d_ff + neuron;\n            per_neuron[neuron].push(data[idx] as f64);\n        }\n    }\n\n    Ok(per_neuron)\n}\n\nfn surface_autocorrelation_scores(\n    surface: &[Vec<f64>],\n    max_lag: usize,\n) -> Result<(f64, f64, f64, f64), String> {\n    let (height, width) = surface_dimensions(surface)?;\n    if height < 2 || width < 2 {\n        return Err(\"surface grid too small\".to_string());\n    }\n\n    let (mean, var) = surface_mean_variance(surface)?;\n    if var == 0.0 {\n        return Err(\"surface variance is zero\".to_string());\n    }\n\n    let max_lag = max_lag.min(height.saturating_sub(1)).min(width.saturating_sub(1));\n    if max_lag == 0 {\n        return Err(\"max_lag too small for surface\".to_string());\n    }\n\n    let diag_main = average_autocorrelation(surface, mean, var, max_lag, 1, 1)?;\n    let diag_anti = average_autocorrelation(surface, mean, var, max_lag, 1, -1)?;\n    let axis_x = average_autocorrelation(surface, mean, var, max_lag, 1, 0)?;\n    let axis_y = average_autocorrelation(surface, mean, var, max_lag, 0, 1)?;\n\n    Ok((diag_main, diag_anti, axis_x, axis_y))\n}\n\nfn surface_dimensions(surface: &[Vec<f64>]) -> Result<(usize, usize), String> {\n    let height = surface.len();\n    if height == 0 {\n        return Err(\"surface is empty\".to_string());\n    }\n    let width = surface[0].len();\n    if width == 0 {\n        return Err(\"surface has empty rows\".to_string());\n    }\n    for (idx, row) in surface.iter().enumerate() {\n        if row.len() != width {\n            return Err(format!(\n                \"surface row {} has length {}, expected {}\",\n                idx,\n                row.len(),\n                width\n            ));\n        }\n    }\n    Ok((height, width))\n}\n\nfn surface_mean_variance(surface: &[Vec<f64>]) -> Result<(f64, f64), String> {\n    let (height, width) = surface_dimensions(surface)?;\n    let mut sum = 0.0;\n    for row in surface {\n        for value in row {\n            sum += *value;\n        }\n    }\n    let count = (height * width) as f64;\n    let mean = sum / count;\n    let mut variance = 0.0;\n    for row in surface {\n        for value in row {\n            let diff = *value - mean;\n            variance += diff * diff;\n        }\n    }\n    variance /= count;\n    Ok((mean, variance))\n}\n\nfn average_autocorrelation(\n    surface: &[Vec<f64>],\n    mean: f64,\n    variance: f64,\n    max_lag: usize,\n    dx: i32,\n    dy: i32,\n) -> Result<f64, String> {\n    let (height, width) = surface_dimensions(surface)?;\n    let mut total = 0.0;\n    let mut lag_count = 0usize;\n\n    for lag in 1..=max_lag {\n        let mut sum = 0.0;\n        let mut count = 0usize;\n        let lag = lag as i32;\n        for x in 0..height {\n            for y in 0..width {\n                let x2 = x as i32 + dx * lag;\n                let y2 = y as i32 + dy * lag;\n                if x2 < 0 || y2 < 0 {\n                    continue;\n                }\n                let x2 = x2 as usize;\n                let y2 = y2 as usize;\n                if x2 >= height || y2 >= width {\n                    continue;\n                }\n                let a = surface[x][y] - mean;\n                let b = surface[x2][y2] - mean;\n                sum += a * b;\n                count += 1;\n            }\n        }\n        if count == 0 {\n            continue;\n        }\n        let corr = sum / (count as f64 * variance);\n        total += corr.abs();\n        lag_count += 1;\n    }\n\n    if lag_count == 0 {\n        return Err(\"no valid lags for autocorrelation\".to_string());\n    }\n\n    Ok(total / lag_count as f64)\n}\n\nfn analyze_mlp_activation_wave(\n    neuron_index: usize,\n    signal: &[f64],\n    config: &MlpWaveVerificationConfig,\n) -> Result<MlpWaveNeuronReport, String> {\n    if signal.len() < 3 {\n        return Err(format!(\n            \"activation signal for neuron {} is too short\",\n            neuron_index\n        ));\n    }\n\n    let negative_count = signal\n        .iter()\n        .filter(|value| **value < -config.negative_tolerance)\n        .count();\n    let negative_fraction = negative_count as f64 / signal.len() as f64;\n    if negative_fraction > config.max_negative_fraction {\n        return Err(format!(\n            \"neuron {} has {:.3} negative activations\",\n            neuron_index, negative_fraction\n        ));\n    }\n\n    let (dominant_frequency, dominant_ratio) = dominant_frequency_ratio(signal)?;\n    if dominant_ratio < config.min_dominant_ratio {\n        return Err(format!(\n            \"neuron {} dominant frequency ratio {:.3} below threshold {:.3}\",\n            neuron_index, dominant_ratio, config.min_dominant_ratio\n        ));\n    }\n\n    let best_correlation =\n        best_rectified_sine_correlation(signal, dominant_frequency, config.phase_steps);\n    if best_correlation < config.min_correlation {\n        return Err(format!(\n            \"neuron {} rectified sine correlation {:.3} below threshold {:.3}\",\n            neuron_index, best_correlation, config.min_correlation\n        ));\n    }\n\n    Ok(MlpWaveNeuronReport {\n        neuron_index,\n        dominant_frequency,\n        dominant_ratio,\n        negative_fraction,\n        best_correlation,\n    })\n}\n\nfn dominant_frequency_ratio(signal: &[f64]) -> Result<(usize, f64), String> {\n    if signal.len() < 2 {\n        return Err(\"signal too short for FFT\".to_string());\n    }\n\n    let mut planner = FftPlanner::new();\n    let fft = planner.plan_fft_forward(signal.len());\n    let mut buffer: Vec<Complex<f64>> = signal\n        .iter()\n        .map(|&x| Complex::new(x, 0.0))\n        .collect();\n    fft.process(&mut buffer);\n    let magnitudes: Vec<f64> = buffer.iter().map(|c| c.norm()).collect();\n\n    if magnitudes.len() < 2 {\n        return Err(\"insufficient FFT bins\".to_string());\n    }\n\n    let mut dominant_idx = 1usize;\n    let mut dominant_mag = magnitudes[1];\n    for (idx, &mag) in magnitudes.iter().enumerate().skip(2) {\n        if mag > dominant_mag {\n            dominant_mag = mag;\n            dominant_idx = idx;\n        }\n    }\n\n    if dominant_idx == 0 {\n        return Err(\"dominant frequency is zero\".to_string());\n    }\n\n    let mut nonzero = magnitudes[1..].to_vec();\n    nonzero.sort_by(|a, b| a.partial_cmp(b).unwrap_or(std::cmp::Ordering::Equal));\n    let median_mag = nonzero[nonzero.len() / 2];\n    let ratio = if median_mag == 0.0 {\n        dominant_mag\n    } else {\n        dominant_mag / median_mag\n    };\n\n    Ok((dominant_idx, ratio))\n}\n\nfn best_rectified_sine_correlation(signal: &[f64], frequency: usize, phase_steps: usize) -> f64 {\n    if signal.is_empty() || frequency == 0 || phase_steps == 0 {\n        return 0.0;\n    }\n\n    let tau = std::f64::consts::PI * 2.0;\n    let mut best = -1.0;\n    for step in 0..phase_steps {\n        let phase = tau * step as f64 / phase_steps as f64;\n        let wave: Vec<f64> = (0..signal.len())\n            .map(|idx| {\n                let angle = tau * frequency as f64 * idx as f64 / signal.len() as f64 + phase;\n                angle.sin().max(0.0)\n            })\n            .collect();\n        let corr = pearson_correlation(signal, &wave);\n        if corr > best {\n            best = corr;\n        }\n    }\n\n    best\n}\n\nfn pearson_correlation(a: &[f64], b: &[f64]) -> f64 {\n    if a.len() != b.len() || a.is_empty() {\n        return 0.0;\n    }\n\n    let mean_a = a.iter().sum::<f64>() / a.len() as f64;\n    let mean_b = b.iter().sum::<f64>() / b.len() as f64;\n\n    let mut num = 0.0;\n    let mut denom_a = 0.0;\n    let mut denom_b = 0.0;\n    for (x, y) in a.iter().zip(b.iter()) {\n        let da = x - mean_a;\n        let db = y - mean_b;\n        num += da * db;\n        denom_a += da * da;\n        denom_b += db * db;\n    }\n\n    if denom_a == 0.0 || denom_b == 0.0 {\n        return 0.0;\n    }\n\n    num / (denom_a.sqrt() * denom_b.sqrt())\n}\n\n/// Verify that the model actually learned modular addition\n/// by testing it on ALL possible examples systematically\npub fn verify_full_accuracy<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n) -> (f32, f32) {\n    println!(\"ðŸ” Verifying model on COMPLETE dataset (no sampling)...\");\n\n    let train_dataset = ModularAdditionDataset::new(true, 42);\n    let val_dataset = ModularAdditionDataset::new(false, 42);\n\n    let train_acc = test_all_examples(model, &train_dataset, device);\n    let val_acc = test_all_examples(model, &val_dataset, device);\n\n    println!(\"ðŸ“Š FULL Dataset Results:\");\n    println!(\"   Training: {}/{} = {:.2}%\",\n        (train_acc * train_dataset.len() as f32) as usize,\n        train_dataset.len(),\n        train_acc * 100.0);\n    println!(\"   Validation: {}/{} = {:.2}%\",\n        (val_acc * val_dataset.len() as f32) as usize,\n        val_dataset.len(),\n        val_acc * 100.0);\n\n    (train_acc, val_acc)\n}\n\n/// Test model on ALL examples in dataset (no random sampling)\nfn test_all_examples<B: Backend>(\n    model: &Transformer<B>,\n    dataset: &ModularAdditionDataset,\n    device: &B::Device,\n) -> f32 {\n    let mut total_correct = 0;\n    let total = dataset.len();\n\n    // Test in batches to avoid memory issues\n    let batch_size = 100;\n    for batch_start in (0..total).step_by(batch_size) {\n        let batch_end = (batch_start + batch_size).min(total);\n        let batch_len = batch_end - batch_start;\n\n        let mut inputs_vec = Vec::new();\n        let mut targets_vec = Vec::new();\n\n        for idx in batch_start..batch_end {\n            let (input, target) = dataset.get(idx).unwrap();\n            inputs_vec.extend(input);\n            targets_vec.push(target as i32);\n        }\n\n        let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n            .reshape([batch_len, 3]);\n\n        let logits = model.forward(inputs);\n        let predictions = logits.argmax(1).squeeze::<1>();\n        let predictions_vec: Vec<i32> = predictions.into_data().to_vec().unwrap();\n\n        let correct = predictions_vec\n            .iter()\n            .zip(targets_vec.iter())\n            .filter(|(p, t)| p == t)\n            .count();\n\n        total_correct += correct;\n    }\n\n    total_correct as f32 / total as f32\n}\n\n/// Verify that weight norms decrease over time with weight decay\n/// and show correlation with the grokking transition\npub fn verify_weight_norm_decay(\n    weight_norm_history: &[(usize, f64)],\n    grok_step: Option<usize>,\n    config: &WeightNormDecayConfig,\n) -> Result<WeightNormDecayReport, String> {\n    if weight_norm_history.len() < 2 {\n        return Err(\"weight norm history requires at least 2 snapshots\".to_string());\n    }\n\n    let initial_norm = weight_norm_history\n        .first()\n        .ok_or_else(|| \"weight norm history is empty\".to_string())?\n        .1;\n    let final_norm = weight_norm_history\n        .last()\n        .ok_or_else(|| \"weight norm history is empty\".to_string())?\n        .1;\n\n    if initial_norm <= 0.0 {\n        return Err(\"initial weight norm must be positive\".to_string());\n    }\n\n    // Check that norms are generally decreasing (with weight decay)\n    let relative_decrease = (initial_norm - final_norm) / initial_norm;\n    if relative_decrease < config.min_relative_decrease {\n        return Err(format!(\n            \"weight norm decrease {:.4} below threshold {:.4} (initial {:.4}, final {:.4})\",\n            relative_decrease, config.min_relative_decrease, initial_norm, final_norm\n        ));\n    }\n\n    // Check that norms are not increasing (weight decay should monotonically decrease norms)\n    for window in weight_norm_history.windows(2) {\n        let prev_norm = window[0].1;\n        let next_norm = window[1].1;\n        if next_norm > prev_norm * 1.01 {\n            // Allow 1% tolerance for numerical noise\n            return Err(format!(\n                \"weight norm increased from {:.4} to {:.4} at step {}\",\n                prev_norm, next_norm, window[1].0\n            ));\n        }\n    }\n\n    // If grok step is provided, check that norms show a shift around grokking\n    let grok_step_norm = if let Some(step) = grok_step {\n        weight_norm_history\n            .iter()\n            .filter(|(s, _)| {\n                *s >= step.saturating_sub(config.grok_step_window)\n                    && *s <= step + config.grok_step_window\n            })\n            .map(|(_, norm)| *norm)\n            .next()\n    } else {\n        None\n    };\n\n    Ok(WeightNormDecayReport {\n        initial_norm,\n        final_norm,\n        relative_decrease,\n        grok_step_norm,\n    })\n}\n\n/// Verify the characteristic \"snake curve\" shape of grokking phenomenon\n/// on log-scale plots: train accuracy rises early, validation stays flat then jumps\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_snake_curve_shape(\n    train_acc: &[(usize, f32)],\n    val_acc: &[(usize, f32)],\n    config: &SnakeCurveConfig,\n) -> Result<SnakeCurveReport, String> {\n    if train_acc.is_empty() || val_acc.is_empty() {\n        return Err(\"empty accuracy history\".to_string());\n    }\n\n    // Phase 1: Verify early train convergence (immediate drop on log scale)\n    let train_converged_step = train_acc\n        .iter()\n        .find(|(_, acc)| *acc >= config.train_convergence_threshold)\n        .map(|(step, _)| *step)\n        .ok_or_else(|| {\n            format!(\n                \"train accuracy never reaches {} threshold\",\n                config.train_convergence_threshold\n            )\n        })?;\n\n    if train_converged_step > config.early_step_max {\n        return Err(format!(\n            \"train accuracy converges at step {}, expected before {}\",\n            train_converged_step, config.early_step_max\n        ));\n    }\n\n    // Phase 2: Verify validation plateau (flat on log scale)\n    let plateau_samples: Vec<f32> = val_acc\n        .iter()\n        .filter(|(step, _)| *step >= 1 && *step <= config.val_plateau_min_step)\n        .map(|(_, acc)| *acc)\n        .collect();\n\n    if plateau_samples.is_empty() {\n        return Err(\"no validation samples in plateau region\".to_string());\n    }\n\n    let val_plateau_max = plateau_samples\n        .iter()\n        .fold(0.0f32, |max_val, &acc| max_val.max(acc));\n\n    if val_plateau_max > config.val_plateau_max_acc {\n        return Err(format!(\n            \"validation accuracy in plateau region too high: max {:.4} exceeds {:.4}\",\n            val_plateau_max, config.val_plateau_max_acc\n        ));\n    }\n\n    // Phase 3: Verify delayed validation jump (crash up on log scale)\n    let val_grok_step = val_acc\n        .iter()\n        .filter(|(step, _)| *step > config.val_plateau_min_step)\n        .find(|(_, acc)| *acc >= config.val_final_threshold)\n        .map(|(step, _)| *step)\n        .ok_or_else(|| {\n            format!(\n                \"validation accuracy never reaches {} after plateau\",\n                config.val_final_threshold\n            )\n        })?;\n\n    // Phase 4: Verify phase ordering (snake shape: early train, late val)\n    let curve_shape_valid = train_converged_step < config.val_plateau_min_step\n        && val_grok_step > config.val_plateau_min_step\n        && train_converged_step < val_grok_step;\n\n    if !curve_shape_valid {\n        return Err(format!(\n            \"invalid curve shape: train converged at {}, val plateau until {}, val grokked at {}\",\n            train_converged_step, config.val_plateau_min_step, val_grok_step\n        ));\n    }\n\n    Ok(SnakeCurveReport {\n        train_converged_by_step: train_converged_step,\n        val_plateau_max_acc: val_plateau_max,\n        val_grok_step,\n        curve_shape_valid,\n    })\n}\n\n/// Test specific examples to see if model actually computes mod p\npub fn test_specific_examples<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n) {\n    println!();\n    println!(\"ðŸ§ª Testing specific examples:\");\n\n    let modulus = ModularAdditionDataset::modulus();\n    let equals_token = ModularAdditionDataset::equals_token();\n    let test_cases = vec![\n        (0, 0, 0),\n        (1, 1, 2),\n        (50, 50, (50 + 50) % modulus),\n        (112, 1, (112 + 1) % modulus),\n        (56, 57, (56 + 57) % modulus),\n        (112, 112, (112 + 112) % modulus),\n        (10, 20, 30),\n        (60, 60, (60 + 60) % modulus),\n    ];\n\n    for (a, b, expected) in test_cases {\n        let input_vec = vec![a as i32, b as i32, equals_token as i32];\n        let input = Tensor::<B, 1, Int>::from_ints(input_vec.as_slice(), device)\n            .reshape([1, 3]);\n\n        let logits = model.forward(input);\n        let prediction = logits.argmax(1).squeeze::<1>();\n        let pred_value: i32 = prediction.into_data().to_vec().unwrap()[0];\n\n        let correct = if pred_value == expected as i32 { \"âœ“\" } else { \"âœ—\" };\n        println!(\n            \"   {} + {} mod {} = {} | Model: {} {}\",\n            a, b, modulus, expected, pred_value, correct\n        );\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    fn make_histories(\n        train_acc: &[(usize, f32)],\n        val_acc: &[(usize, f32)],\n        val_loss: &[(usize, f64)],\n    ) -> (LossHistory, AccuracyHistory) {\n        let mut loss_history = LossHistory::new();\n        loss_history.val_snapshots = val_loss.to_vec();\n        let mut accuracy_history = AccuracyHistory::new();\n        accuracy_history.train_snapshots = train_acc.to_vec();\n        accuracy_history.val_snapshots = val_acc.to_vec();\n        (loss_history, accuracy_history)\n    }\n\n    #[test]\n    fn grokking_phase_verification_passes_on_synthetic_signal() {\n        let train_acc = vec![(0, 0.2), (500, 0.995), (1000, 0.999)];\n        let val_acc = vec![\n            (0, 0.01),\n            (500, 0.01),\n            (1500, 0.01),\n            (2200, 0.995),\n            (2500, 0.999),\n        ];\n        let val_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 2.0),\n            (2200, 0.2),\n            (2500, 0.15),\n        ];\n\n        let (loss_history, accuracy_history) = make_histories(&train_acc, &val_acc, &val_loss);\n        let config = GrokkingVerificationConfig {\n            early_step_max: 1000,\n            plateau_min_step: 1500,\n            generalization_window: 1500,\n            accuracy_window: 2,\n            loss_window: 2,\n            ..GrokkingVerificationConfig::default_for_modulus(ModularAdditionDataset::modulus())\n        };\n\n        let report = verify_grokking_phase_transition(&loss_history, &accuracy_history, &config)\n            .expect(\"expected grokking verification to pass\");\n        assert!(report.transition_step >= config.plateau_min_step);\n        assert!(report.early_train_step <= config.early_step_max);\n    }\n\n    #[test]\n    fn grokking_phase_verification_fails_without_transition() {\n        let train_acc = vec![(0, 0.2), (500, 0.995)];\n        let val_acc = vec![(0, 0.01), (500, 0.01), (1500, 0.02)];\n        let val_loss = vec![(0, 2.0), (500, 2.0), (1500, 1.9)];\n\n        let (loss_history, accuracy_history) = make_histories(&train_acc, &val_acc, &val_loss);\n        let config = GrokkingVerificationConfig {\n            early_step_max: 1000,\n            plateau_min_step: 1000,\n            generalization_window: 1000,\n            accuracy_window: 2,\n            loss_window: 2,\n            ..GrokkingVerificationConfig::default_for_modulus(ModularAdditionDataset::modulus())\n        };\n\n        let err = verify_grokking_phase_transition(&loss_history, &accuracy_history, &config)\n            .expect_err(\"expected grokking verification to fail\");\n        assert!(err.contains(\"validation accuracy never reaches target\"));\n    }\n\n    #[test]\n    fn grokking_phase_verification_fails_with_slow_accuracy_rise() {\n        let train_acc = vec![(0, 0.2), (500, 0.995), (1000, 0.999)];\n        let val_acc = vec![\n            (0, 0.01),\n            (500, 0.01),\n            (1500, 0.5),\n            (2000, 0.6),\n            (2200, 0.995),\n            (2500, 0.999),\n        ];\n        let val_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 2.0),\n            (2200, 0.2),\n            (2500, 0.15),\n        ];\n\n        let (loss_history, accuracy_history) = make_histories(&train_acc, &val_acc, &val_loss);\n        let config = GrokkingVerificationConfig {\n            early_step_max: 1000,\n            plateau_min_step: 1000,\n            generalization_window: 2000,\n            accuracy_window: 2,\n            loss_window: 2,\n            ..GrokkingVerificationConfig::default_for_modulus(ModularAdditionDataset::modulus())\n        };\n\n        let err = verify_grokking_phase_transition(&loss_history, &accuracy_history, &config)\n            .expect_err(\"expected grokking verification to fail\");\n        assert!(err.contains(\"plateau too high\"));\n    }\n\n    #[test]\n    fn restricted_loss_detection_passes_with_early_drop() {\n        let full_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 2.0),\n            (2500, 2.0),\n            (3500, 0.3),\n        ];\n        let restricted_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 0.8),\n            (2500, 0.4),\n            (3500, 0.3),\n        ];\n\n        let config = RestrictedLossVerificationConfig {\n            plateau_min_step: 1000,\n            drop_window: 2,\n            drop_fraction: 0.5,\n            min_step_lead: 500,\n        };\n\n        let report = verify_restricted_loss_early_drop(&full_loss, &restricted_loss, &config)\n            .expect(\"expected restricted loss verification to pass\");\n        assert!(report.restricted_drop_step < report.full_drop_step);\n    }\n\n    #[test]\n    fn restricted_loss_detection_fails_when_full_drops_first() {\n        let full_loss = vec![(0, 2.0), (1000, 1.0), (1500, 0.8)];\n        let restricted_loss = vec![(0, 2.0), (1000, 2.0), (1500, 0.8)];\n\n        let config = RestrictedLossVerificationConfig {\n            plateau_min_step: 500,\n            drop_window: 1,\n            drop_fraction: 0.3,\n            min_step_lead: 200,\n        };\n\n        let err = verify_restricted_loss_early_drop(&full_loss, &restricted_loss, &config)\n            .expect_err(\"expected restricted loss verification to fail\");\n        assert!(err.contains(\"restricted loss drops\"));\n    }\n\n    #[test]\n    fn excluded_loss_spike_verification_passes() {\n        let excluded_loss = vec![(0, 1.0), (1500, 2.2), (3000, 1.1)];\n        let config = ExcludedLossSpikeConfig {\n            min_relative_increase: 0.5,\n        };\n\n        let report = verify_excluded_loss_spike(&excluded_loss, &config)\n            .expect(\"expected excluded loss spike verification to pass\");\n        assert_eq!(report.spike_step, 1500);\n    }\n\n    #[test]\n    fn excluded_loss_spike_verification_fails_without_spike() {\n        let excluded_loss = vec![(0, 1.0), (1500, 1.1), (3000, 1.0)];\n        let config = ExcludedLossSpikeConfig {\n            min_relative_increase: 0.5,\n        };\n\n        let err = verify_excluded_loss_spike(&excluded_loss, &config)\n            .expect_err(\"expected excluded loss spike verification to fail\");\n        assert!(err.contains(\"excluded loss spike too small\"));\n    }\n\n    #[test]\n    fn restricted_loss_drop_verification_passes() {\n        let restricted_loss = vec![\n            (0, 2.0),\n            (1000, 1.8),\n            (5000, 1.2),\n            (7000, 0.8),\n            (10000, 0.9),\n        ];\n        let config = RestrictedLossDropConfig {\n            min_relative_decrease: 0.3,\n        };\n\n        let report = verify_restricted_loss_drop(&restricted_loss, &config)\n            .expect(\"expected restricted loss drop verification to pass\");\n        assert_eq!(report.drop_step, 7000);\n        assert!(report.baseline_loss > report.drop_loss);\n        assert!((report.baseline_loss - report.drop_loss) / report.baseline_loss >= 0.3);\n    }\n\n    #[test]\n    fn restricted_loss_drop_verification_fails_without_drop() {\n        let restricted_loss = vec![\n            (0, 2.0),\n            (1000, 2.1),\n            (5000, 1.9),\n            (7000, 2.0),\n            (10000, 2.05),\n        ];\n        let config = RestrictedLossDropConfig {\n            min_relative_decrease: 0.3,\n        };\n\n        let err = verify_restricted_loss_drop(&restricted_loss, &config)\n            .expect_err(\"expected restricted loss drop verification to fail\");\n        assert!(err.contains(\"restricted loss decrease too small\"));\n    }\n\n    #[test]\n    fn mlp_activation_wave_verification_passes_on_rectified_sine() {\n        let n = ModularAdditionDataset::modulus();\n        let freq = 5.0;\n        let mut signal = Vec::with_capacity(n);\n        for idx in 0..n {\n            let angle = 2.0 * std::f64::consts::PI * freq * idx as f64 / n as f64;\n            let rectified = angle.sin().max(0.0);\n            let wobble = (2.0 * std::f64::consts::PI * 2.0 * idx as f64 / n as f64)\n                .sin()\n                .abs()\n                * 0.02;\n            signal.push(rectified + wobble);\n        }\n\n        let config = MlpWaveVerificationConfig {\n            max_negative_fraction: 0.0,\n            negative_tolerance: 1e-6,\n            min_dominant_ratio: 3.0,\n            min_correlation: 0.7,\n            phase_steps: 32,\n        };\n\n        let report =\n            verify_mlp_activation_waves_from_signals(&[(0, signal)], &config)\n                .expect(\"expected rectified sine to pass\");\n        assert_eq!(report.neuron_reports.len(), 1);\n    }\n\n    #[test]\n    fn mlp_activation_wave_verification_fails_on_noise() {\n        let n = ModularAdditionDataset::modulus();\n        let signal: Vec<f64> = (0..n).map(|idx| idx as f64 / n as f64).collect();\n\n        let config = MlpWaveVerificationConfig {\n            max_negative_fraction: 0.0,\n            negative_tolerance: 1e-6,\n            min_dominant_ratio: 4.0,\n            min_correlation: 0.8,\n            phase_steps: 16,\n        };\n\n        let err =\n            verify_mlp_activation_waves_from_signals(&[(0, signal)], &config)\n                .expect_err(\"expected linear signal to fail\");\n        assert!(err.contains(\"dominant frequency ratio\") || err.contains(\"correlation\"));\n    }\n\n    #[test]\n    fn pairwise_manifold_transition_passes_on_elliptic_signals() {\n        let n = ModularAdditionDataset::modulus();\n        let freq = 5.0;\n        let mut post_signals = Vec::new();\n        for neuron_idx in 0..7usize {\n            let phase = 2.0 * std::f64::consts::PI * neuron_idx as f64 / 7.0;\n            let signal: Vec<f64> = (0..n)\n                .map(|idx| {\n                    let angle = 2.0 * std::f64::consts::PI * freq * idx as f64 / n as f64 + phase;\n                    angle.sin()\n                })\n                .collect();\n            post_signals.push((neuron_idx, signal));\n        }\n\n        let mut pre_signals = Vec::new();\n        for neuron_idx in 0..7usize {\n            let signal: Vec<f64> = (0..n)\n                .map(|idx| idx as f64 / n as f64 + neuron_idx as f64 * 0.01)\n                .collect();\n            pre_signals.push((neuron_idx, signal));\n        }\n\n        let config = PairwiseManifoldConfig {\n            fixed_y: 0,\n            neuron_indices: (0..7).collect(),\n            min_post_score: 0.75,\n            max_pre_score: 0.55,\n            max_axis_ratio: 8.0,\n            min_pairs_passing: 8,\n        };\n\n        let report = verify_pairwise_manifold_transition_from_signals(\n            &pre_signals,\n            &post_signals,\n            &config,\n        )\n        .expect(\"expected pairwise manifold transition to pass\");\n        assert!(report.passing_pairs >= config.min_pairs_passing);\n    }\n\n    #[test]\n    fn pairwise_manifold_transition_fails_without_rings() {\n        let n = ModularAdditionDataset::modulus();\n        let mut signals = Vec::new();\n        for neuron_idx in 0..7usize {\n            let signal: Vec<f64> = (0..n)\n                .map(|idx| idx as f64 / n as f64 + neuron_idx as f64 * 0.02)\n                .collect();\n            signals.push((neuron_idx, signal));\n        }\n\n        let config = PairwiseManifoldConfig {\n            fixed_y: 0,\n            neuron_indices: (0..7).collect(),\n            min_post_score: 0.8,\n            max_pre_score: 0.55,\n            max_axis_ratio: 4.0,\n            min_pairs_passing: 5,\n        };\n\n        let err = verify_pairwise_manifold_transition_from_signals(\n            &signals,\n            &signals,\n            &config,\n        )\n        .expect_err(\"expected pairwise manifold transition to fail\");\n        assert!(err.contains(\"post-grok\") || err.contains(\"pre-grok\"));\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_passes_on_diagonal_corrugation() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create diagonal ridges: f(x,y) = sin(2Ï€ * freq * (x+y) / n)\n        let freq = 3.0;\n        for x in 0..n {\n            for y in 0..n {\n                let angle = 2.0 * std::f64::consts::PI * freq * (x + y) as f64 / n as f64;\n                surface[x][y] = angle.sin();\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let report = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect(\"expected diagonal ridge verification to pass\");\n        assert!(report.diagonal_score > report.axis_score);\n        assert!(report.ratio >= config.min_diagonal_ratio);\n        assert!(report.diagonal_score >= config.min_diagonal_score);\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_passes_on_anti_diagonal_pattern() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create anti-diagonal ridges: f(x,y) = sin(2Ï€ * freq * (x-y) / n)\n        let freq = 3.0;\n        for x in 0..n {\n            for y in 0..n {\n                let angle = 2.0 * std::f64::consts::PI * freq * (x as i32 - y as i32) as f64 / n as f64;\n                surface[x][y] = angle.sin();\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let report = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect(\"expected anti-diagonal ridge verification to pass\");\n        assert!(report.diagonal_score > report.axis_score);\n        assert!(report.ratio >= config.min_diagonal_ratio);\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_fails_on_axis_aligned_pattern() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create axis-aligned ridges: f(x,y) = sin(2Ï€ * freq * x / n)\n        let freq = 3.0;\n        for x in 0..n {\n            for y in 0..n {\n                let angle = 2.0 * std::f64::consts::PI * freq * x as f64 / n as f64;\n                surface[x][y] = angle.sin();\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let err = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect_err(\"expected axis-aligned pattern to fail diagonal verification\");\n        assert!(err.contains(\"diagonal/axis ratio\") || err.contains(\"diagonal autocorrelation\"));\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_fails_on_flat_surface() {\n        let n = 50;\n        let surface = vec![vec![1.5; n]; n];\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let err = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect_err(\"expected flat surface to fail\");\n        assert!(err.contains(\"surface variance is zero\"));\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_fails_on_random_noise() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create random noise (low autocorrelation everywhere)\n        for x in 0..n {\n            for y in 0..n {\n                surface[x][y] = ((x * 17 + y * 31) % 100) as f64 / 100.0;\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let err = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect_err(\"expected random noise to fail diagonal verification\");\n        assert!(err.contains(\"diagonal/axis ratio\") || err.contains(\"diagonal autocorrelation\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_passes_on_valid_grokking_curve() {\n        // Simulate typical grokking: train converges early, val plateaus then jumps\n        let train_acc = vec![\n            (1, 0.10),\n            (10, 0.50),\n            (100, 0.80),\n            (500, 0.98),\n            (1000, 0.99),\n            (2000, 0.995),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (10, 0.009),\n            (100, 0.01),\n            (500, 0.011),\n            (1000, 0.009),\n            (1500, 0.01),\n            (2000, 0.92),\n            (3000, 0.98),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 1500,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let report = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect(\"expected snake curve verification to pass\");\n\n        assert_eq!(report.train_converged_by_step, 500);\n        assert!(report.val_plateau_max_acc < config.val_plateau_max_acc);\n        assert_eq!(report.val_grok_step, 2000);\n        assert!(report.curve_shape_valid);\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_on_simultaneous_convergence() {\n        // Both train and val converge at the same time (no grokking)\n        let train_acc = vec![\n            (1, 0.10),\n            (100, 0.98),\n            (500, 0.99),\n        ];\n        let val_acc = vec![\n            (1, 0.10),\n            (100, 0.97),\n            (500, 0.98),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 200,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let err = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect_err(\"expected snake curve verification to fail\");\n        assert!(err.contains(\"plateau region too high\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_on_slow_train_convergence() {\n        // Train takes too long to converge (not the typical snake curve)\n        let train_acc = vec![\n            (1, 0.10),\n            (500, 0.50),\n            (1000, 0.70),\n            (2000, 0.98),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (500, 0.01),\n            (1000, 0.01),\n            (2000, 0.92),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 1500,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let err = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect_err(\"expected snake curve verification to fail\");\n        assert!(err.contains(\"converges at step 2000\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_without_val_jump() {\n        // Validation never jumps up (no generalization)\n        let train_acc = vec![\n            (1, 0.10),\n            (100, 0.98),\n            (500, 0.99),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (100, 0.01),\n            (500, 0.01),\n            (2000, 0.02),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 200,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let err = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect_err(\"expected snake curve verification to fail\");\n        assert!(err.contains(\"validation accuracy never reaches\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_on_wrong_phase_ordering() {\n        // Val jumps before train converges (backwards snake)\n        let train_acc = vec![\n            (1, 0.10),\n            (500, 0.50),\n            (2000, 0.98),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (100, 0.95),\n            (500, 0.98),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 200,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let result = verify_snake_curve_shape(&train_acc, &val_acc, &config);\n        // Should fail because val jumps before train converges (not a proper snake curve)\n        // This can fail for multiple reasons, but should definitely fail\n        assert!(result.is_err(), \"Expected snake curve verification to fail but it passed\");\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_passes_on_decreasing_norms() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 85.0),\n            (1000, 72.0),\n            (1500, 63.0),\n            (2000, 58.0),\n            (2500, 55.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let report = verify_weight_norm_decay(&weight_norms, Some(2000), &config)\n            .expect(\"expected weight norm decay verification to pass\");\n\n        assert_eq!(report.initial_norm, 100.0);\n        assert_eq!(report.final_norm, 55.0);\n        assert!((report.relative_decrease - 0.45).abs() < 0.01);\n        assert!(report.grok_step_norm.is_some());\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_fails_on_flat_norms() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 99.0),\n            (1000, 98.5),\n            (1500, 98.0),\n            (2000, 97.5),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let err = verify_weight_norm_decay(&weight_norms, None, &config)\n            .expect_err(\"expected weight norm decay verification to fail\");\n        assert!(err.contains(\"weight norm decrease\"));\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_fails_on_increasing_norms() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 95.0),\n            (1000, 105.0), // Increase here\n            (1500, 90.0),\n            (2000, 85.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let err = verify_weight_norm_decay(&weight_norms, None, &config)\n            .expect_err(\"expected weight norm decay verification to fail\");\n        assert!(err.contains(\"weight norm increased\"));\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_allows_small_noise() {\n        // Weight decay with small numerical noise (within 1% tolerance)\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 95.0),\n            (1000, 90.5),  // Small increase within tolerance\n            (1500, 90.0),\n            (2000, 85.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let report = verify_weight_norm_decay(&weight_norms, None, &config)\n            .expect(\"expected weight norm decay with small noise to pass\");\n\n        assert_eq!(report.initial_norm, 100.0);\n        assert_eq!(report.final_norm, 85.0);\n        assert!((report.relative_decrease - 0.15).abs() < 0.01);\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_finds_grok_step_norm() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (1000, 80.0),\n            (1800, 68.0),  // Within window of grok step (2200 - 500 = 1700)\n            (2000, 65.0),\n            (2200, 62.0),  // Around grok step\n            (3000, 55.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let report = verify_weight_norm_decay(&weight_norms, Some(2200), &config)\n            .expect(\"expected weight norm decay verification to pass\");\n\n        assert!(report.grok_step_norm.is_some());\n        let grok_norm = report.grok_step_norm.unwrap();\n        // Should find the first norm in the window [1700, 2700], which is 68.0 at step 1800\n        assert!((grok_norm - 68.0).abs() < 0.01);\n    }\n}\n","structuredPatch":[{"oldStart":896,"oldLines":6,"newStart":896,"newLines":7,"lines":["     Ok(window.iter().sum::<f64>() / drop_window as f64)"," }"," ","+#[allow(dead_code)]"," fn find_drop_step(","     losses: &[(usize, f64)],","     plateau_min_step: usize,"]}],"userModified":false,"replaceAll":false}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01Fqrd9XgDg9vj4GdELReZB9","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_018XhiYN6RGNAeKSyrjbgtjT","name":"Edit","input":{"replace_all":false,"file_path":"/Users/admin/MechInt/src/verify.rs","old_string":"fn surface_autocorrelation_scores(","new_string":"#[allow(dead_code)]\nfn surface_autocorrelation_scores("}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":429,"cache_read_input_tokens":62301,"cache_creation":{"ephemeral_5m_input_tokens":429,"ephemeral_1h_input_tokens":0},"output_tokens":150,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"7c3d8ee4-3aba-41a9-a1d7-7f04fc5a25b9"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_018XhiYN6RGNAeKSyrjbgtjT","type":"tool_result","content":"The file /Users/admin/MechInt/src/verify.rs has been updated. Here's the result of running `cat -n` on a snippet of the edited file:\n   949â†’\n   950â†’    Ok(per_neuron)\n   951â†’}\n   952â†’\n   953â†’#[allow(dead_code)]\n   954â†’fn surface_autocorrelation_scores(\n   955â†’    surface: &[Vec<f64>],\n   956â†’    max_lag: usize,\n   957â†’) -> Result<(f64, f64, f64, f64), String> {\n   958â†’    let (height, width) = surface_dimensions(surface)?;"}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"661deed1-8bb4-470e-a9b6-731ba7e88666","tool_use_result":{"filePath":"/Users/admin/MechInt/src/verify.rs","oldString":"fn surface_autocorrelation_scores(","newString":"#[allow(dead_code)]\nfn surface_autocorrelation_scores(","originalFile":"use burn::{\n    data::dataset::Dataset,\n    tensor::{backend::Backend, Int, Tensor},\n};\nuse rustfft::{num_complex::Complex, FftPlanner};\n\nuse crate::analysis::{AccuracyHistory, LossHistory};\nuse crate::data::ModularAdditionDataset;\nuse crate::model::Transformer;\n\n#[derive(Debug, Clone)]\npub struct GrokkingVerificationConfig {\n    pub early_train_acc_threshold: f32,\n    pub early_step_max: usize,\n    pub plateau_min_step: usize,\n    pub generalization_window: usize,\n    pub chance_accuracy: f32,\n    pub chance_tolerance: f32,\n    pub target_val_acc_threshold: f32,\n    pub accuracy_window: usize,\n    pub loss_window: usize,\n    pub loss_drop_fraction: f64,\n}\n\nimpl GrokkingVerificationConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        Self {\n            early_train_acc_threshold: 0.99,\n            early_step_max: 1000,\n            plateau_min_step: 1000,\n            generalization_window: 10_000,\n            chance_accuracy: 1.0 / modulus as f32,\n            chance_tolerance: 0.02,\n            target_val_acc_threshold: 0.99,\n            accuracy_window: 5,\n            loss_window: 5,\n            loss_drop_fraction: 0.5,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct GrokkingPhaseReport {\n    pub early_train_step: usize,\n    pub transition_step: usize,\n    pub plateau_max_val_acc: f32,\n    pub loss_drop_ratio: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossVerificationConfig {\n    pub plateau_min_step: usize,\n    pub drop_window: usize,\n    pub drop_fraction: f64,\n    pub min_step_lead: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossReport {\n    pub restricted_drop_step: usize,\n    pub full_drop_step: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossDropConfig {\n    pub min_relative_decrease: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossDropReport {\n    pub drop_step: usize,\n    pub drop_loss: f64,\n    pub baseline_loss: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct ExcludedLossSpikeConfig {\n    pub min_relative_increase: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct ExcludedLossSpikeReport {\n    pub spike_step: usize,\n    pub spike_loss: f64,\n    pub baseline_loss: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct MlpWaveVerificationConfig {\n    pub max_negative_fraction: f64,\n    pub negative_tolerance: f64,\n    pub min_dominant_ratio: f64,\n    pub min_correlation: f64,\n    pub phase_steps: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct MlpWaveNeuronReport {\n    pub neuron_index: usize,\n    pub dominant_frequency: usize,\n    pub dominant_ratio: f64,\n    pub negative_fraction: f64,\n    pub best_correlation: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct MlpWaveReport {\n    pub neuron_reports: Vec<MlpWaveNeuronReport>,\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldConfig {\n    pub fixed_y: usize,\n    pub neuron_indices: Vec<usize>,\n    pub min_post_score: f64,\n    pub max_pre_score: f64,\n    pub max_axis_ratio: f64,\n    pub min_pairs_passing: usize,\n}\n\nimpl PairwiseManifoldConfig {\n    pub fn default_for_modulus(_modulus: usize) -> Self {\n        Self {\n            fixed_y: 0,\n            neuron_indices: (0..7).collect(),\n            min_post_score: 0.75,\n            max_pre_score: 0.55,\n            max_axis_ratio: 8.0,\n            min_pairs_passing: 10,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldPairReport {\n    pub neuron_a: usize,\n    pub neuron_b: usize,\n    pub circularity_score: f64,\n    pub radius_cv: f64,\n    pub axis_ratio: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldReport {\n    pub pair_reports: Vec<PairwiseManifoldPairReport>,\n    pub average_score: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldTransitionReport {\n    pub pre_average_score: f64,\n    pub post_average_score: f64,\n    pub passing_pairs: usize,\n    pub total_pairs: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct DiagonalRidgeConfig {\n    pub neuron_index: usize,\n    pub max_lag: usize,\n    pub min_diagonal_ratio: f64,\n    pub min_diagonal_score: f64,\n}\n\nimpl DiagonalRidgeConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        let max_lag = (modulus / 10).max(4).min(20);\n        Self {\n            neuron_index: 10,\n            max_lag,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct DiagonalRidgeReport {\n    pub neuron_index: usize,\n    pub diagonal_score: f64,\n    pub axis_score: f64,\n    pub ratio: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct SnakeCurveConfig {\n    pub early_step_max: usize,\n    pub train_convergence_threshold: f32,\n    pub val_plateau_min_step: usize,\n    pub val_plateau_max_acc: f32,\n    pub val_final_threshold: f32,\n}\n\nimpl SnakeCurveConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        Self {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 1000,\n            val_plateau_max_acc: 1.0 / modulus as f32 + 0.05,\n            val_final_threshold: 0.90,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct WeightNormDecayConfig {\n    pub min_relative_decrease: f64,\n    pub plateau_min_step: usize,\n    pub grok_step_window: usize,\n}\n\nimpl WeightNormDecayConfig {\n    pub fn default_for_grokking() -> Self {\n        Self {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct WeightNormDecayReport {\n    pub initial_norm: f64,\n    pub final_norm: f64,\n    pub relative_decrease: f64,\n    pub grok_step_norm: Option<f64>,\n}\n\n#[derive(Debug, Clone)]\npub struct SnakeCurveReport {\n    pub train_converged_by_step: usize,\n    pub val_plateau_max_acc: f32,\n    pub val_grok_step: usize,\n    pub curve_shape_valid: bool,\n}\n\npub fn verify_grokking_phase_transition(\n    loss_history: &LossHistory,\n    accuracy_history: &AccuracyHistory,\n    config: &GrokkingVerificationConfig,\n) -> Result<GrokkingPhaseReport, String> {\n    let (early_train_step, _) = accuracy_history\n        .train_snapshots\n        .iter()\n        .find(|(_, acc)| *acc >= config.early_train_acc_threshold)\n        .ok_or_else(|| \"train accuracy never reaches threshold\".to_string())?;\n\n    if *early_train_step > config.early_step_max {\n        return Err(format!(\n            \"train accuracy reaches {:.2} after step {}, expected before {}\",\n            config.early_train_acc_threshold, early_train_step, config.early_step_max\n        ));\n    }\n\n    let plateau_max_val_acc = accuracy_history\n        .val_snapshots\n        .iter()\n        .filter(|(step, _)| *step <= config.plateau_min_step)\n        .map(|(_, acc)| *acc)\n        .fold(None::<f32>, |accum, value| {\n            Some(accum.map_or(value, |max_val| max_val.max(value)))\n        })\n        .ok_or_else(|| \"no validation accuracy samples before plateau_min_step\".to_string())?;\n\n    if plateau_max_val_acc > config.chance_accuracy + config.chance_tolerance {\n        return Err(format!(\n            \"validation accuracy before step {} exceeds chance (max {:.4})\",\n            config.plateau_min_step, plateau_max_val_acc\n        ));\n    }\n\n    let (transition_step, _) = accuracy_history\n        .val_snapshots\n        .iter()\n        .find(|(step, acc)| {\n            *step >= config.plateau_min_step && *acc >= config.target_val_acc_threshold\n        })\n        .ok_or_else(|| \"validation accuracy never reaches target threshold\".to_string())?;\n\n    let latest_allowed = config.plateau_min_step + config.generalization_window;\n    if *transition_step > latest_allowed {\n        return Err(format!(\n            \"validation accuracy reaches {:.2} at step {}, expected before {}\",\n            config.target_val_acc_threshold, transition_step, latest_allowed\n        ));\n    }\n\n    verify_accuracy_jump(\n        &accuracy_history.val_snapshots,\n        *transition_step,\n        config.accuracy_window,\n        config.chance_accuracy,\n        config.chance_tolerance,\n        config.target_val_acc_threshold,\n    )?;\n\n    let loss_drop_ratio = verify_loss_drop(\n        &loss_history.val_snapshots,\n        *transition_step,\n        config.loss_window,\n        config.loss_drop_fraction,\n    )?;\n\n    Ok(GrokkingPhaseReport {\n        early_train_step: *early_train_step,\n        transition_step: *transition_step,\n        plateau_max_val_acc,\n        loss_drop_ratio,\n    })\n}\n\nfn verify_accuracy_jump(\n    val_accuracy: &[(usize, f32)],\n    transition_step: usize,\n    window: usize,\n    chance_accuracy: f32,\n    chance_tolerance: f32,\n    target_val_acc_threshold: f32,\n) -> Result<(), String> {\n    let before_values: Vec<f32> = val_accuracy\n        .iter()\n        .filter(|(step, _)| *step < transition_step)\n        .map(|(_, acc)| *acc)\n        .collect();\n\n    let after_values: Vec<f32> = val_accuracy\n        .iter()\n        .filter(|(step, _)| *step >= transition_step)\n        .map(|(_, acc)| *acc)\n        .collect();\n\n    if before_values.len() < window || after_values.len() < window {\n        return Err(\"insufficient accuracy samples around transition\".to_string());\n    }\n\n    let before_avg: f32 =\n        before_values[before_values.len() - window..].iter().sum::<f32>() / window as f32;\n    let after_avg: f32 = after_values[..window].iter().sum::<f32>() / window as f32;\n\n    if before_avg > chance_accuracy + chance_tolerance {\n        return Err(format!(\n            \"validation accuracy plateau too high before transition (avg {:.4})\",\n            before_avg\n        ));\n    }\n\n    if after_avg < target_val_acc_threshold {\n        return Err(format!(\n            \"validation accuracy after transition too low (avg {:.4})\",\n            after_avg\n        ));\n    }\n\n    Ok(())\n}\n\nfn verify_loss_drop(\n    val_losses: &[(usize, f64)],\n    transition_step: usize,\n    loss_window: usize,\n    loss_drop_fraction: f64,\n) -> Result<f64, String> {\n    let before_losses: Vec<f64> = val_losses\n        .iter()\n        .filter(|(step, _)| *step < transition_step)\n        .map(|(_, loss)| *loss)\n        .collect();\n\n    let after_losses: Vec<f64> = val_losses\n        .iter()\n        .filter(|(step, _)| *step >= transition_step)\n        .map(|(_, loss)| *loss)\n        .collect();\n\n    if before_losses.len() < loss_window || after_losses.len() < loss_window {\n        return Err(\"insufficient loss samples around transition\".to_string());\n    }\n\n    let before_avg: f64 =\n        before_losses[before_losses.len() - loss_window..].iter().sum::<f64>()\n            / loss_window as f64;\n    let after_avg: f64 = after_losses[..loss_window].iter().sum::<f64>() / loss_window as f64;\n\n    if after_avg > before_avg * (1.0 - loss_drop_fraction) {\n        return Err(format!(\n            \"validation loss drop too small (before {:.4}, after {:.4})\",\n            before_avg, after_avg\n        ));\n    }\n\n    Ok(after_avg / before_avg)\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_restricted_loss_early_drop(\n    full_losses: &[(usize, f64)],\n    restricted_losses: &[(usize, f64)],\n    config: &RestrictedLossVerificationConfig,\n) -> Result<RestrictedLossReport, String> {\n    let restricted_baseline =\n        baseline_loss(restricted_losses, config.plateau_min_step, config.drop_window)?;\n    let full_baseline = baseline_loss(full_losses, config.plateau_min_step, config.drop_window)?;\n\n    let restricted_drop_step = find_drop_step(\n        restricted_losses,\n        config.plateau_min_step,\n        restricted_baseline,\n        config.drop_fraction,\n    )?;\n    let full_drop_step = find_drop_step(\n        full_losses,\n        config.plateau_min_step,\n        full_baseline,\n        config.drop_fraction,\n    )?;\n\n    if restricted_drop_step + config.min_step_lead > full_drop_step {\n        return Err(format!(\n            \"restricted loss drops at step {}, expected at least {} steps before full loss (step {})\",\n            restricted_drop_step, config.min_step_lead, full_drop_step\n        ));\n    }\n\n    Ok(RestrictedLossReport {\n        restricted_drop_step,\n        full_drop_step,\n    })\n}\n\npub fn verify_restricted_loss_drop(\n    restricted_losses: &[(usize, f64)],\n    config: &RestrictedLossDropConfig,\n) -> Result<RestrictedLossDropReport, String> {\n    if restricted_losses.len() < 3 {\n        return Err(\"restricted loss history requires at least 3 points\".to_string());\n    }\n\n    let first = restricted_losses.first().ok_or_else(|| {\n        \"restricted loss history is empty\".to_string()\n    })?;\n    let baseline_loss = first.1;\n\n    let (min_idx, (min_step, min_loss)) = restricted_losses\n        .iter()\n        .enumerate()\n        .min_by(|(_, a), (_, b)| {\n            a.1.partial_cmp(&b.1).unwrap_or(std::cmp::Ordering::Equal)\n        })\n        .ok_or_else(|| \"restricted loss history is empty\".to_string())?;\n\n    if min_idx == 0 {\n        return Err(\"restricted loss minimum occurs at start (no drop detected)\".to_string());\n    }\n\n    let relative_decrease = (baseline_loss - min_loss) / baseline_loss;\n    if relative_decrease < config.min_relative_decrease {\n        return Err(format!(\n            \"restricted loss decrease too small (baseline {:.4}, min {:.4}, decrease {:.1}%)\",\n            baseline_loss, min_loss, relative_decrease * 100.0\n        ));\n    }\n\n    Ok(RestrictedLossDropReport {\n        drop_step: *min_step,\n        drop_loss: *min_loss,\n        baseline_loss,\n    })\n}\n\npub fn verify_excluded_loss_spike(\n    excluded_losses: &[(usize, f64)],\n    config: &ExcludedLossSpikeConfig,\n) -> Result<ExcludedLossSpikeReport, String> {\n    if excluded_losses.len() < 3 {\n        return Err(\"excluded loss history requires at least 3 points\".to_string());\n    }\n\n    let first = excluded_losses.first().ok_or_else(|| {\n        \"excluded loss history is empty\".to_string()\n    })?;\n    let last = excluded_losses.last().ok_or_else(|| {\n        \"excluded loss history is empty\".to_string()\n    })?;\n    let baseline_loss = (first.1 + last.1) / 2.0;\n\n    let (peak_idx, (peak_step, peak_loss)) = excluded_losses\n        .iter()\n        .enumerate()\n        .max_by(|(_, a), (_, b)| {\n            a.1.partial_cmp(&b.1).unwrap_or(std::cmp::Ordering::Equal)\n        })\n        .ok_or_else(|| \"excluded loss history is empty\".to_string())?;\n\n    if peak_idx == 0 || peak_idx + 1 == excluded_losses.len() {\n        return Err(\"excluded loss spike occurs at boundary\".to_string());\n    }\n\n    if peak_loss < &(baseline_loss * (1.0 + config.min_relative_increase)) {\n        return Err(format!(\n            \"excluded loss spike too small (baseline {:.4}, peak {:.4})\",\n            baseline_loss, peak_loss\n        ));\n    }\n\n    Ok(ExcludedLossSpikeReport {\n        spike_step: *peak_step,\n        spike_loss: *peak_loss,\n        baseline_loss,\n    })\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_mlp_activation_waves<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n    fixed_y: usize,\n    neuron_indices: &[usize],\n    config: &MlpWaveVerificationConfig,\n) -> Result<MlpWaveReport, String> {\n    let activations = collect_mlp_post_relu_activations(model, device, fixed_y)?;\n    if neuron_indices.is_empty() {\n        return Err(\"neuron indices list is empty\".to_string());\n    }\n\n    let d_ff = activations.len();\n    for &idx in neuron_indices {\n        if idx >= d_ff {\n            return Err(format!(\n                \"neuron index {} out of range (d_ff={})\",\n                idx, d_ff\n            ));\n        }\n    }\n\n    let signals: Vec<(usize, Vec<f64>)> = neuron_indices\n        .iter()\n        .map(|&idx| (idx, activations[idx].clone()))\n        .collect();\n    verify_mlp_activation_waves_from_signals(&signals, config)\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_mlp_activation_waves_from_signals(\n    signals: &[(usize, Vec<f64>)],\n    config: &MlpWaveVerificationConfig,\n) -> Result<MlpWaveReport, String> {\n    if signals.is_empty() {\n        return Err(\"no activation signals provided\".to_string());\n    }\n\n    let mut reports = Vec::with_capacity(signals.len());\n    for (neuron_index, signal) in signals {\n        let report = analyze_mlp_activation_wave(*neuron_index, signal, config)?;\n        reports.push(report);\n    }\n\n    Ok(MlpWaveReport {\n        neuron_reports: reports,\n    })\n}\n\npub fn verify_mlp_pairwise_manifold_transition<B: Backend>(\n    pre_model: &Transformer<B>,\n    post_model: &Transformer<B>,\n    device: &B::Device,\n    config: &PairwiseManifoldConfig,\n) -> Result<PairwiseManifoldTransitionReport, String> {\n    let modulus = ModularAdditionDataset::modulus();\n    if config.fixed_y >= modulus {\n        return Err(format!(\n            \"fixed_y {} out of range (modulus {})\",\n            config.fixed_y, modulus\n        ));\n    }\n\n    let pre_activations = collect_mlp_post_relu_activations(pre_model, device, config.fixed_y)?;\n    let post_activations = collect_mlp_post_relu_activations(post_model, device, config.fixed_y)?;\n\n    let d_ff = pre_activations.len();\n    if post_activations.len() != d_ff {\n        return Err(\"post activations shape mismatch\".to_string());\n    }\n\n    if config.neuron_indices.len() < 2 {\n        return Err(\"need at least two neuron indices\".to_string());\n    }\n\n    for &idx in &config.neuron_indices {\n        if idx >= d_ff {\n            return Err(format!(\n                \"neuron index {} out of range (d_ff={})\",\n                idx, d_ff\n            ));\n        }\n    }\n\n    let pre_signals: Vec<(usize, Vec<f64>)> = config\n        .neuron_indices\n        .iter()\n        .map(|&idx| (idx, pre_activations[idx].clone()))\n        .collect();\n    let post_signals: Vec<(usize, Vec<f64>)> = config\n        .neuron_indices\n        .iter()\n        .map(|&idx| (idx, post_activations[idx].clone()))\n        .collect();\n\n    verify_pairwise_manifold_transition_from_signals(&pre_signals, &post_signals, config)\n}\n\npub fn collect_mlp_activation_surface<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n    neuron_index: usize,\n) -> Result<Vec<Vec<f64>>, String> {\n    let modulus = ModularAdditionDataset::modulus();\n    let equals_token = ModularAdditionDataset::equals_token();\n\n    let mut inputs_vec = Vec::with_capacity(modulus * modulus * 3);\n    for x in 0..modulus {\n        for y in 0..modulus {\n            inputs_vec.push(x as i32);\n            inputs_vec.push(y as i32);\n            inputs_vec.push(equals_token as i32);\n        }\n    }\n\n    let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n        .reshape([modulus * modulus, 3]);\n    let (_, mlp_acts) = model.forward_with_mlp_activations(inputs);\n    let [batch_size, d_ff] = mlp_acts.dims();\n    if batch_size != modulus * modulus {\n        return Err(format!(\n            \"unexpected activation batch size {} (expected {})\",\n            batch_size,\n            modulus * modulus\n        ));\n    }\n    if neuron_index >= d_ff {\n        return Err(format!(\n            \"neuron index {} out of range (d_ff={})\",\n            neuron_index, d_ff\n        ));\n    }\n\n    let data: Vec<f32> = mlp_acts.into_data().to_vec().unwrap();\n    let mut surface = vec![vec![0.0f64; modulus]; modulus];\n    for x in 0..modulus {\n        for y in 0..modulus {\n            let batch = x * modulus + y;\n            let idx = batch * d_ff + neuron_index;\n            surface[x][y] = data[idx] as f64;\n        }\n    }\n\n    Ok(surface)\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_constructive_interference_surface_from_grid(\n    surface: &[Vec<f64>],\n    config: &DiagonalRidgeConfig,\n) -> Result<DiagonalRidgeReport, String> {\n    let (diag_main, diag_anti, axis_x, axis_y) =\n        surface_autocorrelation_scores(surface, config.max_lag)?;\n\n    let diagonal_score = diag_main.max(diag_anti);\n    let axis_score = axis_x.max(axis_y);\n    let ratio = if axis_score > 0.0 {\n        diagonal_score / axis_score\n    } else {\n        f64::INFINITY\n    };\n\n    if diagonal_score < config.min_diagonal_score {\n        return Err(format!(\n            \"diagonal autocorrelation {:.3} below threshold {:.3}\",\n            diagonal_score, config.min_diagonal_score\n        ));\n    }\n\n    if ratio < config.min_diagonal_ratio {\n        return Err(format!(\n            \"diagonal/axis ratio {:.3} below threshold {:.3}\",\n            ratio, config.min_diagonal_ratio\n        ));\n    }\n\n    Ok(DiagonalRidgeReport {\n        neuron_index: config.neuron_index,\n        diagonal_score,\n        axis_score,\n        ratio,\n    })\n}\n\npub fn verify_pairwise_manifold_transition_from_signals(\n    pre_signals: &[(usize, Vec<f64>)],\n    post_signals: &[(usize, Vec<f64>)],\n    config: &PairwiseManifoldConfig,\n) -> Result<PairwiseManifoldTransitionReport, String> {\n    let pre_report = evaluate_pairwise_manifolds_from_signals(pre_signals)?;\n    let post_report = evaluate_pairwise_manifolds_from_signals(post_signals)?;\n\n    if pre_report.average_score > config.max_pre_score {\n        return Err(format!(\n            \"pre-grok average circularity {:.3} exceeds threshold {:.3}\",\n            pre_report.average_score, config.max_pre_score\n        ));\n    }\n\n    let passing_pairs = post_report\n        .pair_reports\n        .iter()\n        .filter(|pair| {\n            pair.circularity_score >= config.min_post_score\n                && pair.axis_ratio <= config.max_axis_ratio\n        })\n        .count();\n    if passing_pairs < config.min_pairs_passing {\n        return Err(format!(\n            \"post-grok only {} pairs meet circularity threshold (need {})\",\n            passing_pairs, config.min_pairs_passing\n        ));\n    }\n\n    Ok(PairwiseManifoldTransitionReport {\n        pre_average_score: pre_report.average_score,\n        post_average_score: post_report.average_score,\n        passing_pairs,\n        total_pairs: post_report.pair_reports.len(),\n    })\n}\n\nfn evaluate_pairwise_manifolds_from_signals(\n    signals: &[(usize, Vec<f64>)],\n) -> Result<PairwiseManifoldReport, String> {\n    if signals.len() < 2 {\n        return Err(\"need at least two activation signals\".to_string());\n    }\n\n    let signal_len = signals[0].1.len();\n    if signal_len < 3 {\n        return Err(\"activation signal too short\".to_string());\n    }\n\n    for (idx, signal) in signals {\n        if signal.len() != signal_len {\n            return Err(format!(\n                \"signal length mismatch for neuron {}\",\n                idx\n            ));\n        }\n    }\n\n    let mut reports = Vec::new();\n    for i in 0..signals.len() {\n        for j in (i + 1)..signals.len() {\n            let (neuron_a, signal_a) = &signals[i];\n            let (neuron_b, signal_b) = &signals[j];\n            let points: Vec<(f64, f64)> = signal_a\n                .iter()\n                .zip(signal_b.iter())\n                .map(|(x, y)| (*x, *y))\n                .collect();\n            let (score, radius_cv, axis_ratio) =\n                match pairwise_circularity_metrics(&points) {\n                    Ok(metrics) => metrics,\n                    Err(_) => (0.0, 1.0, f64::INFINITY),\n                };\n            reports.push(PairwiseManifoldPairReport {\n                neuron_a: *neuron_a,\n                neuron_b: *neuron_b,\n                circularity_score: score,\n                radius_cv,\n                axis_ratio,\n            });\n        }\n    }\n\n    let average_score = reports\n        .iter()\n        .map(|pair| pair.circularity_score)\n        .sum::<f64>()\n        / reports.len() as f64;\n\n    Ok(PairwiseManifoldReport {\n        pair_reports: reports,\n        average_score,\n    })\n}\n\nfn pairwise_circularity_metrics(points: &[(f64, f64)]) -> Result<(f64, f64, f64), String> {\n    if points.len() < 3 {\n        return Err(\"not enough points for circularity\".to_string());\n    }\n\n    let n = points.len() as f64;\n    let mean_x = points.iter().map(|(x, _)| *x).sum::<f64>() / n;\n    let mean_y = points.iter().map(|(_, y)| *y).sum::<f64>() / n;\n\n    let mut var_x = 0.0;\n    let mut var_y = 0.0;\n    let mut cov_xy = 0.0;\n    for (x, y) in points {\n        let dx = x - mean_x;\n        let dy = y - mean_y;\n        var_x += dx * dx;\n        var_y += dy * dy;\n        cov_xy += dx * dy;\n    }\n    var_x /= n;\n    var_y /= n;\n    cov_xy /= n;\n\n    let trace = var_x + var_y;\n    let det = var_x * var_y - cov_xy * cov_xy;\n    if det <= 1.0e-12 || trace <= 0.0 {\n        return Err(\"degenerate covariance\".to_string());\n    }\n\n    let half_trace = trace * 0.5;\n    let disc = (half_trace * half_trace - det).max(0.0).sqrt();\n    let lambda1 = half_trace + disc;\n    let lambda2 = half_trace - disc;\n    if lambda2 <= 0.0 {\n        return Err(\"degenerate eigenvalues\".to_string());\n    }\n\n    let axis_ratio = (lambda1 / lambda2).sqrt();\n\n    let mut v1_x = cov_xy;\n    let mut v1_y = lambda1 - var_x;\n    if v1_x.abs() < 1.0e-12 && v1_y.abs() < 1.0e-12 {\n        v1_x = 1.0;\n        v1_y = 0.0;\n    }\n    let norm = (v1_x * v1_x + v1_y * v1_y).sqrt();\n    v1_x /= norm;\n    v1_y /= norm;\n    let v2_x = -v1_y;\n    let v2_y = v1_x;\n\n    let mut radii = Vec::with_capacity(points.len());\n    for (x, y) in points {\n        let dx = x - mean_x;\n        let dy = y - mean_y;\n        let p1 = dx * v1_x + dy * v1_y;\n        let p2 = dx * v2_x + dy * v2_y;\n        let w1 = p1 / lambda1.sqrt();\n        let w2 = p2 / lambda2.sqrt();\n        radii.push((w1 * w1 + w2 * w2).sqrt());\n    }\n\n    let mean_radius = radii.iter().sum::<f64>() / radii.len() as f64;\n    if mean_radius == 0.0 {\n        return Err(\"zero mean radius\".to_string());\n    }\n\n    let mut variance = 0.0;\n    for r in &radii {\n        let diff = r - mean_radius;\n        variance += diff * diff;\n    }\n    let std = (variance / radii.len() as f64).sqrt();\n    let radius_cv = std / mean_radius;\n    let score = 1.0 / (1.0 + radius_cv);\n\n    Ok((score, radius_cv, axis_ratio))\n}\n\n// Helper functions for verification - reserved for future integration\n#[allow(dead_code)]\nfn baseline_loss(\n    losses: &[(usize, f64)],\n    plateau_min_step: usize,\n    drop_window: usize,\n) -> Result<f64, String> {\n    let before: Vec<f64> = losses\n        .iter()\n        .filter(|(step, _)| *step <= plateau_min_step)\n        .map(|(_, loss)| *loss)\n        .collect();\n\n    if before.len() < drop_window || drop_window == 0 {\n        return Err(\"insufficient loss samples for baseline\".to_string());\n    }\n\n    let window = &before[before.len() - drop_window..];\n    Ok(window.iter().sum::<f64>() / drop_window as f64)\n}\n\n#[allow(dead_code)]\nfn find_drop_step(\n    losses: &[(usize, f64)],\n    plateau_min_step: usize,\n    baseline: f64,\n    drop_fraction: f64,\n) -> Result<usize, String> {\n    losses\n        .iter()\n        .filter(|(step, _)| *step >= plateau_min_step)\n        .find(|(_, loss)| *loss <= baseline * (1.0 - drop_fraction))\n        .map(|(step, _)| *step)\n        .ok_or_else(|| \"loss never drops below threshold\".to_string())\n}\n\nfn collect_mlp_post_relu_activations<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n    fixed_y: usize,\n) -> Result<Vec<Vec<f64>>, String> {\n    let modulus = ModularAdditionDataset::modulus();\n    let equals_token = ModularAdditionDataset::equals_token();\n\n    let mut inputs_vec = Vec::with_capacity(modulus * 3);\n    for x in 0..modulus {\n        inputs_vec.push(x as i32);\n        inputs_vec.push(fixed_y as i32);\n        inputs_vec.push(equals_token as i32);\n    }\n\n    let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n        .reshape([modulus, 3]);\n    let (_, mlp_acts) = model.forward_with_mlp_activations(inputs);\n    let [batch_size, d_ff] = mlp_acts.dims();\n\n    if batch_size != modulus {\n        return Err(format!(\n            \"unexpected activation batch size {} (expected {})\",\n            batch_size, modulus\n        ));\n    }\n\n    let data: Vec<f32> = mlp_acts.into_data().to_vec().unwrap();\n    let mut per_neuron = vec![Vec::with_capacity(batch_size); d_ff];\n    for batch in 0..batch_size {\n        for neuron in 0..d_ff {\n            let idx = batch * d_ff + neuron;\n            per_neuron[neuron].push(data[idx] as f64);\n        }\n    }\n\n    Ok(per_neuron)\n}\n\nfn surface_autocorrelation_scores(\n    surface: &[Vec<f64>],\n    max_lag: usize,\n) -> Result<(f64, f64, f64, f64), String> {\n    let (height, width) = surface_dimensions(surface)?;\n    if height < 2 || width < 2 {\n        return Err(\"surface grid too small\".to_string());\n    }\n\n    let (mean, var) = surface_mean_variance(surface)?;\n    if var == 0.0 {\n        return Err(\"surface variance is zero\".to_string());\n    }\n\n    let max_lag = max_lag.min(height.saturating_sub(1)).min(width.saturating_sub(1));\n    if max_lag == 0 {\n        return Err(\"max_lag too small for surface\".to_string());\n    }\n\n    let diag_main = average_autocorrelation(surface, mean, var, max_lag, 1, 1)?;\n    let diag_anti = average_autocorrelation(surface, mean, var, max_lag, 1, -1)?;\n    let axis_x = average_autocorrelation(surface, mean, var, max_lag, 1, 0)?;\n    let axis_y = average_autocorrelation(surface, mean, var, max_lag, 0, 1)?;\n\n    Ok((diag_main, diag_anti, axis_x, axis_y))\n}\n\nfn surface_dimensions(surface: &[Vec<f64>]) -> Result<(usize, usize), String> {\n    let height = surface.len();\n    if height == 0 {\n        return Err(\"surface is empty\".to_string());\n    }\n    let width = surface[0].len();\n    if width == 0 {\n        return Err(\"surface has empty rows\".to_string());\n    }\n    for (idx, row) in surface.iter().enumerate() {\n        if row.len() != width {\n            return Err(format!(\n                \"surface row {} has length {}, expected {}\",\n                idx,\n                row.len(),\n                width\n            ));\n        }\n    }\n    Ok((height, width))\n}\n\nfn surface_mean_variance(surface: &[Vec<f64>]) -> Result<(f64, f64), String> {\n    let (height, width) = surface_dimensions(surface)?;\n    let mut sum = 0.0;\n    for row in surface {\n        for value in row {\n            sum += *value;\n        }\n    }\n    let count = (height * width) as f64;\n    let mean = sum / count;\n    let mut variance = 0.0;\n    for row in surface {\n        for value in row {\n            let diff = *value - mean;\n            variance += diff * diff;\n        }\n    }\n    variance /= count;\n    Ok((mean, variance))\n}\n\nfn average_autocorrelation(\n    surface: &[Vec<f64>],\n    mean: f64,\n    variance: f64,\n    max_lag: usize,\n    dx: i32,\n    dy: i32,\n) -> Result<f64, String> {\n    let (height, width) = surface_dimensions(surface)?;\n    let mut total = 0.0;\n    let mut lag_count = 0usize;\n\n    for lag in 1..=max_lag {\n        let mut sum = 0.0;\n        let mut count = 0usize;\n        let lag = lag as i32;\n        for x in 0..height {\n            for y in 0..width {\n                let x2 = x as i32 + dx * lag;\n                let y2 = y as i32 + dy * lag;\n                if x2 < 0 || y2 < 0 {\n                    continue;\n                }\n                let x2 = x2 as usize;\n                let y2 = y2 as usize;\n                if x2 >= height || y2 >= width {\n                    continue;\n                }\n                let a = surface[x][y] - mean;\n                let b = surface[x2][y2] - mean;\n                sum += a * b;\n                count += 1;\n            }\n        }\n        if count == 0 {\n            continue;\n        }\n        let corr = sum / (count as f64 * variance);\n        total += corr.abs();\n        lag_count += 1;\n    }\n\n    if lag_count == 0 {\n        return Err(\"no valid lags for autocorrelation\".to_string());\n    }\n\n    Ok(total / lag_count as f64)\n}\n\nfn analyze_mlp_activation_wave(\n    neuron_index: usize,\n    signal: &[f64],\n    config: &MlpWaveVerificationConfig,\n) -> Result<MlpWaveNeuronReport, String> {\n    if signal.len() < 3 {\n        return Err(format!(\n            \"activation signal for neuron {} is too short\",\n            neuron_index\n        ));\n    }\n\n    let negative_count = signal\n        .iter()\n        .filter(|value| **value < -config.negative_tolerance)\n        .count();\n    let negative_fraction = negative_count as f64 / signal.len() as f64;\n    if negative_fraction > config.max_negative_fraction {\n        return Err(format!(\n            \"neuron {} has {:.3} negative activations\",\n            neuron_index, negative_fraction\n        ));\n    }\n\n    let (dominant_frequency, dominant_ratio) = dominant_frequency_ratio(signal)?;\n    if dominant_ratio < config.min_dominant_ratio {\n        return Err(format!(\n            \"neuron {} dominant frequency ratio {:.3} below threshold {:.3}\",\n            neuron_index, dominant_ratio, config.min_dominant_ratio\n        ));\n    }\n\n    let best_correlation =\n        best_rectified_sine_correlation(signal, dominant_frequency, config.phase_steps);\n    if best_correlation < config.min_correlation {\n        return Err(format!(\n            \"neuron {} rectified sine correlation {:.3} below threshold {:.3}\",\n            neuron_index, best_correlation, config.min_correlation\n        ));\n    }\n\n    Ok(MlpWaveNeuronReport {\n        neuron_index,\n        dominant_frequency,\n        dominant_ratio,\n        negative_fraction,\n        best_correlation,\n    })\n}\n\nfn dominant_frequency_ratio(signal: &[f64]) -> Result<(usize, f64), String> {\n    if signal.len() < 2 {\n        return Err(\"signal too short for FFT\".to_string());\n    }\n\n    let mut planner = FftPlanner::new();\n    let fft = planner.plan_fft_forward(signal.len());\n    let mut buffer: Vec<Complex<f64>> = signal\n        .iter()\n        .map(|&x| Complex::new(x, 0.0))\n        .collect();\n    fft.process(&mut buffer);\n    let magnitudes: Vec<f64> = buffer.iter().map(|c| c.norm()).collect();\n\n    if magnitudes.len() < 2 {\n        return Err(\"insufficient FFT bins\".to_string());\n    }\n\n    let mut dominant_idx = 1usize;\n    let mut dominant_mag = magnitudes[1];\n    for (idx, &mag) in magnitudes.iter().enumerate().skip(2) {\n        if mag > dominant_mag {\n            dominant_mag = mag;\n            dominant_idx = idx;\n        }\n    }\n\n    if dominant_idx == 0 {\n        return Err(\"dominant frequency is zero\".to_string());\n    }\n\n    let mut nonzero = magnitudes[1..].to_vec();\n    nonzero.sort_by(|a, b| a.partial_cmp(b).unwrap_or(std::cmp::Ordering::Equal));\n    let median_mag = nonzero[nonzero.len() / 2];\n    let ratio = if median_mag == 0.0 {\n        dominant_mag\n    } else {\n        dominant_mag / median_mag\n    };\n\n    Ok((dominant_idx, ratio))\n}\n\nfn best_rectified_sine_correlation(signal: &[f64], frequency: usize, phase_steps: usize) -> f64 {\n    if signal.is_empty() || frequency == 0 || phase_steps == 0 {\n        return 0.0;\n    }\n\n    let tau = std::f64::consts::PI * 2.0;\n    let mut best = -1.0;\n    for step in 0..phase_steps {\n        let phase = tau * step as f64 / phase_steps as f64;\n        let wave: Vec<f64> = (0..signal.len())\n            .map(|idx| {\n                let angle = tau * frequency as f64 * idx as f64 / signal.len() as f64 + phase;\n                angle.sin().max(0.0)\n            })\n            .collect();\n        let corr = pearson_correlation(signal, &wave);\n        if corr > best {\n            best = corr;\n        }\n    }\n\n    best\n}\n\nfn pearson_correlation(a: &[f64], b: &[f64]) -> f64 {\n    if a.len() != b.len() || a.is_empty() {\n        return 0.0;\n    }\n\n    let mean_a = a.iter().sum::<f64>() / a.len() as f64;\n    let mean_b = b.iter().sum::<f64>() / b.len() as f64;\n\n    let mut num = 0.0;\n    let mut denom_a = 0.0;\n    let mut denom_b = 0.0;\n    for (x, y) in a.iter().zip(b.iter()) {\n        let da = x - mean_a;\n        let db = y - mean_b;\n        num += da * db;\n        denom_a += da * da;\n        denom_b += db * db;\n    }\n\n    if denom_a == 0.0 || denom_b == 0.0 {\n        return 0.0;\n    }\n\n    num / (denom_a.sqrt() * denom_b.sqrt())\n}\n\n/// Verify that the model actually learned modular addition\n/// by testing it on ALL possible examples systematically\npub fn verify_full_accuracy<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n) -> (f32, f32) {\n    println!(\"ðŸ” Verifying model on COMPLETE dataset (no sampling)...\");\n\n    let train_dataset = ModularAdditionDataset::new(true, 42);\n    let val_dataset = ModularAdditionDataset::new(false, 42);\n\n    let train_acc = test_all_examples(model, &train_dataset, device);\n    let val_acc = test_all_examples(model, &val_dataset, device);\n\n    println!(\"ðŸ“Š FULL Dataset Results:\");\n    println!(\"   Training: {}/{} = {:.2}%\",\n        (train_acc * train_dataset.len() as f32) as usize,\n        train_dataset.len(),\n        train_acc * 100.0);\n    println!(\"   Validation: {}/{} = {:.2}%\",\n        (val_acc * val_dataset.len() as f32) as usize,\n        val_dataset.len(),\n        val_acc * 100.0);\n\n    (train_acc, val_acc)\n}\n\n/// Test model on ALL examples in dataset (no random sampling)\nfn test_all_examples<B: Backend>(\n    model: &Transformer<B>,\n    dataset: &ModularAdditionDataset,\n    device: &B::Device,\n) -> f32 {\n    let mut total_correct = 0;\n    let total = dataset.len();\n\n    // Test in batches to avoid memory issues\n    let batch_size = 100;\n    for batch_start in (0..total).step_by(batch_size) {\n        let batch_end = (batch_start + batch_size).min(total);\n        let batch_len = batch_end - batch_start;\n\n        let mut inputs_vec = Vec::new();\n        let mut targets_vec = Vec::new();\n\n        for idx in batch_start..batch_end {\n            let (input, target) = dataset.get(idx).unwrap();\n            inputs_vec.extend(input);\n            targets_vec.push(target as i32);\n        }\n\n        let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n            .reshape([batch_len, 3]);\n\n        let logits = model.forward(inputs);\n        let predictions = logits.argmax(1).squeeze::<1>();\n        let predictions_vec: Vec<i32> = predictions.into_data().to_vec().unwrap();\n\n        let correct = predictions_vec\n            .iter()\n            .zip(targets_vec.iter())\n            .filter(|(p, t)| p == t)\n            .count();\n\n        total_correct += correct;\n    }\n\n    total_correct as f32 / total as f32\n}\n\n/// Verify that weight norms decrease over time with weight decay\n/// and show correlation with the grokking transition\npub fn verify_weight_norm_decay(\n    weight_norm_history: &[(usize, f64)],\n    grok_step: Option<usize>,\n    config: &WeightNormDecayConfig,\n) -> Result<WeightNormDecayReport, String> {\n    if weight_norm_history.len() < 2 {\n        return Err(\"weight norm history requires at least 2 snapshots\".to_string());\n    }\n\n    let initial_norm = weight_norm_history\n        .first()\n        .ok_or_else(|| \"weight norm history is empty\".to_string())?\n        .1;\n    let final_norm = weight_norm_history\n        .last()\n        .ok_or_else(|| \"weight norm history is empty\".to_string())?\n        .1;\n\n    if initial_norm <= 0.0 {\n        return Err(\"initial weight norm must be positive\".to_string());\n    }\n\n    // Check that norms are generally decreasing (with weight decay)\n    let relative_decrease = (initial_norm - final_norm) / initial_norm;\n    if relative_decrease < config.min_relative_decrease {\n        return Err(format!(\n            \"weight norm decrease {:.4} below threshold {:.4} (initial {:.4}, final {:.4})\",\n            relative_decrease, config.min_relative_decrease, initial_norm, final_norm\n        ));\n    }\n\n    // Check that norms are not increasing (weight decay should monotonically decrease norms)\n    for window in weight_norm_history.windows(2) {\n        let prev_norm = window[0].1;\n        let next_norm = window[1].1;\n        if next_norm > prev_norm * 1.01 {\n            // Allow 1% tolerance for numerical noise\n            return Err(format!(\n                \"weight norm increased from {:.4} to {:.4} at step {}\",\n                prev_norm, next_norm, window[1].0\n            ));\n        }\n    }\n\n    // If grok step is provided, check that norms show a shift around grokking\n    let grok_step_norm = if let Some(step) = grok_step {\n        weight_norm_history\n            .iter()\n            .filter(|(s, _)| {\n                *s >= step.saturating_sub(config.grok_step_window)\n                    && *s <= step + config.grok_step_window\n            })\n            .map(|(_, norm)| *norm)\n            .next()\n    } else {\n        None\n    };\n\n    Ok(WeightNormDecayReport {\n        initial_norm,\n        final_norm,\n        relative_decrease,\n        grok_step_norm,\n    })\n}\n\n/// Verify the characteristic \"snake curve\" shape of grokking phenomenon\n/// on log-scale plots: train accuracy rises early, validation stays flat then jumps\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_snake_curve_shape(\n    train_acc: &[(usize, f32)],\n    val_acc: &[(usize, f32)],\n    config: &SnakeCurveConfig,\n) -> Result<SnakeCurveReport, String> {\n    if train_acc.is_empty() || val_acc.is_empty() {\n        return Err(\"empty accuracy history\".to_string());\n    }\n\n    // Phase 1: Verify early train convergence (immediate drop on log scale)\n    let train_converged_step = train_acc\n        .iter()\n        .find(|(_, acc)| *acc >= config.train_convergence_threshold)\n        .map(|(step, _)| *step)\n        .ok_or_else(|| {\n            format!(\n                \"train accuracy never reaches {} threshold\",\n                config.train_convergence_threshold\n            )\n        })?;\n\n    if train_converged_step > config.early_step_max {\n        return Err(format!(\n            \"train accuracy converges at step {}, expected before {}\",\n            train_converged_step, config.early_step_max\n        ));\n    }\n\n    // Phase 2: Verify validation plateau (flat on log scale)\n    let plateau_samples: Vec<f32> = val_acc\n        .iter()\n        .filter(|(step, _)| *step >= 1 && *step <= config.val_plateau_min_step)\n        .map(|(_, acc)| *acc)\n        .collect();\n\n    if plateau_samples.is_empty() {\n        return Err(\"no validation samples in plateau region\".to_string());\n    }\n\n    let val_plateau_max = plateau_samples\n        .iter()\n        .fold(0.0f32, |max_val, &acc| max_val.max(acc));\n\n    if val_plateau_max > config.val_plateau_max_acc {\n        return Err(format!(\n            \"validation accuracy in plateau region too high: max {:.4} exceeds {:.4}\",\n            val_plateau_max, config.val_plateau_max_acc\n        ));\n    }\n\n    // Phase 3: Verify delayed validation jump (crash up on log scale)\n    let val_grok_step = val_acc\n        .iter()\n        .filter(|(step, _)| *step > config.val_plateau_min_step)\n        .find(|(_, acc)| *acc >= config.val_final_threshold)\n        .map(|(step, _)| *step)\n        .ok_or_else(|| {\n            format!(\n                \"validation accuracy never reaches {} after plateau\",\n                config.val_final_threshold\n            )\n        })?;\n\n    // Phase 4: Verify phase ordering (snake shape: early train, late val)\n    let curve_shape_valid = train_converged_step < config.val_plateau_min_step\n        && val_grok_step > config.val_plateau_min_step\n        && train_converged_step < val_grok_step;\n\n    if !curve_shape_valid {\n        return Err(format!(\n            \"invalid curve shape: train converged at {}, val plateau until {}, val grokked at {}\",\n            train_converged_step, config.val_plateau_min_step, val_grok_step\n        ));\n    }\n\n    Ok(SnakeCurveReport {\n        train_converged_by_step: train_converged_step,\n        val_plateau_max_acc: val_plateau_max,\n        val_grok_step,\n        curve_shape_valid,\n    })\n}\n\n/// Test specific examples to see if model actually computes mod p\npub fn test_specific_examples<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n) {\n    println!();\n    println!(\"ðŸ§ª Testing specific examples:\");\n\n    let modulus = ModularAdditionDataset::modulus();\n    let equals_token = ModularAdditionDataset::equals_token();\n    let test_cases = vec![\n        (0, 0, 0),\n        (1, 1, 2),\n        (50, 50, (50 + 50) % modulus),\n        (112, 1, (112 + 1) % modulus),\n        (56, 57, (56 + 57) % modulus),\n        (112, 112, (112 + 112) % modulus),\n        (10, 20, 30),\n        (60, 60, (60 + 60) % modulus),\n    ];\n\n    for (a, b, expected) in test_cases {\n        let input_vec = vec![a as i32, b as i32, equals_token as i32];\n        let input = Tensor::<B, 1, Int>::from_ints(input_vec.as_slice(), device)\n            .reshape([1, 3]);\n\n        let logits = model.forward(input);\n        let prediction = logits.argmax(1).squeeze::<1>();\n        let pred_value: i32 = prediction.into_data().to_vec().unwrap()[0];\n\n        let correct = if pred_value == expected as i32 { \"âœ“\" } else { \"âœ—\" };\n        println!(\n            \"   {} + {} mod {} = {} | Model: {} {}\",\n            a, b, modulus, expected, pred_value, correct\n        );\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    fn make_histories(\n        train_acc: &[(usize, f32)],\n        val_acc: &[(usize, f32)],\n        val_loss: &[(usize, f64)],\n    ) -> (LossHistory, AccuracyHistory) {\n        let mut loss_history = LossHistory::new();\n        loss_history.val_snapshots = val_loss.to_vec();\n        let mut accuracy_history = AccuracyHistory::new();\n        accuracy_history.train_snapshots = train_acc.to_vec();\n        accuracy_history.val_snapshots = val_acc.to_vec();\n        (loss_history, accuracy_history)\n    }\n\n    #[test]\n    fn grokking_phase_verification_passes_on_synthetic_signal() {\n        let train_acc = vec![(0, 0.2), (500, 0.995), (1000, 0.999)];\n        let val_acc = vec![\n            (0, 0.01),\n            (500, 0.01),\n            (1500, 0.01),\n            (2200, 0.995),\n            (2500, 0.999),\n        ];\n        let val_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 2.0),\n            (2200, 0.2),\n            (2500, 0.15),\n        ];\n\n        let (loss_history, accuracy_history) = make_histories(&train_acc, &val_acc, &val_loss);\n        let config = GrokkingVerificationConfig {\n            early_step_max: 1000,\n            plateau_min_step: 1500,\n            generalization_window: 1500,\n            accuracy_window: 2,\n            loss_window: 2,\n            ..GrokkingVerificationConfig::default_for_modulus(ModularAdditionDataset::modulus())\n        };\n\n        let report = verify_grokking_phase_transition(&loss_history, &accuracy_history, &config)\n            .expect(\"expected grokking verification to pass\");\n        assert!(report.transition_step >= config.plateau_min_step);\n        assert!(report.early_train_step <= config.early_step_max);\n    }\n\n    #[test]\n    fn grokking_phase_verification_fails_without_transition() {\n        let train_acc = vec![(0, 0.2), (500, 0.995)];\n        let val_acc = vec![(0, 0.01), (500, 0.01), (1500, 0.02)];\n        let val_loss = vec![(0, 2.0), (500, 2.0), (1500, 1.9)];\n\n        let (loss_history, accuracy_history) = make_histories(&train_acc, &val_acc, &val_loss);\n        let config = GrokkingVerificationConfig {\n            early_step_max: 1000,\n            plateau_min_step: 1000,\n            generalization_window: 1000,\n            accuracy_window: 2,\n            loss_window: 2,\n            ..GrokkingVerificationConfig::default_for_modulus(ModularAdditionDataset::modulus())\n        };\n\n        let err = verify_grokking_phase_transition(&loss_history, &accuracy_history, &config)\n            .expect_err(\"expected grokking verification to fail\");\n        assert!(err.contains(\"validation accuracy never reaches target\"));\n    }\n\n    #[test]\n    fn grokking_phase_verification_fails_with_slow_accuracy_rise() {\n        let train_acc = vec![(0, 0.2), (500, 0.995), (1000, 0.999)];\n        let val_acc = vec![\n            (0, 0.01),\n            (500, 0.01),\n            (1500, 0.5),\n            (2000, 0.6),\n            (2200, 0.995),\n            (2500, 0.999),\n        ];\n        let val_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 2.0),\n            (2200, 0.2),\n            (2500, 0.15),\n        ];\n\n        let (loss_history, accuracy_history) = make_histories(&train_acc, &val_acc, &val_loss);\n        let config = GrokkingVerificationConfig {\n            early_step_max: 1000,\n            plateau_min_step: 1000,\n            generalization_window: 2000,\n            accuracy_window: 2,\n            loss_window: 2,\n            ..GrokkingVerificationConfig::default_for_modulus(ModularAdditionDataset::modulus())\n        };\n\n        let err = verify_grokking_phase_transition(&loss_history, &accuracy_history, &config)\n            .expect_err(\"expected grokking verification to fail\");\n        assert!(err.contains(\"plateau too high\"));\n    }\n\n    #[test]\n    fn restricted_loss_detection_passes_with_early_drop() {\n        let full_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 2.0),\n            (2500, 2.0),\n            (3500, 0.3),\n        ];\n        let restricted_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 0.8),\n            (2500, 0.4),\n            (3500, 0.3),\n        ];\n\n        let config = RestrictedLossVerificationConfig {\n            plateau_min_step: 1000,\n            drop_window: 2,\n            drop_fraction: 0.5,\n            min_step_lead: 500,\n        };\n\n        let report = verify_restricted_loss_early_drop(&full_loss, &restricted_loss, &config)\n            .expect(\"expected restricted loss verification to pass\");\n        assert!(report.restricted_drop_step < report.full_drop_step);\n    }\n\n    #[test]\n    fn restricted_loss_detection_fails_when_full_drops_first() {\n        let full_loss = vec![(0, 2.0), (1000, 1.0), (1500, 0.8)];\n        let restricted_loss = vec![(0, 2.0), (1000, 2.0), (1500, 0.8)];\n\n        let config = RestrictedLossVerificationConfig {\n            plateau_min_step: 500,\n            drop_window: 1,\n            drop_fraction: 0.3,\n            min_step_lead: 200,\n        };\n\n        let err = verify_restricted_loss_early_drop(&full_loss, &restricted_loss, &config)\n            .expect_err(\"expected restricted loss verification to fail\");\n        assert!(err.contains(\"restricted loss drops\"));\n    }\n\n    #[test]\n    fn excluded_loss_spike_verification_passes() {\n        let excluded_loss = vec![(0, 1.0), (1500, 2.2), (3000, 1.1)];\n        let config = ExcludedLossSpikeConfig {\n            min_relative_increase: 0.5,\n        };\n\n        let report = verify_excluded_loss_spike(&excluded_loss, &config)\n            .expect(\"expected excluded loss spike verification to pass\");\n        assert_eq!(report.spike_step, 1500);\n    }\n\n    #[test]\n    fn excluded_loss_spike_verification_fails_without_spike() {\n        let excluded_loss = vec![(0, 1.0), (1500, 1.1), (3000, 1.0)];\n        let config = ExcludedLossSpikeConfig {\n            min_relative_increase: 0.5,\n        };\n\n        let err = verify_excluded_loss_spike(&excluded_loss, &config)\n            .expect_err(\"expected excluded loss spike verification to fail\");\n        assert!(err.contains(\"excluded loss spike too small\"));\n    }\n\n    #[test]\n    fn restricted_loss_drop_verification_passes() {\n        let restricted_loss = vec![\n            (0, 2.0),\n            (1000, 1.8),\n            (5000, 1.2),\n            (7000, 0.8),\n            (10000, 0.9),\n        ];\n        let config = RestrictedLossDropConfig {\n            min_relative_decrease: 0.3,\n        };\n\n        let report = verify_restricted_loss_drop(&restricted_loss, &config)\n            .expect(\"expected restricted loss drop verification to pass\");\n        assert_eq!(report.drop_step, 7000);\n        assert!(report.baseline_loss > report.drop_loss);\n        assert!((report.baseline_loss - report.drop_loss) / report.baseline_loss >= 0.3);\n    }\n\n    #[test]\n    fn restricted_loss_drop_verification_fails_without_drop() {\n        let restricted_loss = vec![\n            (0, 2.0),\n            (1000, 2.1),\n            (5000, 1.9),\n            (7000, 2.0),\n            (10000, 2.05),\n        ];\n        let config = RestrictedLossDropConfig {\n            min_relative_decrease: 0.3,\n        };\n\n        let err = verify_restricted_loss_drop(&restricted_loss, &config)\n            .expect_err(\"expected restricted loss drop verification to fail\");\n        assert!(err.contains(\"restricted loss decrease too small\"));\n    }\n\n    #[test]\n    fn mlp_activation_wave_verification_passes_on_rectified_sine() {\n        let n = ModularAdditionDataset::modulus();\n        let freq = 5.0;\n        let mut signal = Vec::with_capacity(n);\n        for idx in 0..n {\n            let angle = 2.0 * std::f64::consts::PI * freq * idx as f64 / n as f64;\n            let rectified = angle.sin().max(0.0);\n            let wobble = (2.0 * std::f64::consts::PI * 2.0 * idx as f64 / n as f64)\n                .sin()\n                .abs()\n                * 0.02;\n            signal.push(rectified + wobble);\n        }\n\n        let config = MlpWaveVerificationConfig {\n            max_negative_fraction: 0.0,\n            negative_tolerance: 1e-6,\n            min_dominant_ratio: 3.0,\n            min_correlation: 0.7,\n            phase_steps: 32,\n        };\n\n        let report =\n            verify_mlp_activation_waves_from_signals(&[(0, signal)], &config)\n                .expect(\"expected rectified sine to pass\");\n        assert_eq!(report.neuron_reports.len(), 1);\n    }\n\n    #[test]\n    fn mlp_activation_wave_verification_fails_on_noise() {\n        let n = ModularAdditionDataset::modulus();\n        let signal: Vec<f64> = (0..n).map(|idx| idx as f64 / n as f64).collect();\n\n        let config = MlpWaveVerificationConfig {\n            max_negative_fraction: 0.0,\n            negative_tolerance: 1e-6,\n            min_dominant_ratio: 4.0,\n            min_correlation: 0.8,\n            phase_steps: 16,\n        };\n\n        let err =\n            verify_mlp_activation_waves_from_signals(&[(0, signal)], &config)\n                .expect_err(\"expected linear signal to fail\");\n        assert!(err.contains(\"dominant frequency ratio\") || err.contains(\"correlation\"));\n    }\n\n    #[test]\n    fn pairwise_manifold_transition_passes_on_elliptic_signals() {\n        let n = ModularAdditionDataset::modulus();\n        let freq = 5.0;\n        let mut post_signals = Vec::new();\n        for neuron_idx in 0..7usize {\n            let phase = 2.0 * std::f64::consts::PI * neuron_idx as f64 / 7.0;\n            let signal: Vec<f64> = (0..n)\n                .map(|idx| {\n                    let angle = 2.0 * std::f64::consts::PI * freq * idx as f64 / n as f64 + phase;\n                    angle.sin()\n                })\n                .collect();\n            post_signals.push((neuron_idx, signal));\n        }\n\n        let mut pre_signals = Vec::new();\n        for neuron_idx in 0..7usize {\n            let signal: Vec<f64> = (0..n)\n                .map(|idx| idx as f64 / n as f64 + neuron_idx as f64 * 0.01)\n                .collect();\n            pre_signals.push((neuron_idx, signal));\n        }\n\n        let config = PairwiseManifoldConfig {\n            fixed_y: 0,\n            neuron_indices: (0..7).collect(),\n            min_post_score: 0.75,\n            max_pre_score: 0.55,\n            max_axis_ratio: 8.0,\n            min_pairs_passing: 8,\n        };\n\n        let report = verify_pairwise_manifold_transition_from_signals(\n            &pre_signals,\n            &post_signals,\n            &config,\n        )\n        .expect(\"expected pairwise manifold transition to pass\");\n        assert!(report.passing_pairs >= config.min_pairs_passing);\n    }\n\n    #[test]\n    fn pairwise_manifold_transition_fails_without_rings() {\n        let n = ModularAdditionDataset::modulus();\n        let mut signals = Vec::new();\n        for neuron_idx in 0..7usize {\n            let signal: Vec<f64> = (0..n)\n                .map(|idx| idx as f64 / n as f64 + neuron_idx as f64 * 0.02)\n                .collect();\n            signals.push((neuron_idx, signal));\n        }\n\n        let config = PairwiseManifoldConfig {\n            fixed_y: 0,\n            neuron_indices: (0..7).collect(),\n            min_post_score: 0.8,\n            max_pre_score: 0.55,\n            max_axis_ratio: 4.0,\n            min_pairs_passing: 5,\n        };\n\n        let err = verify_pairwise_manifold_transition_from_signals(\n            &signals,\n            &signals,\n            &config,\n        )\n        .expect_err(\"expected pairwise manifold transition to fail\");\n        assert!(err.contains(\"post-grok\") || err.contains(\"pre-grok\"));\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_passes_on_diagonal_corrugation() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create diagonal ridges: f(x,y) = sin(2Ï€ * freq * (x+y) / n)\n        let freq = 3.0;\n        for x in 0..n {\n            for y in 0..n {\n                let angle = 2.0 * std::f64::consts::PI * freq * (x + y) as f64 / n as f64;\n                surface[x][y] = angle.sin();\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let report = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect(\"expected diagonal ridge verification to pass\");\n        assert!(report.diagonal_score > report.axis_score);\n        assert!(report.ratio >= config.min_diagonal_ratio);\n        assert!(report.diagonal_score >= config.min_diagonal_score);\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_passes_on_anti_diagonal_pattern() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create anti-diagonal ridges: f(x,y) = sin(2Ï€ * freq * (x-y) / n)\n        let freq = 3.0;\n        for x in 0..n {\n            for y in 0..n {\n                let angle = 2.0 * std::f64::consts::PI * freq * (x as i32 - y as i32) as f64 / n as f64;\n                surface[x][y] = angle.sin();\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let report = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect(\"expected anti-diagonal ridge verification to pass\");\n        assert!(report.diagonal_score > report.axis_score);\n        assert!(report.ratio >= config.min_diagonal_ratio);\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_fails_on_axis_aligned_pattern() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create axis-aligned ridges: f(x,y) = sin(2Ï€ * freq * x / n)\n        let freq = 3.0;\n        for x in 0..n {\n            for y in 0..n {\n                let angle = 2.0 * std::f64::consts::PI * freq * x as f64 / n as f64;\n                surface[x][y] = angle.sin();\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let err = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect_err(\"expected axis-aligned pattern to fail diagonal verification\");\n        assert!(err.contains(\"diagonal/axis ratio\") || err.contains(\"diagonal autocorrelation\"));\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_fails_on_flat_surface() {\n        let n = 50;\n        let surface = vec![vec![1.5; n]; n];\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let err = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect_err(\"expected flat surface to fail\");\n        assert!(err.contains(\"surface variance is zero\"));\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_fails_on_random_noise() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create random noise (low autocorrelation everywhere)\n        for x in 0..n {\n            for y in 0..n {\n                surface[x][y] = ((x * 17 + y * 31) % 100) as f64 / 100.0;\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let err = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect_err(\"expected random noise to fail diagonal verification\");\n        assert!(err.contains(\"diagonal/axis ratio\") || err.contains(\"diagonal autocorrelation\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_passes_on_valid_grokking_curve() {\n        // Simulate typical grokking: train converges early, val plateaus then jumps\n        let train_acc = vec![\n            (1, 0.10),\n            (10, 0.50),\n            (100, 0.80),\n            (500, 0.98),\n            (1000, 0.99),\n            (2000, 0.995),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (10, 0.009),\n            (100, 0.01),\n            (500, 0.011),\n            (1000, 0.009),\n            (1500, 0.01),\n            (2000, 0.92),\n            (3000, 0.98),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 1500,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let report = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect(\"expected snake curve verification to pass\");\n\n        assert_eq!(report.train_converged_by_step, 500);\n        assert!(report.val_plateau_max_acc < config.val_plateau_max_acc);\n        assert_eq!(report.val_grok_step, 2000);\n        assert!(report.curve_shape_valid);\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_on_simultaneous_convergence() {\n        // Both train and val converge at the same time (no grokking)\n        let train_acc = vec![\n            (1, 0.10),\n            (100, 0.98),\n            (500, 0.99),\n        ];\n        let val_acc = vec![\n            (1, 0.10),\n            (100, 0.97),\n            (500, 0.98),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 200,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let err = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect_err(\"expected snake curve verification to fail\");\n        assert!(err.contains(\"plateau region too high\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_on_slow_train_convergence() {\n        // Train takes too long to converge (not the typical snake curve)\n        let train_acc = vec![\n            (1, 0.10),\n            (500, 0.50),\n            (1000, 0.70),\n            (2000, 0.98),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (500, 0.01),\n            (1000, 0.01),\n            (2000, 0.92),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 1500,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let err = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect_err(\"expected snake curve verification to fail\");\n        assert!(err.contains(\"converges at step 2000\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_without_val_jump() {\n        // Validation never jumps up (no generalization)\n        let train_acc = vec![\n            (1, 0.10),\n            (100, 0.98),\n            (500, 0.99),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (100, 0.01),\n            (500, 0.01),\n            (2000, 0.02),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 200,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let err = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect_err(\"expected snake curve verification to fail\");\n        assert!(err.contains(\"validation accuracy never reaches\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_on_wrong_phase_ordering() {\n        // Val jumps before train converges (backwards snake)\n        let train_acc = vec![\n            (1, 0.10),\n            (500, 0.50),\n            (2000, 0.98),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (100, 0.95),\n            (500, 0.98),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 200,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let result = verify_snake_curve_shape(&train_acc, &val_acc, &config);\n        // Should fail because val jumps before train converges (not a proper snake curve)\n        // This can fail for multiple reasons, but should definitely fail\n        assert!(result.is_err(), \"Expected snake curve verification to fail but it passed\");\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_passes_on_decreasing_norms() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 85.0),\n            (1000, 72.0),\n            (1500, 63.0),\n            (2000, 58.0),\n            (2500, 55.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let report = verify_weight_norm_decay(&weight_norms, Some(2000), &config)\n            .expect(\"expected weight norm decay verification to pass\");\n\n        assert_eq!(report.initial_norm, 100.0);\n        assert_eq!(report.final_norm, 55.0);\n        assert!((report.relative_decrease - 0.45).abs() < 0.01);\n        assert!(report.grok_step_norm.is_some());\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_fails_on_flat_norms() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 99.0),\n            (1000, 98.5),\n            (1500, 98.0),\n            (2000, 97.5),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let err = verify_weight_norm_decay(&weight_norms, None, &config)\n            .expect_err(\"expected weight norm decay verification to fail\");\n        assert!(err.contains(\"weight norm decrease\"));\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_fails_on_increasing_norms() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 95.0),\n            (1000, 105.0), // Increase here\n            (1500, 90.0),\n            (2000, 85.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let err = verify_weight_norm_decay(&weight_norms, None, &config)\n            .expect_err(\"expected weight norm decay verification to fail\");\n        assert!(err.contains(\"weight norm increased\"));\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_allows_small_noise() {\n        // Weight decay with small numerical noise (within 1% tolerance)\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 95.0),\n            (1000, 90.5),  // Small increase within tolerance\n            (1500, 90.0),\n            (2000, 85.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let report = verify_weight_norm_decay(&weight_norms, None, &config)\n            .expect(\"expected weight norm decay with small noise to pass\");\n\n        assert_eq!(report.initial_norm, 100.0);\n        assert_eq!(report.final_norm, 85.0);\n        assert!((report.relative_decrease - 0.15).abs() < 0.01);\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_finds_grok_step_norm() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (1000, 80.0),\n            (1800, 68.0),  // Within window of grok step (2200 - 500 = 1700)\n            (2000, 65.0),\n            (2200, 62.0),  // Around grok step\n            (3000, 55.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let report = verify_weight_norm_decay(&weight_norms, Some(2200), &config)\n            .expect(\"expected weight norm decay verification to pass\");\n\n        assert!(report.grok_step_norm.is_some());\n        let grok_norm = report.grok_step_norm.unwrap();\n        // Should find the first norm in the window [1700, 2700], which is 68.0 at step 1800\n        assert!((grok_norm - 68.0).abs() < 0.01);\n    }\n}\n","structuredPatch":[{"oldStart":950,"oldLines":6,"newStart":950,"newLines":7,"lines":["     Ok(per_neuron)"," }"," ","+#[allow(dead_code)]"," fn surface_autocorrelation_scores(","     surface: &[Vec<f64>],","     max_lag: usize,"]}],"userModified":false,"replaceAll":false}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01Gk51phKJCcVATSWQHt1E2v","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01RBnt84L183mvU47DVaNCKC","name":"Edit","input":{"replace_all":false,"file_path":"/Users/admin/MechInt/src/verify.rs","old_string":"fn surface_dimensions(surface: &[Vec<f64>]) -> Result<(usize, usize), String> {","new_string":"#[allow(dead_code)]\nfn surface_dimensions(surface: &[Vec<f64>]) -> Result<(usize, usize), String> {"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":321,"cache_read_input_tokens":62730,"cache_creation":{"ephemeral_5m_input_tokens":321,"ephemeral_1h_input_tokens":0},"output_tokens":186,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"9ca42f15-8cea-4f1c-81c4-8634d1d9ae10"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01RBnt84L183mvU47DVaNCKC","type":"tool_result","content":"The file /Users/admin/MechInt/src/verify.rs has been updated. Here's the result of running `cat -n` on a snippet of the edited file:\n   977â†’\n   978â†’    Ok((diag_main, diag_anti, axis_x, axis_y))\n   979â†’}\n   980â†’\n   981â†’#[allow(dead_code)]\n   982â†’fn surface_dimensions(surface: &[Vec<f64>]) -> Result<(usize, usize), String> {\n   983â†’    let height = surface.len();\n   984â†’    if height == 0 {\n   985â†’        return Err(\"surface is empty\".to_string());\n   986â†’    }"}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"68e6d565-1bc9-42dd-a91d-0f2027142388","tool_use_result":{"filePath":"/Users/admin/MechInt/src/verify.rs","oldString":"fn surface_dimensions(surface: &[Vec<f64>]) -> Result<(usize, usize), String> {","newString":"#[allow(dead_code)]\nfn surface_dimensions(surface: &[Vec<f64>]) -> Result<(usize, usize), String> {","originalFile":"use burn::{\n    data::dataset::Dataset,\n    tensor::{backend::Backend, Int, Tensor},\n};\nuse rustfft::{num_complex::Complex, FftPlanner};\n\nuse crate::analysis::{AccuracyHistory, LossHistory};\nuse crate::data::ModularAdditionDataset;\nuse crate::model::Transformer;\n\n#[derive(Debug, Clone)]\npub struct GrokkingVerificationConfig {\n    pub early_train_acc_threshold: f32,\n    pub early_step_max: usize,\n    pub plateau_min_step: usize,\n    pub generalization_window: usize,\n    pub chance_accuracy: f32,\n    pub chance_tolerance: f32,\n    pub target_val_acc_threshold: f32,\n    pub accuracy_window: usize,\n    pub loss_window: usize,\n    pub loss_drop_fraction: f64,\n}\n\nimpl GrokkingVerificationConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        Self {\n            early_train_acc_threshold: 0.99,\n            early_step_max: 1000,\n            plateau_min_step: 1000,\n            generalization_window: 10_000,\n            chance_accuracy: 1.0 / modulus as f32,\n            chance_tolerance: 0.02,\n            target_val_acc_threshold: 0.99,\n            accuracy_window: 5,\n            loss_window: 5,\n            loss_drop_fraction: 0.5,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct GrokkingPhaseReport {\n    pub early_train_step: usize,\n    pub transition_step: usize,\n    pub plateau_max_val_acc: f32,\n    pub loss_drop_ratio: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossVerificationConfig {\n    pub plateau_min_step: usize,\n    pub drop_window: usize,\n    pub drop_fraction: f64,\n    pub min_step_lead: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossReport {\n    pub restricted_drop_step: usize,\n    pub full_drop_step: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossDropConfig {\n    pub min_relative_decrease: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossDropReport {\n    pub drop_step: usize,\n    pub drop_loss: f64,\n    pub baseline_loss: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct ExcludedLossSpikeConfig {\n    pub min_relative_increase: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct ExcludedLossSpikeReport {\n    pub spike_step: usize,\n    pub spike_loss: f64,\n    pub baseline_loss: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct MlpWaveVerificationConfig {\n    pub max_negative_fraction: f64,\n    pub negative_tolerance: f64,\n    pub min_dominant_ratio: f64,\n    pub min_correlation: f64,\n    pub phase_steps: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct MlpWaveNeuronReport {\n    pub neuron_index: usize,\n    pub dominant_frequency: usize,\n    pub dominant_ratio: f64,\n    pub negative_fraction: f64,\n    pub best_correlation: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct MlpWaveReport {\n    pub neuron_reports: Vec<MlpWaveNeuronReport>,\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldConfig {\n    pub fixed_y: usize,\n    pub neuron_indices: Vec<usize>,\n    pub min_post_score: f64,\n    pub max_pre_score: f64,\n    pub max_axis_ratio: f64,\n    pub min_pairs_passing: usize,\n}\n\nimpl PairwiseManifoldConfig {\n    pub fn default_for_modulus(_modulus: usize) -> Self {\n        Self {\n            fixed_y: 0,\n            neuron_indices: (0..7).collect(),\n            min_post_score: 0.75,\n            max_pre_score: 0.55,\n            max_axis_ratio: 8.0,\n            min_pairs_passing: 10,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldPairReport {\n    pub neuron_a: usize,\n    pub neuron_b: usize,\n    pub circularity_score: f64,\n    pub radius_cv: f64,\n    pub axis_ratio: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldReport {\n    pub pair_reports: Vec<PairwiseManifoldPairReport>,\n    pub average_score: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldTransitionReport {\n    pub pre_average_score: f64,\n    pub post_average_score: f64,\n    pub passing_pairs: usize,\n    pub total_pairs: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct DiagonalRidgeConfig {\n    pub neuron_index: usize,\n    pub max_lag: usize,\n    pub min_diagonal_ratio: f64,\n    pub min_diagonal_score: f64,\n}\n\nimpl DiagonalRidgeConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        let max_lag = (modulus / 10).max(4).min(20);\n        Self {\n            neuron_index: 10,\n            max_lag,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct DiagonalRidgeReport {\n    pub neuron_index: usize,\n    pub diagonal_score: f64,\n    pub axis_score: f64,\n    pub ratio: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct SnakeCurveConfig {\n    pub early_step_max: usize,\n    pub train_convergence_threshold: f32,\n    pub val_plateau_min_step: usize,\n    pub val_plateau_max_acc: f32,\n    pub val_final_threshold: f32,\n}\n\nimpl SnakeCurveConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        Self {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 1000,\n            val_plateau_max_acc: 1.0 / modulus as f32 + 0.05,\n            val_final_threshold: 0.90,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct WeightNormDecayConfig {\n    pub min_relative_decrease: f64,\n    pub plateau_min_step: usize,\n    pub grok_step_window: usize,\n}\n\nimpl WeightNormDecayConfig {\n    pub fn default_for_grokking() -> Self {\n        Self {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct WeightNormDecayReport {\n    pub initial_norm: f64,\n    pub final_norm: f64,\n    pub relative_decrease: f64,\n    pub grok_step_norm: Option<f64>,\n}\n\n#[derive(Debug, Clone)]\npub struct SnakeCurveReport {\n    pub train_converged_by_step: usize,\n    pub val_plateau_max_acc: f32,\n    pub val_grok_step: usize,\n    pub curve_shape_valid: bool,\n}\n\npub fn verify_grokking_phase_transition(\n    loss_history: &LossHistory,\n    accuracy_history: &AccuracyHistory,\n    config: &GrokkingVerificationConfig,\n) -> Result<GrokkingPhaseReport, String> {\n    let (early_train_step, _) = accuracy_history\n        .train_snapshots\n        .iter()\n        .find(|(_, acc)| *acc >= config.early_train_acc_threshold)\n        .ok_or_else(|| \"train accuracy never reaches threshold\".to_string())?;\n\n    if *early_train_step > config.early_step_max {\n        return Err(format!(\n            \"train accuracy reaches {:.2} after step {}, expected before {}\",\n            config.early_train_acc_threshold, early_train_step, config.early_step_max\n        ));\n    }\n\n    let plateau_max_val_acc = accuracy_history\n        .val_snapshots\n        .iter()\n        .filter(|(step, _)| *step <= config.plateau_min_step)\n        .map(|(_, acc)| *acc)\n        .fold(None::<f32>, |accum, value| {\n            Some(accum.map_or(value, |max_val| max_val.max(value)))\n        })\n        .ok_or_else(|| \"no validation accuracy samples before plateau_min_step\".to_string())?;\n\n    if plateau_max_val_acc > config.chance_accuracy + config.chance_tolerance {\n        return Err(format!(\n            \"validation accuracy before step {} exceeds chance (max {:.4})\",\n            config.plateau_min_step, plateau_max_val_acc\n        ));\n    }\n\n    let (transition_step, _) = accuracy_history\n        .val_snapshots\n        .iter()\n        .find(|(step, acc)| {\n            *step >= config.plateau_min_step && *acc >= config.target_val_acc_threshold\n        })\n        .ok_or_else(|| \"validation accuracy never reaches target threshold\".to_string())?;\n\n    let latest_allowed = config.plateau_min_step + config.generalization_window;\n    if *transition_step > latest_allowed {\n        return Err(format!(\n            \"validation accuracy reaches {:.2} at step {}, expected before {}\",\n            config.target_val_acc_threshold, transition_step, latest_allowed\n        ));\n    }\n\n    verify_accuracy_jump(\n        &accuracy_history.val_snapshots,\n        *transition_step,\n        config.accuracy_window,\n        config.chance_accuracy,\n        config.chance_tolerance,\n        config.target_val_acc_threshold,\n    )?;\n\n    let loss_drop_ratio = verify_loss_drop(\n        &loss_history.val_snapshots,\n        *transition_step,\n        config.loss_window,\n        config.loss_drop_fraction,\n    )?;\n\n    Ok(GrokkingPhaseReport {\n        early_train_step: *early_train_step,\n        transition_step: *transition_step,\n        plateau_max_val_acc,\n        loss_drop_ratio,\n    })\n}\n\nfn verify_accuracy_jump(\n    val_accuracy: &[(usize, f32)],\n    transition_step: usize,\n    window: usize,\n    chance_accuracy: f32,\n    chance_tolerance: f32,\n    target_val_acc_threshold: f32,\n) -> Result<(), String> {\n    let before_values: Vec<f32> = val_accuracy\n        .iter()\n        .filter(|(step, _)| *step < transition_step)\n        .map(|(_, acc)| *acc)\n        .collect();\n\n    let after_values: Vec<f32> = val_accuracy\n        .iter()\n        .filter(|(step, _)| *step >= transition_step)\n        .map(|(_, acc)| *acc)\n        .collect();\n\n    if before_values.len() < window || after_values.len() < window {\n        return Err(\"insufficient accuracy samples around transition\".to_string());\n    }\n\n    let before_avg: f32 =\n        before_values[before_values.len() - window..].iter().sum::<f32>() / window as f32;\n    let after_avg: f32 = after_values[..window].iter().sum::<f32>() / window as f32;\n\n    if before_avg > chance_accuracy + chance_tolerance {\n        return Err(format!(\n            \"validation accuracy plateau too high before transition (avg {:.4})\",\n            before_avg\n        ));\n    }\n\n    if after_avg < target_val_acc_threshold {\n        return Err(format!(\n            \"validation accuracy after transition too low (avg {:.4})\",\n            after_avg\n        ));\n    }\n\n    Ok(())\n}\n\nfn verify_loss_drop(\n    val_losses: &[(usize, f64)],\n    transition_step: usize,\n    loss_window: usize,\n    loss_drop_fraction: f64,\n) -> Result<f64, String> {\n    let before_losses: Vec<f64> = val_losses\n        .iter()\n        .filter(|(step, _)| *step < transition_step)\n        .map(|(_, loss)| *loss)\n        .collect();\n\n    let after_losses: Vec<f64> = val_losses\n        .iter()\n        .filter(|(step, _)| *step >= transition_step)\n        .map(|(_, loss)| *loss)\n        .collect();\n\n    if before_losses.len() < loss_window || after_losses.len() < loss_window {\n        return Err(\"insufficient loss samples around transition\".to_string());\n    }\n\n    let before_avg: f64 =\n        before_losses[before_losses.len() - loss_window..].iter().sum::<f64>()\n            / loss_window as f64;\n    let after_avg: f64 = after_losses[..loss_window].iter().sum::<f64>() / loss_window as f64;\n\n    if after_avg > before_avg * (1.0 - loss_drop_fraction) {\n        return Err(format!(\n            \"validation loss drop too small (before {:.4}, after {:.4})\",\n            before_avg, after_avg\n        ));\n    }\n\n    Ok(after_avg / before_avg)\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_restricted_loss_early_drop(\n    full_losses: &[(usize, f64)],\n    restricted_losses: &[(usize, f64)],\n    config: &RestrictedLossVerificationConfig,\n) -> Result<RestrictedLossReport, String> {\n    let restricted_baseline =\n        baseline_loss(restricted_losses, config.plateau_min_step, config.drop_window)?;\n    let full_baseline = baseline_loss(full_losses, config.plateau_min_step, config.drop_window)?;\n\n    let restricted_drop_step = find_drop_step(\n        restricted_losses,\n        config.plateau_min_step,\n        restricted_baseline,\n        config.drop_fraction,\n    )?;\n    let full_drop_step = find_drop_step(\n        full_losses,\n        config.plateau_min_step,\n        full_baseline,\n        config.drop_fraction,\n    )?;\n\n    if restricted_drop_step + config.min_step_lead > full_drop_step {\n        return Err(format!(\n            \"restricted loss drops at step {}, expected at least {} steps before full loss (step {})\",\n            restricted_drop_step, config.min_step_lead, full_drop_step\n        ));\n    }\n\n    Ok(RestrictedLossReport {\n        restricted_drop_step,\n        full_drop_step,\n    })\n}\n\npub fn verify_restricted_loss_drop(\n    restricted_losses: &[(usize, f64)],\n    config: &RestrictedLossDropConfig,\n) -> Result<RestrictedLossDropReport, String> {\n    if restricted_losses.len() < 3 {\n        return Err(\"restricted loss history requires at least 3 points\".to_string());\n    }\n\n    let first = restricted_losses.first().ok_or_else(|| {\n        \"restricted loss history is empty\".to_string()\n    })?;\n    let baseline_loss = first.1;\n\n    let (min_idx, (min_step, min_loss)) = restricted_losses\n        .iter()\n        .enumerate()\n        .min_by(|(_, a), (_, b)| {\n            a.1.partial_cmp(&b.1).unwrap_or(std::cmp::Ordering::Equal)\n        })\n        .ok_or_else(|| \"restricted loss history is empty\".to_string())?;\n\n    if min_idx == 0 {\n        return Err(\"restricted loss minimum occurs at start (no drop detected)\".to_string());\n    }\n\n    let relative_decrease = (baseline_loss - min_loss) / baseline_loss;\n    if relative_decrease < config.min_relative_decrease {\n        return Err(format!(\n            \"restricted loss decrease too small (baseline {:.4}, min {:.4}, decrease {:.1}%)\",\n            baseline_loss, min_loss, relative_decrease * 100.0\n        ));\n    }\n\n    Ok(RestrictedLossDropReport {\n        drop_step: *min_step,\n        drop_loss: *min_loss,\n        baseline_loss,\n    })\n}\n\npub fn verify_excluded_loss_spike(\n    excluded_losses: &[(usize, f64)],\n    config: &ExcludedLossSpikeConfig,\n) -> Result<ExcludedLossSpikeReport, String> {\n    if excluded_losses.len() < 3 {\n        return Err(\"excluded loss history requires at least 3 points\".to_string());\n    }\n\n    let first = excluded_losses.first().ok_or_else(|| {\n        \"excluded loss history is empty\".to_string()\n    })?;\n    let last = excluded_losses.last().ok_or_else(|| {\n        \"excluded loss history is empty\".to_string()\n    })?;\n    let baseline_loss = (first.1 + last.1) / 2.0;\n\n    let (peak_idx, (peak_step, peak_loss)) = excluded_losses\n        .iter()\n        .enumerate()\n        .max_by(|(_, a), (_, b)| {\n            a.1.partial_cmp(&b.1).unwrap_or(std::cmp::Ordering::Equal)\n        })\n        .ok_or_else(|| \"excluded loss history is empty\".to_string())?;\n\n    if peak_idx == 0 || peak_idx + 1 == excluded_losses.len() {\n        return Err(\"excluded loss spike occurs at boundary\".to_string());\n    }\n\n    if peak_loss < &(baseline_loss * (1.0 + config.min_relative_increase)) {\n        return Err(format!(\n            \"excluded loss spike too small (baseline {:.4}, peak {:.4})\",\n            baseline_loss, peak_loss\n        ));\n    }\n\n    Ok(ExcludedLossSpikeReport {\n        spike_step: *peak_step,\n        spike_loss: *peak_loss,\n        baseline_loss,\n    })\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_mlp_activation_waves<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n    fixed_y: usize,\n    neuron_indices: &[usize],\n    config: &MlpWaveVerificationConfig,\n) -> Result<MlpWaveReport, String> {\n    let activations = collect_mlp_post_relu_activations(model, device, fixed_y)?;\n    if neuron_indices.is_empty() {\n        return Err(\"neuron indices list is empty\".to_string());\n    }\n\n    let d_ff = activations.len();\n    for &idx in neuron_indices {\n        if idx >= d_ff {\n            return Err(format!(\n                \"neuron index {} out of range (d_ff={})\",\n                idx, d_ff\n            ));\n        }\n    }\n\n    let signals: Vec<(usize, Vec<f64>)> = neuron_indices\n        .iter()\n        .map(|&idx| (idx, activations[idx].clone()))\n        .collect();\n    verify_mlp_activation_waves_from_signals(&signals, config)\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_mlp_activation_waves_from_signals(\n    signals: &[(usize, Vec<f64>)],\n    config: &MlpWaveVerificationConfig,\n) -> Result<MlpWaveReport, String> {\n    if signals.is_empty() {\n        return Err(\"no activation signals provided\".to_string());\n    }\n\n    let mut reports = Vec::with_capacity(signals.len());\n    for (neuron_index, signal) in signals {\n        let report = analyze_mlp_activation_wave(*neuron_index, signal, config)?;\n        reports.push(report);\n    }\n\n    Ok(MlpWaveReport {\n        neuron_reports: reports,\n    })\n}\n\npub fn verify_mlp_pairwise_manifold_transition<B: Backend>(\n    pre_model: &Transformer<B>,\n    post_model: &Transformer<B>,\n    device: &B::Device,\n    config: &PairwiseManifoldConfig,\n) -> Result<PairwiseManifoldTransitionReport, String> {\n    let modulus = ModularAdditionDataset::modulus();\n    if config.fixed_y >= modulus {\n        return Err(format!(\n            \"fixed_y {} out of range (modulus {})\",\n            config.fixed_y, modulus\n        ));\n    }\n\n    let pre_activations = collect_mlp_post_relu_activations(pre_model, device, config.fixed_y)?;\n    let post_activations = collect_mlp_post_relu_activations(post_model, device, config.fixed_y)?;\n\n    let d_ff = pre_activations.len();\n    if post_activations.len() != d_ff {\n        return Err(\"post activations shape mismatch\".to_string());\n    }\n\n    if config.neuron_indices.len() < 2 {\n        return Err(\"need at least two neuron indices\".to_string());\n    }\n\n    for &idx in &config.neuron_indices {\n        if idx >= d_ff {\n            return Err(format!(\n                \"neuron index {} out of range (d_ff={})\",\n                idx, d_ff\n            ));\n        }\n    }\n\n    let pre_signals: Vec<(usize, Vec<f64>)> = config\n        .neuron_indices\n        .iter()\n        .map(|&idx| (idx, pre_activations[idx].clone()))\n        .collect();\n    let post_signals: Vec<(usize, Vec<f64>)> = config\n        .neuron_indices\n        .iter()\n        .map(|&idx| (idx, post_activations[idx].clone()))\n        .collect();\n\n    verify_pairwise_manifold_transition_from_signals(&pre_signals, &post_signals, config)\n}\n\npub fn collect_mlp_activation_surface<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n    neuron_index: usize,\n) -> Result<Vec<Vec<f64>>, String> {\n    let modulus = ModularAdditionDataset::modulus();\n    let equals_token = ModularAdditionDataset::equals_token();\n\n    let mut inputs_vec = Vec::with_capacity(modulus * modulus * 3);\n    for x in 0..modulus {\n        for y in 0..modulus {\n            inputs_vec.push(x as i32);\n            inputs_vec.push(y as i32);\n            inputs_vec.push(equals_token as i32);\n        }\n    }\n\n    let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n        .reshape([modulus * modulus, 3]);\n    let (_, mlp_acts) = model.forward_with_mlp_activations(inputs);\n    let [batch_size, d_ff] = mlp_acts.dims();\n    if batch_size != modulus * modulus {\n        return Err(format!(\n            \"unexpected activation batch size {} (expected {})\",\n            batch_size,\n            modulus * modulus\n        ));\n    }\n    if neuron_index >= d_ff {\n        return Err(format!(\n            \"neuron index {} out of range (d_ff={})\",\n            neuron_index, d_ff\n        ));\n    }\n\n    let data: Vec<f32> = mlp_acts.into_data().to_vec().unwrap();\n    let mut surface = vec![vec![0.0f64; modulus]; modulus];\n    for x in 0..modulus {\n        for y in 0..modulus {\n            let batch = x * modulus + y;\n            let idx = batch * d_ff + neuron_index;\n            surface[x][y] = data[idx] as f64;\n        }\n    }\n\n    Ok(surface)\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_constructive_interference_surface_from_grid(\n    surface: &[Vec<f64>],\n    config: &DiagonalRidgeConfig,\n) -> Result<DiagonalRidgeReport, String> {\n    let (diag_main, diag_anti, axis_x, axis_y) =\n        surface_autocorrelation_scores(surface, config.max_lag)?;\n\n    let diagonal_score = diag_main.max(diag_anti);\n    let axis_score = axis_x.max(axis_y);\n    let ratio = if axis_score > 0.0 {\n        diagonal_score / axis_score\n    } else {\n        f64::INFINITY\n    };\n\n    if diagonal_score < config.min_diagonal_score {\n        return Err(format!(\n            \"diagonal autocorrelation {:.3} below threshold {:.3}\",\n            diagonal_score, config.min_diagonal_score\n        ));\n    }\n\n    if ratio < config.min_diagonal_ratio {\n        return Err(format!(\n            \"diagonal/axis ratio {:.3} below threshold {:.3}\",\n            ratio, config.min_diagonal_ratio\n        ));\n    }\n\n    Ok(DiagonalRidgeReport {\n        neuron_index: config.neuron_index,\n        diagonal_score,\n        axis_score,\n        ratio,\n    })\n}\n\npub fn verify_pairwise_manifold_transition_from_signals(\n    pre_signals: &[(usize, Vec<f64>)],\n    post_signals: &[(usize, Vec<f64>)],\n    config: &PairwiseManifoldConfig,\n) -> Result<PairwiseManifoldTransitionReport, String> {\n    let pre_report = evaluate_pairwise_manifolds_from_signals(pre_signals)?;\n    let post_report = evaluate_pairwise_manifolds_from_signals(post_signals)?;\n\n    if pre_report.average_score > config.max_pre_score {\n        return Err(format!(\n            \"pre-grok average circularity {:.3} exceeds threshold {:.3}\",\n            pre_report.average_score, config.max_pre_score\n        ));\n    }\n\n    let passing_pairs = post_report\n        .pair_reports\n        .iter()\n        .filter(|pair| {\n            pair.circularity_score >= config.min_post_score\n                && pair.axis_ratio <= config.max_axis_ratio\n        })\n        .count();\n    if passing_pairs < config.min_pairs_passing {\n        return Err(format!(\n            \"post-grok only {} pairs meet circularity threshold (need {})\",\n            passing_pairs, config.min_pairs_passing\n        ));\n    }\n\n    Ok(PairwiseManifoldTransitionReport {\n        pre_average_score: pre_report.average_score,\n        post_average_score: post_report.average_score,\n        passing_pairs,\n        total_pairs: post_report.pair_reports.len(),\n    })\n}\n\nfn evaluate_pairwise_manifolds_from_signals(\n    signals: &[(usize, Vec<f64>)],\n) -> Result<PairwiseManifoldReport, String> {\n    if signals.len() < 2 {\n        return Err(\"need at least two activation signals\".to_string());\n    }\n\n    let signal_len = signals[0].1.len();\n    if signal_len < 3 {\n        return Err(\"activation signal too short\".to_string());\n    }\n\n    for (idx, signal) in signals {\n        if signal.len() != signal_len {\n            return Err(format!(\n                \"signal length mismatch for neuron {}\",\n                idx\n            ));\n        }\n    }\n\n    let mut reports = Vec::new();\n    for i in 0..signals.len() {\n        for j in (i + 1)..signals.len() {\n            let (neuron_a, signal_a) = &signals[i];\n            let (neuron_b, signal_b) = &signals[j];\n            let points: Vec<(f64, f64)> = signal_a\n                .iter()\n                .zip(signal_b.iter())\n                .map(|(x, y)| (*x, *y))\n                .collect();\n            let (score, radius_cv, axis_ratio) =\n                match pairwise_circularity_metrics(&points) {\n                    Ok(metrics) => metrics,\n                    Err(_) => (0.0, 1.0, f64::INFINITY),\n                };\n            reports.push(PairwiseManifoldPairReport {\n                neuron_a: *neuron_a,\n                neuron_b: *neuron_b,\n                circularity_score: score,\n                radius_cv,\n                axis_ratio,\n            });\n        }\n    }\n\n    let average_score = reports\n        .iter()\n        .map(|pair| pair.circularity_score)\n        .sum::<f64>()\n        / reports.len() as f64;\n\n    Ok(PairwiseManifoldReport {\n        pair_reports: reports,\n        average_score,\n    })\n}\n\nfn pairwise_circularity_metrics(points: &[(f64, f64)]) -> Result<(f64, f64, f64), String> {\n    if points.len() < 3 {\n        return Err(\"not enough points for circularity\".to_string());\n    }\n\n    let n = points.len() as f64;\n    let mean_x = points.iter().map(|(x, _)| *x).sum::<f64>() / n;\n    let mean_y = points.iter().map(|(_, y)| *y).sum::<f64>() / n;\n\n    let mut var_x = 0.0;\n    let mut var_y = 0.0;\n    let mut cov_xy = 0.0;\n    for (x, y) in points {\n        let dx = x - mean_x;\n        let dy = y - mean_y;\n        var_x += dx * dx;\n        var_y += dy * dy;\n        cov_xy += dx * dy;\n    }\n    var_x /= n;\n    var_y /= n;\n    cov_xy /= n;\n\n    let trace = var_x + var_y;\n    let det = var_x * var_y - cov_xy * cov_xy;\n    if det <= 1.0e-12 || trace <= 0.0 {\n        return Err(\"degenerate covariance\".to_string());\n    }\n\n    let half_trace = trace * 0.5;\n    let disc = (half_trace * half_trace - det).max(0.0).sqrt();\n    let lambda1 = half_trace + disc;\n    let lambda2 = half_trace - disc;\n    if lambda2 <= 0.0 {\n        return Err(\"degenerate eigenvalues\".to_string());\n    }\n\n    let axis_ratio = (lambda1 / lambda2).sqrt();\n\n    let mut v1_x = cov_xy;\n    let mut v1_y = lambda1 - var_x;\n    if v1_x.abs() < 1.0e-12 && v1_y.abs() < 1.0e-12 {\n        v1_x = 1.0;\n        v1_y = 0.0;\n    }\n    let norm = (v1_x * v1_x + v1_y * v1_y).sqrt();\n    v1_x /= norm;\n    v1_y /= norm;\n    let v2_x = -v1_y;\n    let v2_y = v1_x;\n\n    let mut radii = Vec::with_capacity(points.len());\n    for (x, y) in points {\n        let dx = x - mean_x;\n        let dy = y - mean_y;\n        let p1 = dx * v1_x + dy * v1_y;\n        let p2 = dx * v2_x + dy * v2_y;\n        let w1 = p1 / lambda1.sqrt();\n        let w2 = p2 / lambda2.sqrt();\n        radii.push((w1 * w1 + w2 * w2).sqrt());\n    }\n\n    let mean_radius = radii.iter().sum::<f64>() / radii.len() as f64;\n    if mean_radius == 0.0 {\n        return Err(\"zero mean radius\".to_string());\n    }\n\n    let mut variance = 0.0;\n    for r in &radii {\n        let diff = r - mean_radius;\n        variance += diff * diff;\n    }\n    let std = (variance / radii.len() as f64).sqrt();\n    let radius_cv = std / mean_radius;\n    let score = 1.0 / (1.0 + radius_cv);\n\n    Ok((score, radius_cv, axis_ratio))\n}\n\n// Helper functions for verification - reserved for future integration\n#[allow(dead_code)]\nfn baseline_loss(\n    losses: &[(usize, f64)],\n    plateau_min_step: usize,\n    drop_window: usize,\n) -> Result<f64, String> {\n    let before: Vec<f64> = losses\n        .iter()\n        .filter(|(step, _)| *step <= plateau_min_step)\n        .map(|(_, loss)| *loss)\n        .collect();\n\n    if before.len() < drop_window || drop_window == 0 {\n        return Err(\"insufficient loss samples for baseline\".to_string());\n    }\n\n    let window = &before[before.len() - drop_window..];\n    Ok(window.iter().sum::<f64>() / drop_window as f64)\n}\n\n#[allow(dead_code)]\nfn find_drop_step(\n    losses: &[(usize, f64)],\n    plateau_min_step: usize,\n    baseline: f64,\n    drop_fraction: f64,\n) -> Result<usize, String> {\n    losses\n        .iter()\n        .filter(|(step, _)| *step >= plateau_min_step)\n        .find(|(_, loss)| *loss <= baseline * (1.0 - drop_fraction))\n        .map(|(step, _)| *step)\n        .ok_or_else(|| \"loss never drops below threshold\".to_string())\n}\n\nfn collect_mlp_post_relu_activations<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n    fixed_y: usize,\n) -> Result<Vec<Vec<f64>>, String> {\n    let modulus = ModularAdditionDataset::modulus();\n    let equals_token = ModularAdditionDataset::equals_token();\n\n    let mut inputs_vec = Vec::with_capacity(modulus * 3);\n    for x in 0..modulus {\n        inputs_vec.push(x as i32);\n        inputs_vec.push(fixed_y as i32);\n        inputs_vec.push(equals_token as i32);\n    }\n\n    let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n        .reshape([modulus, 3]);\n    let (_, mlp_acts) = model.forward_with_mlp_activations(inputs);\n    let [batch_size, d_ff] = mlp_acts.dims();\n\n    if batch_size != modulus {\n        return Err(format!(\n            \"unexpected activation batch size {} (expected {})\",\n            batch_size, modulus\n        ));\n    }\n\n    let data: Vec<f32> = mlp_acts.into_data().to_vec().unwrap();\n    let mut per_neuron = vec![Vec::with_capacity(batch_size); d_ff];\n    for batch in 0..batch_size {\n        for neuron in 0..d_ff {\n            let idx = batch * d_ff + neuron;\n            per_neuron[neuron].push(data[idx] as f64);\n        }\n    }\n\n    Ok(per_neuron)\n}\n\n#[allow(dead_code)]\nfn surface_autocorrelation_scores(\n    surface: &[Vec<f64>],\n    max_lag: usize,\n) -> Result<(f64, f64, f64, f64), String> {\n    let (height, width) = surface_dimensions(surface)?;\n    if height < 2 || width < 2 {\n        return Err(\"surface grid too small\".to_string());\n    }\n\n    let (mean, var) = surface_mean_variance(surface)?;\n    if var == 0.0 {\n        return Err(\"surface variance is zero\".to_string());\n    }\n\n    let max_lag = max_lag.min(height.saturating_sub(1)).min(width.saturating_sub(1));\n    if max_lag == 0 {\n        return Err(\"max_lag too small for surface\".to_string());\n    }\n\n    let diag_main = average_autocorrelation(surface, mean, var, max_lag, 1, 1)?;\n    let diag_anti = average_autocorrelation(surface, mean, var, max_lag, 1, -1)?;\n    let axis_x = average_autocorrelation(surface, mean, var, max_lag, 1, 0)?;\n    let axis_y = average_autocorrelation(surface, mean, var, max_lag, 0, 1)?;\n\n    Ok((diag_main, diag_anti, axis_x, axis_y))\n}\n\nfn surface_dimensions(surface: &[Vec<f64>]) -> Result<(usize, usize), String> {\n    let height = surface.len();\n    if height == 0 {\n        return Err(\"surface is empty\".to_string());\n    }\n    let width = surface[0].len();\n    if width == 0 {\n        return Err(\"surface has empty rows\".to_string());\n    }\n    for (idx, row) in surface.iter().enumerate() {\n        if row.len() != width {\n            return Err(format!(\n                \"surface row {} has length {}, expected {}\",\n                idx,\n                row.len(),\n                width\n            ));\n        }\n    }\n    Ok((height, width))\n}\n\nfn surface_mean_variance(surface: &[Vec<f64>]) -> Result<(f64, f64), String> {\n    let (height, width) = surface_dimensions(surface)?;\n    let mut sum = 0.0;\n    for row in surface {\n        for value in row {\n            sum += *value;\n        }\n    }\n    let count = (height * width) as f64;\n    let mean = sum / count;\n    let mut variance = 0.0;\n    for row in surface {\n        for value in row {\n            let diff = *value - mean;\n            variance += diff * diff;\n        }\n    }\n    variance /= count;\n    Ok((mean, variance))\n}\n\nfn average_autocorrelation(\n    surface: &[Vec<f64>],\n    mean: f64,\n    variance: f64,\n    max_lag: usize,\n    dx: i32,\n    dy: i32,\n) -> Result<f64, String> {\n    let (height, width) = surface_dimensions(surface)?;\n    let mut total = 0.0;\n    let mut lag_count = 0usize;\n\n    for lag in 1..=max_lag {\n        let mut sum = 0.0;\n        let mut count = 0usize;\n        let lag = lag as i32;\n        for x in 0..height {\n            for y in 0..width {\n                let x2 = x as i32 + dx * lag;\n                let y2 = y as i32 + dy * lag;\n                if x2 < 0 || y2 < 0 {\n                    continue;\n                }\n                let x2 = x2 as usize;\n                let y2 = y2 as usize;\n                if x2 >= height || y2 >= width {\n                    continue;\n                }\n                let a = surface[x][y] - mean;\n                let b = surface[x2][y2] - mean;\n                sum += a * b;\n                count += 1;\n            }\n        }\n        if count == 0 {\n            continue;\n        }\n        let corr = sum / (count as f64 * variance);\n        total += corr.abs();\n        lag_count += 1;\n    }\n\n    if lag_count == 0 {\n        return Err(\"no valid lags for autocorrelation\".to_string());\n    }\n\n    Ok(total / lag_count as f64)\n}\n\nfn analyze_mlp_activation_wave(\n    neuron_index: usize,\n    signal: &[f64],\n    config: &MlpWaveVerificationConfig,\n) -> Result<MlpWaveNeuronReport, String> {\n    if signal.len() < 3 {\n        return Err(format!(\n            \"activation signal for neuron {} is too short\",\n            neuron_index\n        ));\n    }\n\n    let negative_count = signal\n        .iter()\n        .filter(|value| **value < -config.negative_tolerance)\n        .count();\n    let negative_fraction = negative_count as f64 / signal.len() as f64;\n    if negative_fraction > config.max_negative_fraction {\n        return Err(format!(\n            \"neuron {} has {:.3} negative activations\",\n            neuron_index, negative_fraction\n        ));\n    }\n\n    let (dominant_frequency, dominant_ratio) = dominant_frequency_ratio(signal)?;\n    if dominant_ratio < config.min_dominant_ratio {\n        return Err(format!(\n            \"neuron {} dominant frequency ratio {:.3} below threshold {:.3}\",\n            neuron_index, dominant_ratio, config.min_dominant_ratio\n        ));\n    }\n\n    let best_correlation =\n        best_rectified_sine_correlation(signal, dominant_frequency, config.phase_steps);\n    if best_correlation < config.min_correlation {\n        return Err(format!(\n            \"neuron {} rectified sine correlation {:.3} below threshold {:.3}\",\n            neuron_index, best_correlation, config.min_correlation\n        ));\n    }\n\n    Ok(MlpWaveNeuronReport {\n        neuron_index,\n        dominant_frequency,\n        dominant_ratio,\n        negative_fraction,\n        best_correlation,\n    })\n}\n\nfn dominant_frequency_ratio(signal: &[f64]) -> Result<(usize, f64), String> {\n    if signal.len() < 2 {\n        return Err(\"signal too short for FFT\".to_string());\n    }\n\n    let mut planner = FftPlanner::new();\n    let fft = planner.plan_fft_forward(signal.len());\n    let mut buffer: Vec<Complex<f64>> = signal\n        .iter()\n        .map(|&x| Complex::new(x, 0.0))\n        .collect();\n    fft.process(&mut buffer);\n    let magnitudes: Vec<f64> = buffer.iter().map(|c| c.norm()).collect();\n\n    if magnitudes.len() < 2 {\n        return Err(\"insufficient FFT bins\".to_string());\n    }\n\n    let mut dominant_idx = 1usize;\n    let mut dominant_mag = magnitudes[1];\n    for (idx, &mag) in magnitudes.iter().enumerate().skip(2) {\n        if mag > dominant_mag {\n            dominant_mag = mag;\n            dominant_idx = idx;\n        }\n    }\n\n    if dominant_idx == 0 {\n        return Err(\"dominant frequency is zero\".to_string());\n    }\n\n    let mut nonzero = magnitudes[1..].to_vec();\n    nonzero.sort_by(|a, b| a.partial_cmp(b).unwrap_or(std::cmp::Ordering::Equal));\n    let median_mag = nonzero[nonzero.len() / 2];\n    let ratio = if median_mag == 0.0 {\n        dominant_mag\n    } else {\n        dominant_mag / median_mag\n    };\n\n    Ok((dominant_idx, ratio))\n}\n\nfn best_rectified_sine_correlation(signal: &[f64], frequency: usize, phase_steps: usize) -> f64 {\n    if signal.is_empty() || frequency == 0 || phase_steps == 0 {\n        return 0.0;\n    }\n\n    let tau = std::f64::consts::PI * 2.0;\n    let mut best = -1.0;\n    for step in 0..phase_steps {\n        let phase = tau * step as f64 / phase_steps as f64;\n        let wave: Vec<f64> = (0..signal.len())\n            .map(|idx| {\n                let angle = tau * frequency as f64 * idx as f64 / signal.len() as f64 + phase;\n                angle.sin().max(0.0)\n            })\n            .collect();\n        let corr = pearson_correlation(signal, &wave);\n        if corr > best {\n            best = corr;\n        }\n    }\n\n    best\n}\n\nfn pearson_correlation(a: &[f64], b: &[f64]) -> f64 {\n    if a.len() != b.len() || a.is_empty() {\n        return 0.0;\n    }\n\n    let mean_a = a.iter().sum::<f64>() / a.len() as f64;\n    let mean_b = b.iter().sum::<f64>() / b.len() as f64;\n\n    let mut num = 0.0;\n    let mut denom_a = 0.0;\n    let mut denom_b = 0.0;\n    for (x, y) in a.iter().zip(b.iter()) {\n        let da = x - mean_a;\n        let db = y - mean_b;\n        num += da * db;\n        denom_a += da * da;\n        denom_b += db * db;\n    }\n\n    if denom_a == 0.0 || denom_b == 0.0 {\n        return 0.0;\n    }\n\n    num / (denom_a.sqrt() * denom_b.sqrt())\n}\n\n/// Verify that the model actually learned modular addition\n/// by testing it on ALL possible examples systematically\npub fn verify_full_accuracy<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n) -> (f32, f32) {\n    println!(\"ðŸ” Verifying model on COMPLETE dataset (no sampling)...\");\n\n    let train_dataset = ModularAdditionDataset::new(true, 42);\n    let val_dataset = ModularAdditionDataset::new(false, 42);\n\n    let train_acc = test_all_examples(model, &train_dataset, device);\n    let val_acc = test_all_examples(model, &val_dataset, device);\n\n    println!(\"ðŸ“Š FULL Dataset Results:\");\n    println!(\"   Training: {}/{} = {:.2}%\",\n        (train_acc * train_dataset.len() as f32) as usize,\n        train_dataset.len(),\n        train_acc * 100.0);\n    println!(\"   Validation: {}/{} = {:.2}%\",\n        (val_acc * val_dataset.len() as f32) as usize,\n        val_dataset.len(),\n        val_acc * 100.0);\n\n    (train_acc, val_acc)\n}\n\n/// Test model on ALL examples in dataset (no random sampling)\nfn test_all_examples<B: Backend>(\n    model: &Transformer<B>,\n    dataset: &ModularAdditionDataset,\n    device: &B::Device,\n) -> f32 {\n    let mut total_correct = 0;\n    let total = dataset.len();\n\n    // Test in batches to avoid memory issues\n    let batch_size = 100;\n    for batch_start in (0..total).step_by(batch_size) {\n        let batch_end = (batch_start + batch_size).min(total);\n        let batch_len = batch_end - batch_start;\n\n        let mut inputs_vec = Vec::new();\n        let mut targets_vec = Vec::new();\n\n        for idx in batch_start..batch_end {\n            let (input, target) = dataset.get(idx).unwrap();\n            inputs_vec.extend(input);\n            targets_vec.push(target as i32);\n        }\n\n        let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n            .reshape([batch_len, 3]);\n\n        let logits = model.forward(inputs);\n        let predictions = logits.argmax(1).squeeze::<1>();\n        let predictions_vec: Vec<i32> = predictions.into_data().to_vec().unwrap();\n\n        let correct = predictions_vec\n            .iter()\n            .zip(targets_vec.iter())\n            .filter(|(p, t)| p == t)\n            .count();\n\n        total_correct += correct;\n    }\n\n    total_correct as f32 / total as f32\n}\n\n/// Verify that weight norms decrease over time with weight decay\n/// and show correlation with the grokking transition\npub fn verify_weight_norm_decay(\n    weight_norm_history: &[(usize, f64)],\n    grok_step: Option<usize>,\n    config: &WeightNormDecayConfig,\n) -> Result<WeightNormDecayReport, String> {\n    if weight_norm_history.len() < 2 {\n        return Err(\"weight norm history requires at least 2 snapshots\".to_string());\n    }\n\n    let initial_norm = weight_norm_history\n        .first()\n        .ok_or_else(|| \"weight norm history is empty\".to_string())?\n        .1;\n    let final_norm = weight_norm_history\n        .last()\n        .ok_or_else(|| \"weight norm history is empty\".to_string())?\n        .1;\n\n    if initial_norm <= 0.0 {\n        return Err(\"initial weight norm must be positive\".to_string());\n    }\n\n    // Check that norms are generally decreasing (with weight decay)\n    let relative_decrease = (initial_norm - final_norm) / initial_norm;\n    if relative_decrease < config.min_relative_decrease {\n        return Err(format!(\n            \"weight norm decrease {:.4} below threshold {:.4} (initial {:.4}, final {:.4})\",\n            relative_decrease, config.min_relative_decrease, initial_norm, final_norm\n        ));\n    }\n\n    // Check that norms are not increasing (weight decay should monotonically decrease norms)\n    for window in weight_norm_history.windows(2) {\n        let prev_norm = window[0].1;\n        let next_norm = window[1].1;\n        if next_norm > prev_norm * 1.01 {\n            // Allow 1% tolerance for numerical noise\n            return Err(format!(\n                \"weight norm increased from {:.4} to {:.4} at step {}\",\n                prev_norm, next_norm, window[1].0\n            ));\n        }\n    }\n\n    // If grok step is provided, check that norms show a shift around grokking\n    let grok_step_norm = if let Some(step) = grok_step {\n        weight_norm_history\n            .iter()\n            .filter(|(s, _)| {\n                *s >= step.saturating_sub(config.grok_step_window)\n                    && *s <= step + config.grok_step_window\n            })\n            .map(|(_, norm)| *norm)\n            .next()\n    } else {\n        None\n    };\n\n    Ok(WeightNormDecayReport {\n        initial_norm,\n        final_norm,\n        relative_decrease,\n        grok_step_norm,\n    })\n}\n\n/// Verify the characteristic \"snake curve\" shape of grokking phenomenon\n/// on log-scale plots: train accuracy rises early, validation stays flat then jumps\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_snake_curve_shape(\n    train_acc: &[(usize, f32)],\n    val_acc: &[(usize, f32)],\n    config: &SnakeCurveConfig,\n) -> Result<SnakeCurveReport, String> {\n    if train_acc.is_empty() || val_acc.is_empty() {\n        return Err(\"empty accuracy history\".to_string());\n    }\n\n    // Phase 1: Verify early train convergence (immediate drop on log scale)\n    let train_converged_step = train_acc\n        .iter()\n        .find(|(_, acc)| *acc >= config.train_convergence_threshold)\n        .map(|(step, _)| *step)\n        .ok_or_else(|| {\n            format!(\n                \"train accuracy never reaches {} threshold\",\n                config.train_convergence_threshold\n            )\n        })?;\n\n    if train_converged_step > config.early_step_max {\n        return Err(format!(\n            \"train accuracy converges at step {}, expected before {}\",\n            train_converged_step, config.early_step_max\n        ));\n    }\n\n    // Phase 2: Verify validation plateau (flat on log scale)\n    let plateau_samples: Vec<f32> = val_acc\n        .iter()\n        .filter(|(step, _)| *step >= 1 && *step <= config.val_plateau_min_step)\n        .map(|(_, acc)| *acc)\n        .collect();\n\n    if plateau_samples.is_empty() {\n        return Err(\"no validation samples in plateau region\".to_string());\n    }\n\n    let val_plateau_max = plateau_samples\n        .iter()\n        .fold(0.0f32, |max_val, &acc| max_val.max(acc));\n\n    if val_plateau_max > config.val_plateau_max_acc {\n        return Err(format!(\n            \"validation accuracy in plateau region too high: max {:.4} exceeds {:.4}\",\n            val_plateau_max, config.val_plateau_max_acc\n        ));\n    }\n\n    // Phase 3: Verify delayed validation jump (crash up on log scale)\n    let val_grok_step = val_acc\n        .iter()\n        .filter(|(step, _)| *step > config.val_plateau_min_step)\n        .find(|(_, acc)| *acc >= config.val_final_threshold)\n        .map(|(step, _)| *step)\n        .ok_or_else(|| {\n            format!(\n                \"validation accuracy never reaches {} after plateau\",\n                config.val_final_threshold\n            )\n        })?;\n\n    // Phase 4: Verify phase ordering (snake shape: early train, late val)\n    let curve_shape_valid = train_converged_step < config.val_plateau_min_step\n        && val_grok_step > config.val_plateau_min_step\n        && train_converged_step < val_grok_step;\n\n    if !curve_shape_valid {\n        return Err(format!(\n            \"invalid curve shape: train converged at {}, val plateau until {}, val grokked at {}\",\n            train_converged_step, config.val_plateau_min_step, val_grok_step\n        ));\n    }\n\n    Ok(SnakeCurveReport {\n        train_converged_by_step: train_converged_step,\n        val_plateau_max_acc: val_plateau_max,\n        val_grok_step,\n        curve_shape_valid,\n    })\n}\n\n/// Test specific examples to see if model actually computes mod p\npub fn test_specific_examples<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n) {\n    println!();\n    println!(\"ðŸ§ª Testing specific examples:\");\n\n    let modulus = ModularAdditionDataset::modulus();\n    let equals_token = ModularAdditionDataset::equals_token();\n    let test_cases = vec![\n        (0, 0, 0),\n        (1, 1, 2),\n        (50, 50, (50 + 50) % modulus),\n        (112, 1, (112 + 1) % modulus),\n        (56, 57, (56 + 57) % modulus),\n        (112, 112, (112 + 112) % modulus),\n        (10, 20, 30),\n        (60, 60, (60 + 60) % modulus),\n    ];\n\n    for (a, b, expected) in test_cases {\n        let input_vec = vec![a as i32, b as i32, equals_token as i32];\n        let input = Tensor::<B, 1, Int>::from_ints(input_vec.as_slice(), device)\n            .reshape([1, 3]);\n\n        let logits = model.forward(input);\n        let prediction = logits.argmax(1).squeeze::<1>();\n        let pred_value: i32 = prediction.into_data().to_vec().unwrap()[0];\n\n        let correct = if pred_value == expected as i32 { \"âœ“\" } else { \"âœ—\" };\n        println!(\n            \"   {} + {} mod {} = {} | Model: {} {}\",\n            a, b, modulus, expected, pred_value, correct\n        );\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    fn make_histories(\n        train_acc: &[(usize, f32)],\n        val_acc: &[(usize, f32)],\n        val_loss: &[(usize, f64)],\n    ) -> (LossHistory, AccuracyHistory) {\n        let mut loss_history = LossHistory::new();\n        loss_history.val_snapshots = val_loss.to_vec();\n        let mut accuracy_history = AccuracyHistory::new();\n        accuracy_history.train_snapshots = train_acc.to_vec();\n        accuracy_history.val_snapshots = val_acc.to_vec();\n        (loss_history, accuracy_history)\n    }\n\n    #[test]\n    fn grokking_phase_verification_passes_on_synthetic_signal() {\n        let train_acc = vec![(0, 0.2), (500, 0.995), (1000, 0.999)];\n        let val_acc = vec![\n            (0, 0.01),\n            (500, 0.01),\n            (1500, 0.01),\n            (2200, 0.995),\n            (2500, 0.999),\n        ];\n        let val_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 2.0),\n            (2200, 0.2),\n            (2500, 0.15),\n        ];\n\n        let (loss_history, accuracy_history) = make_histories(&train_acc, &val_acc, &val_loss);\n        let config = GrokkingVerificationConfig {\n            early_step_max: 1000,\n            plateau_min_step: 1500,\n            generalization_window: 1500,\n            accuracy_window: 2,\n            loss_window: 2,\n            ..GrokkingVerificationConfig::default_for_modulus(ModularAdditionDataset::modulus())\n        };\n\n        let report = verify_grokking_phase_transition(&loss_history, &accuracy_history, &config)\n            .expect(\"expected grokking verification to pass\");\n        assert!(report.transition_step >= config.plateau_min_step);\n        assert!(report.early_train_step <= config.early_step_max);\n    }\n\n    #[test]\n    fn grokking_phase_verification_fails_without_transition() {\n        let train_acc = vec![(0, 0.2), (500, 0.995)];\n        let val_acc = vec![(0, 0.01), (500, 0.01), (1500, 0.02)];\n        let val_loss = vec![(0, 2.0), (500, 2.0), (1500, 1.9)];\n\n        let (loss_history, accuracy_history) = make_histories(&train_acc, &val_acc, &val_loss);\n        let config = GrokkingVerificationConfig {\n            early_step_max: 1000,\n            plateau_min_step: 1000,\n            generalization_window: 1000,\n            accuracy_window: 2,\n            loss_window: 2,\n            ..GrokkingVerificationConfig::default_for_modulus(ModularAdditionDataset::modulus())\n        };\n\n        let err = verify_grokking_phase_transition(&loss_history, &accuracy_history, &config)\n            .expect_err(\"expected grokking verification to fail\");\n        assert!(err.contains(\"validation accuracy never reaches target\"));\n    }\n\n    #[test]\n    fn grokking_phase_verification_fails_with_slow_accuracy_rise() {\n        let train_acc = vec![(0, 0.2), (500, 0.995), (1000, 0.999)];\n        let val_acc = vec![\n            (0, 0.01),\n            (500, 0.01),\n            (1500, 0.5),\n            (2000, 0.6),\n            (2200, 0.995),\n            (2500, 0.999),\n        ];\n        let val_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 2.0),\n            (2200, 0.2),\n            (2500, 0.15),\n        ];\n\n        let (loss_history, accuracy_history) = make_histories(&train_acc, &val_acc, &val_loss);\n        let config = GrokkingVerificationConfig {\n            early_step_max: 1000,\n            plateau_min_step: 1000,\n            generalization_window: 2000,\n            accuracy_window: 2,\n            loss_window: 2,\n            ..GrokkingVerificationConfig::default_for_modulus(ModularAdditionDataset::modulus())\n        };\n\n        let err = verify_grokking_phase_transition(&loss_history, &accuracy_history, &config)\n            .expect_err(\"expected grokking verification to fail\");\n        assert!(err.contains(\"plateau too high\"));\n    }\n\n    #[test]\n    fn restricted_loss_detection_passes_with_early_drop() {\n        let full_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 2.0),\n            (2500, 2.0),\n            (3500, 0.3),\n        ];\n        let restricted_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 0.8),\n            (2500, 0.4),\n            (3500, 0.3),\n        ];\n\n        let config = RestrictedLossVerificationConfig {\n            plateau_min_step: 1000,\n            drop_window: 2,\n            drop_fraction: 0.5,\n            min_step_lead: 500,\n        };\n\n        let report = verify_restricted_loss_early_drop(&full_loss, &restricted_loss, &config)\n            .expect(\"expected restricted loss verification to pass\");\n        assert!(report.restricted_drop_step < report.full_drop_step);\n    }\n\n    #[test]\n    fn restricted_loss_detection_fails_when_full_drops_first() {\n        let full_loss = vec![(0, 2.0), (1000, 1.0), (1500, 0.8)];\n        let restricted_loss = vec![(0, 2.0), (1000, 2.0), (1500, 0.8)];\n\n        let config = RestrictedLossVerificationConfig {\n            plateau_min_step: 500,\n            drop_window: 1,\n            drop_fraction: 0.3,\n            min_step_lead: 200,\n        };\n\n        let err = verify_restricted_loss_early_drop(&full_loss, &restricted_loss, &config)\n            .expect_err(\"expected restricted loss verification to fail\");\n        assert!(err.contains(\"restricted loss drops\"));\n    }\n\n    #[test]\n    fn excluded_loss_spike_verification_passes() {\n        let excluded_loss = vec![(0, 1.0), (1500, 2.2), (3000, 1.1)];\n        let config = ExcludedLossSpikeConfig {\n            min_relative_increase: 0.5,\n        };\n\n        let report = verify_excluded_loss_spike(&excluded_loss, &config)\n            .expect(\"expected excluded loss spike verification to pass\");\n        assert_eq!(report.spike_step, 1500);\n    }\n\n    #[test]\n    fn excluded_loss_spike_verification_fails_without_spike() {\n        let excluded_loss = vec![(0, 1.0), (1500, 1.1), (3000, 1.0)];\n        let config = ExcludedLossSpikeConfig {\n            min_relative_increase: 0.5,\n        };\n\n        let err = verify_excluded_loss_spike(&excluded_loss, &config)\n            .expect_err(\"expected excluded loss spike verification to fail\");\n        assert!(err.contains(\"excluded loss spike too small\"));\n    }\n\n    #[test]\n    fn restricted_loss_drop_verification_passes() {\n        let restricted_loss = vec![\n            (0, 2.0),\n            (1000, 1.8),\n            (5000, 1.2),\n            (7000, 0.8),\n            (10000, 0.9),\n        ];\n        let config = RestrictedLossDropConfig {\n            min_relative_decrease: 0.3,\n        };\n\n        let report = verify_restricted_loss_drop(&restricted_loss, &config)\n            .expect(\"expected restricted loss drop verification to pass\");\n        assert_eq!(report.drop_step, 7000);\n        assert!(report.baseline_loss > report.drop_loss);\n        assert!((report.baseline_loss - report.drop_loss) / report.baseline_loss >= 0.3);\n    }\n\n    #[test]\n    fn restricted_loss_drop_verification_fails_without_drop() {\n        let restricted_loss = vec![\n            (0, 2.0),\n            (1000, 2.1),\n            (5000, 1.9),\n            (7000, 2.0),\n            (10000, 2.05),\n        ];\n        let config = RestrictedLossDropConfig {\n            min_relative_decrease: 0.3,\n        };\n\n        let err = verify_restricted_loss_drop(&restricted_loss, &config)\n            .expect_err(\"expected restricted loss drop verification to fail\");\n        assert!(err.contains(\"restricted loss decrease too small\"));\n    }\n\n    #[test]\n    fn mlp_activation_wave_verification_passes_on_rectified_sine() {\n        let n = ModularAdditionDataset::modulus();\n        let freq = 5.0;\n        let mut signal = Vec::with_capacity(n);\n        for idx in 0..n {\n            let angle = 2.0 * std::f64::consts::PI * freq * idx as f64 / n as f64;\n            let rectified = angle.sin().max(0.0);\n            let wobble = (2.0 * std::f64::consts::PI * 2.0 * idx as f64 / n as f64)\n                .sin()\n                .abs()\n                * 0.02;\n            signal.push(rectified + wobble);\n        }\n\n        let config = MlpWaveVerificationConfig {\n            max_negative_fraction: 0.0,\n            negative_tolerance: 1e-6,\n            min_dominant_ratio: 3.0,\n            min_correlation: 0.7,\n            phase_steps: 32,\n        };\n\n        let report =\n            verify_mlp_activation_waves_from_signals(&[(0, signal)], &config)\n                .expect(\"expected rectified sine to pass\");\n        assert_eq!(report.neuron_reports.len(), 1);\n    }\n\n    #[test]\n    fn mlp_activation_wave_verification_fails_on_noise() {\n        let n = ModularAdditionDataset::modulus();\n        let signal: Vec<f64> = (0..n).map(|idx| idx as f64 / n as f64).collect();\n\n        let config = MlpWaveVerificationConfig {\n            max_negative_fraction: 0.0,\n            negative_tolerance: 1e-6,\n            min_dominant_ratio: 4.0,\n            min_correlation: 0.8,\n            phase_steps: 16,\n        };\n\n        let err =\n            verify_mlp_activation_waves_from_signals(&[(0, signal)], &config)\n                .expect_err(\"expected linear signal to fail\");\n        assert!(err.contains(\"dominant frequency ratio\") || err.contains(\"correlation\"));\n    }\n\n    #[test]\n    fn pairwise_manifold_transition_passes_on_elliptic_signals() {\n        let n = ModularAdditionDataset::modulus();\n        let freq = 5.0;\n        let mut post_signals = Vec::new();\n        for neuron_idx in 0..7usize {\n            let phase = 2.0 * std::f64::consts::PI * neuron_idx as f64 / 7.0;\n            let signal: Vec<f64> = (0..n)\n                .map(|idx| {\n                    let angle = 2.0 * std::f64::consts::PI * freq * idx as f64 / n as f64 + phase;\n                    angle.sin()\n                })\n                .collect();\n            post_signals.push((neuron_idx, signal));\n        }\n\n        let mut pre_signals = Vec::new();\n        for neuron_idx in 0..7usize {\n            let signal: Vec<f64> = (0..n)\n                .map(|idx| idx as f64 / n as f64 + neuron_idx as f64 * 0.01)\n                .collect();\n            pre_signals.push((neuron_idx, signal));\n        }\n\n        let config = PairwiseManifoldConfig {\n            fixed_y: 0,\n            neuron_indices: (0..7).collect(),\n            min_post_score: 0.75,\n            max_pre_score: 0.55,\n            max_axis_ratio: 8.0,\n            min_pairs_passing: 8,\n        };\n\n        let report = verify_pairwise_manifold_transition_from_signals(\n            &pre_signals,\n            &post_signals,\n            &config,\n        )\n        .expect(\"expected pairwise manifold transition to pass\");\n        assert!(report.passing_pairs >= config.min_pairs_passing);\n    }\n\n    #[test]\n    fn pairwise_manifold_transition_fails_without_rings() {\n        let n = ModularAdditionDataset::modulus();\n        let mut signals = Vec::new();\n        for neuron_idx in 0..7usize {\n            let signal: Vec<f64> = (0..n)\n                .map(|idx| idx as f64 / n as f64 + neuron_idx as f64 * 0.02)\n                .collect();\n            signals.push((neuron_idx, signal));\n        }\n\n        let config = PairwiseManifoldConfig {\n            fixed_y: 0,\n            neuron_indices: (0..7).collect(),\n            min_post_score: 0.8,\n            max_pre_score: 0.55,\n            max_axis_ratio: 4.0,\n            min_pairs_passing: 5,\n        };\n\n        let err = verify_pairwise_manifold_transition_from_signals(\n            &signals,\n            &signals,\n            &config,\n        )\n        .expect_err(\"expected pairwise manifold transition to fail\");\n        assert!(err.contains(\"post-grok\") || err.contains(\"pre-grok\"));\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_passes_on_diagonal_corrugation() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create diagonal ridges: f(x,y) = sin(2Ï€ * freq * (x+y) / n)\n        let freq = 3.0;\n        for x in 0..n {\n            for y in 0..n {\n                let angle = 2.0 * std::f64::consts::PI * freq * (x + y) as f64 / n as f64;\n                surface[x][y] = angle.sin();\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let report = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect(\"expected diagonal ridge verification to pass\");\n        assert!(report.diagonal_score > report.axis_score);\n        assert!(report.ratio >= config.min_diagonal_ratio);\n        assert!(report.diagonal_score >= config.min_diagonal_score);\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_passes_on_anti_diagonal_pattern() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create anti-diagonal ridges: f(x,y) = sin(2Ï€ * freq * (x-y) / n)\n        let freq = 3.0;\n        for x in 0..n {\n            for y in 0..n {\n                let angle = 2.0 * std::f64::consts::PI * freq * (x as i32 - y as i32) as f64 / n as f64;\n                surface[x][y] = angle.sin();\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let report = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect(\"expected anti-diagonal ridge verification to pass\");\n        assert!(report.diagonal_score > report.axis_score);\n        assert!(report.ratio >= config.min_diagonal_ratio);\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_fails_on_axis_aligned_pattern() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create axis-aligned ridges: f(x,y) = sin(2Ï€ * freq * x / n)\n        let freq = 3.0;\n        for x in 0..n {\n            for y in 0..n {\n                let angle = 2.0 * std::f64::consts::PI * freq * x as f64 / n as f64;\n                surface[x][y] = angle.sin();\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let err = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect_err(\"expected axis-aligned pattern to fail diagonal verification\");\n        assert!(err.contains(\"diagonal/axis ratio\") || err.contains(\"diagonal autocorrelation\"));\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_fails_on_flat_surface() {\n        let n = 50;\n        let surface = vec![vec![1.5; n]; n];\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let err = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect_err(\"expected flat surface to fail\");\n        assert!(err.contains(\"surface variance is zero\"));\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_fails_on_random_noise() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create random noise (low autocorrelation everywhere)\n        for x in 0..n {\n            for y in 0..n {\n                surface[x][y] = ((x * 17 + y * 31) % 100) as f64 / 100.0;\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let err = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect_err(\"expected random noise to fail diagonal verification\");\n        assert!(err.contains(\"diagonal/axis ratio\") || err.contains(\"diagonal autocorrelation\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_passes_on_valid_grokking_curve() {\n        // Simulate typical grokking: train converges early, val plateaus then jumps\n        let train_acc = vec![\n            (1, 0.10),\n            (10, 0.50),\n            (100, 0.80),\n            (500, 0.98),\n            (1000, 0.99),\n            (2000, 0.995),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (10, 0.009),\n            (100, 0.01),\n            (500, 0.011),\n            (1000, 0.009),\n            (1500, 0.01),\n            (2000, 0.92),\n            (3000, 0.98),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 1500,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let report = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect(\"expected snake curve verification to pass\");\n\n        assert_eq!(report.train_converged_by_step, 500);\n        assert!(report.val_plateau_max_acc < config.val_plateau_max_acc);\n        assert_eq!(report.val_grok_step, 2000);\n        assert!(report.curve_shape_valid);\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_on_simultaneous_convergence() {\n        // Both train and val converge at the same time (no grokking)\n        let train_acc = vec![\n            (1, 0.10),\n            (100, 0.98),\n            (500, 0.99),\n        ];\n        let val_acc = vec![\n            (1, 0.10),\n            (100, 0.97),\n            (500, 0.98),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 200,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let err = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect_err(\"expected snake curve verification to fail\");\n        assert!(err.contains(\"plateau region too high\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_on_slow_train_convergence() {\n        // Train takes too long to converge (not the typical snake curve)\n        let train_acc = vec![\n            (1, 0.10),\n            (500, 0.50),\n            (1000, 0.70),\n            (2000, 0.98),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (500, 0.01),\n            (1000, 0.01),\n            (2000, 0.92),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 1500,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let err = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect_err(\"expected snake curve verification to fail\");\n        assert!(err.contains(\"converges at step 2000\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_without_val_jump() {\n        // Validation never jumps up (no generalization)\n        let train_acc = vec![\n            (1, 0.10),\n            (100, 0.98),\n            (500, 0.99),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (100, 0.01),\n            (500, 0.01),\n            (2000, 0.02),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 200,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let err = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect_err(\"expected snake curve verification to fail\");\n        assert!(err.contains(\"validation accuracy never reaches\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_on_wrong_phase_ordering() {\n        // Val jumps before train converges (backwards snake)\n        let train_acc = vec![\n            (1, 0.10),\n            (500, 0.50),\n            (2000, 0.98),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (100, 0.95),\n            (500, 0.98),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 200,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let result = verify_snake_curve_shape(&train_acc, &val_acc, &config);\n        // Should fail because val jumps before train converges (not a proper snake curve)\n        // This can fail for multiple reasons, but should definitely fail\n        assert!(result.is_err(), \"Expected snake curve verification to fail but it passed\");\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_passes_on_decreasing_norms() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 85.0),\n            (1000, 72.0),\n            (1500, 63.0),\n            (2000, 58.0),\n            (2500, 55.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let report = verify_weight_norm_decay(&weight_norms, Some(2000), &config)\n            .expect(\"expected weight norm decay verification to pass\");\n\n        assert_eq!(report.initial_norm, 100.0);\n        assert_eq!(report.final_norm, 55.0);\n        assert!((report.relative_decrease - 0.45).abs() < 0.01);\n        assert!(report.grok_step_norm.is_some());\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_fails_on_flat_norms() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 99.0),\n            (1000, 98.5),\n            (1500, 98.0),\n            (2000, 97.5),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let err = verify_weight_norm_decay(&weight_norms, None, &config)\n            .expect_err(\"expected weight norm decay verification to fail\");\n        assert!(err.contains(\"weight norm decrease\"));\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_fails_on_increasing_norms() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 95.0),\n            (1000, 105.0), // Increase here\n            (1500, 90.0),\n            (2000, 85.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let err = verify_weight_norm_decay(&weight_norms, None, &config)\n            .expect_err(\"expected weight norm decay verification to fail\");\n        assert!(err.contains(\"weight norm increased\"));\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_allows_small_noise() {\n        // Weight decay with small numerical noise (within 1% tolerance)\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 95.0),\n            (1000, 90.5),  // Small increase within tolerance\n            (1500, 90.0),\n            (2000, 85.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let report = verify_weight_norm_decay(&weight_norms, None, &config)\n            .expect(\"expected weight norm decay with small noise to pass\");\n\n        assert_eq!(report.initial_norm, 100.0);\n        assert_eq!(report.final_norm, 85.0);\n        assert!((report.relative_decrease - 0.15).abs() < 0.01);\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_finds_grok_step_norm() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (1000, 80.0),\n            (1800, 68.0),  // Within window of grok step (2200 - 500 = 1700)\n            (2000, 65.0),\n            (2200, 62.0),  // Around grok step\n            (3000, 55.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let report = verify_weight_norm_decay(&weight_norms, Some(2200), &config)\n            .expect(\"expected weight norm decay verification to pass\");\n\n        assert!(report.grok_step_norm.is_some());\n        let grok_norm = report.grok_step_norm.unwrap();\n        // Should find the first norm in the window [1700, 2700], which is 68.0 at step 1800\n        assert!((grok_norm - 68.0).abs() < 0.01);\n    }\n}\n","structuredPatch":[{"oldStart":978,"oldLines":6,"newStart":978,"newLines":7,"lines":["     Ok((diag_main, diag_anti, axis_x, axis_y))"," }"," ","+#[allow(dead_code)]"," fn surface_dimensions(surface: &[Vec<f64>]) -> Result<(usize, usize), String> {","     let height = surface.len();","     if height == 0 {"]}],"userModified":false,"replaceAll":false}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_014XJgvYjyHvxVxDafaRKrju","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01Xq12JuoWsAvWMXFbfKxdqw","name":"Edit","input":{"replace_all":false,"file_path":"/Users/admin/MechInt/src/verify.rs","old_string":"fn surface_mean_variance(surface: &[Vec<f64>]) -> Result<(f64, f64), String> {","new_string":"#[allow(dead_code)]\nfn surface_mean_variance(surface: &[Vec<f64>]) -> Result<(f64, f64), String> {"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":366,"cache_read_input_tokens":63051,"cache_creation":{"ephemeral_5m_input_tokens":366,"ephemeral_1h_input_tokens":0},"output_tokens":190,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"b4c3ebbf-11cc-4ff5-a74f-8c87413422e3"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01Xq12JuoWsAvWMXFbfKxdqw","type":"tool_result","content":"The file /Users/admin/MechInt/src/verify.rs has been updated. Here's the result of running `cat -n` on a snippet of the edited file:\n  1000â†’    }\n  1001â†’    Ok((height, width))\n  1002â†’}\n  1003â†’\n  1004â†’#[allow(dead_code)]\n  1005â†’fn surface_mean_variance(surface: &[Vec<f64>]) -> Result<(f64, f64), String> {\n  1006â†’    let (height, width) = surface_dimensions(surface)?;\n  1007â†’    let mut sum = 0.0;\n  1008â†’    for row in surface {\n  1009â†’        for value in row {"}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"d431e164-62c9-4473-94c7-d06ba4d0c2f8","tool_use_result":{"filePath":"/Users/admin/MechInt/src/verify.rs","oldString":"fn surface_mean_variance(surface: &[Vec<f64>]) -> Result<(f64, f64), String> {","newString":"#[allow(dead_code)]\nfn surface_mean_variance(surface: &[Vec<f64>]) -> Result<(f64, f64), String> {","originalFile":"use burn::{\n    data::dataset::Dataset,\n    tensor::{backend::Backend, Int, Tensor},\n};\nuse rustfft::{num_complex::Complex, FftPlanner};\n\nuse crate::analysis::{AccuracyHistory, LossHistory};\nuse crate::data::ModularAdditionDataset;\nuse crate::model::Transformer;\n\n#[derive(Debug, Clone)]\npub struct GrokkingVerificationConfig {\n    pub early_train_acc_threshold: f32,\n    pub early_step_max: usize,\n    pub plateau_min_step: usize,\n    pub generalization_window: usize,\n    pub chance_accuracy: f32,\n    pub chance_tolerance: f32,\n    pub target_val_acc_threshold: f32,\n    pub accuracy_window: usize,\n    pub loss_window: usize,\n    pub loss_drop_fraction: f64,\n}\n\nimpl GrokkingVerificationConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        Self {\n            early_train_acc_threshold: 0.99,\n            early_step_max: 1000,\n            plateau_min_step: 1000,\n            generalization_window: 10_000,\n            chance_accuracy: 1.0 / modulus as f32,\n            chance_tolerance: 0.02,\n            target_val_acc_threshold: 0.99,\n            accuracy_window: 5,\n            loss_window: 5,\n            loss_drop_fraction: 0.5,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct GrokkingPhaseReport {\n    pub early_train_step: usize,\n    pub transition_step: usize,\n    pub plateau_max_val_acc: f32,\n    pub loss_drop_ratio: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossVerificationConfig {\n    pub plateau_min_step: usize,\n    pub drop_window: usize,\n    pub drop_fraction: f64,\n    pub min_step_lead: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossReport {\n    pub restricted_drop_step: usize,\n    pub full_drop_step: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossDropConfig {\n    pub min_relative_decrease: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossDropReport {\n    pub drop_step: usize,\n    pub drop_loss: f64,\n    pub baseline_loss: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct ExcludedLossSpikeConfig {\n    pub min_relative_increase: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct ExcludedLossSpikeReport {\n    pub spike_step: usize,\n    pub spike_loss: f64,\n    pub baseline_loss: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct MlpWaveVerificationConfig {\n    pub max_negative_fraction: f64,\n    pub negative_tolerance: f64,\n    pub min_dominant_ratio: f64,\n    pub min_correlation: f64,\n    pub phase_steps: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct MlpWaveNeuronReport {\n    pub neuron_index: usize,\n    pub dominant_frequency: usize,\n    pub dominant_ratio: f64,\n    pub negative_fraction: f64,\n    pub best_correlation: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct MlpWaveReport {\n    pub neuron_reports: Vec<MlpWaveNeuronReport>,\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldConfig {\n    pub fixed_y: usize,\n    pub neuron_indices: Vec<usize>,\n    pub min_post_score: f64,\n    pub max_pre_score: f64,\n    pub max_axis_ratio: f64,\n    pub min_pairs_passing: usize,\n}\n\nimpl PairwiseManifoldConfig {\n    pub fn default_for_modulus(_modulus: usize) -> Self {\n        Self {\n            fixed_y: 0,\n            neuron_indices: (0..7).collect(),\n            min_post_score: 0.75,\n            max_pre_score: 0.55,\n            max_axis_ratio: 8.0,\n            min_pairs_passing: 10,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldPairReport {\n    pub neuron_a: usize,\n    pub neuron_b: usize,\n    pub circularity_score: f64,\n    pub radius_cv: f64,\n    pub axis_ratio: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldReport {\n    pub pair_reports: Vec<PairwiseManifoldPairReport>,\n    pub average_score: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldTransitionReport {\n    pub pre_average_score: f64,\n    pub post_average_score: f64,\n    pub passing_pairs: usize,\n    pub total_pairs: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct DiagonalRidgeConfig {\n    pub neuron_index: usize,\n    pub max_lag: usize,\n    pub min_diagonal_ratio: f64,\n    pub min_diagonal_score: f64,\n}\n\nimpl DiagonalRidgeConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        let max_lag = (modulus / 10).max(4).min(20);\n        Self {\n            neuron_index: 10,\n            max_lag,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct DiagonalRidgeReport {\n    pub neuron_index: usize,\n    pub diagonal_score: f64,\n    pub axis_score: f64,\n    pub ratio: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct SnakeCurveConfig {\n    pub early_step_max: usize,\n    pub train_convergence_threshold: f32,\n    pub val_plateau_min_step: usize,\n    pub val_plateau_max_acc: f32,\n    pub val_final_threshold: f32,\n}\n\nimpl SnakeCurveConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        Self {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 1000,\n            val_plateau_max_acc: 1.0 / modulus as f32 + 0.05,\n            val_final_threshold: 0.90,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct WeightNormDecayConfig {\n    pub min_relative_decrease: f64,\n    pub plateau_min_step: usize,\n    pub grok_step_window: usize,\n}\n\nimpl WeightNormDecayConfig {\n    pub fn default_for_grokking() -> Self {\n        Self {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct WeightNormDecayReport {\n    pub initial_norm: f64,\n    pub final_norm: f64,\n    pub relative_decrease: f64,\n    pub grok_step_norm: Option<f64>,\n}\n\n#[derive(Debug, Clone)]\npub struct SnakeCurveReport {\n    pub train_converged_by_step: usize,\n    pub val_plateau_max_acc: f32,\n    pub val_grok_step: usize,\n    pub curve_shape_valid: bool,\n}\n\npub fn verify_grokking_phase_transition(\n    loss_history: &LossHistory,\n    accuracy_history: &AccuracyHistory,\n    config: &GrokkingVerificationConfig,\n) -> Result<GrokkingPhaseReport, String> {\n    let (early_train_step, _) = accuracy_history\n        .train_snapshots\n        .iter()\n        .find(|(_, acc)| *acc >= config.early_train_acc_threshold)\n        .ok_or_else(|| \"train accuracy never reaches threshold\".to_string())?;\n\n    if *early_train_step > config.early_step_max {\n        return Err(format!(\n            \"train accuracy reaches {:.2} after step {}, expected before {}\",\n            config.early_train_acc_threshold, early_train_step, config.early_step_max\n        ));\n    }\n\n    let plateau_max_val_acc = accuracy_history\n        .val_snapshots\n        .iter()\n        .filter(|(step, _)| *step <= config.plateau_min_step)\n        .map(|(_, acc)| *acc)\n        .fold(None::<f32>, |accum, value| {\n            Some(accum.map_or(value, |max_val| max_val.max(value)))\n        })\n        .ok_or_else(|| \"no validation accuracy samples before plateau_min_step\".to_string())?;\n\n    if plateau_max_val_acc > config.chance_accuracy + config.chance_tolerance {\n        return Err(format!(\n            \"validation accuracy before step {} exceeds chance (max {:.4})\",\n            config.plateau_min_step, plateau_max_val_acc\n        ));\n    }\n\n    let (transition_step, _) = accuracy_history\n        .val_snapshots\n        .iter()\n        .find(|(step, acc)| {\n            *step >= config.plateau_min_step && *acc >= config.target_val_acc_threshold\n        })\n        .ok_or_else(|| \"validation accuracy never reaches target threshold\".to_string())?;\n\n    let latest_allowed = config.plateau_min_step + config.generalization_window;\n    if *transition_step > latest_allowed {\n        return Err(format!(\n            \"validation accuracy reaches {:.2} at step {}, expected before {}\",\n            config.target_val_acc_threshold, transition_step, latest_allowed\n        ));\n    }\n\n    verify_accuracy_jump(\n        &accuracy_history.val_snapshots,\n        *transition_step,\n        config.accuracy_window,\n        config.chance_accuracy,\n        config.chance_tolerance,\n        config.target_val_acc_threshold,\n    )?;\n\n    let loss_drop_ratio = verify_loss_drop(\n        &loss_history.val_snapshots,\n        *transition_step,\n        config.loss_window,\n        config.loss_drop_fraction,\n    )?;\n\n    Ok(GrokkingPhaseReport {\n        early_train_step: *early_train_step,\n        transition_step: *transition_step,\n        plateau_max_val_acc,\n        loss_drop_ratio,\n    })\n}\n\nfn verify_accuracy_jump(\n    val_accuracy: &[(usize, f32)],\n    transition_step: usize,\n    window: usize,\n    chance_accuracy: f32,\n    chance_tolerance: f32,\n    target_val_acc_threshold: f32,\n) -> Result<(), String> {\n    let before_values: Vec<f32> = val_accuracy\n        .iter()\n        .filter(|(step, _)| *step < transition_step)\n        .map(|(_, acc)| *acc)\n        .collect();\n\n    let after_values: Vec<f32> = val_accuracy\n        .iter()\n        .filter(|(step, _)| *step >= transition_step)\n        .map(|(_, acc)| *acc)\n        .collect();\n\n    if before_values.len() < window || after_values.len() < window {\n        return Err(\"insufficient accuracy samples around transition\".to_string());\n    }\n\n    let before_avg: f32 =\n        before_values[before_values.len() - window..].iter().sum::<f32>() / window as f32;\n    let after_avg: f32 = after_values[..window].iter().sum::<f32>() / window as f32;\n\n    if before_avg > chance_accuracy + chance_tolerance {\n        return Err(format!(\n            \"validation accuracy plateau too high before transition (avg {:.4})\",\n            before_avg\n        ));\n    }\n\n    if after_avg < target_val_acc_threshold {\n        return Err(format!(\n            \"validation accuracy after transition too low (avg {:.4})\",\n            after_avg\n        ));\n    }\n\n    Ok(())\n}\n\nfn verify_loss_drop(\n    val_losses: &[(usize, f64)],\n    transition_step: usize,\n    loss_window: usize,\n    loss_drop_fraction: f64,\n) -> Result<f64, String> {\n    let before_losses: Vec<f64> = val_losses\n        .iter()\n        .filter(|(step, _)| *step < transition_step)\n        .map(|(_, loss)| *loss)\n        .collect();\n\n    let after_losses: Vec<f64> = val_losses\n        .iter()\n        .filter(|(step, _)| *step >= transition_step)\n        .map(|(_, loss)| *loss)\n        .collect();\n\n    if before_losses.len() < loss_window || after_losses.len() < loss_window {\n        return Err(\"insufficient loss samples around transition\".to_string());\n    }\n\n    let before_avg: f64 =\n        before_losses[before_losses.len() - loss_window..].iter().sum::<f64>()\n            / loss_window as f64;\n    let after_avg: f64 = after_losses[..loss_window].iter().sum::<f64>() / loss_window as f64;\n\n    if after_avg > before_avg * (1.0 - loss_drop_fraction) {\n        return Err(format!(\n            \"validation loss drop too small (before {:.4}, after {:.4})\",\n            before_avg, after_avg\n        ));\n    }\n\n    Ok(after_avg / before_avg)\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_restricted_loss_early_drop(\n    full_losses: &[(usize, f64)],\n    restricted_losses: &[(usize, f64)],\n    config: &RestrictedLossVerificationConfig,\n) -> Result<RestrictedLossReport, String> {\n    let restricted_baseline =\n        baseline_loss(restricted_losses, config.plateau_min_step, config.drop_window)?;\n    let full_baseline = baseline_loss(full_losses, config.plateau_min_step, config.drop_window)?;\n\n    let restricted_drop_step = find_drop_step(\n        restricted_losses,\n        config.plateau_min_step,\n        restricted_baseline,\n        config.drop_fraction,\n    )?;\n    let full_drop_step = find_drop_step(\n        full_losses,\n        config.plateau_min_step,\n        full_baseline,\n        config.drop_fraction,\n    )?;\n\n    if restricted_drop_step + config.min_step_lead > full_drop_step {\n        return Err(format!(\n            \"restricted loss drops at step {}, expected at least {} steps before full loss (step {})\",\n            restricted_drop_step, config.min_step_lead, full_drop_step\n        ));\n    }\n\n    Ok(RestrictedLossReport {\n        restricted_drop_step,\n        full_drop_step,\n    })\n}\n\npub fn verify_restricted_loss_drop(\n    restricted_losses: &[(usize, f64)],\n    config: &RestrictedLossDropConfig,\n) -> Result<RestrictedLossDropReport, String> {\n    if restricted_losses.len() < 3 {\n        return Err(\"restricted loss history requires at least 3 points\".to_string());\n    }\n\n    let first = restricted_losses.first().ok_or_else(|| {\n        \"restricted loss history is empty\".to_string()\n    })?;\n    let baseline_loss = first.1;\n\n    let (min_idx, (min_step, min_loss)) = restricted_losses\n        .iter()\n        .enumerate()\n        .min_by(|(_, a), (_, b)| {\n            a.1.partial_cmp(&b.1).unwrap_or(std::cmp::Ordering::Equal)\n        })\n        .ok_or_else(|| \"restricted loss history is empty\".to_string())?;\n\n    if min_idx == 0 {\n        return Err(\"restricted loss minimum occurs at start (no drop detected)\".to_string());\n    }\n\n    let relative_decrease = (baseline_loss - min_loss) / baseline_loss;\n    if relative_decrease < config.min_relative_decrease {\n        return Err(format!(\n            \"restricted loss decrease too small (baseline {:.4}, min {:.4}, decrease {:.1}%)\",\n            baseline_loss, min_loss, relative_decrease * 100.0\n        ));\n    }\n\n    Ok(RestrictedLossDropReport {\n        drop_step: *min_step,\n        drop_loss: *min_loss,\n        baseline_loss,\n    })\n}\n\npub fn verify_excluded_loss_spike(\n    excluded_losses: &[(usize, f64)],\n    config: &ExcludedLossSpikeConfig,\n) -> Result<ExcludedLossSpikeReport, String> {\n    if excluded_losses.len() < 3 {\n        return Err(\"excluded loss history requires at least 3 points\".to_string());\n    }\n\n    let first = excluded_losses.first().ok_or_else(|| {\n        \"excluded loss history is empty\".to_string()\n    })?;\n    let last = excluded_losses.last().ok_or_else(|| {\n        \"excluded loss history is empty\".to_string()\n    })?;\n    let baseline_loss = (first.1 + last.1) / 2.0;\n\n    let (peak_idx, (peak_step, peak_loss)) = excluded_losses\n        .iter()\n        .enumerate()\n        .max_by(|(_, a), (_, b)| {\n            a.1.partial_cmp(&b.1).unwrap_or(std::cmp::Ordering::Equal)\n        })\n        .ok_or_else(|| \"excluded loss history is empty\".to_string())?;\n\n    if peak_idx == 0 || peak_idx + 1 == excluded_losses.len() {\n        return Err(\"excluded loss spike occurs at boundary\".to_string());\n    }\n\n    if peak_loss < &(baseline_loss * (1.0 + config.min_relative_increase)) {\n        return Err(format!(\n            \"excluded loss spike too small (baseline {:.4}, peak {:.4})\",\n            baseline_loss, peak_loss\n        ));\n    }\n\n    Ok(ExcludedLossSpikeReport {\n        spike_step: *peak_step,\n        spike_loss: *peak_loss,\n        baseline_loss,\n    })\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_mlp_activation_waves<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n    fixed_y: usize,\n    neuron_indices: &[usize],\n    config: &MlpWaveVerificationConfig,\n) -> Result<MlpWaveReport, String> {\n    let activations = collect_mlp_post_relu_activations(model, device, fixed_y)?;\n    if neuron_indices.is_empty() {\n        return Err(\"neuron indices list is empty\".to_string());\n    }\n\n    let d_ff = activations.len();\n    for &idx in neuron_indices {\n        if idx >= d_ff {\n            return Err(format!(\n                \"neuron index {} out of range (d_ff={})\",\n                idx, d_ff\n            ));\n        }\n    }\n\n    let signals: Vec<(usize, Vec<f64>)> = neuron_indices\n        .iter()\n        .map(|&idx| (idx, activations[idx].clone()))\n        .collect();\n    verify_mlp_activation_waves_from_signals(&signals, config)\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_mlp_activation_waves_from_signals(\n    signals: &[(usize, Vec<f64>)],\n    config: &MlpWaveVerificationConfig,\n) -> Result<MlpWaveReport, String> {\n    if signals.is_empty() {\n        return Err(\"no activation signals provided\".to_string());\n    }\n\n    let mut reports = Vec::with_capacity(signals.len());\n    for (neuron_index, signal) in signals {\n        let report = analyze_mlp_activation_wave(*neuron_index, signal, config)?;\n        reports.push(report);\n    }\n\n    Ok(MlpWaveReport {\n        neuron_reports: reports,\n    })\n}\n\npub fn verify_mlp_pairwise_manifold_transition<B: Backend>(\n    pre_model: &Transformer<B>,\n    post_model: &Transformer<B>,\n    device: &B::Device,\n    config: &PairwiseManifoldConfig,\n) -> Result<PairwiseManifoldTransitionReport, String> {\n    let modulus = ModularAdditionDataset::modulus();\n    if config.fixed_y >= modulus {\n        return Err(format!(\n            \"fixed_y {} out of range (modulus {})\",\n            config.fixed_y, modulus\n        ));\n    }\n\n    let pre_activations = collect_mlp_post_relu_activations(pre_model, device, config.fixed_y)?;\n    let post_activations = collect_mlp_post_relu_activations(post_model, device, config.fixed_y)?;\n\n    let d_ff = pre_activations.len();\n    if post_activations.len() != d_ff {\n        return Err(\"post activations shape mismatch\".to_string());\n    }\n\n    if config.neuron_indices.len() < 2 {\n        return Err(\"need at least two neuron indices\".to_string());\n    }\n\n    for &idx in &config.neuron_indices {\n        if idx >= d_ff {\n            return Err(format!(\n                \"neuron index {} out of range (d_ff={})\",\n                idx, d_ff\n            ));\n        }\n    }\n\n    let pre_signals: Vec<(usize, Vec<f64>)> = config\n        .neuron_indices\n        .iter()\n        .map(|&idx| (idx, pre_activations[idx].clone()))\n        .collect();\n    let post_signals: Vec<(usize, Vec<f64>)> = config\n        .neuron_indices\n        .iter()\n        .map(|&idx| (idx, post_activations[idx].clone()))\n        .collect();\n\n    verify_pairwise_manifold_transition_from_signals(&pre_signals, &post_signals, config)\n}\n\npub fn collect_mlp_activation_surface<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n    neuron_index: usize,\n) -> Result<Vec<Vec<f64>>, String> {\n    let modulus = ModularAdditionDataset::modulus();\n    let equals_token = ModularAdditionDataset::equals_token();\n\n    let mut inputs_vec = Vec::with_capacity(modulus * modulus * 3);\n    for x in 0..modulus {\n        for y in 0..modulus {\n            inputs_vec.push(x as i32);\n            inputs_vec.push(y as i32);\n            inputs_vec.push(equals_token as i32);\n        }\n    }\n\n    let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n        .reshape([modulus * modulus, 3]);\n    let (_, mlp_acts) = model.forward_with_mlp_activations(inputs);\n    let [batch_size, d_ff] = mlp_acts.dims();\n    if batch_size != modulus * modulus {\n        return Err(format!(\n            \"unexpected activation batch size {} (expected {})\",\n            batch_size,\n            modulus * modulus\n        ));\n    }\n    if neuron_index >= d_ff {\n        return Err(format!(\n            \"neuron index {} out of range (d_ff={})\",\n            neuron_index, d_ff\n        ));\n    }\n\n    let data: Vec<f32> = mlp_acts.into_data().to_vec().unwrap();\n    let mut surface = vec![vec![0.0f64; modulus]; modulus];\n    for x in 0..modulus {\n        for y in 0..modulus {\n            let batch = x * modulus + y;\n            let idx = batch * d_ff + neuron_index;\n            surface[x][y] = data[idx] as f64;\n        }\n    }\n\n    Ok(surface)\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_constructive_interference_surface_from_grid(\n    surface: &[Vec<f64>],\n    config: &DiagonalRidgeConfig,\n) -> Result<DiagonalRidgeReport, String> {\n    let (diag_main, diag_anti, axis_x, axis_y) =\n        surface_autocorrelation_scores(surface, config.max_lag)?;\n\n    let diagonal_score = diag_main.max(diag_anti);\n    let axis_score = axis_x.max(axis_y);\n    let ratio = if axis_score > 0.0 {\n        diagonal_score / axis_score\n    } else {\n        f64::INFINITY\n    };\n\n    if diagonal_score < config.min_diagonal_score {\n        return Err(format!(\n            \"diagonal autocorrelation {:.3} below threshold {:.3}\",\n            diagonal_score, config.min_diagonal_score\n        ));\n    }\n\n    if ratio < config.min_diagonal_ratio {\n        return Err(format!(\n            \"diagonal/axis ratio {:.3} below threshold {:.3}\",\n            ratio, config.min_diagonal_ratio\n        ));\n    }\n\n    Ok(DiagonalRidgeReport {\n        neuron_index: config.neuron_index,\n        diagonal_score,\n        axis_score,\n        ratio,\n    })\n}\n\npub fn verify_pairwise_manifold_transition_from_signals(\n    pre_signals: &[(usize, Vec<f64>)],\n    post_signals: &[(usize, Vec<f64>)],\n    config: &PairwiseManifoldConfig,\n) -> Result<PairwiseManifoldTransitionReport, String> {\n    let pre_report = evaluate_pairwise_manifolds_from_signals(pre_signals)?;\n    let post_report = evaluate_pairwise_manifolds_from_signals(post_signals)?;\n\n    if pre_report.average_score > config.max_pre_score {\n        return Err(format!(\n            \"pre-grok average circularity {:.3} exceeds threshold {:.3}\",\n            pre_report.average_score, config.max_pre_score\n        ));\n    }\n\n    let passing_pairs = post_report\n        .pair_reports\n        .iter()\n        .filter(|pair| {\n            pair.circularity_score >= config.min_post_score\n                && pair.axis_ratio <= config.max_axis_ratio\n        })\n        .count();\n    if passing_pairs < config.min_pairs_passing {\n        return Err(format!(\n            \"post-grok only {} pairs meet circularity threshold (need {})\",\n            passing_pairs, config.min_pairs_passing\n        ));\n    }\n\n    Ok(PairwiseManifoldTransitionReport {\n        pre_average_score: pre_report.average_score,\n        post_average_score: post_report.average_score,\n        passing_pairs,\n        total_pairs: post_report.pair_reports.len(),\n    })\n}\n\nfn evaluate_pairwise_manifolds_from_signals(\n    signals: &[(usize, Vec<f64>)],\n) -> Result<PairwiseManifoldReport, String> {\n    if signals.len() < 2 {\n        return Err(\"need at least two activation signals\".to_string());\n    }\n\n    let signal_len = signals[0].1.len();\n    if signal_len < 3 {\n        return Err(\"activation signal too short\".to_string());\n    }\n\n    for (idx, signal) in signals {\n        if signal.len() != signal_len {\n            return Err(format!(\n                \"signal length mismatch for neuron {}\",\n                idx\n            ));\n        }\n    }\n\n    let mut reports = Vec::new();\n    for i in 0..signals.len() {\n        for j in (i + 1)..signals.len() {\n            let (neuron_a, signal_a) = &signals[i];\n            let (neuron_b, signal_b) = &signals[j];\n            let points: Vec<(f64, f64)> = signal_a\n                .iter()\n                .zip(signal_b.iter())\n                .map(|(x, y)| (*x, *y))\n                .collect();\n            let (score, radius_cv, axis_ratio) =\n                match pairwise_circularity_metrics(&points) {\n                    Ok(metrics) => metrics,\n                    Err(_) => (0.0, 1.0, f64::INFINITY),\n                };\n            reports.push(PairwiseManifoldPairReport {\n                neuron_a: *neuron_a,\n                neuron_b: *neuron_b,\n                circularity_score: score,\n                radius_cv,\n                axis_ratio,\n            });\n        }\n    }\n\n    let average_score = reports\n        .iter()\n        .map(|pair| pair.circularity_score)\n        .sum::<f64>()\n        / reports.len() as f64;\n\n    Ok(PairwiseManifoldReport {\n        pair_reports: reports,\n        average_score,\n    })\n}\n\nfn pairwise_circularity_metrics(points: &[(f64, f64)]) -> Result<(f64, f64, f64), String> {\n    if points.len() < 3 {\n        return Err(\"not enough points for circularity\".to_string());\n    }\n\n    let n = points.len() as f64;\n    let mean_x = points.iter().map(|(x, _)| *x).sum::<f64>() / n;\n    let mean_y = points.iter().map(|(_, y)| *y).sum::<f64>() / n;\n\n    let mut var_x = 0.0;\n    let mut var_y = 0.0;\n    let mut cov_xy = 0.0;\n    for (x, y) in points {\n        let dx = x - mean_x;\n        let dy = y - mean_y;\n        var_x += dx * dx;\n        var_y += dy * dy;\n        cov_xy += dx * dy;\n    }\n    var_x /= n;\n    var_y /= n;\n    cov_xy /= n;\n\n    let trace = var_x + var_y;\n    let det = var_x * var_y - cov_xy * cov_xy;\n    if det <= 1.0e-12 || trace <= 0.0 {\n        return Err(\"degenerate covariance\".to_string());\n    }\n\n    let half_trace = trace * 0.5;\n    let disc = (half_trace * half_trace - det).max(0.0).sqrt();\n    let lambda1 = half_trace + disc;\n    let lambda2 = half_trace - disc;\n    if lambda2 <= 0.0 {\n        return Err(\"degenerate eigenvalues\".to_string());\n    }\n\n    let axis_ratio = (lambda1 / lambda2).sqrt();\n\n    let mut v1_x = cov_xy;\n    let mut v1_y = lambda1 - var_x;\n    if v1_x.abs() < 1.0e-12 && v1_y.abs() < 1.0e-12 {\n        v1_x = 1.0;\n        v1_y = 0.0;\n    }\n    let norm = (v1_x * v1_x + v1_y * v1_y).sqrt();\n    v1_x /= norm;\n    v1_y /= norm;\n    let v2_x = -v1_y;\n    let v2_y = v1_x;\n\n    let mut radii = Vec::with_capacity(points.len());\n    for (x, y) in points {\n        let dx = x - mean_x;\n        let dy = y - mean_y;\n        let p1 = dx * v1_x + dy * v1_y;\n        let p2 = dx * v2_x + dy * v2_y;\n        let w1 = p1 / lambda1.sqrt();\n        let w2 = p2 / lambda2.sqrt();\n        radii.push((w1 * w1 + w2 * w2).sqrt());\n    }\n\n    let mean_radius = radii.iter().sum::<f64>() / radii.len() as f64;\n    if mean_radius == 0.0 {\n        return Err(\"zero mean radius\".to_string());\n    }\n\n    let mut variance = 0.0;\n    for r in &radii {\n        let diff = r - mean_radius;\n        variance += diff * diff;\n    }\n    let std = (variance / radii.len() as f64).sqrt();\n    let radius_cv = std / mean_radius;\n    let score = 1.0 / (1.0 + radius_cv);\n\n    Ok((score, radius_cv, axis_ratio))\n}\n\n// Helper functions for verification - reserved for future integration\n#[allow(dead_code)]\nfn baseline_loss(\n    losses: &[(usize, f64)],\n    plateau_min_step: usize,\n    drop_window: usize,\n) -> Result<f64, String> {\n    let before: Vec<f64> = losses\n        .iter()\n        .filter(|(step, _)| *step <= plateau_min_step)\n        .map(|(_, loss)| *loss)\n        .collect();\n\n    if before.len() < drop_window || drop_window == 0 {\n        return Err(\"insufficient loss samples for baseline\".to_string());\n    }\n\n    let window = &before[before.len() - drop_window..];\n    Ok(window.iter().sum::<f64>() / drop_window as f64)\n}\n\n#[allow(dead_code)]\nfn find_drop_step(\n    losses: &[(usize, f64)],\n    plateau_min_step: usize,\n    baseline: f64,\n    drop_fraction: f64,\n) -> Result<usize, String> {\n    losses\n        .iter()\n        .filter(|(step, _)| *step >= plateau_min_step)\n        .find(|(_, loss)| *loss <= baseline * (1.0 - drop_fraction))\n        .map(|(step, _)| *step)\n        .ok_or_else(|| \"loss never drops below threshold\".to_string())\n}\n\nfn collect_mlp_post_relu_activations<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n    fixed_y: usize,\n) -> Result<Vec<Vec<f64>>, String> {\n    let modulus = ModularAdditionDataset::modulus();\n    let equals_token = ModularAdditionDataset::equals_token();\n\n    let mut inputs_vec = Vec::with_capacity(modulus * 3);\n    for x in 0..modulus {\n        inputs_vec.push(x as i32);\n        inputs_vec.push(fixed_y as i32);\n        inputs_vec.push(equals_token as i32);\n    }\n\n    let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n        .reshape([modulus, 3]);\n    let (_, mlp_acts) = model.forward_with_mlp_activations(inputs);\n    let [batch_size, d_ff] = mlp_acts.dims();\n\n    if batch_size != modulus {\n        return Err(format!(\n            \"unexpected activation batch size {} (expected {})\",\n            batch_size, modulus\n        ));\n    }\n\n    let data: Vec<f32> = mlp_acts.into_data().to_vec().unwrap();\n    let mut per_neuron = vec![Vec::with_capacity(batch_size); d_ff];\n    for batch in 0..batch_size {\n        for neuron in 0..d_ff {\n            let idx = batch * d_ff + neuron;\n            per_neuron[neuron].push(data[idx] as f64);\n        }\n    }\n\n    Ok(per_neuron)\n}\n\n#[allow(dead_code)]\nfn surface_autocorrelation_scores(\n    surface: &[Vec<f64>],\n    max_lag: usize,\n) -> Result<(f64, f64, f64, f64), String> {\n    let (height, width) = surface_dimensions(surface)?;\n    if height < 2 || width < 2 {\n        return Err(\"surface grid too small\".to_string());\n    }\n\n    let (mean, var) = surface_mean_variance(surface)?;\n    if var == 0.0 {\n        return Err(\"surface variance is zero\".to_string());\n    }\n\n    let max_lag = max_lag.min(height.saturating_sub(1)).min(width.saturating_sub(1));\n    if max_lag == 0 {\n        return Err(\"max_lag too small for surface\".to_string());\n    }\n\n    let diag_main = average_autocorrelation(surface, mean, var, max_lag, 1, 1)?;\n    let diag_anti = average_autocorrelation(surface, mean, var, max_lag, 1, -1)?;\n    let axis_x = average_autocorrelation(surface, mean, var, max_lag, 1, 0)?;\n    let axis_y = average_autocorrelation(surface, mean, var, max_lag, 0, 1)?;\n\n    Ok((diag_main, diag_anti, axis_x, axis_y))\n}\n\n#[allow(dead_code)]\nfn surface_dimensions(surface: &[Vec<f64>]) -> Result<(usize, usize), String> {\n    let height = surface.len();\n    if height == 0 {\n        return Err(\"surface is empty\".to_string());\n    }\n    let width = surface[0].len();\n    if width == 0 {\n        return Err(\"surface has empty rows\".to_string());\n    }\n    for (idx, row) in surface.iter().enumerate() {\n        if row.len() != width {\n            return Err(format!(\n                \"surface row {} has length {}, expected {}\",\n                idx,\n                row.len(),\n                width\n            ));\n        }\n    }\n    Ok((height, width))\n}\n\nfn surface_mean_variance(surface: &[Vec<f64>]) -> Result<(f64, f64), String> {\n    let (height, width) = surface_dimensions(surface)?;\n    let mut sum = 0.0;\n    for row in surface {\n        for value in row {\n            sum += *value;\n        }\n    }\n    let count = (height * width) as f64;\n    let mean = sum / count;\n    let mut variance = 0.0;\n    for row in surface {\n        for value in row {\n            let diff = *value - mean;\n            variance += diff * diff;\n        }\n    }\n    variance /= count;\n    Ok((mean, variance))\n}\n\nfn average_autocorrelation(\n    surface: &[Vec<f64>],\n    mean: f64,\n    variance: f64,\n    max_lag: usize,\n    dx: i32,\n    dy: i32,\n) -> Result<f64, String> {\n    let (height, width) = surface_dimensions(surface)?;\n    let mut total = 0.0;\n    let mut lag_count = 0usize;\n\n    for lag in 1..=max_lag {\n        let mut sum = 0.0;\n        let mut count = 0usize;\n        let lag = lag as i32;\n        for x in 0..height {\n            for y in 0..width {\n                let x2 = x as i32 + dx * lag;\n                let y2 = y as i32 + dy * lag;\n                if x2 < 0 || y2 < 0 {\n                    continue;\n                }\n                let x2 = x2 as usize;\n                let y2 = y2 as usize;\n                if x2 >= height || y2 >= width {\n                    continue;\n                }\n                let a = surface[x][y] - mean;\n                let b = surface[x2][y2] - mean;\n                sum += a * b;\n                count += 1;\n            }\n        }\n        if count == 0 {\n            continue;\n        }\n        let corr = sum / (count as f64 * variance);\n        total += corr.abs();\n        lag_count += 1;\n    }\n\n    if lag_count == 0 {\n        return Err(\"no valid lags for autocorrelation\".to_string());\n    }\n\n    Ok(total / lag_count as f64)\n}\n\nfn analyze_mlp_activation_wave(\n    neuron_index: usize,\n    signal: &[f64],\n    config: &MlpWaveVerificationConfig,\n) -> Result<MlpWaveNeuronReport, String> {\n    if signal.len() < 3 {\n        return Err(format!(\n            \"activation signal for neuron {} is too short\",\n            neuron_index\n        ));\n    }\n\n    let negative_count = signal\n        .iter()\n        .filter(|value| **value < -config.negative_tolerance)\n        .count();\n    let negative_fraction = negative_count as f64 / signal.len() as f64;\n    if negative_fraction > config.max_negative_fraction {\n        return Err(format!(\n            \"neuron {} has {:.3} negative activations\",\n            neuron_index, negative_fraction\n        ));\n    }\n\n    let (dominant_frequency, dominant_ratio) = dominant_frequency_ratio(signal)?;\n    if dominant_ratio < config.min_dominant_ratio {\n        return Err(format!(\n            \"neuron {} dominant frequency ratio {:.3} below threshold {:.3}\",\n            neuron_index, dominant_ratio, config.min_dominant_ratio\n        ));\n    }\n\n    let best_correlation =\n        best_rectified_sine_correlation(signal, dominant_frequency, config.phase_steps);\n    if best_correlation < config.min_correlation {\n        return Err(format!(\n            \"neuron {} rectified sine correlation {:.3} below threshold {:.3}\",\n            neuron_index, best_correlation, config.min_correlation\n        ));\n    }\n\n    Ok(MlpWaveNeuronReport {\n        neuron_index,\n        dominant_frequency,\n        dominant_ratio,\n        negative_fraction,\n        best_correlation,\n    })\n}\n\nfn dominant_frequency_ratio(signal: &[f64]) -> Result<(usize, f64), String> {\n    if signal.len() < 2 {\n        return Err(\"signal too short for FFT\".to_string());\n    }\n\n    let mut planner = FftPlanner::new();\n    let fft = planner.plan_fft_forward(signal.len());\n    let mut buffer: Vec<Complex<f64>> = signal\n        .iter()\n        .map(|&x| Complex::new(x, 0.0))\n        .collect();\n    fft.process(&mut buffer);\n    let magnitudes: Vec<f64> = buffer.iter().map(|c| c.norm()).collect();\n\n    if magnitudes.len() < 2 {\n        return Err(\"insufficient FFT bins\".to_string());\n    }\n\n    let mut dominant_idx = 1usize;\n    let mut dominant_mag = magnitudes[1];\n    for (idx, &mag) in magnitudes.iter().enumerate().skip(2) {\n        if mag > dominant_mag {\n            dominant_mag = mag;\n            dominant_idx = idx;\n        }\n    }\n\n    if dominant_idx == 0 {\n        return Err(\"dominant frequency is zero\".to_string());\n    }\n\n    let mut nonzero = magnitudes[1..].to_vec();\n    nonzero.sort_by(|a, b| a.partial_cmp(b).unwrap_or(std::cmp::Ordering::Equal));\n    let median_mag = nonzero[nonzero.len() / 2];\n    let ratio = if median_mag == 0.0 {\n        dominant_mag\n    } else {\n        dominant_mag / median_mag\n    };\n\n    Ok((dominant_idx, ratio))\n}\n\nfn best_rectified_sine_correlation(signal: &[f64], frequency: usize, phase_steps: usize) -> f64 {\n    if signal.is_empty() || frequency == 0 || phase_steps == 0 {\n        return 0.0;\n    }\n\n    let tau = std::f64::consts::PI * 2.0;\n    let mut best = -1.0;\n    for step in 0..phase_steps {\n        let phase = tau * step as f64 / phase_steps as f64;\n        let wave: Vec<f64> = (0..signal.len())\n            .map(|idx| {\n                let angle = tau * frequency as f64 * idx as f64 / signal.len() as f64 + phase;\n                angle.sin().max(0.0)\n            })\n            .collect();\n        let corr = pearson_correlation(signal, &wave);\n        if corr > best {\n            best = corr;\n        }\n    }\n\n    best\n}\n\nfn pearson_correlation(a: &[f64], b: &[f64]) -> f64 {\n    if a.len() != b.len() || a.is_empty() {\n        return 0.0;\n    }\n\n    let mean_a = a.iter().sum::<f64>() / a.len() as f64;\n    let mean_b = b.iter().sum::<f64>() / b.len() as f64;\n\n    let mut num = 0.0;\n    let mut denom_a = 0.0;\n    let mut denom_b = 0.0;\n    for (x, y) in a.iter().zip(b.iter()) {\n        let da = x - mean_a;\n        let db = y - mean_b;\n        num += da * db;\n        denom_a += da * da;\n        denom_b += db * db;\n    }\n\n    if denom_a == 0.0 || denom_b == 0.0 {\n        return 0.0;\n    }\n\n    num / (denom_a.sqrt() * denom_b.sqrt())\n}\n\n/// Verify that the model actually learned modular addition\n/// by testing it on ALL possible examples systematically\npub fn verify_full_accuracy<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n) -> (f32, f32) {\n    println!(\"ðŸ” Verifying model on COMPLETE dataset (no sampling)...\");\n\n    let train_dataset = ModularAdditionDataset::new(true, 42);\n    let val_dataset = ModularAdditionDataset::new(false, 42);\n\n    let train_acc = test_all_examples(model, &train_dataset, device);\n    let val_acc = test_all_examples(model, &val_dataset, device);\n\n    println!(\"ðŸ“Š FULL Dataset Results:\");\n    println!(\"   Training: {}/{} = {:.2}%\",\n        (train_acc * train_dataset.len() as f32) as usize,\n        train_dataset.len(),\n        train_acc * 100.0);\n    println!(\"   Validation: {}/{} = {:.2}%\",\n        (val_acc * val_dataset.len() as f32) as usize,\n        val_dataset.len(),\n        val_acc * 100.0);\n\n    (train_acc, val_acc)\n}\n\n/// Test model on ALL examples in dataset (no random sampling)\nfn test_all_examples<B: Backend>(\n    model: &Transformer<B>,\n    dataset: &ModularAdditionDataset,\n    device: &B::Device,\n) -> f32 {\n    let mut total_correct = 0;\n    let total = dataset.len();\n\n    // Test in batches to avoid memory issues\n    let batch_size = 100;\n    for batch_start in (0..total).step_by(batch_size) {\n        let batch_end = (batch_start + batch_size).min(total);\n        let batch_len = batch_end - batch_start;\n\n        let mut inputs_vec = Vec::new();\n        let mut targets_vec = Vec::new();\n\n        for idx in batch_start..batch_end {\n            let (input, target) = dataset.get(idx).unwrap();\n            inputs_vec.extend(input);\n            targets_vec.push(target as i32);\n        }\n\n        let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n            .reshape([batch_len, 3]);\n\n        let logits = model.forward(inputs);\n        let predictions = logits.argmax(1).squeeze::<1>();\n        let predictions_vec: Vec<i32> = predictions.into_data().to_vec().unwrap();\n\n        let correct = predictions_vec\n            .iter()\n            .zip(targets_vec.iter())\n            .filter(|(p, t)| p == t)\n            .count();\n\n        total_correct += correct;\n    }\n\n    total_correct as f32 / total as f32\n}\n\n/// Verify that weight norms decrease over time with weight decay\n/// and show correlation with the grokking transition\npub fn verify_weight_norm_decay(\n    weight_norm_history: &[(usize, f64)],\n    grok_step: Option<usize>,\n    config: &WeightNormDecayConfig,\n) -> Result<WeightNormDecayReport, String> {\n    if weight_norm_history.len() < 2 {\n        return Err(\"weight norm history requires at least 2 snapshots\".to_string());\n    }\n\n    let initial_norm = weight_norm_history\n        .first()\n        .ok_or_else(|| \"weight norm history is empty\".to_string())?\n        .1;\n    let final_norm = weight_norm_history\n        .last()\n        .ok_or_else(|| \"weight norm history is empty\".to_string())?\n        .1;\n\n    if initial_norm <= 0.0 {\n        return Err(\"initial weight norm must be positive\".to_string());\n    }\n\n    // Check that norms are generally decreasing (with weight decay)\n    let relative_decrease = (initial_norm - final_norm) / initial_norm;\n    if relative_decrease < config.min_relative_decrease {\n        return Err(format!(\n            \"weight norm decrease {:.4} below threshold {:.4} (initial {:.4}, final {:.4})\",\n            relative_decrease, config.min_relative_decrease, initial_norm, final_norm\n        ));\n    }\n\n    // Check that norms are not increasing (weight decay should monotonically decrease norms)\n    for window in weight_norm_history.windows(2) {\n        let prev_norm = window[0].1;\n        let next_norm = window[1].1;\n        if next_norm > prev_norm * 1.01 {\n            // Allow 1% tolerance for numerical noise\n            return Err(format!(\n                \"weight norm increased from {:.4} to {:.4} at step {}\",\n                prev_norm, next_norm, window[1].0\n            ));\n        }\n    }\n\n    // If grok step is provided, check that norms show a shift around grokking\n    let grok_step_norm = if let Some(step) = grok_step {\n        weight_norm_history\n            .iter()\n            .filter(|(s, _)| {\n                *s >= step.saturating_sub(config.grok_step_window)\n                    && *s <= step + config.grok_step_window\n            })\n            .map(|(_, norm)| *norm)\n            .next()\n    } else {\n        None\n    };\n\n    Ok(WeightNormDecayReport {\n        initial_norm,\n        final_norm,\n        relative_decrease,\n        grok_step_norm,\n    })\n}\n\n/// Verify the characteristic \"snake curve\" shape of grokking phenomenon\n/// on log-scale plots: train accuracy rises early, validation stays flat then jumps\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_snake_curve_shape(\n    train_acc: &[(usize, f32)],\n    val_acc: &[(usize, f32)],\n    config: &SnakeCurveConfig,\n) -> Result<SnakeCurveReport, String> {\n    if train_acc.is_empty() || val_acc.is_empty() {\n        return Err(\"empty accuracy history\".to_string());\n    }\n\n    // Phase 1: Verify early train convergence (immediate drop on log scale)\n    let train_converged_step = train_acc\n        .iter()\n        .find(|(_, acc)| *acc >= config.train_convergence_threshold)\n        .map(|(step, _)| *step)\n        .ok_or_else(|| {\n            format!(\n                \"train accuracy never reaches {} threshold\",\n                config.train_convergence_threshold\n            )\n        })?;\n\n    if train_converged_step > config.early_step_max {\n        return Err(format!(\n            \"train accuracy converges at step {}, expected before {}\",\n            train_converged_step, config.early_step_max\n        ));\n    }\n\n    // Phase 2: Verify validation plateau (flat on log scale)\n    let plateau_samples: Vec<f32> = val_acc\n        .iter()\n        .filter(|(step, _)| *step >= 1 && *step <= config.val_plateau_min_step)\n        .map(|(_, acc)| *acc)\n        .collect();\n\n    if plateau_samples.is_empty() {\n        return Err(\"no validation samples in plateau region\".to_string());\n    }\n\n    let val_plateau_max = plateau_samples\n        .iter()\n        .fold(0.0f32, |max_val, &acc| max_val.max(acc));\n\n    if val_plateau_max > config.val_plateau_max_acc {\n        return Err(format!(\n            \"validation accuracy in plateau region too high: max {:.4} exceeds {:.4}\",\n            val_plateau_max, config.val_plateau_max_acc\n        ));\n    }\n\n    // Phase 3: Verify delayed validation jump (crash up on log scale)\n    let val_grok_step = val_acc\n        .iter()\n        .filter(|(step, _)| *step > config.val_plateau_min_step)\n        .find(|(_, acc)| *acc >= config.val_final_threshold)\n        .map(|(step, _)| *step)\n        .ok_or_else(|| {\n            format!(\n                \"validation accuracy never reaches {} after plateau\",\n                config.val_final_threshold\n            )\n        })?;\n\n    // Phase 4: Verify phase ordering (snake shape: early train, late val)\n    let curve_shape_valid = train_converged_step < config.val_plateau_min_step\n        && val_grok_step > config.val_plateau_min_step\n        && train_converged_step < val_grok_step;\n\n    if !curve_shape_valid {\n        return Err(format!(\n            \"invalid curve shape: train converged at {}, val plateau until {}, val grokked at {}\",\n            train_converged_step, config.val_plateau_min_step, val_grok_step\n        ));\n    }\n\n    Ok(SnakeCurveReport {\n        train_converged_by_step: train_converged_step,\n        val_plateau_max_acc: val_plateau_max,\n        val_grok_step,\n        curve_shape_valid,\n    })\n}\n\n/// Test specific examples to see if model actually computes mod p\npub fn test_specific_examples<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n) {\n    println!();\n    println!(\"ðŸ§ª Testing specific examples:\");\n\n    let modulus = ModularAdditionDataset::modulus();\n    let equals_token = ModularAdditionDataset::equals_token();\n    let test_cases = vec![\n        (0, 0, 0),\n        (1, 1, 2),\n        (50, 50, (50 + 50) % modulus),\n        (112, 1, (112 + 1) % modulus),\n        (56, 57, (56 + 57) % modulus),\n        (112, 112, (112 + 112) % modulus),\n        (10, 20, 30),\n        (60, 60, (60 + 60) % modulus),\n    ];\n\n    for (a, b, expected) in test_cases {\n        let input_vec = vec![a as i32, b as i32, equals_token as i32];\n        let input = Tensor::<B, 1, Int>::from_ints(input_vec.as_slice(), device)\n            .reshape([1, 3]);\n\n        let logits = model.forward(input);\n        let prediction = logits.argmax(1).squeeze::<1>();\n        let pred_value: i32 = prediction.into_data().to_vec().unwrap()[0];\n\n        let correct = if pred_value == expected as i32 { \"âœ“\" } else { \"âœ—\" };\n        println!(\n            \"   {} + {} mod {} = {} | Model: {} {}\",\n            a, b, modulus, expected, pred_value, correct\n        );\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    fn make_histories(\n        train_acc: &[(usize, f32)],\n        val_acc: &[(usize, f32)],\n        val_loss: &[(usize, f64)],\n    ) -> (LossHistory, AccuracyHistory) {\n        let mut loss_history = LossHistory::new();\n        loss_history.val_snapshots = val_loss.to_vec();\n        let mut accuracy_history = AccuracyHistory::new();\n        accuracy_history.train_snapshots = train_acc.to_vec();\n        accuracy_history.val_snapshots = val_acc.to_vec();\n        (loss_history, accuracy_history)\n    }\n\n    #[test]\n    fn grokking_phase_verification_passes_on_synthetic_signal() {\n        let train_acc = vec![(0, 0.2), (500, 0.995), (1000, 0.999)];\n        let val_acc = vec![\n            (0, 0.01),\n            (500, 0.01),\n            (1500, 0.01),\n            (2200, 0.995),\n            (2500, 0.999),\n        ];\n        let val_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 2.0),\n            (2200, 0.2),\n            (2500, 0.15),\n        ];\n\n        let (loss_history, accuracy_history) = make_histories(&train_acc, &val_acc, &val_loss);\n        let config = GrokkingVerificationConfig {\n            early_step_max: 1000,\n            plateau_min_step: 1500,\n            generalization_window: 1500,\n            accuracy_window: 2,\n            loss_window: 2,\n            ..GrokkingVerificationConfig::default_for_modulus(ModularAdditionDataset::modulus())\n        };\n\n        let report = verify_grokking_phase_transition(&loss_history, &accuracy_history, &config)\n            .expect(\"expected grokking verification to pass\");\n        assert!(report.transition_step >= config.plateau_min_step);\n        assert!(report.early_train_step <= config.early_step_max);\n    }\n\n    #[test]\n    fn grokking_phase_verification_fails_without_transition() {\n        let train_acc = vec![(0, 0.2), (500, 0.995)];\n        let val_acc = vec![(0, 0.01), (500, 0.01), (1500, 0.02)];\n        let val_loss = vec![(0, 2.0), (500, 2.0), (1500, 1.9)];\n\n        let (loss_history, accuracy_history) = make_histories(&train_acc, &val_acc, &val_loss);\n        let config = GrokkingVerificationConfig {\n            early_step_max: 1000,\n            plateau_min_step: 1000,\n            generalization_window: 1000,\n            accuracy_window: 2,\n            loss_window: 2,\n            ..GrokkingVerificationConfig::default_for_modulus(ModularAdditionDataset::modulus())\n        };\n\n        let err = verify_grokking_phase_transition(&loss_history, &accuracy_history, &config)\n            .expect_err(\"expected grokking verification to fail\");\n        assert!(err.contains(\"validation accuracy never reaches target\"));\n    }\n\n    #[test]\n    fn grokking_phase_verification_fails_with_slow_accuracy_rise() {\n        let train_acc = vec![(0, 0.2), (500, 0.995), (1000, 0.999)];\n        let val_acc = vec![\n            (0, 0.01),\n            (500, 0.01),\n            (1500, 0.5),\n            (2000, 0.6),\n            (2200, 0.995),\n            (2500, 0.999),\n        ];\n        let val_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 2.0),\n            (2200, 0.2),\n            (2500, 0.15),\n        ];\n\n        let (loss_history, accuracy_history) = make_histories(&train_acc, &val_acc, &val_loss);\n        let config = GrokkingVerificationConfig {\n            early_step_max: 1000,\n            plateau_min_step: 1000,\n            generalization_window: 2000,\n            accuracy_window: 2,\n            loss_window: 2,\n            ..GrokkingVerificationConfig::default_for_modulus(ModularAdditionDataset::modulus())\n        };\n\n        let err = verify_grokking_phase_transition(&loss_history, &accuracy_history, &config)\n            .expect_err(\"expected grokking verification to fail\");\n        assert!(err.contains(\"plateau too high\"));\n    }\n\n    #[test]\n    fn restricted_loss_detection_passes_with_early_drop() {\n        let full_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 2.0),\n            (2500, 2.0),\n            (3500, 0.3),\n        ];\n        let restricted_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 0.8),\n            (2500, 0.4),\n            (3500, 0.3),\n        ];\n\n        let config = RestrictedLossVerificationConfig {\n            plateau_min_step: 1000,\n            drop_window: 2,\n            drop_fraction: 0.5,\n            min_step_lead: 500,\n        };\n\n        let report = verify_restricted_loss_early_drop(&full_loss, &restricted_loss, &config)\n            .expect(\"expected restricted loss verification to pass\");\n        assert!(report.restricted_drop_step < report.full_drop_step);\n    }\n\n    #[test]\n    fn restricted_loss_detection_fails_when_full_drops_first() {\n        let full_loss = vec![(0, 2.0), (1000, 1.0), (1500, 0.8)];\n        let restricted_loss = vec![(0, 2.0), (1000, 2.0), (1500, 0.8)];\n\n        let config = RestrictedLossVerificationConfig {\n            plateau_min_step: 500,\n            drop_window: 1,\n            drop_fraction: 0.3,\n            min_step_lead: 200,\n        };\n\n        let err = verify_restricted_loss_early_drop(&full_loss, &restricted_loss, &config)\n            .expect_err(\"expected restricted loss verification to fail\");\n        assert!(err.contains(\"restricted loss drops\"));\n    }\n\n    #[test]\n    fn excluded_loss_spike_verification_passes() {\n        let excluded_loss = vec![(0, 1.0), (1500, 2.2), (3000, 1.1)];\n        let config = ExcludedLossSpikeConfig {\n            min_relative_increase: 0.5,\n        };\n\n        let report = verify_excluded_loss_spike(&excluded_loss, &config)\n            .expect(\"expected excluded loss spike verification to pass\");\n        assert_eq!(report.spike_step, 1500);\n    }\n\n    #[test]\n    fn excluded_loss_spike_verification_fails_without_spike() {\n        let excluded_loss = vec![(0, 1.0), (1500, 1.1), (3000, 1.0)];\n        let config = ExcludedLossSpikeConfig {\n            min_relative_increase: 0.5,\n        };\n\n        let err = verify_excluded_loss_spike(&excluded_loss, &config)\n            .expect_err(\"expected excluded loss spike verification to fail\");\n        assert!(err.contains(\"excluded loss spike too small\"));\n    }\n\n    #[test]\n    fn restricted_loss_drop_verification_passes() {\n        let restricted_loss = vec![\n            (0, 2.0),\n            (1000, 1.8),\n            (5000, 1.2),\n            (7000, 0.8),\n            (10000, 0.9),\n        ];\n        let config = RestrictedLossDropConfig {\n            min_relative_decrease: 0.3,\n        };\n\n        let report = verify_restricted_loss_drop(&restricted_loss, &config)\n            .expect(\"expected restricted loss drop verification to pass\");\n        assert_eq!(report.drop_step, 7000);\n        assert!(report.baseline_loss > report.drop_loss);\n        assert!((report.baseline_loss - report.drop_loss) / report.baseline_loss >= 0.3);\n    }\n\n    #[test]\n    fn restricted_loss_drop_verification_fails_without_drop() {\n        let restricted_loss = vec![\n            (0, 2.0),\n            (1000, 2.1),\n            (5000, 1.9),\n            (7000, 2.0),\n            (10000, 2.05),\n        ];\n        let config = RestrictedLossDropConfig {\n            min_relative_decrease: 0.3,\n        };\n\n        let err = verify_restricted_loss_drop(&restricted_loss, &config)\n            .expect_err(\"expected restricted loss drop verification to fail\");\n        assert!(err.contains(\"restricted loss decrease too small\"));\n    }\n\n    #[test]\n    fn mlp_activation_wave_verification_passes_on_rectified_sine() {\n        let n = ModularAdditionDataset::modulus();\n        let freq = 5.0;\n        let mut signal = Vec::with_capacity(n);\n        for idx in 0..n {\n            let angle = 2.0 * std::f64::consts::PI * freq * idx as f64 / n as f64;\n            let rectified = angle.sin().max(0.0);\n            let wobble = (2.0 * std::f64::consts::PI * 2.0 * idx as f64 / n as f64)\n                .sin()\n                .abs()\n                * 0.02;\n            signal.push(rectified + wobble);\n        }\n\n        let config = MlpWaveVerificationConfig {\n            max_negative_fraction: 0.0,\n            negative_tolerance: 1e-6,\n            min_dominant_ratio: 3.0,\n            min_correlation: 0.7,\n            phase_steps: 32,\n        };\n\n        let report =\n            verify_mlp_activation_waves_from_signals(&[(0, signal)], &config)\n                .expect(\"expected rectified sine to pass\");\n        assert_eq!(report.neuron_reports.len(), 1);\n    }\n\n    #[test]\n    fn mlp_activation_wave_verification_fails_on_noise() {\n        let n = ModularAdditionDataset::modulus();\n        let signal: Vec<f64> = (0..n).map(|idx| idx as f64 / n as f64).collect();\n\n        let config = MlpWaveVerificationConfig {\n            max_negative_fraction: 0.0,\n            negative_tolerance: 1e-6,\n            min_dominant_ratio: 4.0,\n            min_correlation: 0.8,\n            phase_steps: 16,\n        };\n\n        let err =\n            verify_mlp_activation_waves_from_signals(&[(0, signal)], &config)\n                .expect_err(\"expected linear signal to fail\");\n        assert!(err.contains(\"dominant frequency ratio\") || err.contains(\"correlation\"));\n    }\n\n    #[test]\n    fn pairwise_manifold_transition_passes_on_elliptic_signals() {\n        let n = ModularAdditionDataset::modulus();\n        let freq = 5.0;\n        let mut post_signals = Vec::new();\n        for neuron_idx in 0..7usize {\n            let phase = 2.0 * std::f64::consts::PI * neuron_idx as f64 / 7.0;\n            let signal: Vec<f64> = (0..n)\n                .map(|idx| {\n                    let angle = 2.0 * std::f64::consts::PI * freq * idx as f64 / n as f64 + phase;\n                    angle.sin()\n                })\n                .collect();\n            post_signals.push((neuron_idx, signal));\n        }\n\n        let mut pre_signals = Vec::new();\n        for neuron_idx in 0..7usize {\n            let signal: Vec<f64> = (0..n)\n                .map(|idx| idx as f64 / n as f64 + neuron_idx as f64 * 0.01)\n                .collect();\n            pre_signals.push((neuron_idx, signal));\n        }\n\n        let config = PairwiseManifoldConfig {\n            fixed_y: 0,\n            neuron_indices: (0..7).collect(),\n            min_post_score: 0.75,\n            max_pre_score: 0.55,\n            max_axis_ratio: 8.0,\n            min_pairs_passing: 8,\n        };\n\n        let report = verify_pairwise_manifold_transition_from_signals(\n            &pre_signals,\n            &post_signals,\n            &config,\n        )\n        .expect(\"expected pairwise manifold transition to pass\");\n        assert!(report.passing_pairs >= config.min_pairs_passing);\n    }\n\n    #[test]\n    fn pairwise_manifold_transition_fails_without_rings() {\n        let n = ModularAdditionDataset::modulus();\n        let mut signals = Vec::new();\n        for neuron_idx in 0..7usize {\n            let signal: Vec<f64> = (0..n)\n                .map(|idx| idx as f64 / n as f64 + neuron_idx as f64 * 0.02)\n                .collect();\n            signals.push((neuron_idx, signal));\n        }\n\n        let config = PairwiseManifoldConfig {\n            fixed_y: 0,\n            neuron_indices: (0..7).collect(),\n            min_post_score: 0.8,\n            max_pre_score: 0.55,\n            max_axis_ratio: 4.0,\n            min_pairs_passing: 5,\n        };\n\n        let err = verify_pairwise_manifold_transition_from_signals(\n            &signals,\n            &signals,\n            &config,\n        )\n        .expect_err(\"expected pairwise manifold transition to fail\");\n        assert!(err.contains(\"post-grok\") || err.contains(\"pre-grok\"));\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_passes_on_diagonal_corrugation() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create diagonal ridges: f(x,y) = sin(2Ï€ * freq * (x+y) / n)\n        let freq = 3.0;\n        for x in 0..n {\n            for y in 0..n {\n                let angle = 2.0 * std::f64::consts::PI * freq * (x + y) as f64 / n as f64;\n                surface[x][y] = angle.sin();\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let report = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect(\"expected diagonal ridge verification to pass\");\n        assert!(report.diagonal_score > report.axis_score);\n        assert!(report.ratio >= config.min_diagonal_ratio);\n        assert!(report.diagonal_score >= config.min_diagonal_score);\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_passes_on_anti_diagonal_pattern() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create anti-diagonal ridges: f(x,y) = sin(2Ï€ * freq * (x-y) / n)\n        let freq = 3.0;\n        for x in 0..n {\n            for y in 0..n {\n                let angle = 2.0 * std::f64::consts::PI * freq * (x as i32 - y as i32) as f64 / n as f64;\n                surface[x][y] = angle.sin();\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let report = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect(\"expected anti-diagonal ridge verification to pass\");\n        assert!(report.diagonal_score > report.axis_score);\n        assert!(report.ratio >= config.min_diagonal_ratio);\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_fails_on_axis_aligned_pattern() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create axis-aligned ridges: f(x,y) = sin(2Ï€ * freq * x / n)\n        let freq = 3.0;\n        for x in 0..n {\n            for y in 0..n {\n                let angle = 2.0 * std::f64::consts::PI * freq * x as f64 / n as f64;\n                surface[x][y] = angle.sin();\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let err = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect_err(\"expected axis-aligned pattern to fail diagonal verification\");\n        assert!(err.contains(\"diagonal/axis ratio\") || err.contains(\"diagonal autocorrelation\"));\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_fails_on_flat_surface() {\n        let n = 50;\n        let surface = vec![vec![1.5; n]; n];\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let err = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect_err(\"expected flat surface to fail\");\n        assert!(err.contains(\"surface variance is zero\"));\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_fails_on_random_noise() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create random noise (low autocorrelation everywhere)\n        for x in 0..n {\n            for y in 0..n {\n                surface[x][y] = ((x * 17 + y * 31) % 100) as f64 / 100.0;\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let err = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect_err(\"expected random noise to fail diagonal verification\");\n        assert!(err.contains(\"diagonal/axis ratio\") || err.contains(\"diagonal autocorrelation\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_passes_on_valid_grokking_curve() {\n        // Simulate typical grokking: train converges early, val plateaus then jumps\n        let train_acc = vec![\n            (1, 0.10),\n            (10, 0.50),\n            (100, 0.80),\n            (500, 0.98),\n            (1000, 0.99),\n            (2000, 0.995),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (10, 0.009),\n            (100, 0.01),\n            (500, 0.011),\n            (1000, 0.009),\n            (1500, 0.01),\n            (2000, 0.92),\n            (3000, 0.98),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 1500,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let report = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect(\"expected snake curve verification to pass\");\n\n        assert_eq!(report.train_converged_by_step, 500);\n        assert!(report.val_plateau_max_acc < config.val_plateau_max_acc);\n        assert_eq!(report.val_grok_step, 2000);\n        assert!(report.curve_shape_valid);\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_on_simultaneous_convergence() {\n        // Both train and val converge at the same time (no grokking)\n        let train_acc = vec![\n            (1, 0.10),\n            (100, 0.98),\n            (500, 0.99),\n        ];\n        let val_acc = vec![\n            (1, 0.10),\n            (100, 0.97),\n            (500, 0.98),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 200,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let err = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect_err(\"expected snake curve verification to fail\");\n        assert!(err.contains(\"plateau region too high\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_on_slow_train_convergence() {\n        // Train takes too long to converge (not the typical snake curve)\n        let train_acc = vec![\n            (1, 0.10),\n            (500, 0.50),\n            (1000, 0.70),\n            (2000, 0.98),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (500, 0.01),\n            (1000, 0.01),\n            (2000, 0.92),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 1500,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let err = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect_err(\"expected snake curve verification to fail\");\n        assert!(err.contains(\"converges at step 2000\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_without_val_jump() {\n        // Validation never jumps up (no generalization)\n        let train_acc = vec![\n            (1, 0.10),\n            (100, 0.98),\n            (500, 0.99),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (100, 0.01),\n            (500, 0.01),\n            (2000, 0.02),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 200,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let err = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect_err(\"expected snake curve verification to fail\");\n        assert!(err.contains(\"validation accuracy never reaches\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_on_wrong_phase_ordering() {\n        // Val jumps before train converges (backwards snake)\n        let train_acc = vec![\n            (1, 0.10),\n            (500, 0.50),\n            (2000, 0.98),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (100, 0.95),\n            (500, 0.98),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 200,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let result = verify_snake_curve_shape(&train_acc, &val_acc, &config);\n        // Should fail because val jumps before train converges (not a proper snake curve)\n        // This can fail for multiple reasons, but should definitely fail\n        assert!(result.is_err(), \"Expected snake curve verification to fail but it passed\");\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_passes_on_decreasing_norms() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 85.0),\n            (1000, 72.0),\n            (1500, 63.0),\n            (2000, 58.0),\n            (2500, 55.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let report = verify_weight_norm_decay(&weight_norms, Some(2000), &config)\n            .expect(\"expected weight norm decay verification to pass\");\n\n        assert_eq!(report.initial_norm, 100.0);\n        assert_eq!(report.final_norm, 55.0);\n        assert!((report.relative_decrease - 0.45).abs() < 0.01);\n        assert!(report.grok_step_norm.is_some());\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_fails_on_flat_norms() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 99.0),\n            (1000, 98.5),\n            (1500, 98.0),\n            (2000, 97.5),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let err = verify_weight_norm_decay(&weight_norms, None, &config)\n            .expect_err(\"expected weight norm decay verification to fail\");\n        assert!(err.contains(\"weight norm decrease\"));\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_fails_on_increasing_norms() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 95.0),\n            (1000, 105.0), // Increase here\n            (1500, 90.0),\n            (2000, 85.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let err = verify_weight_norm_decay(&weight_norms, None, &config)\n            .expect_err(\"expected weight norm decay verification to fail\");\n        assert!(err.contains(\"weight norm increased\"));\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_allows_small_noise() {\n        // Weight decay with small numerical noise (within 1% tolerance)\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 95.0),\n            (1000, 90.5),  // Small increase within tolerance\n            (1500, 90.0),\n            (2000, 85.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let report = verify_weight_norm_decay(&weight_norms, None, &config)\n            .expect(\"expected weight norm decay with small noise to pass\");\n\n        assert_eq!(report.initial_norm, 100.0);\n        assert_eq!(report.final_norm, 85.0);\n        assert!((report.relative_decrease - 0.15).abs() < 0.01);\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_finds_grok_step_norm() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (1000, 80.0),\n            (1800, 68.0),  // Within window of grok step (2200 - 500 = 1700)\n            (2000, 65.0),\n            (2200, 62.0),  // Around grok step\n            (3000, 55.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let report = verify_weight_norm_decay(&weight_norms, Some(2200), &config)\n            .expect(\"expected weight norm decay verification to pass\");\n\n        assert!(report.grok_step_norm.is_some());\n        let grok_norm = report.grok_step_norm.unwrap();\n        // Should find the first norm in the window [1700, 2700], which is 68.0 at step 1800\n        assert!((grok_norm - 68.0).abs() < 0.01);\n    }\n}\n","structuredPatch":[{"oldStart":1001,"oldLines":6,"newStart":1001,"newLines":7,"lines":["     Ok((height, width))"," }"," ","+#[allow(dead_code)]"," fn surface_mean_variance(surface: &[Vec<f64>]) -> Result<(f64, f64), String> {","     let (height, width) = surface_dimensions(surface)?;","     let mut sum = 0.0;"]}],"userModified":false,"replaceAll":false}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_014NagqF26mQxEiUDg1CiTRN","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01N79aPSKsubyZ49U6gH1Fgt","name":"Edit","input":{"replace_all":false,"file_path":"/Users/admin/MechInt/src/verify.rs","old_string":"fn average_autocorrelation(","new_string":"#[allow(dead_code)]\nfn average_autocorrelation("}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":376,"cache_read_input_tokens":63417,"cache_creation":{"ephemeral_5m_input_tokens":376,"ephemeral_1h_input_tokens":0},"output_tokens":146,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"aa4e58a9-98cf-4b44-8415-4f7428ff160d"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01N79aPSKsubyZ49U6gH1Fgt","type":"tool_result","content":"The file /Users/admin/MechInt/src/verify.rs has been updated. Here's the result of running `cat -n` on a snippet of the edited file:\n  1022â†’    variance /= count;\n  1023â†’    Ok((mean, variance))\n  1024â†’}\n  1025â†’\n  1026â†’#[allow(dead_code)]\n  1027â†’fn average_autocorrelation(\n  1028â†’    surface: &[Vec<f64>],\n  1029â†’    mean: f64,\n  1030â†’    variance: f64,\n  1031â†’    max_lag: usize,"}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"fe035cb2-3ab6-49b2-b09c-1b84ea57c4a5","tool_use_result":{"filePath":"/Users/admin/MechInt/src/verify.rs","oldString":"fn average_autocorrelation(","newString":"#[allow(dead_code)]\nfn average_autocorrelation(","originalFile":"use burn::{\n    data::dataset::Dataset,\n    tensor::{backend::Backend, Int, Tensor},\n};\nuse rustfft::{num_complex::Complex, FftPlanner};\n\nuse crate::analysis::{AccuracyHistory, LossHistory};\nuse crate::data::ModularAdditionDataset;\nuse crate::model::Transformer;\n\n#[derive(Debug, Clone)]\npub struct GrokkingVerificationConfig {\n    pub early_train_acc_threshold: f32,\n    pub early_step_max: usize,\n    pub plateau_min_step: usize,\n    pub generalization_window: usize,\n    pub chance_accuracy: f32,\n    pub chance_tolerance: f32,\n    pub target_val_acc_threshold: f32,\n    pub accuracy_window: usize,\n    pub loss_window: usize,\n    pub loss_drop_fraction: f64,\n}\n\nimpl GrokkingVerificationConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        Self {\n            early_train_acc_threshold: 0.99,\n            early_step_max: 1000,\n            plateau_min_step: 1000,\n            generalization_window: 10_000,\n            chance_accuracy: 1.0 / modulus as f32,\n            chance_tolerance: 0.02,\n            target_val_acc_threshold: 0.99,\n            accuracy_window: 5,\n            loss_window: 5,\n            loss_drop_fraction: 0.5,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct GrokkingPhaseReport {\n    pub early_train_step: usize,\n    pub transition_step: usize,\n    pub plateau_max_val_acc: f32,\n    pub loss_drop_ratio: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossVerificationConfig {\n    pub plateau_min_step: usize,\n    pub drop_window: usize,\n    pub drop_fraction: f64,\n    pub min_step_lead: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossReport {\n    pub restricted_drop_step: usize,\n    pub full_drop_step: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossDropConfig {\n    pub min_relative_decrease: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossDropReport {\n    pub drop_step: usize,\n    pub drop_loss: f64,\n    pub baseline_loss: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct ExcludedLossSpikeConfig {\n    pub min_relative_increase: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct ExcludedLossSpikeReport {\n    pub spike_step: usize,\n    pub spike_loss: f64,\n    pub baseline_loss: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct MlpWaveVerificationConfig {\n    pub max_negative_fraction: f64,\n    pub negative_tolerance: f64,\n    pub min_dominant_ratio: f64,\n    pub min_correlation: f64,\n    pub phase_steps: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct MlpWaveNeuronReport {\n    pub neuron_index: usize,\n    pub dominant_frequency: usize,\n    pub dominant_ratio: f64,\n    pub negative_fraction: f64,\n    pub best_correlation: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct MlpWaveReport {\n    pub neuron_reports: Vec<MlpWaveNeuronReport>,\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldConfig {\n    pub fixed_y: usize,\n    pub neuron_indices: Vec<usize>,\n    pub min_post_score: f64,\n    pub max_pre_score: f64,\n    pub max_axis_ratio: f64,\n    pub min_pairs_passing: usize,\n}\n\nimpl PairwiseManifoldConfig {\n    pub fn default_for_modulus(_modulus: usize) -> Self {\n        Self {\n            fixed_y: 0,\n            neuron_indices: (0..7).collect(),\n            min_post_score: 0.75,\n            max_pre_score: 0.55,\n            max_axis_ratio: 8.0,\n            min_pairs_passing: 10,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldPairReport {\n    pub neuron_a: usize,\n    pub neuron_b: usize,\n    pub circularity_score: f64,\n    pub radius_cv: f64,\n    pub axis_ratio: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldReport {\n    pub pair_reports: Vec<PairwiseManifoldPairReport>,\n    pub average_score: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldTransitionReport {\n    pub pre_average_score: f64,\n    pub post_average_score: f64,\n    pub passing_pairs: usize,\n    pub total_pairs: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct DiagonalRidgeConfig {\n    pub neuron_index: usize,\n    pub max_lag: usize,\n    pub min_diagonal_ratio: f64,\n    pub min_diagonal_score: f64,\n}\n\nimpl DiagonalRidgeConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        let max_lag = (modulus / 10).max(4).min(20);\n        Self {\n            neuron_index: 10,\n            max_lag,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct DiagonalRidgeReport {\n    pub neuron_index: usize,\n    pub diagonal_score: f64,\n    pub axis_score: f64,\n    pub ratio: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct SnakeCurveConfig {\n    pub early_step_max: usize,\n    pub train_convergence_threshold: f32,\n    pub val_plateau_min_step: usize,\n    pub val_plateau_max_acc: f32,\n    pub val_final_threshold: f32,\n}\n\nimpl SnakeCurveConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        Self {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 1000,\n            val_plateau_max_acc: 1.0 / modulus as f32 + 0.05,\n            val_final_threshold: 0.90,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct WeightNormDecayConfig {\n    pub min_relative_decrease: f64,\n    pub plateau_min_step: usize,\n    pub grok_step_window: usize,\n}\n\nimpl WeightNormDecayConfig {\n    pub fn default_for_grokking() -> Self {\n        Self {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct WeightNormDecayReport {\n    pub initial_norm: f64,\n    pub final_norm: f64,\n    pub relative_decrease: f64,\n    pub grok_step_norm: Option<f64>,\n}\n\n#[derive(Debug, Clone)]\npub struct SnakeCurveReport {\n    pub train_converged_by_step: usize,\n    pub val_plateau_max_acc: f32,\n    pub val_grok_step: usize,\n    pub curve_shape_valid: bool,\n}\n\npub fn verify_grokking_phase_transition(\n    loss_history: &LossHistory,\n    accuracy_history: &AccuracyHistory,\n    config: &GrokkingVerificationConfig,\n) -> Result<GrokkingPhaseReport, String> {\n    let (early_train_step, _) = accuracy_history\n        .train_snapshots\n        .iter()\n        .find(|(_, acc)| *acc >= config.early_train_acc_threshold)\n        .ok_or_else(|| \"train accuracy never reaches threshold\".to_string())?;\n\n    if *early_train_step > config.early_step_max {\n        return Err(format!(\n            \"train accuracy reaches {:.2} after step {}, expected before {}\",\n            config.early_train_acc_threshold, early_train_step, config.early_step_max\n        ));\n    }\n\n    let plateau_max_val_acc = accuracy_history\n        .val_snapshots\n        .iter()\n        .filter(|(step, _)| *step <= config.plateau_min_step)\n        .map(|(_, acc)| *acc)\n        .fold(None::<f32>, |accum, value| {\n            Some(accum.map_or(value, |max_val| max_val.max(value)))\n        })\n        .ok_or_else(|| \"no validation accuracy samples before plateau_min_step\".to_string())?;\n\n    if plateau_max_val_acc > config.chance_accuracy + config.chance_tolerance {\n        return Err(format!(\n            \"validation accuracy before step {} exceeds chance (max {:.4})\",\n            config.plateau_min_step, plateau_max_val_acc\n        ));\n    }\n\n    let (transition_step, _) = accuracy_history\n        .val_snapshots\n        .iter()\n        .find(|(step, acc)| {\n            *step >= config.plateau_min_step && *acc >= config.target_val_acc_threshold\n        })\n        .ok_or_else(|| \"validation accuracy never reaches target threshold\".to_string())?;\n\n    let latest_allowed = config.plateau_min_step + config.generalization_window;\n    if *transition_step > latest_allowed {\n        return Err(format!(\n            \"validation accuracy reaches {:.2} at step {}, expected before {}\",\n            config.target_val_acc_threshold, transition_step, latest_allowed\n        ));\n    }\n\n    verify_accuracy_jump(\n        &accuracy_history.val_snapshots,\n        *transition_step,\n        config.accuracy_window,\n        config.chance_accuracy,\n        config.chance_tolerance,\n        config.target_val_acc_threshold,\n    )?;\n\n    let loss_drop_ratio = verify_loss_drop(\n        &loss_history.val_snapshots,\n        *transition_step,\n        config.loss_window,\n        config.loss_drop_fraction,\n    )?;\n\n    Ok(GrokkingPhaseReport {\n        early_train_step: *early_train_step,\n        transition_step: *transition_step,\n        plateau_max_val_acc,\n        loss_drop_ratio,\n    })\n}\n\nfn verify_accuracy_jump(\n    val_accuracy: &[(usize, f32)],\n    transition_step: usize,\n    window: usize,\n    chance_accuracy: f32,\n    chance_tolerance: f32,\n    target_val_acc_threshold: f32,\n) -> Result<(), String> {\n    let before_values: Vec<f32> = val_accuracy\n        .iter()\n        .filter(|(step, _)| *step < transition_step)\n        .map(|(_, acc)| *acc)\n        .collect();\n\n    let after_values: Vec<f32> = val_accuracy\n        .iter()\n        .filter(|(step, _)| *step >= transition_step)\n        .map(|(_, acc)| *acc)\n        .collect();\n\n    if before_values.len() < window || after_values.len() < window {\n        return Err(\"insufficient accuracy samples around transition\".to_string());\n    }\n\n    let before_avg: f32 =\n        before_values[before_values.len() - window..].iter().sum::<f32>() / window as f32;\n    let after_avg: f32 = after_values[..window].iter().sum::<f32>() / window as f32;\n\n    if before_avg > chance_accuracy + chance_tolerance {\n        return Err(format!(\n            \"validation accuracy plateau too high before transition (avg {:.4})\",\n            before_avg\n        ));\n    }\n\n    if after_avg < target_val_acc_threshold {\n        return Err(format!(\n            \"validation accuracy after transition too low (avg {:.4})\",\n            after_avg\n        ));\n    }\n\n    Ok(())\n}\n\nfn verify_loss_drop(\n    val_losses: &[(usize, f64)],\n    transition_step: usize,\n    loss_window: usize,\n    loss_drop_fraction: f64,\n) -> Result<f64, String> {\n    let before_losses: Vec<f64> = val_losses\n        .iter()\n        .filter(|(step, _)| *step < transition_step)\n        .map(|(_, loss)| *loss)\n        .collect();\n\n    let after_losses: Vec<f64> = val_losses\n        .iter()\n        .filter(|(step, _)| *step >= transition_step)\n        .map(|(_, loss)| *loss)\n        .collect();\n\n    if before_losses.len() < loss_window || after_losses.len() < loss_window {\n        return Err(\"insufficient loss samples around transition\".to_string());\n    }\n\n    let before_avg: f64 =\n        before_losses[before_losses.len() - loss_window..].iter().sum::<f64>()\n            / loss_window as f64;\n    let after_avg: f64 = after_losses[..loss_window].iter().sum::<f64>() / loss_window as f64;\n\n    if after_avg > before_avg * (1.0 - loss_drop_fraction) {\n        return Err(format!(\n            \"validation loss drop too small (before {:.4}, after {:.4})\",\n            before_avg, after_avg\n        ));\n    }\n\n    Ok(after_avg / before_avg)\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_restricted_loss_early_drop(\n    full_losses: &[(usize, f64)],\n    restricted_losses: &[(usize, f64)],\n    config: &RestrictedLossVerificationConfig,\n) -> Result<RestrictedLossReport, String> {\n    let restricted_baseline =\n        baseline_loss(restricted_losses, config.plateau_min_step, config.drop_window)?;\n    let full_baseline = baseline_loss(full_losses, config.plateau_min_step, config.drop_window)?;\n\n    let restricted_drop_step = find_drop_step(\n        restricted_losses,\n        config.plateau_min_step,\n        restricted_baseline,\n        config.drop_fraction,\n    )?;\n    let full_drop_step = find_drop_step(\n        full_losses,\n        config.plateau_min_step,\n        full_baseline,\n        config.drop_fraction,\n    )?;\n\n    if restricted_drop_step + config.min_step_lead > full_drop_step {\n        return Err(format!(\n            \"restricted loss drops at step {}, expected at least {} steps before full loss (step {})\",\n            restricted_drop_step, config.min_step_lead, full_drop_step\n        ));\n    }\n\n    Ok(RestrictedLossReport {\n        restricted_drop_step,\n        full_drop_step,\n    })\n}\n\npub fn verify_restricted_loss_drop(\n    restricted_losses: &[(usize, f64)],\n    config: &RestrictedLossDropConfig,\n) -> Result<RestrictedLossDropReport, String> {\n    if restricted_losses.len() < 3 {\n        return Err(\"restricted loss history requires at least 3 points\".to_string());\n    }\n\n    let first = restricted_losses.first().ok_or_else(|| {\n        \"restricted loss history is empty\".to_string()\n    })?;\n    let baseline_loss = first.1;\n\n    let (min_idx, (min_step, min_loss)) = restricted_losses\n        .iter()\n        .enumerate()\n        .min_by(|(_, a), (_, b)| {\n            a.1.partial_cmp(&b.1).unwrap_or(std::cmp::Ordering::Equal)\n        })\n        .ok_or_else(|| \"restricted loss history is empty\".to_string())?;\n\n    if min_idx == 0 {\n        return Err(\"restricted loss minimum occurs at start (no drop detected)\".to_string());\n    }\n\n    let relative_decrease = (baseline_loss - min_loss) / baseline_loss;\n    if relative_decrease < config.min_relative_decrease {\n        return Err(format!(\n            \"restricted loss decrease too small (baseline {:.4}, min {:.4}, decrease {:.1}%)\",\n            baseline_loss, min_loss, relative_decrease * 100.0\n        ));\n    }\n\n    Ok(RestrictedLossDropReport {\n        drop_step: *min_step,\n        drop_loss: *min_loss,\n        baseline_loss,\n    })\n}\n\npub fn verify_excluded_loss_spike(\n    excluded_losses: &[(usize, f64)],\n    config: &ExcludedLossSpikeConfig,\n) -> Result<ExcludedLossSpikeReport, String> {\n    if excluded_losses.len() < 3 {\n        return Err(\"excluded loss history requires at least 3 points\".to_string());\n    }\n\n    let first = excluded_losses.first().ok_or_else(|| {\n        \"excluded loss history is empty\".to_string()\n    })?;\n    let last = excluded_losses.last().ok_or_else(|| {\n        \"excluded loss history is empty\".to_string()\n    })?;\n    let baseline_loss = (first.1 + last.1) / 2.0;\n\n    let (peak_idx, (peak_step, peak_loss)) = excluded_losses\n        .iter()\n        .enumerate()\n        .max_by(|(_, a), (_, b)| {\n            a.1.partial_cmp(&b.1).unwrap_or(std::cmp::Ordering::Equal)\n        })\n        .ok_or_else(|| \"excluded loss history is empty\".to_string())?;\n\n    if peak_idx == 0 || peak_idx + 1 == excluded_losses.len() {\n        return Err(\"excluded loss spike occurs at boundary\".to_string());\n    }\n\n    if peak_loss < &(baseline_loss * (1.0 + config.min_relative_increase)) {\n        return Err(format!(\n            \"excluded loss spike too small (baseline {:.4}, peak {:.4})\",\n            baseline_loss, peak_loss\n        ));\n    }\n\n    Ok(ExcludedLossSpikeReport {\n        spike_step: *peak_step,\n        spike_loss: *peak_loss,\n        baseline_loss,\n    })\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_mlp_activation_waves<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n    fixed_y: usize,\n    neuron_indices: &[usize],\n    config: &MlpWaveVerificationConfig,\n) -> Result<MlpWaveReport, String> {\n    let activations = collect_mlp_post_relu_activations(model, device, fixed_y)?;\n    if neuron_indices.is_empty() {\n        return Err(\"neuron indices list is empty\".to_string());\n    }\n\n    let d_ff = activations.len();\n    for &idx in neuron_indices {\n        if idx >= d_ff {\n            return Err(format!(\n                \"neuron index {} out of range (d_ff={})\",\n                idx, d_ff\n            ));\n        }\n    }\n\n    let signals: Vec<(usize, Vec<f64>)> = neuron_indices\n        .iter()\n        .map(|&idx| (idx, activations[idx].clone()))\n        .collect();\n    verify_mlp_activation_waves_from_signals(&signals, config)\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_mlp_activation_waves_from_signals(\n    signals: &[(usize, Vec<f64>)],\n    config: &MlpWaveVerificationConfig,\n) -> Result<MlpWaveReport, String> {\n    if signals.is_empty() {\n        return Err(\"no activation signals provided\".to_string());\n    }\n\n    let mut reports = Vec::with_capacity(signals.len());\n    for (neuron_index, signal) in signals {\n        let report = analyze_mlp_activation_wave(*neuron_index, signal, config)?;\n        reports.push(report);\n    }\n\n    Ok(MlpWaveReport {\n        neuron_reports: reports,\n    })\n}\n\npub fn verify_mlp_pairwise_manifold_transition<B: Backend>(\n    pre_model: &Transformer<B>,\n    post_model: &Transformer<B>,\n    device: &B::Device,\n    config: &PairwiseManifoldConfig,\n) -> Result<PairwiseManifoldTransitionReport, String> {\n    let modulus = ModularAdditionDataset::modulus();\n    if config.fixed_y >= modulus {\n        return Err(format!(\n            \"fixed_y {} out of range (modulus {})\",\n            config.fixed_y, modulus\n        ));\n    }\n\n    let pre_activations = collect_mlp_post_relu_activations(pre_model, device, config.fixed_y)?;\n    let post_activations = collect_mlp_post_relu_activations(post_model, device, config.fixed_y)?;\n\n    let d_ff = pre_activations.len();\n    if post_activations.len() != d_ff {\n        return Err(\"post activations shape mismatch\".to_string());\n    }\n\n    if config.neuron_indices.len() < 2 {\n        return Err(\"need at least two neuron indices\".to_string());\n    }\n\n    for &idx in &config.neuron_indices {\n        if idx >= d_ff {\n            return Err(format!(\n                \"neuron index {} out of range (d_ff={})\",\n                idx, d_ff\n            ));\n        }\n    }\n\n    let pre_signals: Vec<(usize, Vec<f64>)> = config\n        .neuron_indices\n        .iter()\n        .map(|&idx| (idx, pre_activations[idx].clone()))\n        .collect();\n    let post_signals: Vec<(usize, Vec<f64>)> = config\n        .neuron_indices\n        .iter()\n        .map(|&idx| (idx, post_activations[idx].clone()))\n        .collect();\n\n    verify_pairwise_manifold_transition_from_signals(&pre_signals, &post_signals, config)\n}\n\npub fn collect_mlp_activation_surface<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n    neuron_index: usize,\n) -> Result<Vec<Vec<f64>>, String> {\n    let modulus = ModularAdditionDataset::modulus();\n    let equals_token = ModularAdditionDataset::equals_token();\n\n    let mut inputs_vec = Vec::with_capacity(modulus * modulus * 3);\n    for x in 0..modulus {\n        for y in 0..modulus {\n            inputs_vec.push(x as i32);\n            inputs_vec.push(y as i32);\n            inputs_vec.push(equals_token as i32);\n        }\n    }\n\n    let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n        .reshape([modulus * modulus, 3]);\n    let (_, mlp_acts) = model.forward_with_mlp_activations(inputs);\n    let [batch_size, d_ff] = mlp_acts.dims();\n    if batch_size != modulus * modulus {\n        return Err(format!(\n            \"unexpected activation batch size {} (expected {})\",\n            batch_size,\n            modulus * modulus\n        ));\n    }\n    if neuron_index >= d_ff {\n        return Err(format!(\n            \"neuron index {} out of range (d_ff={})\",\n            neuron_index, d_ff\n        ));\n    }\n\n    let data: Vec<f32> = mlp_acts.into_data().to_vec().unwrap();\n    let mut surface = vec![vec![0.0f64; modulus]; modulus];\n    for x in 0..modulus {\n        for y in 0..modulus {\n            let batch = x * modulus + y;\n            let idx = batch * d_ff + neuron_index;\n            surface[x][y] = data[idx] as f64;\n        }\n    }\n\n    Ok(surface)\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_constructive_interference_surface_from_grid(\n    surface: &[Vec<f64>],\n    config: &DiagonalRidgeConfig,\n) -> Result<DiagonalRidgeReport, String> {\n    let (diag_main, diag_anti, axis_x, axis_y) =\n        surface_autocorrelation_scores(surface, config.max_lag)?;\n\n    let diagonal_score = diag_main.max(diag_anti);\n    let axis_score = axis_x.max(axis_y);\n    let ratio = if axis_score > 0.0 {\n        diagonal_score / axis_score\n    } else {\n        f64::INFINITY\n    };\n\n    if diagonal_score < config.min_diagonal_score {\n        return Err(format!(\n            \"diagonal autocorrelation {:.3} below threshold {:.3}\",\n            diagonal_score, config.min_diagonal_score\n        ));\n    }\n\n    if ratio < config.min_diagonal_ratio {\n        return Err(format!(\n            \"diagonal/axis ratio {:.3} below threshold {:.3}\",\n            ratio, config.min_diagonal_ratio\n        ));\n    }\n\n    Ok(DiagonalRidgeReport {\n        neuron_index: config.neuron_index,\n        diagonal_score,\n        axis_score,\n        ratio,\n    })\n}\n\npub fn verify_pairwise_manifold_transition_from_signals(\n    pre_signals: &[(usize, Vec<f64>)],\n    post_signals: &[(usize, Vec<f64>)],\n    config: &PairwiseManifoldConfig,\n) -> Result<PairwiseManifoldTransitionReport, String> {\n    let pre_report = evaluate_pairwise_manifolds_from_signals(pre_signals)?;\n    let post_report = evaluate_pairwise_manifolds_from_signals(post_signals)?;\n\n    if pre_report.average_score > config.max_pre_score {\n        return Err(format!(\n            \"pre-grok average circularity {:.3} exceeds threshold {:.3}\",\n            pre_report.average_score, config.max_pre_score\n        ));\n    }\n\n    let passing_pairs = post_report\n        .pair_reports\n        .iter()\n        .filter(|pair| {\n            pair.circularity_score >= config.min_post_score\n                && pair.axis_ratio <= config.max_axis_ratio\n        })\n        .count();\n    if passing_pairs < config.min_pairs_passing {\n        return Err(format!(\n            \"post-grok only {} pairs meet circularity threshold (need {})\",\n            passing_pairs, config.min_pairs_passing\n        ));\n    }\n\n    Ok(PairwiseManifoldTransitionReport {\n        pre_average_score: pre_report.average_score,\n        post_average_score: post_report.average_score,\n        passing_pairs,\n        total_pairs: post_report.pair_reports.len(),\n    })\n}\n\nfn evaluate_pairwise_manifolds_from_signals(\n    signals: &[(usize, Vec<f64>)],\n) -> Result<PairwiseManifoldReport, String> {\n    if signals.len() < 2 {\n        return Err(\"need at least two activation signals\".to_string());\n    }\n\n    let signal_len = signals[0].1.len();\n    if signal_len < 3 {\n        return Err(\"activation signal too short\".to_string());\n    }\n\n    for (idx, signal) in signals {\n        if signal.len() != signal_len {\n            return Err(format!(\n                \"signal length mismatch for neuron {}\",\n                idx\n            ));\n        }\n    }\n\n    let mut reports = Vec::new();\n    for i in 0..signals.len() {\n        for j in (i + 1)..signals.len() {\n            let (neuron_a, signal_a) = &signals[i];\n            let (neuron_b, signal_b) = &signals[j];\n            let points: Vec<(f64, f64)> = signal_a\n                .iter()\n                .zip(signal_b.iter())\n                .map(|(x, y)| (*x, *y))\n                .collect();\n            let (score, radius_cv, axis_ratio) =\n                match pairwise_circularity_metrics(&points) {\n                    Ok(metrics) => metrics,\n                    Err(_) => (0.0, 1.0, f64::INFINITY),\n                };\n            reports.push(PairwiseManifoldPairReport {\n                neuron_a: *neuron_a,\n                neuron_b: *neuron_b,\n                circularity_score: score,\n                radius_cv,\n                axis_ratio,\n            });\n        }\n    }\n\n    let average_score = reports\n        .iter()\n        .map(|pair| pair.circularity_score)\n        .sum::<f64>()\n        / reports.len() as f64;\n\n    Ok(PairwiseManifoldReport {\n        pair_reports: reports,\n        average_score,\n    })\n}\n\nfn pairwise_circularity_metrics(points: &[(f64, f64)]) -> Result<(f64, f64, f64), String> {\n    if points.len() < 3 {\n        return Err(\"not enough points for circularity\".to_string());\n    }\n\n    let n = points.len() as f64;\n    let mean_x = points.iter().map(|(x, _)| *x).sum::<f64>() / n;\n    let mean_y = points.iter().map(|(_, y)| *y).sum::<f64>() / n;\n\n    let mut var_x = 0.0;\n    let mut var_y = 0.0;\n    let mut cov_xy = 0.0;\n    for (x, y) in points {\n        let dx = x - mean_x;\n        let dy = y - mean_y;\n        var_x += dx * dx;\n        var_y += dy * dy;\n        cov_xy += dx * dy;\n    }\n    var_x /= n;\n    var_y /= n;\n    cov_xy /= n;\n\n    let trace = var_x + var_y;\n    let det = var_x * var_y - cov_xy * cov_xy;\n    if det <= 1.0e-12 || trace <= 0.0 {\n        return Err(\"degenerate covariance\".to_string());\n    }\n\n    let half_trace = trace * 0.5;\n    let disc = (half_trace * half_trace - det).max(0.0).sqrt();\n    let lambda1 = half_trace + disc;\n    let lambda2 = half_trace - disc;\n    if lambda2 <= 0.0 {\n        return Err(\"degenerate eigenvalues\".to_string());\n    }\n\n    let axis_ratio = (lambda1 / lambda2).sqrt();\n\n    let mut v1_x = cov_xy;\n    let mut v1_y = lambda1 - var_x;\n    if v1_x.abs() < 1.0e-12 && v1_y.abs() < 1.0e-12 {\n        v1_x = 1.0;\n        v1_y = 0.0;\n    }\n    let norm = (v1_x * v1_x + v1_y * v1_y).sqrt();\n    v1_x /= norm;\n    v1_y /= norm;\n    let v2_x = -v1_y;\n    let v2_y = v1_x;\n\n    let mut radii = Vec::with_capacity(points.len());\n    for (x, y) in points {\n        let dx = x - mean_x;\n        let dy = y - mean_y;\n        let p1 = dx * v1_x + dy * v1_y;\n        let p2 = dx * v2_x + dy * v2_y;\n        let w1 = p1 / lambda1.sqrt();\n        let w2 = p2 / lambda2.sqrt();\n        radii.push((w1 * w1 + w2 * w2).sqrt());\n    }\n\n    let mean_radius = radii.iter().sum::<f64>() / radii.len() as f64;\n    if mean_radius == 0.0 {\n        return Err(\"zero mean radius\".to_string());\n    }\n\n    let mut variance = 0.0;\n    for r in &radii {\n        let diff = r - mean_radius;\n        variance += diff * diff;\n    }\n    let std = (variance / radii.len() as f64).sqrt();\n    let radius_cv = std / mean_radius;\n    let score = 1.0 / (1.0 + radius_cv);\n\n    Ok((score, radius_cv, axis_ratio))\n}\n\n// Helper functions for verification - reserved for future integration\n#[allow(dead_code)]\nfn baseline_loss(\n    losses: &[(usize, f64)],\n    plateau_min_step: usize,\n    drop_window: usize,\n) -> Result<f64, String> {\n    let before: Vec<f64> = losses\n        .iter()\n        .filter(|(step, _)| *step <= plateau_min_step)\n        .map(|(_, loss)| *loss)\n        .collect();\n\n    if before.len() < drop_window || drop_window == 0 {\n        return Err(\"insufficient loss samples for baseline\".to_string());\n    }\n\n    let window = &before[before.len() - drop_window..];\n    Ok(window.iter().sum::<f64>() / drop_window as f64)\n}\n\n#[allow(dead_code)]\nfn find_drop_step(\n    losses: &[(usize, f64)],\n    plateau_min_step: usize,\n    baseline: f64,\n    drop_fraction: f64,\n) -> Result<usize, String> {\n    losses\n        .iter()\n        .filter(|(step, _)| *step >= plateau_min_step)\n        .find(|(_, loss)| *loss <= baseline * (1.0 - drop_fraction))\n        .map(|(step, _)| *step)\n        .ok_or_else(|| \"loss never drops below threshold\".to_string())\n}\n\nfn collect_mlp_post_relu_activations<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n    fixed_y: usize,\n) -> Result<Vec<Vec<f64>>, String> {\n    let modulus = ModularAdditionDataset::modulus();\n    let equals_token = ModularAdditionDataset::equals_token();\n\n    let mut inputs_vec = Vec::with_capacity(modulus * 3);\n    for x in 0..modulus {\n        inputs_vec.push(x as i32);\n        inputs_vec.push(fixed_y as i32);\n        inputs_vec.push(equals_token as i32);\n    }\n\n    let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n        .reshape([modulus, 3]);\n    let (_, mlp_acts) = model.forward_with_mlp_activations(inputs);\n    let [batch_size, d_ff] = mlp_acts.dims();\n\n    if batch_size != modulus {\n        return Err(format!(\n            \"unexpected activation batch size {} (expected {})\",\n            batch_size, modulus\n        ));\n    }\n\n    let data: Vec<f32> = mlp_acts.into_data().to_vec().unwrap();\n    let mut per_neuron = vec![Vec::with_capacity(batch_size); d_ff];\n    for batch in 0..batch_size {\n        for neuron in 0..d_ff {\n            let idx = batch * d_ff + neuron;\n            per_neuron[neuron].push(data[idx] as f64);\n        }\n    }\n\n    Ok(per_neuron)\n}\n\n#[allow(dead_code)]\nfn surface_autocorrelation_scores(\n    surface: &[Vec<f64>],\n    max_lag: usize,\n) -> Result<(f64, f64, f64, f64), String> {\n    let (height, width) = surface_dimensions(surface)?;\n    if height < 2 || width < 2 {\n        return Err(\"surface grid too small\".to_string());\n    }\n\n    let (mean, var) = surface_mean_variance(surface)?;\n    if var == 0.0 {\n        return Err(\"surface variance is zero\".to_string());\n    }\n\n    let max_lag = max_lag.min(height.saturating_sub(1)).min(width.saturating_sub(1));\n    if max_lag == 0 {\n        return Err(\"max_lag too small for surface\".to_string());\n    }\n\n    let diag_main = average_autocorrelation(surface, mean, var, max_lag, 1, 1)?;\n    let diag_anti = average_autocorrelation(surface, mean, var, max_lag, 1, -1)?;\n    let axis_x = average_autocorrelation(surface, mean, var, max_lag, 1, 0)?;\n    let axis_y = average_autocorrelation(surface, mean, var, max_lag, 0, 1)?;\n\n    Ok((diag_main, diag_anti, axis_x, axis_y))\n}\n\n#[allow(dead_code)]\nfn surface_dimensions(surface: &[Vec<f64>]) -> Result<(usize, usize), String> {\n    let height = surface.len();\n    if height == 0 {\n        return Err(\"surface is empty\".to_string());\n    }\n    let width = surface[0].len();\n    if width == 0 {\n        return Err(\"surface has empty rows\".to_string());\n    }\n    for (idx, row) in surface.iter().enumerate() {\n        if row.len() != width {\n            return Err(format!(\n                \"surface row {} has length {}, expected {}\",\n                idx,\n                row.len(),\n                width\n            ));\n        }\n    }\n    Ok((height, width))\n}\n\n#[allow(dead_code)]\nfn surface_mean_variance(surface: &[Vec<f64>]) -> Result<(f64, f64), String> {\n    let (height, width) = surface_dimensions(surface)?;\n    let mut sum = 0.0;\n    for row in surface {\n        for value in row {\n            sum += *value;\n        }\n    }\n    let count = (height * width) as f64;\n    let mean = sum / count;\n    let mut variance = 0.0;\n    for row in surface {\n        for value in row {\n            let diff = *value - mean;\n            variance += diff * diff;\n        }\n    }\n    variance /= count;\n    Ok((mean, variance))\n}\n\nfn average_autocorrelation(\n    surface: &[Vec<f64>],\n    mean: f64,\n    variance: f64,\n    max_lag: usize,\n    dx: i32,\n    dy: i32,\n) -> Result<f64, String> {\n    let (height, width) = surface_dimensions(surface)?;\n    let mut total = 0.0;\n    let mut lag_count = 0usize;\n\n    for lag in 1..=max_lag {\n        let mut sum = 0.0;\n        let mut count = 0usize;\n        let lag = lag as i32;\n        for x in 0..height {\n            for y in 0..width {\n                let x2 = x as i32 + dx * lag;\n                let y2 = y as i32 + dy * lag;\n                if x2 < 0 || y2 < 0 {\n                    continue;\n                }\n                let x2 = x2 as usize;\n                let y2 = y2 as usize;\n                if x2 >= height || y2 >= width {\n                    continue;\n                }\n                let a = surface[x][y] - mean;\n                let b = surface[x2][y2] - mean;\n                sum += a * b;\n                count += 1;\n            }\n        }\n        if count == 0 {\n            continue;\n        }\n        let corr = sum / (count as f64 * variance);\n        total += corr.abs();\n        lag_count += 1;\n    }\n\n    if lag_count == 0 {\n        return Err(\"no valid lags for autocorrelation\".to_string());\n    }\n\n    Ok(total / lag_count as f64)\n}\n\nfn analyze_mlp_activation_wave(\n    neuron_index: usize,\n    signal: &[f64],\n    config: &MlpWaveVerificationConfig,\n) -> Result<MlpWaveNeuronReport, String> {\n    if signal.len() < 3 {\n        return Err(format!(\n            \"activation signal for neuron {} is too short\",\n            neuron_index\n        ));\n    }\n\n    let negative_count = signal\n        .iter()\n        .filter(|value| **value < -config.negative_tolerance)\n        .count();\n    let negative_fraction = negative_count as f64 / signal.len() as f64;\n    if negative_fraction > config.max_negative_fraction {\n        return Err(format!(\n            \"neuron {} has {:.3} negative activations\",\n            neuron_index, negative_fraction\n        ));\n    }\n\n    let (dominant_frequency, dominant_ratio) = dominant_frequency_ratio(signal)?;\n    if dominant_ratio < config.min_dominant_ratio {\n        return Err(format!(\n            \"neuron {} dominant frequency ratio {:.3} below threshold {:.3}\",\n            neuron_index, dominant_ratio, config.min_dominant_ratio\n        ));\n    }\n\n    let best_correlation =\n        best_rectified_sine_correlation(signal, dominant_frequency, config.phase_steps);\n    if best_correlation < config.min_correlation {\n        return Err(format!(\n            \"neuron {} rectified sine correlation {:.3} below threshold {:.3}\",\n            neuron_index, best_correlation, config.min_correlation\n        ));\n    }\n\n    Ok(MlpWaveNeuronReport {\n        neuron_index,\n        dominant_frequency,\n        dominant_ratio,\n        negative_fraction,\n        best_correlation,\n    })\n}\n\nfn dominant_frequency_ratio(signal: &[f64]) -> Result<(usize, f64), String> {\n    if signal.len() < 2 {\n        return Err(\"signal too short for FFT\".to_string());\n    }\n\n    let mut planner = FftPlanner::new();\n    let fft = planner.plan_fft_forward(signal.len());\n    let mut buffer: Vec<Complex<f64>> = signal\n        .iter()\n        .map(|&x| Complex::new(x, 0.0))\n        .collect();\n    fft.process(&mut buffer);\n    let magnitudes: Vec<f64> = buffer.iter().map(|c| c.norm()).collect();\n\n    if magnitudes.len() < 2 {\n        return Err(\"insufficient FFT bins\".to_string());\n    }\n\n    let mut dominant_idx = 1usize;\n    let mut dominant_mag = magnitudes[1];\n    for (idx, &mag) in magnitudes.iter().enumerate().skip(2) {\n        if mag > dominant_mag {\n            dominant_mag = mag;\n            dominant_idx = idx;\n        }\n    }\n\n    if dominant_idx == 0 {\n        return Err(\"dominant frequency is zero\".to_string());\n    }\n\n    let mut nonzero = magnitudes[1..].to_vec();\n    nonzero.sort_by(|a, b| a.partial_cmp(b).unwrap_or(std::cmp::Ordering::Equal));\n    let median_mag = nonzero[nonzero.len() / 2];\n    let ratio = if median_mag == 0.0 {\n        dominant_mag\n    } else {\n        dominant_mag / median_mag\n    };\n\n    Ok((dominant_idx, ratio))\n}\n\nfn best_rectified_sine_correlation(signal: &[f64], frequency: usize, phase_steps: usize) -> f64 {\n    if signal.is_empty() || frequency == 0 || phase_steps == 0 {\n        return 0.0;\n    }\n\n    let tau = std::f64::consts::PI * 2.0;\n    let mut best = -1.0;\n    for step in 0..phase_steps {\n        let phase = tau * step as f64 / phase_steps as f64;\n        let wave: Vec<f64> = (0..signal.len())\n            .map(|idx| {\n                let angle = tau * frequency as f64 * idx as f64 / signal.len() as f64 + phase;\n                angle.sin().max(0.0)\n            })\n            .collect();\n        let corr = pearson_correlation(signal, &wave);\n        if corr > best {\n            best = corr;\n        }\n    }\n\n    best\n}\n\nfn pearson_correlation(a: &[f64], b: &[f64]) -> f64 {\n    if a.len() != b.len() || a.is_empty() {\n        return 0.0;\n    }\n\n    let mean_a = a.iter().sum::<f64>() / a.len() as f64;\n    let mean_b = b.iter().sum::<f64>() / b.len() as f64;\n\n    let mut num = 0.0;\n    let mut denom_a = 0.0;\n    let mut denom_b = 0.0;\n    for (x, y) in a.iter().zip(b.iter()) {\n        let da = x - mean_a;\n        let db = y - mean_b;\n        num += da * db;\n        denom_a += da * da;\n        denom_b += db * db;\n    }\n\n    if denom_a == 0.0 || denom_b == 0.0 {\n        return 0.0;\n    }\n\n    num / (denom_a.sqrt() * denom_b.sqrt())\n}\n\n/// Verify that the model actually learned modular addition\n/// by testing it on ALL possible examples systematically\npub fn verify_full_accuracy<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n) -> (f32, f32) {\n    println!(\"ðŸ” Verifying model on COMPLETE dataset (no sampling)...\");\n\n    let train_dataset = ModularAdditionDataset::new(true, 42);\n    let val_dataset = ModularAdditionDataset::new(false, 42);\n\n    let train_acc = test_all_examples(model, &train_dataset, device);\n    let val_acc = test_all_examples(model, &val_dataset, device);\n\n    println!(\"ðŸ“Š FULL Dataset Results:\");\n    println!(\"   Training: {}/{} = {:.2}%\",\n        (train_acc * train_dataset.len() as f32) as usize,\n        train_dataset.len(),\n        train_acc * 100.0);\n    println!(\"   Validation: {}/{} = {:.2}%\",\n        (val_acc * val_dataset.len() as f32) as usize,\n        val_dataset.len(),\n        val_acc * 100.0);\n\n    (train_acc, val_acc)\n}\n\n/// Test model on ALL examples in dataset (no random sampling)\nfn test_all_examples<B: Backend>(\n    model: &Transformer<B>,\n    dataset: &ModularAdditionDataset,\n    device: &B::Device,\n) -> f32 {\n    let mut total_correct = 0;\n    let total = dataset.len();\n\n    // Test in batches to avoid memory issues\n    let batch_size = 100;\n    for batch_start in (0..total).step_by(batch_size) {\n        let batch_end = (batch_start + batch_size).min(total);\n        let batch_len = batch_end - batch_start;\n\n        let mut inputs_vec = Vec::new();\n        let mut targets_vec = Vec::new();\n\n        for idx in batch_start..batch_end {\n            let (input, target) = dataset.get(idx).unwrap();\n            inputs_vec.extend(input);\n            targets_vec.push(target as i32);\n        }\n\n        let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n            .reshape([batch_len, 3]);\n\n        let logits = model.forward(inputs);\n        let predictions = logits.argmax(1).squeeze::<1>();\n        let predictions_vec: Vec<i32> = predictions.into_data().to_vec().unwrap();\n\n        let correct = predictions_vec\n            .iter()\n            .zip(targets_vec.iter())\n            .filter(|(p, t)| p == t)\n            .count();\n\n        total_correct += correct;\n    }\n\n    total_correct as f32 / total as f32\n}\n\n/// Verify that weight norms decrease over time with weight decay\n/// and show correlation with the grokking transition\npub fn verify_weight_norm_decay(\n    weight_norm_history: &[(usize, f64)],\n    grok_step: Option<usize>,\n    config: &WeightNormDecayConfig,\n) -> Result<WeightNormDecayReport, String> {\n    if weight_norm_history.len() < 2 {\n        return Err(\"weight norm history requires at least 2 snapshots\".to_string());\n    }\n\n    let initial_norm = weight_norm_history\n        .first()\n        .ok_or_else(|| \"weight norm history is empty\".to_string())?\n        .1;\n    let final_norm = weight_norm_history\n        .last()\n        .ok_or_else(|| \"weight norm history is empty\".to_string())?\n        .1;\n\n    if initial_norm <= 0.0 {\n        return Err(\"initial weight norm must be positive\".to_string());\n    }\n\n    // Check that norms are generally decreasing (with weight decay)\n    let relative_decrease = (initial_norm - final_norm) / initial_norm;\n    if relative_decrease < config.min_relative_decrease {\n        return Err(format!(\n            \"weight norm decrease {:.4} below threshold {:.4} (initial {:.4}, final {:.4})\",\n            relative_decrease, config.min_relative_decrease, initial_norm, final_norm\n        ));\n    }\n\n    // Check that norms are not increasing (weight decay should monotonically decrease norms)\n    for window in weight_norm_history.windows(2) {\n        let prev_norm = window[0].1;\n        let next_norm = window[1].1;\n        if next_norm > prev_norm * 1.01 {\n            // Allow 1% tolerance for numerical noise\n            return Err(format!(\n                \"weight norm increased from {:.4} to {:.4} at step {}\",\n                prev_norm, next_norm, window[1].0\n            ));\n        }\n    }\n\n    // If grok step is provided, check that norms show a shift around grokking\n    let grok_step_norm = if let Some(step) = grok_step {\n        weight_norm_history\n            .iter()\n            .filter(|(s, _)| {\n                *s >= step.saturating_sub(config.grok_step_window)\n                    && *s <= step + config.grok_step_window\n            })\n            .map(|(_, norm)| *norm)\n            .next()\n    } else {\n        None\n    };\n\n    Ok(WeightNormDecayReport {\n        initial_norm,\n        final_norm,\n        relative_decrease,\n        grok_step_norm,\n    })\n}\n\n/// Verify the characteristic \"snake curve\" shape of grokking phenomenon\n/// on log-scale plots: train accuracy rises early, validation stays flat then jumps\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_snake_curve_shape(\n    train_acc: &[(usize, f32)],\n    val_acc: &[(usize, f32)],\n    config: &SnakeCurveConfig,\n) -> Result<SnakeCurveReport, String> {\n    if train_acc.is_empty() || val_acc.is_empty() {\n        return Err(\"empty accuracy history\".to_string());\n    }\n\n    // Phase 1: Verify early train convergence (immediate drop on log scale)\n    let train_converged_step = train_acc\n        .iter()\n        .find(|(_, acc)| *acc >= config.train_convergence_threshold)\n        .map(|(step, _)| *step)\n        .ok_or_else(|| {\n            format!(\n                \"train accuracy never reaches {} threshold\",\n                config.train_convergence_threshold\n            )\n        })?;\n\n    if train_converged_step > config.early_step_max {\n        return Err(format!(\n            \"train accuracy converges at step {}, expected before {}\",\n            train_converged_step, config.early_step_max\n        ));\n    }\n\n    // Phase 2: Verify validation plateau (flat on log scale)\n    let plateau_samples: Vec<f32> = val_acc\n        .iter()\n        .filter(|(step, _)| *step >= 1 && *step <= config.val_plateau_min_step)\n        .map(|(_, acc)| *acc)\n        .collect();\n\n    if plateau_samples.is_empty() {\n        return Err(\"no validation samples in plateau region\".to_string());\n    }\n\n    let val_plateau_max = plateau_samples\n        .iter()\n        .fold(0.0f32, |max_val, &acc| max_val.max(acc));\n\n    if val_plateau_max > config.val_plateau_max_acc {\n        return Err(format!(\n            \"validation accuracy in plateau region too high: max {:.4} exceeds {:.4}\",\n            val_plateau_max, config.val_plateau_max_acc\n        ));\n    }\n\n    // Phase 3: Verify delayed validation jump (crash up on log scale)\n    let val_grok_step = val_acc\n        .iter()\n        .filter(|(step, _)| *step > config.val_plateau_min_step)\n        .find(|(_, acc)| *acc >= config.val_final_threshold)\n        .map(|(step, _)| *step)\n        .ok_or_else(|| {\n            format!(\n                \"validation accuracy never reaches {} after plateau\",\n                config.val_final_threshold\n            )\n        })?;\n\n    // Phase 4: Verify phase ordering (snake shape: early train, late val)\n    let curve_shape_valid = train_converged_step < config.val_plateau_min_step\n        && val_grok_step > config.val_plateau_min_step\n        && train_converged_step < val_grok_step;\n\n    if !curve_shape_valid {\n        return Err(format!(\n            \"invalid curve shape: train converged at {}, val plateau until {}, val grokked at {}\",\n            train_converged_step, config.val_plateau_min_step, val_grok_step\n        ));\n    }\n\n    Ok(SnakeCurveReport {\n        train_converged_by_step: train_converged_step,\n        val_plateau_max_acc: val_plateau_max,\n        val_grok_step,\n        curve_shape_valid,\n    })\n}\n\n/// Test specific examples to see if model actually computes mod p\npub fn test_specific_examples<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n) {\n    println!();\n    println!(\"ðŸ§ª Testing specific examples:\");\n\n    let modulus = ModularAdditionDataset::modulus();\n    let equals_token = ModularAdditionDataset::equals_token();\n    let test_cases = vec![\n        (0, 0, 0),\n        (1, 1, 2),\n        (50, 50, (50 + 50) % modulus),\n        (112, 1, (112 + 1) % modulus),\n        (56, 57, (56 + 57) % modulus),\n        (112, 112, (112 + 112) % modulus),\n        (10, 20, 30),\n        (60, 60, (60 + 60) % modulus),\n    ];\n\n    for (a, b, expected) in test_cases {\n        let input_vec = vec![a as i32, b as i32, equals_token as i32];\n        let input = Tensor::<B, 1, Int>::from_ints(input_vec.as_slice(), device)\n            .reshape([1, 3]);\n\n        let logits = model.forward(input);\n        let prediction = logits.argmax(1).squeeze::<1>();\n        let pred_value: i32 = prediction.into_data().to_vec().unwrap()[0];\n\n        let correct = if pred_value == expected as i32 { \"âœ“\" } else { \"âœ—\" };\n        println!(\n            \"   {} + {} mod {} = {} | Model: {} {}\",\n            a, b, modulus, expected, pred_value, correct\n        );\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    fn make_histories(\n        train_acc: &[(usize, f32)],\n        val_acc: &[(usize, f32)],\n        val_loss: &[(usize, f64)],\n    ) -> (LossHistory, AccuracyHistory) {\n        let mut loss_history = LossHistory::new();\n        loss_history.val_snapshots = val_loss.to_vec();\n        let mut accuracy_history = AccuracyHistory::new();\n        accuracy_history.train_snapshots = train_acc.to_vec();\n        accuracy_history.val_snapshots = val_acc.to_vec();\n        (loss_history, accuracy_history)\n    }\n\n    #[test]\n    fn grokking_phase_verification_passes_on_synthetic_signal() {\n        let train_acc = vec![(0, 0.2), (500, 0.995), (1000, 0.999)];\n        let val_acc = vec![\n            (0, 0.01),\n            (500, 0.01),\n            (1500, 0.01),\n            (2200, 0.995),\n            (2500, 0.999),\n        ];\n        let val_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 2.0),\n            (2200, 0.2),\n            (2500, 0.15),\n        ];\n\n        let (loss_history, accuracy_history) = make_histories(&train_acc, &val_acc, &val_loss);\n        let config = GrokkingVerificationConfig {\n            early_step_max: 1000,\n            plateau_min_step: 1500,\n            generalization_window: 1500,\n            accuracy_window: 2,\n            loss_window: 2,\n            ..GrokkingVerificationConfig::default_for_modulus(ModularAdditionDataset::modulus())\n        };\n\n        let report = verify_grokking_phase_transition(&loss_history, &accuracy_history, &config)\n            .expect(\"expected grokking verification to pass\");\n        assert!(report.transition_step >= config.plateau_min_step);\n        assert!(report.early_train_step <= config.early_step_max);\n    }\n\n    #[test]\n    fn grokking_phase_verification_fails_without_transition() {\n        let train_acc = vec![(0, 0.2), (500, 0.995)];\n        let val_acc = vec![(0, 0.01), (500, 0.01), (1500, 0.02)];\n        let val_loss = vec![(0, 2.0), (500, 2.0), (1500, 1.9)];\n\n        let (loss_history, accuracy_history) = make_histories(&train_acc, &val_acc, &val_loss);\n        let config = GrokkingVerificationConfig {\n            early_step_max: 1000,\n            plateau_min_step: 1000,\n            generalization_window: 1000,\n            accuracy_window: 2,\n            loss_window: 2,\n            ..GrokkingVerificationConfig::default_for_modulus(ModularAdditionDataset::modulus())\n        };\n\n        let err = verify_grokking_phase_transition(&loss_history, &accuracy_history, &config)\n            .expect_err(\"expected grokking verification to fail\");\n        assert!(err.contains(\"validation accuracy never reaches target\"));\n    }\n\n    #[test]\n    fn grokking_phase_verification_fails_with_slow_accuracy_rise() {\n        let train_acc = vec![(0, 0.2), (500, 0.995), (1000, 0.999)];\n        let val_acc = vec![\n            (0, 0.01),\n            (500, 0.01),\n            (1500, 0.5),\n            (2000, 0.6),\n            (2200, 0.995),\n            (2500, 0.999),\n        ];\n        let val_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 2.0),\n            (2200, 0.2),\n            (2500, 0.15),\n        ];\n\n        let (loss_history, accuracy_history) = make_histories(&train_acc, &val_acc, &val_loss);\n        let config = GrokkingVerificationConfig {\n            early_step_max: 1000,\n            plateau_min_step: 1000,\n            generalization_window: 2000,\n            accuracy_window: 2,\n            loss_window: 2,\n            ..GrokkingVerificationConfig::default_for_modulus(ModularAdditionDataset::modulus())\n        };\n\n        let err = verify_grokking_phase_transition(&loss_history, &accuracy_history, &config)\n            .expect_err(\"expected grokking verification to fail\");\n        assert!(err.contains(\"plateau too high\"));\n    }\n\n    #[test]\n    fn restricted_loss_detection_passes_with_early_drop() {\n        let full_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 2.0),\n            (2500, 2.0),\n            (3500, 0.3),\n        ];\n        let restricted_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 0.8),\n            (2500, 0.4),\n            (3500, 0.3),\n        ];\n\n        let config = RestrictedLossVerificationConfig {\n            plateau_min_step: 1000,\n            drop_window: 2,\n            drop_fraction: 0.5,\n            min_step_lead: 500,\n        };\n\n        let report = verify_restricted_loss_early_drop(&full_loss, &restricted_loss, &config)\n            .expect(\"expected restricted loss verification to pass\");\n        assert!(report.restricted_drop_step < report.full_drop_step);\n    }\n\n    #[test]\n    fn restricted_loss_detection_fails_when_full_drops_first() {\n        let full_loss = vec![(0, 2.0), (1000, 1.0), (1500, 0.8)];\n        let restricted_loss = vec![(0, 2.0), (1000, 2.0), (1500, 0.8)];\n\n        let config = RestrictedLossVerificationConfig {\n            plateau_min_step: 500,\n            drop_window: 1,\n            drop_fraction: 0.3,\n            min_step_lead: 200,\n        };\n\n        let err = verify_restricted_loss_early_drop(&full_loss, &restricted_loss, &config)\n            .expect_err(\"expected restricted loss verification to fail\");\n        assert!(err.contains(\"restricted loss drops\"));\n    }\n\n    #[test]\n    fn excluded_loss_spike_verification_passes() {\n        let excluded_loss = vec![(0, 1.0), (1500, 2.2), (3000, 1.1)];\n        let config = ExcludedLossSpikeConfig {\n            min_relative_increase: 0.5,\n        };\n\n        let report = verify_excluded_loss_spike(&excluded_loss, &config)\n            .expect(\"expected excluded loss spike verification to pass\");\n        assert_eq!(report.spike_step, 1500);\n    }\n\n    #[test]\n    fn excluded_loss_spike_verification_fails_without_spike() {\n        let excluded_loss = vec![(0, 1.0), (1500, 1.1), (3000, 1.0)];\n        let config = ExcludedLossSpikeConfig {\n            min_relative_increase: 0.5,\n        };\n\n        let err = verify_excluded_loss_spike(&excluded_loss, &config)\n            .expect_err(\"expected excluded loss spike verification to fail\");\n        assert!(err.contains(\"excluded loss spike too small\"));\n    }\n\n    #[test]\n    fn restricted_loss_drop_verification_passes() {\n        let restricted_loss = vec![\n            (0, 2.0),\n            (1000, 1.8),\n            (5000, 1.2),\n            (7000, 0.8),\n            (10000, 0.9),\n        ];\n        let config = RestrictedLossDropConfig {\n            min_relative_decrease: 0.3,\n        };\n\n        let report = verify_restricted_loss_drop(&restricted_loss, &config)\n            .expect(\"expected restricted loss drop verification to pass\");\n        assert_eq!(report.drop_step, 7000);\n        assert!(report.baseline_loss > report.drop_loss);\n        assert!((report.baseline_loss - report.drop_loss) / report.baseline_loss >= 0.3);\n    }\n\n    #[test]\n    fn restricted_loss_drop_verification_fails_without_drop() {\n        let restricted_loss = vec![\n            (0, 2.0),\n            (1000, 2.1),\n            (5000, 1.9),\n            (7000, 2.0),\n            (10000, 2.05),\n        ];\n        let config = RestrictedLossDropConfig {\n            min_relative_decrease: 0.3,\n        };\n\n        let err = verify_restricted_loss_drop(&restricted_loss, &config)\n            .expect_err(\"expected restricted loss drop verification to fail\");\n        assert!(err.contains(\"restricted loss decrease too small\"));\n    }\n\n    #[test]\n    fn mlp_activation_wave_verification_passes_on_rectified_sine() {\n        let n = ModularAdditionDataset::modulus();\n        let freq = 5.0;\n        let mut signal = Vec::with_capacity(n);\n        for idx in 0..n {\n            let angle = 2.0 * std::f64::consts::PI * freq * idx as f64 / n as f64;\n            let rectified = angle.sin().max(0.0);\n            let wobble = (2.0 * std::f64::consts::PI * 2.0 * idx as f64 / n as f64)\n                .sin()\n                .abs()\n                * 0.02;\n            signal.push(rectified + wobble);\n        }\n\n        let config = MlpWaveVerificationConfig {\n            max_negative_fraction: 0.0,\n            negative_tolerance: 1e-6,\n            min_dominant_ratio: 3.0,\n            min_correlation: 0.7,\n            phase_steps: 32,\n        };\n\n        let report =\n            verify_mlp_activation_waves_from_signals(&[(0, signal)], &config)\n                .expect(\"expected rectified sine to pass\");\n        assert_eq!(report.neuron_reports.len(), 1);\n    }\n\n    #[test]\n    fn mlp_activation_wave_verification_fails_on_noise() {\n        let n = ModularAdditionDataset::modulus();\n        let signal: Vec<f64> = (0..n).map(|idx| idx as f64 / n as f64).collect();\n\n        let config = MlpWaveVerificationConfig {\n            max_negative_fraction: 0.0,\n            negative_tolerance: 1e-6,\n            min_dominant_ratio: 4.0,\n            min_correlation: 0.8,\n            phase_steps: 16,\n        };\n\n        let err =\n            verify_mlp_activation_waves_from_signals(&[(0, signal)], &config)\n                .expect_err(\"expected linear signal to fail\");\n        assert!(err.contains(\"dominant frequency ratio\") || err.contains(\"correlation\"));\n    }\n\n    #[test]\n    fn pairwise_manifold_transition_passes_on_elliptic_signals() {\n        let n = ModularAdditionDataset::modulus();\n        let freq = 5.0;\n        let mut post_signals = Vec::new();\n        for neuron_idx in 0..7usize {\n            let phase = 2.0 * std::f64::consts::PI * neuron_idx as f64 / 7.0;\n            let signal: Vec<f64> = (0..n)\n                .map(|idx| {\n                    let angle = 2.0 * std::f64::consts::PI * freq * idx as f64 / n as f64 + phase;\n                    angle.sin()\n                })\n                .collect();\n            post_signals.push((neuron_idx, signal));\n        }\n\n        let mut pre_signals = Vec::new();\n        for neuron_idx in 0..7usize {\n            let signal: Vec<f64> = (0..n)\n                .map(|idx| idx as f64 / n as f64 + neuron_idx as f64 * 0.01)\n                .collect();\n            pre_signals.push((neuron_idx, signal));\n        }\n\n        let config = PairwiseManifoldConfig {\n            fixed_y: 0,\n            neuron_indices: (0..7).collect(),\n            min_post_score: 0.75,\n            max_pre_score: 0.55,\n            max_axis_ratio: 8.0,\n            min_pairs_passing: 8,\n        };\n\n        let report = verify_pairwise_manifold_transition_from_signals(\n            &pre_signals,\n            &post_signals,\n            &config,\n        )\n        .expect(\"expected pairwise manifold transition to pass\");\n        assert!(report.passing_pairs >= config.min_pairs_passing);\n    }\n\n    #[test]\n    fn pairwise_manifold_transition_fails_without_rings() {\n        let n = ModularAdditionDataset::modulus();\n        let mut signals = Vec::new();\n        for neuron_idx in 0..7usize {\n            let signal: Vec<f64> = (0..n)\n                .map(|idx| idx as f64 / n as f64 + neuron_idx as f64 * 0.02)\n                .collect();\n            signals.push((neuron_idx, signal));\n        }\n\n        let config = PairwiseManifoldConfig {\n            fixed_y: 0,\n            neuron_indices: (0..7).collect(),\n            min_post_score: 0.8,\n            max_pre_score: 0.55,\n            max_axis_ratio: 4.0,\n            min_pairs_passing: 5,\n        };\n\n        let err = verify_pairwise_manifold_transition_from_signals(\n            &signals,\n            &signals,\n            &config,\n        )\n        .expect_err(\"expected pairwise manifold transition to fail\");\n        assert!(err.contains(\"post-grok\") || err.contains(\"pre-grok\"));\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_passes_on_diagonal_corrugation() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create diagonal ridges: f(x,y) = sin(2Ï€ * freq * (x+y) / n)\n        let freq = 3.0;\n        for x in 0..n {\n            for y in 0..n {\n                let angle = 2.0 * std::f64::consts::PI * freq * (x + y) as f64 / n as f64;\n                surface[x][y] = angle.sin();\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let report = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect(\"expected diagonal ridge verification to pass\");\n        assert!(report.diagonal_score > report.axis_score);\n        assert!(report.ratio >= config.min_diagonal_ratio);\n        assert!(report.diagonal_score >= config.min_diagonal_score);\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_passes_on_anti_diagonal_pattern() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create anti-diagonal ridges: f(x,y) = sin(2Ï€ * freq * (x-y) / n)\n        let freq = 3.0;\n        for x in 0..n {\n            for y in 0..n {\n                let angle = 2.0 * std::f64::consts::PI * freq * (x as i32 - y as i32) as f64 / n as f64;\n                surface[x][y] = angle.sin();\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let report = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect(\"expected anti-diagonal ridge verification to pass\");\n        assert!(report.diagonal_score > report.axis_score);\n        assert!(report.ratio >= config.min_diagonal_ratio);\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_fails_on_axis_aligned_pattern() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create axis-aligned ridges: f(x,y) = sin(2Ï€ * freq * x / n)\n        let freq = 3.0;\n        for x in 0..n {\n            for y in 0..n {\n                let angle = 2.0 * std::f64::consts::PI * freq * x as f64 / n as f64;\n                surface[x][y] = angle.sin();\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let err = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect_err(\"expected axis-aligned pattern to fail diagonal verification\");\n        assert!(err.contains(\"diagonal/axis ratio\") || err.contains(\"diagonal autocorrelation\"));\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_fails_on_flat_surface() {\n        let n = 50;\n        let surface = vec![vec![1.5; n]; n];\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let err = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect_err(\"expected flat surface to fail\");\n        assert!(err.contains(\"surface variance is zero\"));\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_fails_on_random_noise() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create random noise (low autocorrelation everywhere)\n        for x in 0..n {\n            for y in 0..n {\n                surface[x][y] = ((x * 17 + y * 31) % 100) as f64 / 100.0;\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let err = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect_err(\"expected random noise to fail diagonal verification\");\n        assert!(err.contains(\"diagonal/axis ratio\") || err.contains(\"diagonal autocorrelation\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_passes_on_valid_grokking_curve() {\n        // Simulate typical grokking: train converges early, val plateaus then jumps\n        let train_acc = vec![\n            (1, 0.10),\n            (10, 0.50),\n            (100, 0.80),\n            (500, 0.98),\n            (1000, 0.99),\n            (2000, 0.995),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (10, 0.009),\n            (100, 0.01),\n            (500, 0.011),\n            (1000, 0.009),\n            (1500, 0.01),\n            (2000, 0.92),\n            (3000, 0.98),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 1500,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let report = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect(\"expected snake curve verification to pass\");\n\n        assert_eq!(report.train_converged_by_step, 500);\n        assert!(report.val_plateau_max_acc < config.val_plateau_max_acc);\n        assert_eq!(report.val_grok_step, 2000);\n        assert!(report.curve_shape_valid);\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_on_simultaneous_convergence() {\n        // Both train and val converge at the same time (no grokking)\n        let train_acc = vec![\n            (1, 0.10),\n            (100, 0.98),\n            (500, 0.99),\n        ];\n        let val_acc = vec![\n            (1, 0.10),\n            (100, 0.97),\n            (500, 0.98),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 200,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let err = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect_err(\"expected snake curve verification to fail\");\n        assert!(err.contains(\"plateau region too high\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_on_slow_train_convergence() {\n        // Train takes too long to converge (not the typical snake curve)\n        let train_acc = vec![\n            (1, 0.10),\n            (500, 0.50),\n            (1000, 0.70),\n            (2000, 0.98),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (500, 0.01),\n            (1000, 0.01),\n            (2000, 0.92),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 1500,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let err = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect_err(\"expected snake curve verification to fail\");\n        assert!(err.contains(\"converges at step 2000\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_without_val_jump() {\n        // Validation never jumps up (no generalization)\n        let train_acc = vec![\n            (1, 0.10),\n            (100, 0.98),\n            (500, 0.99),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (100, 0.01),\n            (500, 0.01),\n            (2000, 0.02),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 200,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let err = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect_err(\"expected snake curve verification to fail\");\n        assert!(err.contains(\"validation accuracy never reaches\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_on_wrong_phase_ordering() {\n        // Val jumps before train converges (backwards snake)\n        let train_acc = vec![\n            (1, 0.10),\n            (500, 0.50),\n            (2000, 0.98),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (100, 0.95),\n            (500, 0.98),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 200,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let result = verify_snake_curve_shape(&train_acc, &val_acc, &config);\n        // Should fail because val jumps before train converges (not a proper snake curve)\n        // This can fail for multiple reasons, but should definitely fail\n        assert!(result.is_err(), \"Expected snake curve verification to fail but it passed\");\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_passes_on_decreasing_norms() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 85.0),\n            (1000, 72.0),\n            (1500, 63.0),\n            (2000, 58.0),\n            (2500, 55.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let report = verify_weight_norm_decay(&weight_norms, Some(2000), &config)\n            .expect(\"expected weight norm decay verification to pass\");\n\n        assert_eq!(report.initial_norm, 100.0);\n        assert_eq!(report.final_norm, 55.0);\n        assert!((report.relative_decrease - 0.45).abs() < 0.01);\n        assert!(report.grok_step_norm.is_some());\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_fails_on_flat_norms() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 99.0),\n            (1000, 98.5),\n            (1500, 98.0),\n            (2000, 97.5),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let err = verify_weight_norm_decay(&weight_norms, None, &config)\n            .expect_err(\"expected weight norm decay verification to fail\");\n        assert!(err.contains(\"weight norm decrease\"));\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_fails_on_increasing_norms() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 95.0),\n            (1000, 105.0), // Increase here\n            (1500, 90.0),\n            (2000, 85.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let err = verify_weight_norm_decay(&weight_norms, None, &config)\n            .expect_err(\"expected weight norm decay verification to fail\");\n        assert!(err.contains(\"weight norm increased\"));\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_allows_small_noise() {\n        // Weight decay with small numerical noise (within 1% tolerance)\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 95.0),\n            (1000, 90.5),  // Small increase within tolerance\n            (1500, 90.0),\n            (2000, 85.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let report = verify_weight_norm_decay(&weight_norms, None, &config)\n            .expect(\"expected weight norm decay with small noise to pass\");\n\n        assert_eq!(report.initial_norm, 100.0);\n        assert_eq!(report.final_norm, 85.0);\n        assert!((report.relative_decrease - 0.15).abs() < 0.01);\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_finds_grok_step_norm() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (1000, 80.0),\n            (1800, 68.0),  // Within window of grok step (2200 - 500 = 1700)\n            (2000, 65.0),\n            (2200, 62.0),  // Around grok step\n            (3000, 55.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let report = verify_weight_norm_decay(&weight_norms, Some(2200), &config)\n            .expect(\"expected weight norm decay verification to pass\");\n\n        assert!(report.grok_step_norm.is_some());\n        let grok_norm = report.grok_step_norm.unwrap();\n        // Should find the first norm in the window [1700, 2700], which is 68.0 at step 1800\n        assert!((grok_norm - 68.0).abs() < 0.01);\n    }\n}\n","structuredPatch":[{"oldStart":1023,"oldLines":6,"newStart":1023,"newLines":7,"lines":["     Ok((mean, variance))"," }"," ","+#[allow(dead_code)]"," fn average_autocorrelation(","     surface: &[Vec<f64>],","     mean: f64,"]}],"userModified":false,"replaceAll":false}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01SVREmJyRoe1zo1WVxH4hdE","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01P5kZVzooVBxpq7CpS84XoU","name":"Edit","input":{"replace_all":false,"file_path":"/Users/admin/MechInt/src/verify.rs","old_string":"fn analyze_mlp_activation_wave(","new_string":"#[allow(dead_code)]\nfn analyze_mlp_activation_wave("}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":308,"cache_read_input_tokens":63793,"cache_creation":{"ephemeral_5m_input_tokens":308,"ephemeral_1h_input_tokens":0},"output_tokens":150,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"72698db5-aae6-42aa-b3ee-55b3e584fd8e"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01P5kZVzooVBxpq7CpS84XoU","type":"tool_result","content":"The file /Users/admin/MechInt/src/verify.rs has been updated. Here's the result of running `cat -n` on a snippet of the edited file:\n  1072â†’\n  1073â†’    Ok(total / lag_count as f64)\n  1074â†’}\n  1075â†’\n  1076â†’#[allow(dead_code)]\n  1077â†’fn analyze_mlp_activation_wave(\n  1078â†’    neuron_index: usize,\n  1079â†’    signal: &[f64],\n  1080â†’    config: &MlpWaveVerificationConfig,\n  1081â†’) -> Result<MlpWaveNeuronReport, String> {"}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"c77f93e4-54e5-4c88-91ac-7f6ce9cb71a9","tool_use_result":{"filePath":"/Users/admin/MechInt/src/verify.rs","oldString":"fn analyze_mlp_activation_wave(","newString":"#[allow(dead_code)]\nfn analyze_mlp_activation_wave(","originalFile":"use burn::{\n    data::dataset::Dataset,\n    tensor::{backend::Backend, Int, Tensor},\n};\nuse rustfft::{num_complex::Complex, FftPlanner};\n\nuse crate::analysis::{AccuracyHistory, LossHistory};\nuse crate::data::ModularAdditionDataset;\nuse crate::model::Transformer;\n\n#[derive(Debug, Clone)]\npub struct GrokkingVerificationConfig {\n    pub early_train_acc_threshold: f32,\n    pub early_step_max: usize,\n    pub plateau_min_step: usize,\n    pub generalization_window: usize,\n    pub chance_accuracy: f32,\n    pub chance_tolerance: f32,\n    pub target_val_acc_threshold: f32,\n    pub accuracy_window: usize,\n    pub loss_window: usize,\n    pub loss_drop_fraction: f64,\n}\n\nimpl GrokkingVerificationConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        Self {\n            early_train_acc_threshold: 0.99,\n            early_step_max: 1000,\n            plateau_min_step: 1000,\n            generalization_window: 10_000,\n            chance_accuracy: 1.0 / modulus as f32,\n            chance_tolerance: 0.02,\n            target_val_acc_threshold: 0.99,\n            accuracy_window: 5,\n            loss_window: 5,\n            loss_drop_fraction: 0.5,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct GrokkingPhaseReport {\n    pub early_train_step: usize,\n    pub transition_step: usize,\n    pub plateau_max_val_acc: f32,\n    pub loss_drop_ratio: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossVerificationConfig {\n    pub plateau_min_step: usize,\n    pub drop_window: usize,\n    pub drop_fraction: f64,\n    pub min_step_lead: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossReport {\n    pub restricted_drop_step: usize,\n    pub full_drop_step: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossDropConfig {\n    pub min_relative_decrease: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossDropReport {\n    pub drop_step: usize,\n    pub drop_loss: f64,\n    pub baseline_loss: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct ExcludedLossSpikeConfig {\n    pub min_relative_increase: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct ExcludedLossSpikeReport {\n    pub spike_step: usize,\n    pub spike_loss: f64,\n    pub baseline_loss: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct MlpWaveVerificationConfig {\n    pub max_negative_fraction: f64,\n    pub negative_tolerance: f64,\n    pub min_dominant_ratio: f64,\n    pub min_correlation: f64,\n    pub phase_steps: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct MlpWaveNeuronReport {\n    pub neuron_index: usize,\n    pub dominant_frequency: usize,\n    pub dominant_ratio: f64,\n    pub negative_fraction: f64,\n    pub best_correlation: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct MlpWaveReport {\n    pub neuron_reports: Vec<MlpWaveNeuronReport>,\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldConfig {\n    pub fixed_y: usize,\n    pub neuron_indices: Vec<usize>,\n    pub min_post_score: f64,\n    pub max_pre_score: f64,\n    pub max_axis_ratio: f64,\n    pub min_pairs_passing: usize,\n}\n\nimpl PairwiseManifoldConfig {\n    pub fn default_for_modulus(_modulus: usize) -> Self {\n        Self {\n            fixed_y: 0,\n            neuron_indices: (0..7).collect(),\n            min_post_score: 0.75,\n            max_pre_score: 0.55,\n            max_axis_ratio: 8.0,\n            min_pairs_passing: 10,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldPairReport {\n    pub neuron_a: usize,\n    pub neuron_b: usize,\n    pub circularity_score: f64,\n    pub radius_cv: f64,\n    pub axis_ratio: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldReport {\n    pub pair_reports: Vec<PairwiseManifoldPairReport>,\n    pub average_score: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldTransitionReport {\n    pub pre_average_score: f64,\n    pub post_average_score: f64,\n    pub passing_pairs: usize,\n    pub total_pairs: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct DiagonalRidgeConfig {\n    pub neuron_index: usize,\n    pub max_lag: usize,\n    pub min_diagonal_ratio: f64,\n    pub min_diagonal_score: f64,\n}\n\nimpl DiagonalRidgeConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        let max_lag = (modulus / 10).max(4).min(20);\n        Self {\n            neuron_index: 10,\n            max_lag,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct DiagonalRidgeReport {\n    pub neuron_index: usize,\n    pub diagonal_score: f64,\n    pub axis_score: f64,\n    pub ratio: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct SnakeCurveConfig {\n    pub early_step_max: usize,\n    pub train_convergence_threshold: f32,\n    pub val_plateau_min_step: usize,\n    pub val_plateau_max_acc: f32,\n    pub val_final_threshold: f32,\n}\n\nimpl SnakeCurveConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        Self {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 1000,\n            val_plateau_max_acc: 1.0 / modulus as f32 + 0.05,\n            val_final_threshold: 0.90,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct WeightNormDecayConfig {\n    pub min_relative_decrease: f64,\n    pub plateau_min_step: usize,\n    pub grok_step_window: usize,\n}\n\nimpl WeightNormDecayConfig {\n    pub fn default_for_grokking() -> Self {\n        Self {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct WeightNormDecayReport {\n    pub initial_norm: f64,\n    pub final_norm: f64,\n    pub relative_decrease: f64,\n    pub grok_step_norm: Option<f64>,\n}\n\n#[derive(Debug, Clone)]\npub struct SnakeCurveReport {\n    pub train_converged_by_step: usize,\n    pub val_plateau_max_acc: f32,\n    pub val_grok_step: usize,\n    pub curve_shape_valid: bool,\n}\n\npub fn verify_grokking_phase_transition(\n    loss_history: &LossHistory,\n    accuracy_history: &AccuracyHistory,\n    config: &GrokkingVerificationConfig,\n) -> Result<GrokkingPhaseReport, String> {\n    let (early_train_step, _) = accuracy_history\n        .train_snapshots\n        .iter()\n        .find(|(_, acc)| *acc >= config.early_train_acc_threshold)\n        .ok_or_else(|| \"train accuracy never reaches threshold\".to_string())?;\n\n    if *early_train_step > config.early_step_max {\n        return Err(format!(\n            \"train accuracy reaches {:.2} after step {}, expected before {}\",\n            config.early_train_acc_threshold, early_train_step, config.early_step_max\n        ));\n    }\n\n    let plateau_max_val_acc = accuracy_history\n        .val_snapshots\n        .iter()\n        .filter(|(step, _)| *step <= config.plateau_min_step)\n        .map(|(_, acc)| *acc)\n        .fold(None::<f32>, |accum, value| {\n            Some(accum.map_or(value, |max_val| max_val.max(value)))\n        })\n        .ok_or_else(|| \"no validation accuracy samples before plateau_min_step\".to_string())?;\n\n    if plateau_max_val_acc > config.chance_accuracy + config.chance_tolerance {\n        return Err(format!(\n            \"validation accuracy before step {} exceeds chance (max {:.4})\",\n            config.plateau_min_step, plateau_max_val_acc\n        ));\n    }\n\n    let (transition_step, _) = accuracy_history\n        .val_snapshots\n        .iter()\n        .find(|(step, acc)| {\n            *step >= config.plateau_min_step && *acc >= config.target_val_acc_threshold\n        })\n        .ok_or_else(|| \"validation accuracy never reaches target threshold\".to_string())?;\n\n    let latest_allowed = config.plateau_min_step + config.generalization_window;\n    if *transition_step > latest_allowed {\n        return Err(format!(\n            \"validation accuracy reaches {:.2} at step {}, expected before {}\",\n            config.target_val_acc_threshold, transition_step, latest_allowed\n        ));\n    }\n\n    verify_accuracy_jump(\n        &accuracy_history.val_snapshots,\n        *transition_step,\n        config.accuracy_window,\n        config.chance_accuracy,\n        config.chance_tolerance,\n        config.target_val_acc_threshold,\n    )?;\n\n    let loss_drop_ratio = verify_loss_drop(\n        &loss_history.val_snapshots,\n        *transition_step,\n        config.loss_window,\n        config.loss_drop_fraction,\n    )?;\n\n    Ok(GrokkingPhaseReport {\n        early_train_step: *early_train_step,\n        transition_step: *transition_step,\n        plateau_max_val_acc,\n        loss_drop_ratio,\n    })\n}\n\nfn verify_accuracy_jump(\n    val_accuracy: &[(usize, f32)],\n    transition_step: usize,\n    window: usize,\n    chance_accuracy: f32,\n    chance_tolerance: f32,\n    target_val_acc_threshold: f32,\n) -> Result<(), String> {\n    let before_values: Vec<f32> = val_accuracy\n        .iter()\n        .filter(|(step, _)| *step < transition_step)\n        .map(|(_, acc)| *acc)\n        .collect();\n\n    let after_values: Vec<f32> = val_accuracy\n        .iter()\n        .filter(|(step, _)| *step >= transition_step)\n        .map(|(_, acc)| *acc)\n        .collect();\n\n    if before_values.len() < window || after_values.len() < window {\n        return Err(\"insufficient accuracy samples around transition\".to_string());\n    }\n\n    let before_avg: f32 =\n        before_values[before_values.len() - window..].iter().sum::<f32>() / window as f32;\n    let after_avg: f32 = after_values[..window].iter().sum::<f32>() / window as f32;\n\n    if before_avg > chance_accuracy + chance_tolerance {\n        return Err(format!(\n            \"validation accuracy plateau too high before transition (avg {:.4})\",\n            before_avg\n        ));\n    }\n\n    if after_avg < target_val_acc_threshold {\n        return Err(format!(\n            \"validation accuracy after transition too low (avg {:.4})\",\n            after_avg\n        ));\n    }\n\n    Ok(())\n}\n\nfn verify_loss_drop(\n    val_losses: &[(usize, f64)],\n    transition_step: usize,\n    loss_window: usize,\n    loss_drop_fraction: f64,\n) -> Result<f64, String> {\n    let before_losses: Vec<f64> = val_losses\n        .iter()\n        .filter(|(step, _)| *step < transition_step)\n        .map(|(_, loss)| *loss)\n        .collect();\n\n    let after_losses: Vec<f64> = val_losses\n        .iter()\n        .filter(|(step, _)| *step >= transition_step)\n        .map(|(_, loss)| *loss)\n        .collect();\n\n    if before_losses.len() < loss_window || after_losses.len() < loss_window {\n        return Err(\"insufficient loss samples around transition\".to_string());\n    }\n\n    let before_avg: f64 =\n        before_losses[before_losses.len() - loss_window..].iter().sum::<f64>()\n            / loss_window as f64;\n    let after_avg: f64 = after_losses[..loss_window].iter().sum::<f64>() / loss_window as f64;\n\n    if after_avg > before_avg * (1.0 - loss_drop_fraction) {\n        return Err(format!(\n            \"validation loss drop too small (before {:.4}, after {:.4})\",\n            before_avg, after_avg\n        ));\n    }\n\n    Ok(after_avg / before_avg)\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_restricted_loss_early_drop(\n    full_losses: &[(usize, f64)],\n    restricted_losses: &[(usize, f64)],\n    config: &RestrictedLossVerificationConfig,\n) -> Result<RestrictedLossReport, String> {\n    let restricted_baseline =\n        baseline_loss(restricted_losses, config.plateau_min_step, config.drop_window)?;\n    let full_baseline = baseline_loss(full_losses, config.plateau_min_step, config.drop_window)?;\n\n    let restricted_drop_step = find_drop_step(\n        restricted_losses,\n        config.plateau_min_step,\n        restricted_baseline,\n        config.drop_fraction,\n    )?;\n    let full_drop_step = find_drop_step(\n        full_losses,\n        config.plateau_min_step,\n        full_baseline,\n        config.drop_fraction,\n    )?;\n\n    if restricted_drop_step + config.min_step_lead > full_drop_step {\n        return Err(format!(\n            \"restricted loss drops at step {}, expected at least {} steps before full loss (step {})\",\n            restricted_drop_step, config.min_step_lead, full_drop_step\n        ));\n    }\n\n    Ok(RestrictedLossReport {\n        restricted_drop_step,\n        full_drop_step,\n    })\n}\n\npub fn verify_restricted_loss_drop(\n    restricted_losses: &[(usize, f64)],\n    config: &RestrictedLossDropConfig,\n) -> Result<RestrictedLossDropReport, String> {\n    if restricted_losses.len() < 3 {\n        return Err(\"restricted loss history requires at least 3 points\".to_string());\n    }\n\n    let first = restricted_losses.first().ok_or_else(|| {\n        \"restricted loss history is empty\".to_string()\n    })?;\n    let baseline_loss = first.1;\n\n    let (min_idx, (min_step, min_loss)) = restricted_losses\n        .iter()\n        .enumerate()\n        .min_by(|(_, a), (_, b)| {\n            a.1.partial_cmp(&b.1).unwrap_or(std::cmp::Ordering::Equal)\n        })\n        .ok_or_else(|| \"restricted loss history is empty\".to_string())?;\n\n    if min_idx == 0 {\n        return Err(\"restricted loss minimum occurs at start (no drop detected)\".to_string());\n    }\n\n    let relative_decrease = (baseline_loss - min_loss) / baseline_loss;\n    if relative_decrease < config.min_relative_decrease {\n        return Err(format!(\n            \"restricted loss decrease too small (baseline {:.4}, min {:.4}, decrease {:.1}%)\",\n            baseline_loss, min_loss, relative_decrease * 100.0\n        ));\n    }\n\n    Ok(RestrictedLossDropReport {\n        drop_step: *min_step,\n        drop_loss: *min_loss,\n        baseline_loss,\n    })\n}\n\npub fn verify_excluded_loss_spike(\n    excluded_losses: &[(usize, f64)],\n    config: &ExcludedLossSpikeConfig,\n) -> Result<ExcludedLossSpikeReport, String> {\n    if excluded_losses.len() < 3 {\n        return Err(\"excluded loss history requires at least 3 points\".to_string());\n    }\n\n    let first = excluded_losses.first().ok_or_else(|| {\n        \"excluded loss history is empty\".to_string()\n    })?;\n    let last = excluded_losses.last().ok_or_else(|| {\n        \"excluded loss history is empty\".to_string()\n    })?;\n    let baseline_loss = (first.1 + last.1) / 2.0;\n\n    let (peak_idx, (peak_step, peak_loss)) = excluded_losses\n        .iter()\n        .enumerate()\n        .max_by(|(_, a), (_, b)| {\n            a.1.partial_cmp(&b.1).unwrap_or(std::cmp::Ordering::Equal)\n        })\n        .ok_or_else(|| \"excluded loss history is empty\".to_string())?;\n\n    if peak_idx == 0 || peak_idx + 1 == excluded_losses.len() {\n        return Err(\"excluded loss spike occurs at boundary\".to_string());\n    }\n\n    if peak_loss < &(baseline_loss * (1.0 + config.min_relative_increase)) {\n        return Err(format!(\n            \"excluded loss spike too small (baseline {:.4}, peak {:.4})\",\n            baseline_loss, peak_loss\n        ));\n    }\n\n    Ok(ExcludedLossSpikeReport {\n        spike_step: *peak_step,\n        spike_loss: *peak_loss,\n        baseline_loss,\n    })\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_mlp_activation_waves<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n    fixed_y: usize,\n    neuron_indices: &[usize],\n    config: &MlpWaveVerificationConfig,\n) -> Result<MlpWaveReport, String> {\n    let activations = collect_mlp_post_relu_activations(model, device, fixed_y)?;\n    if neuron_indices.is_empty() {\n        return Err(\"neuron indices list is empty\".to_string());\n    }\n\n    let d_ff = activations.len();\n    for &idx in neuron_indices {\n        if idx >= d_ff {\n            return Err(format!(\n                \"neuron index {} out of range (d_ff={})\",\n                idx, d_ff\n            ));\n        }\n    }\n\n    let signals: Vec<(usize, Vec<f64>)> = neuron_indices\n        .iter()\n        .map(|&idx| (idx, activations[idx].clone()))\n        .collect();\n    verify_mlp_activation_waves_from_signals(&signals, config)\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_mlp_activation_waves_from_signals(\n    signals: &[(usize, Vec<f64>)],\n    config: &MlpWaveVerificationConfig,\n) -> Result<MlpWaveReport, String> {\n    if signals.is_empty() {\n        return Err(\"no activation signals provided\".to_string());\n    }\n\n    let mut reports = Vec::with_capacity(signals.len());\n    for (neuron_index, signal) in signals {\n        let report = analyze_mlp_activation_wave(*neuron_index, signal, config)?;\n        reports.push(report);\n    }\n\n    Ok(MlpWaveReport {\n        neuron_reports: reports,\n    })\n}\n\npub fn verify_mlp_pairwise_manifold_transition<B: Backend>(\n    pre_model: &Transformer<B>,\n    post_model: &Transformer<B>,\n    device: &B::Device,\n    config: &PairwiseManifoldConfig,\n) -> Result<PairwiseManifoldTransitionReport, String> {\n    let modulus = ModularAdditionDataset::modulus();\n    if config.fixed_y >= modulus {\n        return Err(format!(\n            \"fixed_y {} out of range (modulus {})\",\n            config.fixed_y, modulus\n        ));\n    }\n\n    let pre_activations = collect_mlp_post_relu_activations(pre_model, device, config.fixed_y)?;\n    let post_activations = collect_mlp_post_relu_activations(post_model, device, config.fixed_y)?;\n\n    let d_ff = pre_activations.len();\n    if post_activations.len() != d_ff {\n        return Err(\"post activations shape mismatch\".to_string());\n    }\n\n    if config.neuron_indices.len() < 2 {\n        return Err(\"need at least two neuron indices\".to_string());\n    }\n\n    for &idx in &config.neuron_indices {\n        if idx >= d_ff {\n            return Err(format!(\n                \"neuron index {} out of range (d_ff={})\",\n                idx, d_ff\n            ));\n        }\n    }\n\n    let pre_signals: Vec<(usize, Vec<f64>)> = config\n        .neuron_indices\n        .iter()\n        .map(|&idx| (idx, pre_activations[idx].clone()))\n        .collect();\n    let post_signals: Vec<(usize, Vec<f64>)> = config\n        .neuron_indices\n        .iter()\n        .map(|&idx| (idx, post_activations[idx].clone()))\n        .collect();\n\n    verify_pairwise_manifold_transition_from_signals(&pre_signals, &post_signals, config)\n}\n\npub fn collect_mlp_activation_surface<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n    neuron_index: usize,\n) -> Result<Vec<Vec<f64>>, String> {\n    let modulus = ModularAdditionDataset::modulus();\n    let equals_token = ModularAdditionDataset::equals_token();\n\n    let mut inputs_vec = Vec::with_capacity(modulus * modulus * 3);\n    for x in 0..modulus {\n        for y in 0..modulus {\n            inputs_vec.push(x as i32);\n            inputs_vec.push(y as i32);\n            inputs_vec.push(equals_token as i32);\n        }\n    }\n\n    let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n        .reshape([modulus * modulus, 3]);\n    let (_, mlp_acts) = model.forward_with_mlp_activations(inputs);\n    let [batch_size, d_ff] = mlp_acts.dims();\n    if batch_size != modulus * modulus {\n        return Err(format!(\n            \"unexpected activation batch size {} (expected {})\",\n            batch_size,\n            modulus * modulus\n        ));\n    }\n    if neuron_index >= d_ff {\n        return Err(format!(\n            \"neuron index {} out of range (d_ff={})\",\n            neuron_index, d_ff\n        ));\n    }\n\n    let data: Vec<f32> = mlp_acts.into_data().to_vec().unwrap();\n    let mut surface = vec![vec![0.0f64; modulus]; modulus];\n    for x in 0..modulus {\n        for y in 0..modulus {\n            let batch = x * modulus + y;\n            let idx = batch * d_ff + neuron_index;\n            surface[x][y] = data[idx] as f64;\n        }\n    }\n\n    Ok(surface)\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_constructive_interference_surface_from_grid(\n    surface: &[Vec<f64>],\n    config: &DiagonalRidgeConfig,\n) -> Result<DiagonalRidgeReport, String> {\n    let (diag_main, diag_anti, axis_x, axis_y) =\n        surface_autocorrelation_scores(surface, config.max_lag)?;\n\n    let diagonal_score = diag_main.max(diag_anti);\n    let axis_score = axis_x.max(axis_y);\n    let ratio = if axis_score > 0.0 {\n        diagonal_score / axis_score\n    } else {\n        f64::INFINITY\n    };\n\n    if diagonal_score < config.min_diagonal_score {\n        return Err(format!(\n            \"diagonal autocorrelation {:.3} below threshold {:.3}\",\n            diagonal_score, config.min_diagonal_score\n        ));\n    }\n\n    if ratio < config.min_diagonal_ratio {\n        return Err(format!(\n            \"diagonal/axis ratio {:.3} below threshold {:.3}\",\n            ratio, config.min_diagonal_ratio\n        ));\n    }\n\n    Ok(DiagonalRidgeReport {\n        neuron_index: config.neuron_index,\n        diagonal_score,\n        axis_score,\n        ratio,\n    })\n}\n\npub fn verify_pairwise_manifold_transition_from_signals(\n    pre_signals: &[(usize, Vec<f64>)],\n    post_signals: &[(usize, Vec<f64>)],\n    config: &PairwiseManifoldConfig,\n) -> Result<PairwiseManifoldTransitionReport, String> {\n    let pre_report = evaluate_pairwise_manifolds_from_signals(pre_signals)?;\n    let post_report = evaluate_pairwise_manifolds_from_signals(post_signals)?;\n\n    if pre_report.average_score > config.max_pre_score {\n        return Err(format!(\n            \"pre-grok average circularity {:.3} exceeds threshold {:.3}\",\n            pre_report.average_score, config.max_pre_score\n        ));\n    }\n\n    let passing_pairs = post_report\n        .pair_reports\n        .iter()\n        .filter(|pair| {\n            pair.circularity_score >= config.min_post_score\n                && pair.axis_ratio <= config.max_axis_ratio\n        })\n        .count();\n    if passing_pairs < config.min_pairs_passing {\n        return Err(format!(\n            \"post-grok only {} pairs meet circularity threshold (need {})\",\n            passing_pairs, config.min_pairs_passing\n        ));\n    }\n\n    Ok(PairwiseManifoldTransitionReport {\n        pre_average_score: pre_report.average_score,\n        post_average_score: post_report.average_score,\n        passing_pairs,\n        total_pairs: post_report.pair_reports.len(),\n    })\n}\n\nfn evaluate_pairwise_manifolds_from_signals(\n    signals: &[(usize, Vec<f64>)],\n) -> Result<PairwiseManifoldReport, String> {\n    if signals.len() < 2 {\n        return Err(\"need at least two activation signals\".to_string());\n    }\n\n    let signal_len = signals[0].1.len();\n    if signal_len < 3 {\n        return Err(\"activation signal too short\".to_string());\n    }\n\n    for (idx, signal) in signals {\n        if signal.len() != signal_len {\n            return Err(format!(\n                \"signal length mismatch for neuron {}\",\n                idx\n            ));\n        }\n    }\n\n    let mut reports = Vec::new();\n    for i in 0..signals.len() {\n        for j in (i + 1)..signals.len() {\n            let (neuron_a, signal_a) = &signals[i];\n            let (neuron_b, signal_b) = &signals[j];\n            let points: Vec<(f64, f64)> = signal_a\n                .iter()\n                .zip(signal_b.iter())\n                .map(|(x, y)| (*x, *y))\n                .collect();\n            let (score, radius_cv, axis_ratio) =\n                match pairwise_circularity_metrics(&points) {\n                    Ok(metrics) => metrics,\n                    Err(_) => (0.0, 1.0, f64::INFINITY),\n                };\n            reports.push(PairwiseManifoldPairReport {\n                neuron_a: *neuron_a,\n                neuron_b: *neuron_b,\n                circularity_score: score,\n                radius_cv,\n                axis_ratio,\n            });\n        }\n    }\n\n    let average_score = reports\n        .iter()\n        .map(|pair| pair.circularity_score)\n        .sum::<f64>()\n        / reports.len() as f64;\n\n    Ok(PairwiseManifoldReport {\n        pair_reports: reports,\n        average_score,\n    })\n}\n\nfn pairwise_circularity_metrics(points: &[(f64, f64)]) -> Result<(f64, f64, f64), String> {\n    if points.len() < 3 {\n        return Err(\"not enough points for circularity\".to_string());\n    }\n\n    let n = points.len() as f64;\n    let mean_x = points.iter().map(|(x, _)| *x).sum::<f64>() / n;\n    let mean_y = points.iter().map(|(_, y)| *y).sum::<f64>() / n;\n\n    let mut var_x = 0.0;\n    let mut var_y = 0.0;\n    let mut cov_xy = 0.0;\n    for (x, y) in points {\n        let dx = x - mean_x;\n        let dy = y - mean_y;\n        var_x += dx * dx;\n        var_y += dy * dy;\n        cov_xy += dx * dy;\n    }\n    var_x /= n;\n    var_y /= n;\n    cov_xy /= n;\n\n    let trace = var_x + var_y;\n    let det = var_x * var_y - cov_xy * cov_xy;\n    if det <= 1.0e-12 || trace <= 0.0 {\n        return Err(\"degenerate covariance\".to_string());\n    }\n\n    let half_trace = trace * 0.5;\n    let disc = (half_trace * half_trace - det).max(0.0).sqrt();\n    let lambda1 = half_trace + disc;\n    let lambda2 = half_trace - disc;\n    if lambda2 <= 0.0 {\n        return Err(\"degenerate eigenvalues\".to_string());\n    }\n\n    let axis_ratio = (lambda1 / lambda2).sqrt();\n\n    let mut v1_x = cov_xy;\n    let mut v1_y = lambda1 - var_x;\n    if v1_x.abs() < 1.0e-12 && v1_y.abs() < 1.0e-12 {\n        v1_x = 1.0;\n        v1_y = 0.0;\n    }\n    let norm = (v1_x * v1_x + v1_y * v1_y).sqrt();\n    v1_x /= norm;\n    v1_y /= norm;\n    let v2_x = -v1_y;\n    let v2_y = v1_x;\n\n    let mut radii = Vec::with_capacity(points.len());\n    for (x, y) in points {\n        let dx = x - mean_x;\n        let dy = y - mean_y;\n        let p1 = dx * v1_x + dy * v1_y;\n        let p2 = dx * v2_x + dy * v2_y;\n        let w1 = p1 / lambda1.sqrt();\n        let w2 = p2 / lambda2.sqrt();\n        radii.push((w1 * w1 + w2 * w2).sqrt());\n    }\n\n    let mean_radius = radii.iter().sum::<f64>() / radii.len() as f64;\n    if mean_radius == 0.0 {\n        return Err(\"zero mean radius\".to_string());\n    }\n\n    let mut variance = 0.0;\n    for r in &radii {\n        let diff = r - mean_radius;\n        variance += diff * diff;\n    }\n    let std = (variance / radii.len() as f64).sqrt();\n    let radius_cv = std / mean_radius;\n    let score = 1.0 / (1.0 + radius_cv);\n\n    Ok((score, radius_cv, axis_ratio))\n}\n\n// Helper functions for verification - reserved for future integration\n#[allow(dead_code)]\nfn baseline_loss(\n    losses: &[(usize, f64)],\n    plateau_min_step: usize,\n    drop_window: usize,\n) -> Result<f64, String> {\n    let before: Vec<f64> = losses\n        .iter()\n        .filter(|(step, _)| *step <= plateau_min_step)\n        .map(|(_, loss)| *loss)\n        .collect();\n\n    if before.len() < drop_window || drop_window == 0 {\n        return Err(\"insufficient loss samples for baseline\".to_string());\n    }\n\n    let window = &before[before.len() - drop_window..];\n    Ok(window.iter().sum::<f64>() / drop_window as f64)\n}\n\n#[allow(dead_code)]\nfn find_drop_step(\n    losses: &[(usize, f64)],\n    plateau_min_step: usize,\n    baseline: f64,\n    drop_fraction: f64,\n) -> Result<usize, String> {\n    losses\n        .iter()\n        .filter(|(step, _)| *step >= plateau_min_step)\n        .find(|(_, loss)| *loss <= baseline * (1.0 - drop_fraction))\n        .map(|(step, _)| *step)\n        .ok_or_else(|| \"loss never drops below threshold\".to_string())\n}\n\nfn collect_mlp_post_relu_activations<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n    fixed_y: usize,\n) -> Result<Vec<Vec<f64>>, String> {\n    let modulus = ModularAdditionDataset::modulus();\n    let equals_token = ModularAdditionDataset::equals_token();\n\n    let mut inputs_vec = Vec::with_capacity(modulus * 3);\n    for x in 0..modulus {\n        inputs_vec.push(x as i32);\n        inputs_vec.push(fixed_y as i32);\n        inputs_vec.push(equals_token as i32);\n    }\n\n    let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n        .reshape([modulus, 3]);\n    let (_, mlp_acts) = model.forward_with_mlp_activations(inputs);\n    let [batch_size, d_ff] = mlp_acts.dims();\n\n    if batch_size != modulus {\n        return Err(format!(\n            \"unexpected activation batch size {} (expected {})\",\n            batch_size, modulus\n        ));\n    }\n\n    let data: Vec<f32> = mlp_acts.into_data().to_vec().unwrap();\n    let mut per_neuron = vec![Vec::with_capacity(batch_size); d_ff];\n    for batch in 0..batch_size {\n        for neuron in 0..d_ff {\n            let idx = batch * d_ff + neuron;\n            per_neuron[neuron].push(data[idx] as f64);\n        }\n    }\n\n    Ok(per_neuron)\n}\n\n#[allow(dead_code)]\nfn surface_autocorrelation_scores(\n    surface: &[Vec<f64>],\n    max_lag: usize,\n) -> Result<(f64, f64, f64, f64), String> {\n    let (height, width) = surface_dimensions(surface)?;\n    if height < 2 || width < 2 {\n        return Err(\"surface grid too small\".to_string());\n    }\n\n    let (mean, var) = surface_mean_variance(surface)?;\n    if var == 0.0 {\n        return Err(\"surface variance is zero\".to_string());\n    }\n\n    let max_lag = max_lag.min(height.saturating_sub(1)).min(width.saturating_sub(1));\n    if max_lag == 0 {\n        return Err(\"max_lag too small for surface\".to_string());\n    }\n\n    let diag_main = average_autocorrelation(surface, mean, var, max_lag, 1, 1)?;\n    let diag_anti = average_autocorrelation(surface, mean, var, max_lag, 1, -1)?;\n    let axis_x = average_autocorrelation(surface, mean, var, max_lag, 1, 0)?;\n    let axis_y = average_autocorrelation(surface, mean, var, max_lag, 0, 1)?;\n\n    Ok((diag_main, diag_anti, axis_x, axis_y))\n}\n\n#[allow(dead_code)]\nfn surface_dimensions(surface: &[Vec<f64>]) -> Result<(usize, usize), String> {\n    let height = surface.len();\n    if height == 0 {\n        return Err(\"surface is empty\".to_string());\n    }\n    let width = surface[0].len();\n    if width == 0 {\n        return Err(\"surface has empty rows\".to_string());\n    }\n    for (idx, row) in surface.iter().enumerate() {\n        if row.len() != width {\n            return Err(format!(\n                \"surface row {} has length {}, expected {}\",\n                idx,\n                row.len(),\n                width\n            ));\n        }\n    }\n    Ok((height, width))\n}\n\n#[allow(dead_code)]\nfn surface_mean_variance(surface: &[Vec<f64>]) -> Result<(f64, f64), String> {\n    let (height, width) = surface_dimensions(surface)?;\n    let mut sum = 0.0;\n    for row in surface {\n        for value in row {\n            sum += *value;\n        }\n    }\n    let count = (height * width) as f64;\n    let mean = sum / count;\n    let mut variance = 0.0;\n    for row in surface {\n        for value in row {\n            let diff = *value - mean;\n            variance += diff * diff;\n        }\n    }\n    variance /= count;\n    Ok((mean, variance))\n}\n\n#[allow(dead_code)]\nfn average_autocorrelation(\n    surface: &[Vec<f64>],\n    mean: f64,\n    variance: f64,\n    max_lag: usize,\n    dx: i32,\n    dy: i32,\n) -> Result<f64, String> {\n    let (height, width) = surface_dimensions(surface)?;\n    let mut total = 0.0;\n    let mut lag_count = 0usize;\n\n    for lag in 1..=max_lag {\n        let mut sum = 0.0;\n        let mut count = 0usize;\n        let lag = lag as i32;\n        for x in 0..height {\n            for y in 0..width {\n                let x2 = x as i32 + dx * lag;\n                let y2 = y as i32 + dy * lag;\n                if x2 < 0 || y2 < 0 {\n                    continue;\n                }\n                let x2 = x2 as usize;\n                let y2 = y2 as usize;\n                if x2 >= height || y2 >= width {\n                    continue;\n                }\n                let a = surface[x][y] - mean;\n                let b = surface[x2][y2] - mean;\n                sum += a * b;\n                count += 1;\n            }\n        }\n        if count == 0 {\n            continue;\n        }\n        let corr = sum / (count as f64 * variance);\n        total += corr.abs();\n        lag_count += 1;\n    }\n\n    if lag_count == 0 {\n        return Err(\"no valid lags for autocorrelation\".to_string());\n    }\n\n    Ok(total / lag_count as f64)\n}\n\nfn analyze_mlp_activation_wave(\n    neuron_index: usize,\n    signal: &[f64],\n    config: &MlpWaveVerificationConfig,\n) -> Result<MlpWaveNeuronReport, String> {\n    if signal.len() < 3 {\n        return Err(format!(\n            \"activation signal for neuron {} is too short\",\n            neuron_index\n        ));\n    }\n\n    let negative_count = signal\n        .iter()\n        .filter(|value| **value < -config.negative_tolerance)\n        .count();\n    let negative_fraction = negative_count as f64 / signal.len() as f64;\n    if negative_fraction > config.max_negative_fraction {\n        return Err(format!(\n            \"neuron {} has {:.3} negative activations\",\n            neuron_index, negative_fraction\n        ));\n    }\n\n    let (dominant_frequency, dominant_ratio) = dominant_frequency_ratio(signal)?;\n    if dominant_ratio < config.min_dominant_ratio {\n        return Err(format!(\n            \"neuron {} dominant frequency ratio {:.3} below threshold {:.3}\",\n            neuron_index, dominant_ratio, config.min_dominant_ratio\n        ));\n    }\n\n    let best_correlation =\n        best_rectified_sine_correlation(signal, dominant_frequency, config.phase_steps);\n    if best_correlation < config.min_correlation {\n        return Err(format!(\n            \"neuron {} rectified sine correlation {:.3} below threshold {:.3}\",\n            neuron_index, best_correlation, config.min_correlation\n        ));\n    }\n\n    Ok(MlpWaveNeuronReport {\n        neuron_index,\n        dominant_frequency,\n        dominant_ratio,\n        negative_fraction,\n        best_correlation,\n    })\n}\n\nfn dominant_frequency_ratio(signal: &[f64]) -> Result<(usize, f64), String> {\n    if signal.len() < 2 {\n        return Err(\"signal too short for FFT\".to_string());\n    }\n\n    let mut planner = FftPlanner::new();\n    let fft = planner.plan_fft_forward(signal.len());\n    let mut buffer: Vec<Complex<f64>> = signal\n        .iter()\n        .map(|&x| Complex::new(x, 0.0))\n        .collect();\n    fft.process(&mut buffer);\n    let magnitudes: Vec<f64> = buffer.iter().map(|c| c.norm()).collect();\n\n    if magnitudes.len() < 2 {\n        return Err(\"insufficient FFT bins\".to_string());\n    }\n\n    let mut dominant_idx = 1usize;\n    let mut dominant_mag = magnitudes[1];\n    for (idx, &mag) in magnitudes.iter().enumerate().skip(2) {\n        if mag > dominant_mag {\n            dominant_mag = mag;\n            dominant_idx = idx;\n        }\n    }\n\n    if dominant_idx == 0 {\n        return Err(\"dominant frequency is zero\".to_string());\n    }\n\n    let mut nonzero = magnitudes[1..].to_vec();\n    nonzero.sort_by(|a, b| a.partial_cmp(b).unwrap_or(std::cmp::Ordering::Equal));\n    let median_mag = nonzero[nonzero.len() / 2];\n    let ratio = if median_mag == 0.0 {\n        dominant_mag\n    } else {\n        dominant_mag / median_mag\n    };\n\n    Ok((dominant_idx, ratio))\n}\n\nfn best_rectified_sine_correlation(signal: &[f64], frequency: usize, phase_steps: usize) -> f64 {\n    if signal.is_empty() || frequency == 0 || phase_steps == 0 {\n        return 0.0;\n    }\n\n    let tau = std::f64::consts::PI * 2.0;\n    let mut best = -1.0;\n    for step in 0..phase_steps {\n        let phase = tau * step as f64 / phase_steps as f64;\n        let wave: Vec<f64> = (0..signal.len())\n            .map(|idx| {\n                let angle = tau * frequency as f64 * idx as f64 / signal.len() as f64 + phase;\n                angle.sin().max(0.0)\n            })\n            .collect();\n        let corr = pearson_correlation(signal, &wave);\n        if corr > best {\n            best = corr;\n        }\n    }\n\n    best\n}\n\nfn pearson_correlation(a: &[f64], b: &[f64]) -> f64 {\n    if a.len() != b.len() || a.is_empty() {\n        return 0.0;\n    }\n\n    let mean_a = a.iter().sum::<f64>() / a.len() as f64;\n    let mean_b = b.iter().sum::<f64>() / b.len() as f64;\n\n    let mut num = 0.0;\n    let mut denom_a = 0.0;\n    let mut denom_b = 0.0;\n    for (x, y) in a.iter().zip(b.iter()) {\n        let da = x - mean_a;\n        let db = y - mean_b;\n        num += da * db;\n        denom_a += da * da;\n        denom_b += db * db;\n    }\n\n    if denom_a == 0.0 || denom_b == 0.0 {\n        return 0.0;\n    }\n\n    num / (denom_a.sqrt() * denom_b.sqrt())\n}\n\n/// Verify that the model actually learned modular addition\n/// by testing it on ALL possible examples systematically\npub fn verify_full_accuracy<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n) -> (f32, f32) {\n    println!(\"ðŸ” Verifying model on COMPLETE dataset (no sampling)...\");\n\n    let train_dataset = ModularAdditionDataset::new(true, 42);\n    let val_dataset = ModularAdditionDataset::new(false, 42);\n\n    let train_acc = test_all_examples(model, &train_dataset, device);\n    let val_acc = test_all_examples(model, &val_dataset, device);\n\n    println!(\"ðŸ“Š FULL Dataset Results:\");\n    println!(\"   Training: {}/{} = {:.2}%\",\n        (train_acc * train_dataset.len() as f32) as usize,\n        train_dataset.len(),\n        train_acc * 100.0);\n    println!(\"   Validation: {}/{} = {:.2}%\",\n        (val_acc * val_dataset.len() as f32) as usize,\n        val_dataset.len(),\n        val_acc * 100.0);\n\n    (train_acc, val_acc)\n}\n\n/// Test model on ALL examples in dataset (no random sampling)\nfn test_all_examples<B: Backend>(\n    model: &Transformer<B>,\n    dataset: &ModularAdditionDataset,\n    device: &B::Device,\n) -> f32 {\n    let mut total_correct = 0;\n    let total = dataset.len();\n\n    // Test in batches to avoid memory issues\n    let batch_size = 100;\n    for batch_start in (0..total).step_by(batch_size) {\n        let batch_end = (batch_start + batch_size).min(total);\n        let batch_len = batch_end - batch_start;\n\n        let mut inputs_vec = Vec::new();\n        let mut targets_vec = Vec::new();\n\n        for idx in batch_start..batch_end {\n            let (input, target) = dataset.get(idx).unwrap();\n            inputs_vec.extend(input);\n            targets_vec.push(target as i32);\n        }\n\n        let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n            .reshape([batch_len, 3]);\n\n        let logits = model.forward(inputs);\n        let predictions = logits.argmax(1).squeeze::<1>();\n        let predictions_vec: Vec<i32> = predictions.into_data().to_vec().unwrap();\n\n        let correct = predictions_vec\n            .iter()\n            .zip(targets_vec.iter())\n            .filter(|(p, t)| p == t)\n            .count();\n\n        total_correct += correct;\n    }\n\n    total_correct as f32 / total as f32\n}\n\n/// Verify that weight norms decrease over time with weight decay\n/// and show correlation with the grokking transition\npub fn verify_weight_norm_decay(\n    weight_norm_history: &[(usize, f64)],\n    grok_step: Option<usize>,\n    config: &WeightNormDecayConfig,\n) -> Result<WeightNormDecayReport, String> {\n    if weight_norm_history.len() < 2 {\n        return Err(\"weight norm history requires at least 2 snapshots\".to_string());\n    }\n\n    let initial_norm = weight_norm_history\n        .first()\n        .ok_or_else(|| \"weight norm history is empty\".to_string())?\n        .1;\n    let final_norm = weight_norm_history\n        .last()\n        .ok_or_else(|| \"weight norm history is empty\".to_string())?\n        .1;\n\n    if initial_norm <= 0.0 {\n        return Err(\"initial weight norm must be positive\".to_string());\n    }\n\n    // Check that norms are generally decreasing (with weight decay)\n    let relative_decrease = (initial_norm - final_norm) / initial_norm;\n    if relative_decrease < config.min_relative_decrease {\n        return Err(format!(\n            \"weight norm decrease {:.4} below threshold {:.4} (initial {:.4}, final {:.4})\",\n            relative_decrease, config.min_relative_decrease, initial_norm, final_norm\n        ));\n    }\n\n    // Check that norms are not increasing (weight decay should monotonically decrease norms)\n    for window in weight_norm_history.windows(2) {\n        let prev_norm = window[0].1;\n        let next_norm = window[1].1;\n        if next_norm > prev_norm * 1.01 {\n            // Allow 1% tolerance for numerical noise\n            return Err(format!(\n                \"weight norm increased from {:.4} to {:.4} at step {}\",\n                prev_norm, next_norm, window[1].0\n            ));\n        }\n    }\n\n    // If grok step is provided, check that norms show a shift around grokking\n    let grok_step_norm = if let Some(step) = grok_step {\n        weight_norm_history\n            .iter()\n            .filter(|(s, _)| {\n                *s >= step.saturating_sub(config.grok_step_window)\n                    && *s <= step + config.grok_step_window\n            })\n            .map(|(_, norm)| *norm)\n            .next()\n    } else {\n        None\n    };\n\n    Ok(WeightNormDecayReport {\n        initial_norm,\n        final_norm,\n        relative_decrease,\n        grok_step_norm,\n    })\n}\n\n/// Verify the characteristic \"snake curve\" shape of grokking phenomenon\n/// on log-scale plots: train accuracy rises early, validation stays flat then jumps\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_snake_curve_shape(\n    train_acc: &[(usize, f32)],\n    val_acc: &[(usize, f32)],\n    config: &SnakeCurveConfig,\n) -> Result<SnakeCurveReport, String> {\n    if train_acc.is_empty() || val_acc.is_empty() {\n        return Err(\"empty accuracy history\".to_string());\n    }\n\n    // Phase 1: Verify early train convergence (immediate drop on log scale)\n    let train_converged_step = train_acc\n        .iter()\n        .find(|(_, acc)| *acc >= config.train_convergence_threshold)\n        .map(|(step, _)| *step)\n        .ok_or_else(|| {\n            format!(\n                \"train accuracy never reaches {} threshold\",\n                config.train_convergence_threshold\n            )\n        })?;\n\n    if train_converged_step > config.early_step_max {\n        return Err(format!(\n            \"train accuracy converges at step {}, expected before {}\",\n            train_converged_step, config.early_step_max\n        ));\n    }\n\n    // Phase 2: Verify validation plateau (flat on log scale)\n    let plateau_samples: Vec<f32> = val_acc\n        .iter()\n        .filter(|(step, _)| *step >= 1 && *step <= config.val_plateau_min_step)\n        .map(|(_, acc)| *acc)\n        .collect();\n\n    if plateau_samples.is_empty() {\n        return Err(\"no validation samples in plateau region\".to_string());\n    }\n\n    let val_plateau_max = plateau_samples\n        .iter()\n        .fold(0.0f32, |max_val, &acc| max_val.max(acc));\n\n    if val_plateau_max > config.val_plateau_max_acc {\n        return Err(format!(\n            \"validation accuracy in plateau region too high: max {:.4} exceeds {:.4}\",\n            val_plateau_max, config.val_plateau_max_acc\n        ));\n    }\n\n    // Phase 3: Verify delayed validation jump (crash up on log scale)\n    let val_grok_step = val_acc\n        .iter()\n        .filter(|(step, _)| *step > config.val_plateau_min_step)\n        .find(|(_, acc)| *acc >= config.val_final_threshold)\n        .map(|(step, _)| *step)\n        .ok_or_else(|| {\n            format!(\n                \"validation accuracy never reaches {} after plateau\",\n                config.val_final_threshold\n            )\n        })?;\n\n    // Phase 4: Verify phase ordering (snake shape: early train, late val)\n    let curve_shape_valid = train_converged_step < config.val_plateau_min_step\n        && val_grok_step > config.val_plateau_min_step\n        && train_converged_step < val_grok_step;\n\n    if !curve_shape_valid {\n        return Err(format!(\n            \"invalid curve shape: train converged at {}, val plateau until {}, val grokked at {}\",\n            train_converged_step, config.val_plateau_min_step, val_grok_step\n        ));\n    }\n\n    Ok(SnakeCurveReport {\n        train_converged_by_step: train_converged_step,\n        val_plateau_max_acc: val_plateau_max,\n        val_grok_step,\n        curve_shape_valid,\n    })\n}\n\n/// Test specific examples to see if model actually computes mod p\npub fn test_specific_examples<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n) {\n    println!();\n    println!(\"ðŸ§ª Testing specific examples:\");\n\n    let modulus = ModularAdditionDataset::modulus();\n    let equals_token = ModularAdditionDataset::equals_token();\n    let test_cases = vec![\n        (0, 0, 0),\n        (1, 1, 2),\n        (50, 50, (50 + 50) % modulus),\n        (112, 1, (112 + 1) % modulus),\n        (56, 57, (56 + 57) % modulus),\n        (112, 112, (112 + 112) % modulus),\n        (10, 20, 30),\n        (60, 60, (60 + 60) % modulus),\n    ];\n\n    for (a, b, expected) in test_cases {\n        let input_vec = vec![a as i32, b as i32, equals_token as i32];\n        let input = Tensor::<B, 1, Int>::from_ints(input_vec.as_slice(), device)\n            .reshape([1, 3]);\n\n        let logits = model.forward(input);\n        let prediction = logits.argmax(1).squeeze::<1>();\n        let pred_value: i32 = prediction.into_data().to_vec().unwrap()[0];\n\n        let correct = if pred_value == expected as i32 { \"âœ“\" } else { \"âœ—\" };\n        println!(\n            \"   {} + {} mod {} = {} | Model: {} {}\",\n            a, b, modulus, expected, pred_value, correct\n        );\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    fn make_histories(\n        train_acc: &[(usize, f32)],\n        val_acc: &[(usize, f32)],\n        val_loss: &[(usize, f64)],\n    ) -> (LossHistory, AccuracyHistory) {\n        let mut loss_history = LossHistory::new();\n        loss_history.val_snapshots = val_loss.to_vec();\n        let mut accuracy_history = AccuracyHistory::new();\n        accuracy_history.train_snapshots = train_acc.to_vec();\n        accuracy_history.val_snapshots = val_acc.to_vec();\n        (loss_history, accuracy_history)\n    }\n\n    #[test]\n    fn grokking_phase_verification_passes_on_synthetic_signal() {\n        let train_acc = vec![(0, 0.2), (500, 0.995), (1000, 0.999)];\n        let val_acc = vec![\n            (0, 0.01),\n            (500, 0.01),\n            (1500, 0.01),\n            (2200, 0.995),\n            (2500, 0.999),\n        ];\n        let val_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 2.0),\n            (2200, 0.2),\n            (2500, 0.15),\n        ];\n\n        let (loss_history, accuracy_history) = make_histories(&train_acc, &val_acc, &val_loss);\n        let config = GrokkingVerificationConfig {\n            early_step_max: 1000,\n            plateau_min_step: 1500,\n            generalization_window: 1500,\n            accuracy_window: 2,\n            loss_window: 2,\n            ..GrokkingVerificationConfig::default_for_modulus(ModularAdditionDataset::modulus())\n        };\n\n        let report = verify_grokking_phase_transition(&loss_history, &accuracy_history, &config)\n            .expect(\"expected grokking verification to pass\");\n        assert!(report.transition_step >= config.plateau_min_step);\n        assert!(report.early_train_step <= config.early_step_max);\n    }\n\n    #[test]\n    fn grokking_phase_verification_fails_without_transition() {\n        let train_acc = vec![(0, 0.2), (500, 0.995)];\n        let val_acc = vec![(0, 0.01), (500, 0.01), (1500, 0.02)];\n        let val_loss = vec![(0, 2.0), (500, 2.0), (1500, 1.9)];\n\n        let (loss_history, accuracy_history) = make_histories(&train_acc, &val_acc, &val_loss);\n        let config = GrokkingVerificationConfig {\n            early_step_max: 1000,\n            plateau_min_step: 1000,\n            generalization_window: 1000,\n            accuracy_window: 2,\n            loss_window: 2,\n            ..GrokkingVerificationConfig::default_for_modulus(ModularAdditionDataset::modulus())\n        };\n\n        let err = verify_grokking_phase_transition(&loss_history, &accuracy_history, &config)\n            .expect_err(\"expected grokking verification to fail\");\n        assert!(err.contains(\"validation accuracy never reaches target\"));\n    }\n\n    #[test]\n    fn grokking_phase_verification_fails_with_slow_accuracy_rise() {\n        let train_acc = vec![(0, 0.2), (500, 0.995), (1000, 0.999)];\n        let val_acc = vec![\n            (0, 0.01),\n            (500, 0.01),\n            (1500, 0.5),\n            (2000, 0.6),\n            (2200, 0.995),\n            (2500, 0.999),\n        ];\n        let val_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 2.0),\n            (2200, 0.2),\n            (2500, 0.15),\n        ];\n\n        let (loss_history, accuracy_history) = make_histories(&train_acc, &val_acc, &val_loss);\n        let config = GrokkingVerificationConfig {\n            early_step_max: 1000,\n            plateau_min_step: 1000,\n            generalization_window: 2000,\n            accuracy_window: 2,\n            loss_window: 2,\n            ..GrokkingVerificationConfig::default_for_modulus(ModularAdditionDataset::modulus())\n        };\n\n        let err = verify_grokking_phase_transition(&loss_history, &accuracy_history, &config)\n            .expect_err(\"expected grokking verification to fail\");\n        assert!(err.contains(\"plateau too high\"));\n    }\n\n    #[test]\n    fn restricted_loss_detection_passes_with_early_drop() {\n        let full_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 2.0),\n            (2500, 2.0),\n            (3500, 0.3),\n        ];\n        let restricted_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 0.8),\n            (2500, 0.4),\n            (3500, 0.3),\n        ];\n\n        let config = RestrictedLossVerificationConfig {\n            plateau_min_step: 1000,\n            drop_window: 2,\n            drop_fraction: 0.5,\n            min_step_lead: 500,\n        };\n\n        let report = verify_restricted_loss_early_drop(&full_loss, &restricted_loss, &config)\n            .expect(\"expected restricted loss verification to pass\");\n        assert!(report.restricted_drop_step < report.full_drop_step);\n    }\n\n    #[test]\n    fn restricted_loss_detection_fails_when_full_drops_first() {\n        let full_loss = vec![(0, 2.0), (1000, 1.0), (1500, 0.8)];\n        let restricted_loss = vec![(0, 2.0), (1000, 2.0), (1500, 0.8)];\n\n        let config = RestrictedLossVerificationConfig {\n            plateau_min_step: 500,\n            drop_window: 1,\n            drop_fraction: 0.3,\n            min_step_lead: 200,\n        };\n\n        let err = verify_restricted_loss_early_drop(&full_loss, &restricted_loss, &config)\n            .expect_err(\"expected restricted loss verification to fail\");\n        assert!(err.contains(\"restricted loss drops\"));\n    }\n\n    #[test]\n    fn excluded_loss_spike_verification_passes() {\n        let excluded_loss = vec![(0, 1.0), (1500, 2.2), (3000, 1.1)];\n        let config = ExcludedLossSpikeConfig {\n            min_relative_increase: 0.5,\n        };\n\n        let report = verify_excluded_loss_spike(&excluded_loss, &config)\n            .expect(\"expected excluded loss spike verification to pass\");\n        assert_eq!(report.spike_step, 1500);\n    }\n\n    #[test]\n    fn excluded_loss_spike_verification_fails_without_spike() {\n        let excluded_loss = vec![(0, 1.0), (1500, 1.1), (3000, 1.0)];\n        let config = ExcludedLossSpikeConfig {\n            min_relative_increase: 0.5,\n        };\n\n        let err = verify_excluded_loss_spike(&excluded_loss, &config)\n            .expect_err(\"expected excluded loss spike verification to fail\");\n        assert!(err.contains(\"excluded loss spike too small\"));\n    }\n\n    #[test]\n    fn restricted_loss_drop_verification_passes() {\n        let restricted_loss = vec![\n            (0, 2.0),\n            (1000, 1.8),\n            (5000, 1.2),\n            (7000, 0.8),\n            (10000, 0.9),\n        ];\n        let config = RestrictedLossDropConfig {\n            min_relative_decrease: 0.3,\n        };\n\n        let report = verify_restricted_loss_drop(&restricted_loss, &config)\n            .expect(\"expected restricted loss drop verification to pass\");\n        assert_eq!(report.drop_step, 7000);\n        assert!(report.baseline_loss > report.drop_loss);\n        assert!((report.baseline_loss - report.drop_loss) / report.baseline_loss >= 0.3);\n    }\n\n    #[test]\n    fn restricted_loss_drop_verification_fails_without_drop() {\n        let restricted_loss = vec![\n            (0, 2.0),\n            (1000, 2.1),\n            (5000, 1.9),\n            (7000, 2.0),\n            (10000, 2.05),\n        ];\n        let config = RestrictedLossDropConfig {\n            min_relative_decrease: 0.3,\n        };\n\n        let err = verify_restricted_loss_drop(&restricted_loss, &config)\n            .expect_err(\"expected restricted loss drop verification to fail\");\n        assert!(err.contains(\"restricted loss decrease too small\"));\n    }\n\n    #[test]\n    fn mlp_activation_wave_verification_passes_on_rectified_sine() {\n        let n = ModularAdditionDataset::modulus();\n        let freq = 5.0;\n        let mut signal = Vec::with_capacity(n);\n        for idx in 0..n {\n            let angle = 2.0 * std::f64::consts::PI * freq * idx as f64 / n as f64;\n            let rectified = angle.sin().max(0.0);\n            let wobble = (2.0 * std::f64::consts::PI * 2.0 * idx as f64 / n as f64)\n                .sin()\n                .abs()\n                * 0.02;\n            signal.push(rectified + wobble);\n        }\n\n        let config = MlpWaveVerificationConfig {\n            max_negative_fraction: 0.0,\n            negative_tolerance: 1e-6,\n            min_dominant_ratio: 3.0,\n            min_correlation: 0.7,\n            phase_steps: 32,\n        };\n\n        let report =\n            verify_mlp_activation_waves_from_signals(&[(0, signal)], &config)\n                .expect(\"expected rectified sine to pass\");\n        assert_eq!(report.neuron_reports.len(), 1);\n    }\n\n    #[test]\n    fn mlp_activation_wave_verification_fails_on_noise() {\n        let n = ModularAdditionDataset::modulus();\n        let signal: Vec<f64> = (0..n).map(|idx| idx as f64 / n as f64).collect();\n\n        let config = MlpWaveVerificationConfig {\n            max_negative_fraction: 0.0,\n            negative_tolerance: 1e-6,\n            min_dominant_ratio: 4.0,\n            min_correlation: 0.8,\n            phase_steps: 16,\n        };\n\n        let err =\n            verify_mlp_activation_waves_from_signals(&[(0, signal)], &config)\n                .expect_err(\"expected linear signal to fail\");\n        assert!(err.contains(\"dominant frequency ratio\") || err.contains(\"correlation\"));\n    }\n\n    #[test]\n    fn pairwise_manifold_transition_passes_on_elliptic_signals() {\n        let n = ModularAdditionDataset::modulus();\n        let freq = 5.0;\n        let mut post_signals = Vec::new();\n        for neuron_idx in 0..7usize {\n            let phase = 2.0 * std::f64::consts::PI * neuron_idx as f64 / 7.0;\n            let signal: Vec<f64> = (0..n)\n                .map(|idx| {\n                    let angle = 2.0 * std::f64::consts::PI * freq * idx as f64 / n as f64 + phase;\n                    angle.sin()\n                })\n                .collect();\n            post_signals.push((neuron_idx, signal));\n        }\n\n        let mut pre_signals = Vec::new();\n        for neuron_idx in 0..7usize {\n            let signal: Vec<f64> = (0..n)\n                .map(|idx| idx as f64 / n as f64 + neuron_idx as f64 * 0.01)\n                .collect();\n            pre_signals.push((neuron_idx, signal));\n        }\n\n        let config = PairwiseManifoldConfig {\n            fixed_y: 0,\n            neuron_indices: (0..7).collect(),\n            min_post_score: 0.75,\n            max_pre_score: 0.55,\n            max_axis_ratio: 8.0,\n            min_pairs_passing: 8,\n        };\n\n        let report = verify_pairwise_manifold_transition_from_signals(\n            &pre_signals,\n            &post_signals,\n            &config,\n        )\n        .expect(\"expected pairwise manifold transition to pass\");\n        assert!(report.passing_pairs >= config.min_pairs_passing);\n    }\n\n    #[test]\n    fn pairwise_manifold_transition_fails_without_rings() {\n        let n = ModularAdditionDataset::modulus();\n        let mut signals = Vec::new();\n        for neuron_idx in 0..7usize {\n            let signal: Vec<f64> = (0..n)\n                .map(|idx| idx as f64 / n as f64 + neuron_idx as f64 * 0.02)\n                .collect();\n            signals.push((neuron_idx, signal));\n        }\n\n        let config = PairwiseManifoldConfig {\n            fixed_y: 0,\n            neuron_indices: (0..7).collect(),\n            min_post_score: 0.8,\n            max_pre_score: 0.55,\n            max_axis_ratio: 4.0,\n            min_pairs_passing: 5,\n        };\n\n        let err = verify_pairwise_manifold_transition_from_signals(\n            &signals,\n            &signals,\n            &config,\n        )\n        .expect_err(\"expected pairwise manifold transition to fail\");\n        assert!(err.contains(\"post-grok\") || err.contains(\"pre-grok\"));\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_passes_on_diagonal_corrugation() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create diagonal ridges: f(x,y) = sin(2Ï€ * freq * (x+y) / n)\n        let freq = 3.0;\n        for x in 0..n {\n            for y in 0..n {\n                let angle = 2.0 * std::f64::consts::PI * freq * (x + y) as f64 / n as f64;\n                surface[x][y] = angle.sin();\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let report = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect(\"expected diagonal ridge verification to pass\");\n        assert!(report.diagonal_score > report.axis_score);\n        assert!(report.ratio >= config.min_diagonal_ratio);\n        assert!(report.diagonal_score >= config.min_diagonal_score);\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_passes_on_anti_diagonal_pattern() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create anti-diagonal ridges: f(x,y) = sin(2Ï€ * freq * (x-y) / n)\n        let freq = 3.0;\n        for x in 0..n {\n            for y in 0..n {\n                let angle = 2.0 * std::f64::consts::PI * freq * (x as i32 - y as i32) as f64 / n as f64;\n                surface[x][y] = angle.sin();\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let report = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect(\"expected anti-diagonal ridge verification to pass\");\n        assert!(report.diagonal_score > report.axis_score);\n        assert!(report.ratio >= config.min_diagonal_ratio);\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_fails_on_axis_aligned_pattern() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create axis-aligned ridges: f(x,y) = sin(2Ï€ * freq * x / n)\n        let freq = 3.0;\n        for x in 0..n {\n            for y in 0..n {\n                let angle = 2.0 * std::f64::consts::PI * freq * x as f64 / n as f64;\n                surface[x][y] = angle.sin();\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let err = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect_err(\"expected axis-aligned pattern to fail diagonal verification\");\n        assert!(err.contains(\"diagonal/axis ratio\") || err.contains(\"diagonal autocorrelation\"));\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_fails_on_flat_surface() {\n        let n = 50;\n        let surface = vec![vec![1.5; n]; n];\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let err = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect_err(\"expected flat surface to fail\");\n        assert!(err.contains(\"surface variance is zero\"));\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_fails_on_random_noise() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create random noise (low autocorrelation everywhere)\n        for x in 0..n {\n            for y in 0..n {\n                surface[x][y] = ((x * 17 + y * 31) % 100) as f64 / 100.0;\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let err = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect_err(\"expected random noise to fail diagonal verification\");\n        assert!(err.contains(\"diagonal/axis ratio\") || err.contains(\"diagonal autocorrelation\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_passes_on_valid_grokking_curve() {\n        // Simulate typical grokking: train converges early, val plateaus then jumps\n        let train_acc = vec![\n            (1, 0.10),\n            (10, 0.50),\n            (100, 0.80),\n            (500, 0.98),\n            (1000, 0.99),\n            (2000, 0.995),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (10, 0.009),\n            (100, 0.01),\n            (500, 0.011),\n            (1000, 0.009),\n            (1500, 0.01),\n            (2000, 0.92),\n            (3000, 0.98),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 1500,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let report = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect(\"expected snake curve verification to pass\");\n\n        assert_eq!(report.train_converged_by_step, 500);\n        assert!(report.val_plateau_max_acc < config.val_plateau_max_acc);\n        assert_eq!(report.val_grok_step, 2000);\n        assert!(report.curve_shape_valid);\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_on_simultaneous_convergence() {\n        // Both train and val converge at the same time (no grokking)\n        let train_acc = vec![\n            (1, 0.10),\n            (100, 0.98),\n            (500, 0.99),\n        ];\n        let val_acc = vec![\n            (1, 0.10),\n            (100, 0.97),\n            (500, 0.98),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 200,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let err = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect_err(\"expected snake curve verification to fail\");\n        assert!(err.contains(\"plateau region too high\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_on_slow_train_convergence() {\n        // Train takes too long to converge (not the typical snake curve)\n        let train_acc = vec![\n            (1, 0.10),\n            (500, 0.50),\n            (1000, 0.70),\n            (2000, 0.98),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (500, 0.01),\n            (1000, 0.01),\n            (2000, 0.92),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 1500,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let err = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect_err(\"expected snake curve verification to fail\");\n        assert!(err.contains(\"converges at step 2000\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_without_val_jump() {\n        // Validation never jumps up (no generalization)\n        let train_acc = vec![\n            (1, 0.10),\n            (100, 0.98),\n            (500, 0.99),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (100, 0.01),\n            (500, 0.01),\n            (2000, 0.02),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 200,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let err = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect_err(\"expected snake curve verification to fail\");\n        assert!(err.contains(\"validation accuracy never reaches\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_on_wrong_phase_ordering() {\n        // Val jumps before train converges (backwards snake)\n        let train_acc = vec![\n            (1, 0.10),\n            (500, 0.50),\n            (2000, 0.98),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (100, 0.95),\n            (500, 0.98),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 200,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let result = verify_snake_curve_shape(&train_acc, &val_acc, &config);\n        // Should fail because val jumps before train converges (not a proper snake curve)\n        // This can fail for multiple reasons, but should definitely fail\n        assert!(result.is_err(), \"Expected snake curve verification to fail but it passed\");\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_passes_on_decreasing_norms() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 85.0),\n            (1000, 72.0),\n            (1500, 63.0),\n            (2000, 58.0),\n            (2500, 55.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let report = verify_weight_norm_decay(&weight_norms, Some(2000), &config)\n            .expect(\"expected weight norm decay verification to pass\");\n\n        assert_eq!(report.initial_norm, 100.0);\n        assert_eq!(report.final_norm, 55.0);\n        assert!((report.relative_decrease - 0.45).abs() < 0.01);\n        assert!(report.grok_step_norm.is_some());\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_fails_on_flat_norms() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 99.0),\n            (1000, 98.5),\n            (1500, 98.0),\n            (2000, 97.5),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let err = verify_weight_norm_decay(&weight_norms, None, &config)\n            .expect_err(\"expected weight norm decay verification to fail\");\n        assert!(err.contains(\"weight norm decrease\"));\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_fails_on_increasing_norms() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 95.0),\n            (1000, 105.0), // Increase here\n            (1500, 90.0),\n            (2000, 85.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let err = verify_weight_norm_decay(&weight_norms, None, &config)\n            .expect_err(\"expected weight norm decay verification to fail\");\n        assert!(err.contains(\"weight norm increased\"));\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_allows_small_noise() {\n        // Weight decay with small numerical noise (within 1% tolerance)\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 95.0),\n            (1000, 90.5),  // Small increase within tolerance\n            (1500, 90.0),\n            (2000, 85.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let report = verify_weight_norm_decay(&weight_norms, None, &config)\n            .expect(\"expected weight norm decay with small noise to pass\");\n\n        assert_eq!(report.initial_norm, 100.0);\n        assert_eq!(report.final_norm, 85.0);\n        assert!((report.relative_decrease - 0.15).abs() < 0.01);\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_finds_grok_step_norm() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (1000, 80.0),\n            (1800, 68.0),  // Within window of grok step (2200 - 500 = 1700)\n            (2000, 65.0),\n            (2200, 62.0),  // Around grok step\n            (3000, 55.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let report = verify_weight_norm_decay(&weight_norms, Some(2200), &config)\n            .expect(\"expected weight norm decay verification to pass\");\n\n        assert!(report.grok_step_norm.is_some());\n        let grok_norm = report.grok_step_norm.unwrap();\n        // Should find the first norm in the window [1700, 2700], which is 68.0 at step 1800\n        assert!((grok_norm - 68.0).abs() < 0.01);\n    }\n}\n","structuredPatch":[{"oldStart":1073,"oldLines":6,"newStart":1073,"newLines":7,"lines":["     Ok(total / lag_count as f64)"," }"," ","+#[allow(dead_code)]"," fn analyze_mlp_activation_wave(","     neuron_index: usize,","     signal: &[f64],"]}],"userModified":false,"replaceAll":false}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01MD7yhkBFWSVyAtUSixSEtd","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01DMy96VLEQbNKys9k38YeRg","name":"Edit","input":{"replace_all":false,"file_path":"/Users/admin/MechInt/src/verify.rs","old_string":"fn dominant_frequency_ratio(signal: &[f64]) -> Result<(usize, f64), String> {","new_string":"#[allow(dead_code)]\nfn dominant_frequency_ratio(signal: &[f64]) -> Result<(usize, f64), String> {"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":329,"cache_read_input_tokens":64101,"cache_creation":{"ephemeral_5m_input_tokens":329,"ephemeral_1h_input_tokens":0},"output_tokens":184,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"4fb97bcf-c5f5-4559-b4b0-b40542024239"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01DMy96VLEQbNKys9k38YeRg","type":"tool_result","content":"The file /Users/admin/MechInt/src/verify.rs has been updated. Here's the result of running `cat -n` on a snippet of the edited file:\n  1123â†’        best_correlation,\n  1124â†’    })\n  1125â†’}\n  1126â†’\n  1127â†’#[allow(dead_code)]\n  1128â†’fn dominant_frequency_ratio(signal: &[f64]) -> Result<(usize, f64), String> {\n  1129â†’    if signal.len() < 2 {\n  1130â†’        return Err(\"signal too short for FFT\".to_string());\n  1131â†’    }\n  1132â†’"}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"739175c7-f835-4fd9-af58-7487b0828264","tool_use_result":{"filePath":"/Users/admin/MechInt/src/verify.rs","oldString":"fn dominant_frequency_ratio(signal: &[f64]) -> Result<(usize, f64), String> {","newString":"#[allow(dead_code)]\nfn dominant_frequency_ratio(signal: &[f64]) -> Result<(usize, f64), String> {","originalFile":"use burn::{\n    data::dataset::Dataset,\n    tensor::{backend::Backend, Int, Tensor},\n};\nuse rustfft::{num_complex::Complex, FftPlanner};\n\nuse crate::analysis::{AccuracyHistory, LossHistory};\nuse crate::data::ModularAdditionDataset;\nuse crate::model::Transformer;\n\n#[derive(Debug, Clone)]\npub struct GrokkingVerificationConfig {\n    pub early_train_acc_threshold: f32,\n    pub early_step_max: usize,\n    pub plateau_min_step: usize,\n    pub generalization_window: usize,\n    pub chance_accuracy: f32,\n    pub chance_tolerance: f32,\n    pub target_val_acc_threshold: f32,\n    pub accuracy_window: usize,\n    pub loss_window: usize,\n    pub loss_drop_fraction: f64,\n}\n\nimpl GrokkingVerificationConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        Self {\n            early_train_acc_threshold: 0.99,\n            early_step_max: 1000,\n            plateau_min_step: 1000,\n            generalization_window: 10_000,\n            chance_accuracy: 1.0 / modulus as f32,\n            chance_tolerance: 0.02,\n            target_val_acc_threshold: 0.99,\n            accuracy_window: 5,\n            loss_window: 5,\n            loss_drop_fraction: 0.5,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct GrokkingPhaseReport {\n    pub early_train_step: usize,\n    pub transition_step: usize,\n    pub plateau_max_val_acc: f32,\n    pub loss_drop_ratio: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossVerificationConfig {\n    pub plateau_min_step: usize,\n    pub drop_window: usize,\n    pub drop_fraction: f64,\n    pub min_step_lead: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossReport {\n    pub restricted_drop_step: usize,\n    pub full_drop_step: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossDropConfig {\n    pub min_relative_decrease: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossDropReport {\n    pub drop_step: usize,\n    pub drop_loss: f64,\n    pub baseline_loss: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct ExcludedLossSpikeConfig {\n    pub min_relative_increase: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct ExcludedLossSpikeReport {\n    pub spike_step: usize,\n    pub spike_loss: f64,\n    pub baseline_loss: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct MlpWaveVerificationConfig {\n    pub max_negative_fraction: f64,\n    pub negative_tolerance: f64,\n    pub min_dominant_ratio: f64,\n    pub min_correlation: f64,\n    pub phase_steps: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct MlpWaveNeuronReport {\n    pub neuron_index: usize,\n    pub dominant_frequency: usize,\n    pub dominant_ratio: f64,\n    pub negative_fraction: f64,\n    pub best_correlation: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct MlpWaveReport {\n    pub neuron_reports: Vec<MlpWaveNeuronReport>,\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldConfig {\n    pub fixed_y: usize,\n    pub neuron_indices: Vec<usize>,\n    pub min_post_score: f64,\n    pub max_pre_score: f64,\n    pub max_axis_ratio: f64,\n    pub min_pairs_passing: usize,\n}\n\nimpl PairwiseManifoldConfig {\n    pub fn default_for_modulus(_modulus: usize) -> Self {\n        Self {\n            fixed_y: 0,\n            neuron_indices: (0..7).collect(),\n            min_post_score: 0.75,\n            max_pre_score: 0.55,\n            max_axis_ratio: 8.0,\n            min_pairs_passing: 10,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldPairReport {\n    pub neuron_a: usize,\n    pub neuron_b: usize,\n    pub circularity_score: f64,\n    pub radius_cv: f64,\n    pub axis_ratio: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldReport {\n    pub pair_reports: Vec<PairwiseManifoldPairReport>,\n    pub average_score: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldTransitionReport {\n    pub pre_average_score: f64,\n    pub post_average_score: f64,\n    pub passing_pairs: usize,\n    pub total_pairs: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct DiagonalRidgeConfig {\n    pub neuron_index: usize,\n    pub max_lag: usize,\n    pub min_diagonal_ratio: f64,\n    pub min_diagonal_score: f64,\n}\n\nimpl DiagonalRidgeConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        let max_lag = (modulus / 10).max(4).min(20);\n        Self {\n            neuron_index: 10,\n            max_lag,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct DiagonalRidgeReport {\n    pub neuron_index: usize,\n    pub diagonal_score: f64,\n    pub axis_score: f64,\n    pub ratio: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct SnakeCurveConfig {\n    pub early_step_max: usize,\n    pub train_convergence_threshold: f32,\n    pub val_plateau_min_step: usize,\n    pub val_plateau_max_acc: f32,\n    pub val_final_threshold: f32,\n}\n\nimpl SnakeCurveConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        Self {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 1000,\n            val_plateau_max_acc: 1.0 / modulus as f32 + 0.05,\n            val_final_threshold: 0.90,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct WeightNormDecayConfig {\n    pub min_relative_decrease: f64,\n    pub plateau_min_step: usize,\n    pub grok_step_window: usize,\n}\n\nimpl WeightNormDecayConfig {\n    pub fn default_for_grokking() -> Self {\n        Self {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct WeightNormDecayReport {\n    pub initial_norm: f64,\n    pub final_norm: f64,\n    pub relative_decrease: f64,\n    pub grok_step_norm: Option<f64>,\n}\n\n#[derive(Debug, Clone)]\npub struct SnakeCurveReport {\n    pub train_converged_by_step: usize,\n    pub val_plateau_max_acc: f32,\n    pub val_grok_step: usize,\n    pub curve_shape_valid: bool,\n}\n\npub fn verify_grokking_phase_transition(\n    loss_history: &LossHistory,\n    accuracy_history: &AccuracyHistory,\n    config: &GrokkingVerificationConfig,\n) -> Result<GrokkingPhaseReport, String> {\n    let (early_train_step, _) = accuracy_history\n        .train_snapshots\n        .iter()\n        .find(|(_, acc)| *acc >= config.early_train_acc_threshold)\n        .ok_or_else(|| \"train accuracy never reaches threshold\".to_string())?;\n\n    if *early_train_step > config.early_step_max {\n        return Err(format!(\n            \"train accuracy reaches {:.2} after step {}, expected before {}\",\n            config.early_train_acc_threshold, early_train_step, config.early_step_max\n        ));\n    }\n\n    let plateau_max_val_acc = accuracy_history\n        .val_snapshots\n        .iter()\n        .filter(|(step, _)| *step <= config.plateau_min_step)\n        .map(|(_, acc)| *acc)\n        .fold(None::<f32>, |accum, value| {\n            Some(accum.map_or(value, |max_val| max_val.max(value)))\n        })\n        .ok_or_else(|| \"no validation accuracy samples before plateau_min_step\".to_string())?;\n\n    if plateau_max_val_acc > config.chance_accuracy + config.chance_tolerance {\n        return Err(format!(\n            \"validation accuracy before step {} exceeds chance (max {:.4})\",\n            config.plateau_min_step, plateau_max_val_acc\n        ));\n    }\n\n    let (transition_step, _) = accuracy_history\n        .val_snapshots\n        .iter()\n        .find(|(step, acc)| {\n            *step >= config.plateau_min_step && *acc >= config.target_val_acc_threshold\n        })\n        .ok_or_else(|| \"validation accuracy never reaches target threshold\".to_string())?;\n\n    let latest_allowed = config.plateau_min_step + config.generalization_window;\n    if *transition_step > latest_allowed {\n        return Err(format!(\n            \"validation accuracy reaches {:.2} at step {}, expected before {}\",\n            config.target_val_acc_threshold, transition_step, latest_allowed\n        ));\n    }\n\n    verify_accuracy_jump(\n        &accuracy_history.val_snapshots,\n        *transition_step,\n        config.accuracy_window,\n        config.chance_accuracy,\n        config.chance_tolerance,\n        config.target_val_acc_threshold,\n    )?;\n\n    let loss_drop_ratio = verify_loss_drop(\n        &loss_history.val_snapshots,\n        *transition_step,\n        config.loss_window,\n        config.loss_drop_fraction,\n    )?;\n\n    Ok(GrokkingPhaseReport {\n        early_train_step: *early_train_step,\n        transition_step: *transition_step,\n        plateau_max_val_acc,\n        loss_drop_ratio,\n    })\n}\n\nfn verify_accuracy_jump(\n    val_accuracy: &[(usize, f32)],\n    transition_step: usize,\n    window: usize,\n    chance_accuracy: f32,\n    chance_tolerance: f32,\n    target_val_acc_threshold: f32,\n) -> Result<(), String> {\n    let before_values: Vec<f32> = val_accuracy\n        .iter()\n        .filter(|(step, _)| *step < transition_step)\n        .map(|(_, acc)| *acc)\n        .collect();\n\n    let after_values: Vec<f32> = val_accuracy\n        .iter()\n        .filter(|(step, _)| *step >= transition_step)\n        .map(|(_, acc)| *acc)\n        .collect();\n\n    if before_values.len() < window || after_values.len() < window {\n        return Err(\"insufficient accuracy samples around transition\".to_string());\n    }\n\n    let before_avg: f32 =\n        before_values[before_values.len() - window..].iter().sum::<f32>() / window as f32;\n    let after_avg: f32 = after_values[..window].iter().sum::<f32>() / window as f32;\n\n    if before_avg > chance_accuracy + chance_tolerance {\n        return Err(format!(\n            \"validation accuracy plateau too high before transition (avg {:.4})\",\n            before_avg\n        ));\n    }\n\n    if after_avg < target_val_acc_threshold {\n        return Err(format!(\n            \"validation accuracy after transition too low (avg {:.4})\",\n            after_avg\n        ));\n    }\n\n    Ok(())\n}\n\nfn verify_loss_drop(\n    val_losses: &[(usize, f64)],\n    transition_step: usize,\n    loss_window: usize,\n    loss_drop_fraction: f64,\n) -> Result<f64, String> {\n    let before_losses: Vec<f64> = val_losses\n        .iter()\n        .filter(|(step, _)| *step < transition_step)\n        .map(|(_, loss)| *loss)\n        .collect();\n\n    let after_losses: Vec<f64> = val_losses\n        .iter()\n        .filter(|(step, _)| *step >= transition_step)\n        .map(|(_, loss)| *loss)\n        .collect();\n\n    if before_losses.len() < loss_window || after_losses.len() < loss_window {\n        return Err(\"insufficient loss samples around transition\".to_string());\n    }\n\n    let before_avg: f64 =\n        before_losses[before_losses.len() - loss_window..].iter().sum::<f64>()\n            / loss_window as f64;\n    let after_avg: f64 = after_losses[..loss_window].iter().sum::<f64>() / loss_window as f64;\n\n    if after_avg > before_avg * (1.0 - loss_drop_fraction) {\n        return Err(format!(\n            \"validation loss drop too small (before {:.4}, after {:.4})\",\n            before_avg, after_avg\n        ));\n    }\n\n    Ok(after_avg / before_avg)\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_restricted_loss_early_drop(\n    full_losses: &[(usize, f64)],\n    restricted_losses: &[(usize, f64)],\n    config: &RestrictedLossVerificationConfig,\n) -> Result<RestrictedLossReport, String> {\n    let restricted_baseline =\n        baseline_loss(restricted_losses, config.plateau_min_step, config.drop_window)?;\n    let full_baseline = baseline_loss(full_losses, config.plateau_min_step, config.drop_window)?;\n\n    let restricted_drop_step = find_drop_step(\n        restricted_losses,\n        config.plateau_min_step,\n        restricted_baseline,\n        config.drop_fraction,\n    )?;\n    let full_drop_step = find_drop_step(\n        full_losses,\n        config.plateau_min_step,\n        full_baseline,\n        config.drop_fraction,\n    )?;\n\n    if restricted_drop_step + config.min_step_lead > full_drop_step {\n        return Err(format!(\n            \"restricted loss drops at step {}, expected at least {} steps before full loss (step {})\",\n            restricted_drop_step, config.min_step_lead, full_drop_step\n        ));\n    }\n\n    Ok(RestrictedLossReport {\n        restricted_drop_step,\n        full_drop_step,\n    })\n}\n\npub fn verify_restricted_loss_drop(\n    restricted_losses: &[(usize, f64)],\n    config: &RestrictedLossDropConfig,\n) -> Result<RestrictedLossDropReport, String> {\n    if restricted_losses.len() < 3 {\n        return Err(\"restricted loss history requires at least 3 points\".to_string());\n    }\n\n    let first = restricted_losses.first().ok_or_else(|| {\n        \"restricted loss history is empty\".to_string()\n    })?;\n    let baseline_loss = first.1;\n\n    let (min_idx, (min_step, min_loss)) = restricted_losses\n        .iter()\n        .enumerate()\n        .min_by(|(_, a), (_, b)| {\n            a.1.partial_cmp(&b.1).unwrap_or(std::cmp::Ordering::Equal)\n        })\n        .ok_or_else(|| \"restricted loss history is empty\".to_string())?;\n\n    if min_idx == 0 {\n        return Err(\"restricted loss minimum occurs at start (no drop detected)\".to_string());\n    }\n\n    let relative_decrease = (baseline_loss - min_loss) / baseline_loss;\n    if relative_decrease < config.min_relative_decrease {\n        return Err(format!(\n            \"restricted loss decrease too small (baseline {:.4}, min {:.4}, decrease {:.1}%)\",\n            baseline_loss, min_loss, relative_decrease * 100.0\n        ));\n    }\n\n    Ok(RestrictedLossDropReport {\n        drop_step: *min_step,\n        drop_loss: *min_loss,\n        baseline_loss,\n    })\n}\n\npub fn verify_excluded_loss_spike(\n    excluded_losses: &[(usize, f64)],\n    config: &ExcludedLossSpikeConfig,\n) -> Result<ExcludedLossSpikeReport, String> {\n    if excluded_losses.len() < 3 {\n        return Err(\"excluded loss history requires at least 3 points\".to_string());\n    }\n\n    let first = excluded_losses.first().ok_or_else(|| {\n        \"excluded loss history is empty\".to_string()\n    })?;\n    let last = excluded_losses.last().ok_or_else(|| {\n        \"excluded loss history is empty\".to_string()\n    })?;\n    let baseline_loss = (first.1 + last.1) / 2.0;\n\n    let (peak_idx, (peak_step, peak_loss)) = excluded_losses\n        .iter()\n        .enumerate()\n        .max_by(|(_, a), (_, b)| {\n            a.1.partial_cmp(&b.1).unwrap_or(std::cmp::Ordering::Equal)\n        })\n        .ok_or_else(|| \"excluded loss history is empty\".to_string())?;\n\n    if peak_idx == 0 || peak_idx + 1 == excluded_losses.len() {\n        return Err(\"excluded loss spike occurs at boundary\".to_string());\n    }\n\n    if peak_loss < &(baseline_loss * (1.0 + config.min_relative_increase)) {\n        return Err(format!(\n            \"excluded loss spike too small (baseline {:.4}, peak {:.4})\",\n            baseline_loss, peak_loss\n        ));\n    }\n\n    Ok(ExcludedLossSpikeReport {\n        spike_step: *peak_step,\n        spike_loss: *peak_loss,\n        baseline_loss,\n    })\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_mlp_activation_waves<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n    fixed_y: usize,\n    neuron_indices: &[usize],\n    config: &MlpWaveVerificationConfig,\n) -> Result<MlpWaveReport, String> {\n    let activations = collect_mlp_post_relu_activations(model, device, fixed_y)?;\n    if neuron_indices.is_empty() {\n        return Err(\"neuron indices list is empty\".to_string());\n    }\n\n    let d_ff = activations.len();\n    for &idx in neuron_indices {\n        if idx >= d_ff {\n            return Err(format!(\n                \"neuron index {} out of range (d_ff={})\",\n                idx, d_ff\n            ));\n        }\n    }\n\n    let signals: Vec<(usize, Vec<f64>)> = neuron_indices\n        .iter()\n        .map(|&idx| (idx, activations[idx].clone()))\n        .collect();\n    verify_mlp_activation_waves_from_signals(&signals, config)\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_mlp_activation_waves_from_signals(\n    signals: &[(usize, Vec<f64>)],\n    config: &MlpWaveVerificationConfig,\n) -> Result<MlpWaveReport, String> {\n    if signals.is_empty() {\n        return Err(\"no activation signals provided\".to_string());\n    }\n\n    let mut reports = Vec::with_capacity(signals.len());\n    for (neuron_index, signal) in signals {\n        let report = analyze_mlp_activation_wave(*neuron_index, signal, config)?;\n        reports.push(report);\n    }\n\n    Ok(MlpWaveReport {\n        neuron_reports: reports,\n    })\n}\n\npub fn verify_mlp_pairwise_manifold_transition<B: Backend>(\n    pre_model: &Transformer<B>,\n    post_model: &Transformer<B>,\n    device: &B::Device,\n    config: &PairwiseManifoldConfig,\n) -> Result<PairwiseManifoldTransitionReport, String> {\n    let modulus = ModularAdditionDataset::modulus();\n    if config.fixed_y >= modulus {\n        return Err(format!(\n            \"fixed_y {} out of range (modulus {})\",\n            config.fixed_y, modulus\n        ));\n    }\n\n    let pre_activations = collect_mlp_post_relu_activations(pre_model, device, config.fixed_y)?;\n    let post_activations = collect_mlp_post_relu_activations(post_model, device, config.fixed_y)?;\n\n    let d_ff = pre_activations.len();\n    if post_activations.len() != d_ff {\n        return Err(\"post activations shape mismatch\".to_string());\n    }\n\n    if config.neuron_indices.len() < 2 {\n        return Err(\"need at least two neuron indices\".to_string());\n    }\n\n    for &idx in &config.neuron_indices {\n        if idx >= d_ff {\n            return Err(format!(\n                \"neuron index {} out of range (d_ff={})\",\n                idx, d_ff\n            ));\n        }\n    }\n\n    let pre_signals: Vec<(usize, Vec<f64>)> = config\n        .neuron_indices\n        .iter()\n        .map(|&idx| (idx, pre_activations[idx].clone()))\n        .collect();\n    let post_signals: Vec<(usize, Vec<f64>)> = config\n        .neuron_indices\n        .iter()\n        .map(|&idx| (idx, post_activations[idx].clone()))\n        .collect();\n\n    verify_pairwise_manifold_transition_from_signals(&pre_signals, &post_signals, config)\n}\n\npub fn collect_mlp_activation_surface<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n    neuron_index: usize,\n) -> Result<Vec<Vec<f64>>, String> {\n    let modulus = ModularAdditionDataset::modulus();\n    let equals_token = ModularAdditionDataset::equals_token();\n\n    let mut inputs_vec = Vec::with_capacity(modulus * modulus * 3);\n    for x in 0..modulus {\n        for y in 0..modulus {\n            inputs_vec.push(x as i32);\n            inputs_vec.push(y as i32);\n            inputs_vec.push(equals_token as i32);\n        }\n    }\n\n    let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n        .reshape([modulus * modulus, 3]);\n    let (_, mlp_acts) = model.forward_with_mlp_activations(inputs);\n    let [batch_size, d_ff] = mlp_acts.dims();\n    if batch_size != modulus * modulus {\n        return Err(format!(\n            \"unexpected activation batch size {} (expected {})\",\n            batch_size,\n            modulus * modulus\n        ));\n    }\n    if neuron_index >= d_ff {\n        return Err(format!(\n            \"neuron index {} out of range (d_ff={})\",\n            neuron_index, d_ff\n        ));\n    }\n\n    let data: Vec<f32> = mlp_acts.into_data().to_vec().unwrap();\n    let mut surface = vec![vec![0.0f64; modulus]; modulus];\n    for x in 0..modulus {\n        for y in 0..modulus {\n            let batch = x * modulus + y;\n            let idx = batch * d_ff + neuron_index;\n            surface[x][y] = data[idx] as f64;\n        }\n    }\n\n    Ok(surface)\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_constructive_interference_surface_from_grid(\n    surface: &[Vec<f64>],\n    config: &DiagonalRidgeConfig,\n) -> Result<DiagonalRidgeReport, String> {\n    let (diag_main, diag_anti, axis_x, axis_y) =\n        surface_autocorrelation_scores(surface, config.max_lag)?;\n\n    let diagonal_score = diag_main.max(diag_anti);\n    let axis_score = axis_x.max(axis_y);\n    let ratio = if axis_score > 0.0 {\n        diagonal_score / axis_score\n    } else {\n        f64::INFINITY\n    };\n\n    if diagonal_score < config.min_diagonal_score {\n        return Err(format!(\n            \"diagonal autocorrelation {:.3} below threshold {:.3}\",\n            diagonal_score, config.min_diagonal_score\n        ));\n    }\n\n    if ratio < config.min_diagonal_ratio {\n        return Err(format!(\n            \"diagonal/axis ratio {:.3} below threshold {:.3}\",\n            ratio, config.min_diagonal_ratio\n        ));\n    }\n\n    Ok(DiagonalRidgeReport {\n        neuron_index: config.neuron_index,\n        diagonal_score,\n        axis_score,\n        ratio,\n    })\n}\n\npub fn verify_pairwise_manifold_transition_from_signals(\n    pre_signals: &[(usize, Vec<f64>)],\n    post_signals: &[(usize, Vec<f64>)],\n    config: &PairwiseManifoldConfig,\n) -> Result<PairwiseManifoldTransitionReport, String> {\n    let pre_report = evaluate_pairwise_manifolds_from_signals(pre_signals)?;\n    let post_report = evaluate_pairwise_manifolds_from_signals(post_signals)?;\n\n    if pre_report.average_score > config.max_pre_score {\n        return Err(format!(\n            \"pre-grok average circularity {:.3} exceeds threshold {:.3}\",\n            pre_report.average_score, config.max_pre_score\n        ));\n    }\n\n    let passing_pairs = post_report\n        .pair_reports\n        .iter()\n        .filter(|pair| {\n            pair.circularity_score >= config.min_post_score\n                && pair.axis_ratio <= config.max_axis_ratio\n        })\n        .count();\n    if passing_pairs < config.min_pairs_passing {\n        return Err(format!(\n            \"post-grok only {} pairs meet circularity threshold (need {})\",\n            passing_pairs, config.min_pairs_passing\n        ));\n    }\n\n    Ok(PairwiseManifoldTransitionReport {\n        pre_average_score: pre_report.average_score,\n        post_average_score: post_report.average_score,\n        passing_pairs,\n        total_pairs: post_report.pair_reports.len(),\n    })\n}\n\nfn evaluate_pairwise_manifolds_from_signals(\n    signals: &[(usize, Vec<f64>)],\n) -> Result<PairwiseManifoldReport, String> {\n    if signals.len() < 2 {\n        return Err(\"need at least two activation signals\".to_string());\n    }\n\n    let signal_len = signals[0].1.len();\n    if signal_len < 3 {\n        return Err(\"activation signal too short\".to_string());\n    }\n\n    for (idx, signal) in signals {\n        if signal.len() != signal_len {\n            return Err(format!(\n                \"signal length mismatch for neuron {}\",\n                idx\n            ));\n        }\n    }\n\n    let mut reports = Vec::new();\n    for i in 0..signals.len() {\n        for j in (i + 1)..signals.len() {\n            let (neuron_a, signal_a) = &signals[i];\n            let (neuron_b, signal_b) = &signals[j];\n            let points: Vec<(f64, f64)> = signal_a\n                .iter()\n                .zip(signal_b.iter())\n                .map(|(x, y)| (*x, *y))\n                .collect();\n            let (score, radius_cv, axis_ratio) =\n                match pairwise_circularity_metrics(&points) {\n                    Ok(metrics) => metrics,\n                    Err(_) => (0.0, 1.0, f64::INFINITY),\n                };\n            reports.push(PairwiseManifoldPairReport {\n                neuron_a: *neuron_a,\n                neuron_b: *neuron_b,\n                circularity_score: score,\n                radius_cv,\n                axis_ratio,\n            });\n        }\n    }\n\n    let average_score = reports\n        .iter()\n        .map(|pair| pair.circularity_score)\n        .sum::<f64>()\n        / reports.len() as f64;\n\n    Ok(PairwiseManifoldReport {\n        pair_reports: reports,\n        average_score,\n    })\n}\n\nfn pairwise_circularity_metrics(points: &[(f64, f64)]) -> Result<(f64, f64, f64), String> {\n    if points.len() < 3 {\n        return Err(\"not enough points for circularity\".to_string());\n    }\n\n    let n = points.len() as f64;\n    let mean_x = points.iter().map(|(x, _)| *x).sum::<f64>() / n;\n    let mean_y = points.iter().map(|(_, y)| *y).sum::<f64>() / n;\n\n    let mut var_x = 0.0;\n    let mut var_y = 0.0;\n    let mut cov_xy = 0.0;\n    for (x, y) in points {\n        let dx = x - mean_x;\n        let dy = y - mean_y;\n        var_x += dx * dx;\n        var_y += dy * dy;\n        cov_xy += dx * dy;\n    }\n    var_x /= n;\n    var_y /= n;\n    cov_xy /= n;\n\n    let trace = var_x + var_y;\n    let det = var_x * var_y - cov_xy * cov_xy;\n    if det <= 1.0e-12 || trace <= 0.0 {\n        return Err(\"degenerate covariance\".to_string());\n    }\n\n    let half_trace = trace * 0.5;\n    let disc = (half_trace * half_trace - det).max(0.0).sqrt();\n    let lambda1 = half_trace + disc;\n    let lambda2 = half_trace - disc;\n    if lambda2 <= 0.0 {\n        return Err(\"degenerate eigenvalues\".to_string());\n    }\n\n    let axis_ratio = (lambda1 / lambda2).sqrt();\n\n    let mut v1_x = cov_xy;\n    let mut v1_y = lambda1 - var_x;\n    if v1_x.abs() < 1.0e-12 && v1_y.abs() < 1.0e-12 {\n        v1_x = 1.0;\n        v1_y = 0.0;\n    }\n    let norm = (v1_x * v1_x + v1_y * v1_y).sqrt();\n    v1_x /= norm;\n    v1_y /= norm;\n    let v2_x = -v1_y;\n    let v2_y = v1_x;\n\n    let mut radii = Vec::with_capacity(points.len());\n    for (x, y) in points {\n        let dx = x - mean_x;\n        let dy = y - mean_y;\n        let p1 = dx * v1_x + dy * v1_y;\n        let p2 = dx * v2_x + dy * v2_y;\n        let w1 = p1 / lambda1.sqrt();\n        let w2 = p2 / lambda2.sqrt();\n        radii.push((w1 * w1 + w2 * w2).sqrt());\n    }\n\n    let mean_radius = radii.iter().sum::<f64>() / radii.len() as f64;\n    if mean_radius == 0.0 {\n        return Err(\"zero mean radius\".to_string());\n    }\n\n    let mut variance = 0.0;\n    for r in &radii {\n        let diff = r - mean_radius;\n        variance += diff * diff;\n    }\n    let std = (variance / radii.len() as f64).sqrt();\n    let radius_cv = std / mean_radius;\n    let score = 1.0 / (1.0 + radius_cv);\n\n    Ok((score, radius_cv, axis_ratio))\n}\n\n// Helper functions for verification - reserved for future integration\n#[allow(dead_code)]\nfn baseline_loss(\n    losses: &[(usize, f64)],\n    plateau_min_step: usize,\n    drop_window: usize,\n) -> Result<f64, String> {\n    let before: Vec<f64> = losses\n        .iter()\n        .filter(|(step, _)| *step <= plateau_min_step)\n        .map(|(_, loss)| *loss)\n        .collect();\n\n    if before.len() < drop_window || drop_window == 0 {\n        return Err(\"insufficient loss samples for baseline\".to_string());\n    }\n\n    let window = &before[before.len() - drop_window..];\n    Ok(window.iter().sum::<f64>() / drop_window as f64)\n}\n\n#[allow(dead_code)]\nfn find_drop_step(\n    losses: &[(usize, f64)],\n    plateau_min_step: usize,\n    baseline: f64,\n    drop_fraction: f64,\n) -> Result<usize, String> {\n    losses\n        .iter()\n        .filter(|(step, _)| *step >= plateau_min_step)\n        .find(|(_, loss)| *loss <= baseline * (1.0 - drop_fraction))\n        .map(|(step, _)| *step)\n        .ok_or_else(|| \"loss never drops below threshold\".to_string())\n}\n\nfn collect_mlp_post_relu_activations<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n    fixed_y: usize,\n) -> Result<Vec<Vec<f64>>, String> {\n    let modulus = ModularAdditionDataset::modulus();\n    let equals_token = ModularAdditionDataset::equals_token();\n\n    let mut inputs_vec = Vec::with_capacity(modulus * 3);\n    for x in 0..modulus {\n        inputs_vec.push(x as i32);\n        inputs_vec.push(fixed_y as i32);\n        inputs_vec.push(equals_token as i32);\n    }\n\n    let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n        .reshape([modulus, 3]);\n    let (_, mlp_acts) = model.forward_with_mlp_activations(inputs);\n    let [batch_size, d_ff] = mlp_acts.dims();\n\n    if batch_size != modulus {\n        return Err(format!(\n            \"unexpected activation batch size {} (expected {})\",\n            batch_size, modulus\n        ));\n    }\n\n    let data: Vec<f32> = mlp_acts.into_data().to_vec().unwrap();\n    let mut per_neuron = vec![Vec::with_capacity(batch_size); d_ff];\n    for batch in 0..batch_size {\n        for neuron in 0..d_ff {\n            let idx = batch * d_ff + neuron;\n            per_neuron[neuron].push(data[idx] as f64);\n        }\n    }\n\n    Ok(per_neuron)\n}\n\n#[allow(dead_code)]\nfn surface_autocorrelation_scores(\n    surface: &[Vec<f64>],\n    max_lag: usize,\n) -> Result<(f64, f64, f64, f64), String> {\n    let (height, width) = surface_dimensions(surface)?;\n    if height < 2 || width < 2 {\n        return Err(\"surface grid too small\".to_string());\n    }\n\n    let (mean, var) = surface_mean_variance(surface)?;\n    if var == 0.0 {\n        return Err(\"surface variance is zero\".to_string());\n    }\n\n    let max_lag = max_lag.min(height.saturating_sub(1)).min(width.saturating_sub(1));\n    if max_lag == 0 {\n        return Err(\"max_lag too small for surface\".to_string());\n    }\n\n    let diag_main = average_autocorrelation(surface, mean, var, max_lag, 1, 1)?;\n    let diag_anti = average_autocorrelation(surface, mean, var, max_lag, 1, -1)?;\n    let axis_x = average_autocorrelation(surface, mean, var, max_lag, 1, 0)?;\n    let axis_y = average_autocorrelation(surface, mean, var, max_lag, 0, 1)?;\n\n    Ok((diag_main, diag_anti, axis_x, axis_y))\n}\n\n#[allow(dead_code)]\nfn surface_dimensions(surface: &[Vec<f64>]) -> Result<(usize, usize), String> {\n    let height = surface.len();\n    if height == 0 {\n        return Err(\"surface is empty\".to_string());\n    }\n    let width = surface[0].len();\n    if width == 0 {\n        return Err(\"surface has empty rows\".to_string());\n    }\n    for (idx, row) in surface.iter().enumerate() {\n        if row.len() != width {\n            return Err(format!(\n                \"surface row {} has length {}, expected {}\",\n                idx,\n                row.len(),\n                width\n            ));\n        }\n    }\n    Ok((height, width))\n}\n\n#[allow(dead_code)]\nfn surface_mean_variance(surface: &[Vec<f64>]) -> Result<(f64, f64), String> {\n    let (height, width) = surface_dimensions(surface)?;\n    let mut sum = 0.0;\n    for row in surface {\n        for value in row {\n            sum += *value;\n        }\n    }\n    let count = (height * width) as f64;\n    let mean = sum / count;\n    let mut variance = 0.0;\n    for row in surface {\n        for value in row {\n            let diff = *value - mean;\n            variance += diff * diff;\n        }\n    }\n    variance /= count;\n    Ok((mean, variance))\n}\n\n#[allow(dead_code)]\nfn average_autocorrelation(\n    surface: &[Vec<f64>],\n    mean: f64,\n    variance: f64,\n    max_lag: usize,\n    dx: i32,\n    dy: i32,\n) -> Result<f64, String> {\n    let (height, width) = surface_dimensions(surface)?;\n    let mut total = 0.0;\n    let mut lag_count = 0usize;\n\n    for lag in 1..=max_lag {\n        let mut sum = 0.0;\n        let mut count = 0usize;\n        let lag = lag as i32;\n        for x in 0..height {\n            for y in 0..width {\n                let x2 = x as i32 + dx * lag;\n                let y2 = y as i32 + dy * lag;\n                if x2 < 0 || y2 < 0 {\n                    continue;\n                }\n                let x2 = x2 as usize;\n                let y2 = y2 as usize;\n                if x2 >= height || y2 >= width {\n                    continue;\n                }\n                let a = surface[x][y] - mean;\n                let b = surface[x2][y2] - mean;\n                sum += a * b;\n                count += 1;\n            }\n        }\n        if count == 0 {\n            continue;\n        }\n        let corr = sum / (count as f64 * variance);\n        total += corr.abs();\n        lag_count += 1;\n    }\n\n    if lag_count == 0 {\n        return Err(\"no valid lags for autocorrelation\".to_string());\n    }\n\n    Ok(total / lag_count as f64)\n}\n\n#[allow(dead_code)]\nfn analyze_mlp_activation_wave(\n    neuron_index: usize,\n    signal: &[f64],\n    config: &MlpWaveVerificationConfig,\n) -> Result<MlpWaveNeuronReport, String> {\n    if signal.len() < 3 {\n        return Err(format!(\n            \"activation signal for neuron {} is too short\",\n            neuron_index\n        ));\n    }\n\n    let negative_count = signal\n        .iter()\n        .filter(|value| **value < -config.negative_tolerance)\n        .count();\n    let negative_fraction = negative_count as f64 / signal.len() as f64;\n    if negative_fraction > config.max_negative_fraction {\n        return Err(format!(\n            \"neuron {} has {:.3} negative activations\",\n            neuron_index, negative_fraction\n        ));\n    }\n\n    let (dominant_frequency, dominant_ratio) = dominant_frequency_ratio(signal)?;\n    if dominant_ratio < config.min_dominant_ratio {\n        return Err(format!(\n            \"neuron {} dominant frequency ratio {:.3} below threshold {:.3}\",\n            neuron_index, dominant_ratio, config.min_dominant_ratio\n        ));\n    }\n\n    let best_correlation =\n        best_rectified_sine_correlation(signal, dominant_frequency, config.phase_steps);\n    if best_correlation < config.min_correlation {\n        return Err(format!(\n            \"neuron {} rectified sine correlation {:.3} below threshold {:.3}\",\n            neuron_index, best_correlation, config.min_correlation\n        ));\n    }\n\n    Ok(MlpWaveNeuronReport {\n        neuron_index,\n        dominant_frequency,\n        dominant_ratio,\n        negative_fraction,\n        best_correlation,\n    })\n}\n\nfn dominant_frequency_ratio(signal: &[f64]) -> Result<(usize, f64), String> {\n    if signal.len() < 2 {\n        return Err(\"signal too short for FFT\".to_string());\n    }\n\n    let mut planner = FftPlanner::new();\n    let fft = planner.plan_fft_forward(signal.len());\n    let mut buffer: Vec<Complex<f64>> = signal\n        .iter()\n        .map(|&x| Complex::new(x, 0.0))\n        .collect();\n    fft.process(&mut buffer);\n    let magnitudes: Vec<f64> = buffer.iter().map(|c| c.norm()).collect();\n\n    if magnitudes.len() < 2 {\n        return Err(\"insufficient FFT bins\".to_string());\n    }\n\n    let mut dominant_idx = 1usize;\n    let mut dominant_mag = magnitudes[1];\n    for (idx, &mag) in magnitudes.iter().enumerate().skip(2) {\n        if mag > dominant_mag {\n            dominant_mag = mag;\n            dominant_idx = idx;\n        }\n    }\n\n    if dominant_idx == 0 {\n        return Err(\"dominant frequency is zero\".to_string());\n    }\n\n    let mut nonzero = magnitudes[1..].to_vec();\n    nonzero.sort_by(|a, b| a.partial_cmp(b).unwrap_or(std::cmp::Ordering::Equal));\n    let median_mag = nonzero[nonzero.len() / 2];\n    let ratio = if median_mag == 0.0 {\n        dominant_mag\n    } else {\n        dominant_mag / median_mag\n    };\n\n    Ok((dominant_idx, ratio))\n}\n\nfn best_rectified_sine_correlation(signal: &[f64], frequency: usize, phase_steps: usize) -> f64 {\n    if signal.is_empty() || frequency == 0 || phase_steps == 0 {\n        return 0.0;\n    }\n\n    let tau = std::f64::consts::PI * 2.0;\n    let mut best = -1.0;\n    for step in 0..phase_steps {\n        let phase = tau * step as f64 / phase_steps as f64;\n        let wave: Vec<f64> = (0..signal.len())\n            .map(|idx| {\n                let angle = tau * frequency as f64 * idx as f64 / signal.len() as f64 + phase;\n                angle.sin().max(0.0)\n            })\n            .collect();\n        let corr = pearson_correlation(signal, &wave);\n        if corr > best {\n            best = corr;\n        }\n    }\n\n    best\n}\n\nfn pearson_correlation(a: &[f64], b: &[f64]) -> f64 {\n    if a.len() != b.len() || a.is_empty() {\n        return 0.0;\n    }\n\n    let mean_a = a.iter().sum::<f64>() / a.len() as f64;\n    let mean_b = b.iter().sum::<f64>() / b.len() as f64;\n\n    let mut num = 0.0;\n    let mut denom_a = 0.0;\n    let mut denom_b = 0.0;\n    for (x, y) in a.iter().zip(b.iter()) {\n        let da = x - mean_a;\n        let db = y - mean_b;\n        num += da * db;\n        denom_a += da * da;\n        denom_b += db * db;\n    }\n\n    if denom_a == 0.0 || denom_b == 0.0 {\n        return 0.0;\n    }\n\n    num / (denom_a.sqrt() * denom_b.sqrt())\n}\n\n/// Verify that the model actually learned modular addition\n/// by testing it on ALL possible examples systematically\npub fn verify_full_accuracy<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n) -> (f32, f32) {\n    println!(\"ðŸ” Verifying model on COMPLETE dataset (no sampling)...\");\n\n    let train_dataset = ModularAdditionDataset::new(true, 42);\n    let val_dataset = ModularAdditionDataset::new(false, 42);\n\n    let train_acc = test_all_examples(model, &train_dataset, device);\n    let val_acc = test_all_examples(model, &val_dataset, device);\n\n    println!(\"ðŸ“Š FULL Dataset Results:\");\n    println!(\"   Training: {}/{} = {:.2}%\",\n        (train_acc * train_dataset.len() as f32) as usize,\n        train_dataset.len(),\n        train_acc * 100.0);\n    println!(\"   Validation: {}/{} = {:.2}%\",\n        (val_acc * val_dataset.len() as f32) as usize,\n        val_dataset.len(),\n        val_acc * 100.0);\n\n    (train_acc, val_acc)\n}\n\n/// Test model on ALL examples in dataset (no random sampling)\nfn test_all_examples<B: Backend>(\n    model: &Transformer<B>,\n    dataset: &ModularAdditionDataset,\n    device: &B::Device,\n) -> f32 {\n    let mut total_correct = 0;\n    let total = dataset.len();\n\n    // Test in batches to avoid memory issues\n    let batch_size = 100;\n    for batch_start in (0..total).step_by(batch_size) {\n        let batch_end = (batch_start + batch_size).min(total);\n        let batch_len = batch_end - batch_start;\n\n        let mut inputs_vec = Vec::new();\n        let mut targets_vec = Vec::new();\n\n        for idx in batch_start..batch_end {\n            let (input, target) = dataset.get(idx).unwrap();\n            inputs_vec.extend(input);\n            targets_vec.push(target as i32);\n        }\n\n        let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n            .reshape([batch_len, 3]);\n\n        let logits = model.forward(inputs);\n        let predictions = logits.argmax(1).squeeze::<1>();\n        let predictions_vec: Vec<i32> = predictions.into_data().to_vec().unwrap();\n\n        let correct = predictions_vec\n            .iter()\n            .zip(targets_vec.iter())\n            .filter(|(p, t)| p == t)\n            .count();\n\n        total_correct += correct;\n    }\n\n    total_correct as f32 / total as f32\n}\n\n/// Verify that weight norms decrease over time with weight decay\n/// and show correlation with the grokking transition\npub fn verify_weight_norm_decay(\n    weight_norm_history: &[(usize, f64)],\n    grok_step: Option<usize>,\n    config: &WeightNormDecayConfig,\n) -> Result<WeightNormDecayReport, String> {\n    if weight_norm_history.len() < 2 {\n        return Err(\"weight norm history requires at least 2 snapshots\".to_string());\n    }\n\n    let initial_norm = weight_norm_history\n        .first()\n        .ok_or_else(|| \"weight norm history is empty\".to_string())?\n        .1;\n    let final_norm = weight_norm_history\n        .last()\n        .ok_or_else(|| \"weight norm history is empty\".to_string())?\n        .1;\n\n    if initial_norm <= 0.0 {\n        return Err(\"initial weight norm must be positive\".to_string());\n    }\n\n    // Check that norms are generally decreasing (with weight decay)\n    let relative_decrease = (initial_norm - final_norm) / initial_norm;\n    if relative_decrease < config.min_relative_decrease {\n        return Err(format!(\n            \"weight norm decrease {:.4} below threshold {:.4} (initial {:.4}, final {:.4})\",\n            relative_decrease, config.min_relative_decrease, initial_norm, final_norm\n        ));\n    }\n\n    // Check that norms are not increasing (weight decay should monotonically decrease norms)\n    for window in weight_norm_history.windows(2) {\n        let prev_norm = window[0].1;\n        let next_norm = window[1].1;\n        if next_norm > prev_norm * 1.01 {\n            // Allow 1% tolerance for numerical noise\n            return Err(format!(\n                \"weight norm increased from {:.4} to {:.4} at step {}\",\n                prev_norm, next_norm, window[1].0\n            ));\n        }\n    }\n\n    // If grok step is provided, check that norms show a shift around grokking\n    let grok_step_norm = if let Some(step) = grok_step {\n        weight_norm_history\n            .iter()\n            .filter(|(s, _)| {\n                *s >= step.saturating_sub(config.grok_step_window)\n                    && *s <= step + config.grok_step_window\n            })\n            .map(|(_, norm)| *norm)\n            .next()\n    } else {\n        None\n    };\n\n    Ok(WeightNormDecayReport {\n        initial_norm,\n        final_norm,\n        relative_decrease,\n        grok_step_norm,\n    })\n}\n\n/// Verify the characteristic \"snake curve\" shape of grokking phenomenon\n/// on log-scale plots: train accuracy rises early, validation stays flat then jumps\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_snake_curve_shape(\n    train_acc: &[(usize, f32)],\n    val_acc: &[(usize, f32)],\n    config: &SnakeCurveConfig,\n) -> Result<SnakeCurveReport, String> {\n    if train_acc.is_empty() || val_acc.is_empty() {\n        return Err(\"empty accuracy history\".to_string());\n    }\n\n    // Phase 1: Verify early train convergence (immediate drop on log scale)\n    let train_converged_step = train_acc\n        .iter()\n        .find(|(_, acc)| *acc >= config.train_convergence_threshold)\n        .map(|(step, _)| *step)\n        .ok_or_else(|| {\n            format!(\n                \"train accuracy never reaches {} threshold\",\n                config.train_convergence_threshold\n            )\n        })?;\n\n    if train_converged_step > config.early_step_max {\n        return Err(format!(\n            \"train accuracy converges at step {}, expected before {}\",\n            train_converged_step, config.early_step_max\n        ));\n    }\n\n    // Phase 2: Verify validation plateau (flat on log scale)\n    let plateau_samples: Vec<f32> = val_acc\n        .iter()\n        .filter(|(step, _)| *step >= 1 && *step <= config.val_plateau_min_step)\n        .map(|(_, acc)| *acc)\n        .collect();\n\n    if plateau_samples.is_empty() {\n        return Err(\"no validation samples in plateau region\".to_string());\n    }\n\n    let val_plateau_max = plateau_samples\n        .iter()\n        .fold(0.0f32, |max_val, &acc| max_val.max(acc));\n\n    if val_plateau_max > config.val_plateau_max_acc {\n        return Err(format!(\n            \"validation accuracy in plateau region too high: max {:.4} exceeds {:.4}\",\n            val_plateau_max, config.val_plateau_max_acc\n        ));\n    }\n\n    // Phase 3: Verify delayed validation jump (crash up on log scale)\n    let val_grok_step = val_acc\n        .iter()\n        .filter(|(step, _)| *step > config.val_plateau_min_step)\n        .find(|(_, acc)| *acc >= config.val_final_threshold)\n        .map(|(step, _)| *step)\n        .ok_or_else(|| {\n            format!(\n                \"validation accuracy never reaches {} after plateau\",\n                config.val_final_threshold\n            )\n        })?;\n\n    // Phase 4: Verify phase ordering (snake shape: early train, late val)\n    let curve_shape_valid = train_converged_step < config.val_plateau_min_step\n        && val_grok_step > config.val_plateau_min_step\n        && train_converged_step < val_grok_step;\n\n    if !curve_shape_valid {\n        return Err(format!(\n            \"invalid curve shape: train converged at {}, val plateau until {}, val grokked at {}\",\n            train_converged_step, config.val_plateau_min_step, val_grok_step\n        ));\n    }\n\n    Ok(SnakeCurveReport {\n        train_converged_by_step: train_converged_step,\n        val_plateau_max_acc: val_plateau_max,\n        val_grok_step,\n        curve_shape_valid,\n    })\n}\n\n/// Test specific examples to see if model actually computes mod p\npub fn test_specific_examples<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n) {\n    println!();\n    println!(\"ðŸ§ª Testing specific examples:\");\n\n    let modulus = ModularAdditionDataset::modulus();\n    let equals_token = ModularAdditionDataset::equals_token();\n    let test_cases = vec![\n        (0, 0, 0),\n        (1, 1, 2),\n        (50, 50, (50 + 50) % modulus),\n        (112, 1, (112 + 1) % modulus),\n        (56, 57, (56 + 57) % modulus),\n        (112, 112, (112 + 112) % modulus),\n        (10, 20, 30),\n        (60, 60, (60 + 60) % modulus),\n    ];\n\n    for (a, b, expected) in test_cases {\n        let input_vec = vec![a as i32, b as i32, equals_token as i32];\n        let input = Tensor::<B, 1, Int>::from_ints(input_vec.as_slice(), device)\n            .reshape([1, 3]);\n\n        let logits = model.forward(input);\n        let prediction = logits.argmax(1).squeeze::<1>();\n        let pred_value: i32 = prediction.into_data().to_vec().unwrap()[0];\n\n        let correct = if pred_value == expected as i32 { \"âœ“\" } else { \"âœ—\" };\n        println!(\n            \"   {} + {} mod {} = {} | Model: {} {}\",\n            a, b, modulus, expected, pred_value, correct\n        );\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    fn make_histories(\n        train_acc: &[(usize, f32)],\n        val_acc: &[(usize, f32)],\n        val_loss: &[(usize, f64)],\n    ) -> (LossHistory, AccuracyHistory) {\n        let mut loss_history = LossHistory::new();\n        loss_history.val_snapshots = val_loss.to_vec();\n        let mut accuracy_history = AccuracyHistory::new();\n        accuracy_history.train_snapshots = train_acc.to_vec();\n        accuracy_history.val_snapshots = val_acc.to_vec();\n        (loss_history, accuracy_history)\n    }\n\n    #[test]\n    fn grokking_phase_verification_passes_on_synthetic_signal() {\n        let train_acc = vec![(0, 0.2), (500, 0.995), (1000, 0.999)];\n        let val_acc = vec![\n            (0, 0.01),\n            (500, 0.01),\n            (1500, 0.01),\n            (2200, 0.995),\n            (2500, 0.999),\n        ];\n        let val_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 2.0),\n            (2200, 0.2),\n            (2500, 0.15),\n        ];\n\n        let (loss_history, accuracy_history) = make_histories(&train_acc, &val_acc, &val_loss);\n        let config = GrokkingVerificationConfig {\n            early_step_max: 1000,\n            plateau_min_step: 1500,\n            generalization_window: 1500,\n            accuracy_window: 2,\n            loss_window: 2,\n            ..GrokkingVerificationConfig::default_for_modulus(ModularAdditionDataset::modulus())\n        };\n\n        let report = verify_grokking_phase_transition(&loss_history, &accuracy_history, &config)\n            .expect(\"expected grokking verification to pass\");\n        assert!(report.transition_step >= config.plateau_min_step);\n        assert!(report.early_train_step <= config.early_step_max);\n    }\n\n    #[test]\n    fn grokking_phase_verification_fails_without_transition() {\n        let train_acc = vec![(0, 0.2), (500, 0.995)];\n        let val_acc = vec![(0, 0.01), (500, 0.01), (1500, 0.02)];\n        let val_loss = vec![(0, 2.0), (500, 2.0), (1500, 1.9)];\n\n        let (loss_history, accuracy_history) = make_histories(&train_acc, &val_acc, &val_loss);\n        let config = GrokkingVerificationConfig {\n            early_step_max: 1000,\n            plateau_min_step: 1000,\n            generalization_window: 1000,\n            accuracy_window: 2,\n            loss_window: 2,\n            ..GrokkingVerificationConfig::default_for_modulus(ModularAdditionDataset::modulus())\n        };\n\n        let err = verify_grokking_phase_transition(&loss_history, &accuracy_history, &config)\n            .expect_err(\"expected grokking verification to fail\");\n        assert!(err.contains(\"validation accuracy never reaches target\"));\n    }\n\n    #[test]\n    fn grokking_phase_verification_fails_with_slow_accuracy_rise() {\n        let train_acc = vec![(0, 0.2), (500, 0.995), (1000, 0.999)];\n        let val_acc = vec![\n            (0, 0.01),\n            (500, 0.01),\n            (1500, 0.5),\n            (2000, 0.6),\n            (2200, 0.995),\n            (2500, 0.999),\n        ];\n        let val_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 2.0),\n            (2200, 0.2),\n            (2500, 0.15),\n        ];\n\n        let (loss_history, accuracy_history) = make_histories(&train_acc, &val_acc, &val_loss);\n        let config = GrokkingVerificationConfig {\n            early_step_max: 1000,\n            plateau_min_step: 1000,\n            generalization_window: 2000,\n            accuracy_window: 2,\n            loss_window: 2,\n            ..GrokkingVerificationConfig::default_for_modulus(ModularAdditionDataset::modulus())\n        };\n\n        let err = verify_grokking_phase_transition(&loss_history, &accuracy_history, &config)\n            .expect_err(\"expected grokking verification to fail\");\n        assert!(err.contains(\"plateau too high\"));\n    }\n\n    #[test]\n    fn restricted_loss_detection_passes_with_early_drop() {\n        let full_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 2.0),\n            (2500, 2.0),\n            (3500, 0.3),\n        ];\n        let restricted_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 0.8),\n            (2500, 0.4),\n            (3500, 0.3),\n        ];\n\n        let config = RestrictedLossVerificationConfig {\n            plateau_min_step: 1000,\n            drop_window: 2,\n            drop_fraction: 0.5,\n            min_step_lead: 500,\n        };\n\n        let report = verify_restricted_loss_early_drop(&full_loss, &restricted_loss, &config)\n            .expect(\"expected restricted loss verification to pass\");\n        assert!(report.restricted_drop_step < report.full_drop_step);\n    }\n\n    #[test]\n    fn restricted_loss_detection_fails_when_full_drops_first() {\n        let full_loss = vec![(0, 2.0), (1000, 1.0), (1500, 0.8)];\n        let restricted_loss = vec![(0, 2.0), (1000, 2.0), (1500, 0.8)];\n\n        let config = RestrictedLossVerificationConfig {\n            plateau_min_step: 500,\n            drop_window: 1,\n            drop_fraction: 0.3,\n            min_step_lead: 200,\n        };\n\n        let err = verify_restricted_loss_early_drop(&full_loss, &restricted_loss, &config)\n            .expect_err(\"expected restricted loss verification to fail\");\n        assert!(err.contains(\"restricted loss drops\"));\n    }\n\n    #[test]\n    fn excluded_loss_spike_verification_passes() {\n        let excluded_loss = vec![(0, 1.0), (1500, 2.2), (3000, 1.1)];\n        let config = ExcludedLossSpikeConfig {\n            min_relative_increase: 0.5,\n        };\n\n        let report = verify_excluded_loss_spike(&excluded_loss, &config)\n            .expect(\"expected excluded loss spike verification to pass\");\n        assert_eq!(report.spike_step, 1500);\n    }\n\n    #[test]\n    fn excluded_loss_spike_verification_fails_without_spike() {\n        let excluded_loss = vec![(0, 1.0), (1500, 1.1), (3000, 1.0)];\n        let config = ExcludedLossSpikeConfig {\n            min_relative_increase: 0.5,\n        };\n\n        let err = verify_excluded_loss_spike(&excluded_loss, &config)\n            .expect_err(\"expected excluded loss spike verification to fail\");\n        assert!(err.contains(\"excluded loss spike too small\"));\n    }\n\n    #[test]\n    fn restricted_loss_drop_verification_passes() {\n        let restricted_loss = vec![\n            (0, 2.0),\n            (1000, 1.8),\n            (5000, 1.2),\n            (7000, 0.8),\n            (10000, 0.9),\n        ];\n        let config = RestrictedLossDropConfig {\n            min_relative_decrease: 0.3,\n        };\n\n        let report = verify_restricted_loss_drop(&restricted_loss, &config)\n            .expect(\"expected restricted loss drop verification to pass\");\n        assert_eq!(report.drop_step, 7000);\n        assert!(report.baseline_loss > report.drop_loss);\n        assert!((report.baseline_loss - report.drop_loss) / report.baseline_loss >= 0.3);\n    }\n\n    #[test]\n    fn restricted_loss_drop_verification_fails_without_drop() {\n        let restricted_loss = vec![\n            (0, 2.0),\n            (1000, 2.1),\n            (5000, 1.9),\n            (7000, 2.0),\n            (10000, 2.05),\n        ];\n        let config = RestrictedLossDropConfig {\n            min_relative_decrease: 0.3,\n        };\n\n        let err = verify_restricted_loss_drop(&restricted_loss, &config)\n            .expect_err(\"expected restricted loss drop verification to fail\");\n        assert!(err.contains(\"restricted loss decrease too small\"));\n    }\n\n    #[test]\n    fn mlp_activation_wave_verification_passes_on_rectified_sine() {\n        let n = ModularAdditionDataset::modulus();\n        let freq = 5.0;\n        let mut signal = Vec::with_capacity(n);\n        for idx in 0..n {\n            let angle = 2.0 * std::f64::consts::PI * freq * idx as f64 / n as f64;\n            let rectified = angle.sin().max(0.0);\n            let wobble = (2.0 * std::f64::consts::PI * 2.0 * idx as f64 / n as f64)\n                .sin()\n                .abs()\n                * 0.02;\n            signal.push(rectified + wobble);\n        }\n\n        let config = MlpWaveVerificationConfig {\n            max_negative_fraction: 0.0,\n            negative_tolerance: 1e-6,\n            min_dominant_ratio: 3.0,\n            min_correlation: 0.7,\n            phase_steps: 32,\n        };\n\n        let report =\n            verify_mlp_activation_waves_from_signals(&[(0, signal)], &config)\n                .expect(\"expected rectified sine to pass\");\n        assert_eq!(report.neuron_reports.len(), 1);\n    }\n\n    #[test]\n    fn mlp_activation_wave_verification_fails_on_noise() {\n        let n = ModularAdditionDataset::modulus();\n        let signal: Vec<f64> = (0..n).map(|idx| idx as f64 / n as f64).collect();\n\n        let config = MlpWaveVerificationConfig {\n            max_negative_fraction: 0.0,\n            negative_tolerance: 1e-6,\n            min_dominant_ratio: 4.0,\n            min_correlation: 0.8,\n            phase_steps: 16,\n        };\n\n        let err =\n            verify_mlp_activation_waves_from_signals(&[(0, signal)], &config)\n                .expect_err(\"expected linear signal to fail\");\n        assert!(err.contains(\"dominant frequency ratio\") || err.contains(\"correlation\"));\n    }\n\n    #[test]\n    fn pairwise_manifold_transition_passes_on_elliptic_signals() {\n        let n = ModularAdditionDataset::modulus();\n        let freq = 5.0;\n        let mut post_signals = Vec::new();\n        for neuron_idx in 0..7usize {\n            let phase = 2.0 * std::f64::consts::PI * neuron_idx as f64 / 7.0;\n            let signal: Vec<f64> = (0..n)\n                .map(|idx| {\n                    let angle = 2.0 * std::f64::consts::PI * freq * idx as f64 / n as f64 + phase;\n                    angle.sin()\n                })\n                .collect();\n            post_signals.push((neuron_idx, signal));\n        }\n\n        let mut pre_signals = Vec::new();\n        for neuron_idx in 0..7usize {\n            let signal: Vec<f64> = (0..n)\n                .map(|idx| idx as f64 / n as f64 + neuron_idx as f64 * 0.01)\n                .collect();\n            pre_signals.push((neuron_idx, signal));\n        }\n\n        let config = PairwiseManifoldConfig {\n            fixed_y: 0,\n            neuron_indices: (0..7).collect(),\n            min_post_score: 0.75,\n            max_pre_score: 0.55,\n            max_axis_ratio: 8.0,\n            min_pairs_passing: 8,\n        };\n\n        let report = verify_pairwise_manifold_transition_from_signals(\n            &pre_signals,\n            &post_signals,\n            &config,\n        )\n        .expect(\"expected pairwise manifold transition to pass\");\n        assert!(report.passing_pairs >= config.min_pairs_passing);\n    }\n\n    #[test]\n    fn pairwise_manifold_transition_fails_without_rings() {\n        let n = ModularAdditionDataset::modulus();\n        let mut signals = Vec::new();\n        for neuron_idx in 0..7usize {\n            let signal: Vec<f64> = (0..n)\n                .map(|idx| idx as f64 / n as f64 + neuron_idx as f64 * 0.02)\n                .collect();\n            signals.push((neuron_idx, signal));\n        }\n\n        let config = PairwiseManifoldConfig {\n            fixed_y: 0,\n            neuron_indices: (0..7).collect(),\n            min_post_score: 0.8,\n            max_pre_score: 0.55,\n            max_axis_ratio: 4.0,\n            min_pairs_passing: 5,\n        };\n\n        let err = verify_pairwise_manifold_transition_from_signals(\n            &signals,\n            &signals,\n            &config,\n        )\n        .expect_err(\"expected pairwise manifold transition to fail\");\n        assert!(err.contains(\"post-grok\") || err.contains(\"pre-grok\"));\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_passes_on_diagonal_corrugation() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create diagonal ridges: f(x,y) = sin(2Ï€ * freq * (x+y) / n)\n        let freq = 3.0;\n        for x in 0..n {\n            for y in 0..n {\n                let angle = 2.0 * std::f64::consts::PI * freq * (x + y) as f64 / n as f64;\n                surface[x][y] = angle.sin();\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let report = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect(\"expected diagonal ridge verification to pass\");\n        assert!(report.diagonal_score > report.axis_score);\n        assert!(report.ratio >= config.min_diagonal_ratio);\n        assert!(report.diagonal_score >= config.min_diagonal_score);\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_passes_on_anti_diagonal_pattern() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create anti-diagonal ridges: f(x,y) = sin(2Ï€ * freq * (x-y) / n)\n        let freq = 3.0;\n        for x in 0..n {\n            for y in 0..n {\n                let angle = 2.0 * std::f64::consts::PI * freq * (x as i32 - y as i32) as f64 / n as f64;\n                surface[x][y] = angle.sin();\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let report = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect(\"expected anti-diagonal ridge verification to pass\");\n        assert!(report.diagonal_score > report.axis_score);\n        assert!(report.ratio >= config.min_diagonal_ratio);\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_fails_on_axis_aligned_pattern() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create axis-aligned ridges: f(x,y) = sin(2Ï€ * freq * x / n)\n        let freq = 3.0;\n        for x in 0..n {\n            for y in 0..n {\n                let angle = 2.0 * std::f64::consts::PI * freq * x as f64 / n as f64;\n                surface[x][y] = angle.sin();\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let err = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect_err(\"expected axis-aligned pattern to fail diagonal verification\");\n        assert!(err.contains(\"diagonal/axis ratio\") || err.contains(\"diagonal autocorrelation\"));\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_fails_on_flat_surface() {\n        let n = 50;\n        let surface = vec![vec![1.5; n]; n];\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let err = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect_err(\"expected flat surface to fail\");\n        assert!(err.contains(\"surface variance is zero\"));\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_fails_on_random_noise() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create random noise (low autocorrelation everywhere)\n        for x in 0..n {\n            for y in 0..n {\n                surface[x][y] = ((x * 17 + y * 31) % 100) as f64 / 100.0;\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let err = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect_err(\"expected random noise to fail diagonal verification\");\n        assert!(err.contains(\"diagonal/axis ratio\") || err.contains(\"diagonal autocorrelation\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_passes_on_valid_grokking_curve() {\n        // Simulate typical grokking: train converges early, val plateaus then jumps\n        let train_acc = vec![\n            (1, 0.10),\n            (10, 0.50),\n            (100, 0.80),\n            (500, 0.98),\n            (1000, 0.99),\n            (2000, 0.995),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (10, 0.009),\n            (100, 0.01),\n            (500, 0.011),\n            (1000, 0.009),\n            (1500, 0.01),\n            (2000, 0.92),\n            (3000, 0.98),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 1500,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let report = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect(\"expected snake curve verification to pass\");\n\n        assert_eq!(report.train_converged_by_step, 500);\n        assert!(report.val_plateau_max_acc < config.val_plateau_max_acc);\n        assert_eq!(report.val_grok_step, 2000);\n        assert!(report.curve_shape_valid);\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_on_simultaneous_convergence() {\n        // Both train and val converge at the same time (no grokking)\n        let train_acc = vec![\n            (1, 0.10),\n            (100, 0.98),\n            (500, 0.99),\n        ];\n        let val_acc = vec![\n            (1, 0.10),\n            (100, 0.97),\n            (500, 0.98),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 200,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let err = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect_err(\"expected snake curve verification to fail\");\n        assert!(err.contains(\"plateau region too high\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_on_slow_train_convergence() {\n        // Train takes too long to converge (not the typical snake curve)\n        let train_acc = vec![\n            (1, 0.10),\n            (500, 0.50),\n            (1000, 0.70),\n            (2000, 0.98),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (500, 0.01),\n            (1000, 0.01),\n            (2000, 0.92),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 1500,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let err = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect_err(\"expected snake curve verification to fail\");\n        assert!(err.contains(\"converges at step 2000\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_without_val_jump() {\n        // Validation never jumps up (no generalization)\n        let train_acc = vec![\n            (1, 0.10),\n            (100, 0.98),\n            (500, 0.99),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (100, 0.01),\n            (500, 0.01),\n            (2000, 0.02),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 200,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let err = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect_err(\"expected snake curve verification to fail\");\n        assert!(err.contains(\"validation accuracy never reaches\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_on_wrong_phase_ordering() {\n        // Val jumps before train converges (backwards snake)\n        let train_acc = vec![\n            (1, 0.10),\n            (500, 0.50),\n            (2000, 0.98),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (100, 0.95),\n            (500, 0.98),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 200,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let result = verify_snake_curve_shape(&train_acc, &val_acc, &config);\n        // Should fail because val jumps before train converges (not a proper snake curve)\n        // This can fail for multiple reasons, but should definitely fail\n        assert!(result.is_err(), \"Expected snake curve verification to fail but it passed\");\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_passes_on_decreasing_norms() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 85.0),\n            (1000, 72.0),\n            (1500, 63.0),\n            (2000, 58.0),\n            (2500, 55.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let report = verify_weight_norm_decay(&weight_norms, Some(2000), &config)\n            .expect(\"expected weight norm decay verification to pass\");\n\n        assert_eq!(report.initial_norm, 100.0);\n        assert_eq!(report.final_norm, 55.0);\n        assert!((report.relative_decrease - 0.45).abs() < 0.01);\n        assert!(report.grok_step_norm.is_some());\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_fails_on_flat_norms() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 99.0),\n            (1000, 98.5),\n            (1500, 98.0),\n            (2000, 97.5),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let err = verify_weight_norm_decay(&weight_norms, None, &config)\n            .expect_err(\"expected weight norm decay verification to fail\");\n        assert!(err.contains(\"weight norm decrease\"));\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_fails_on_increasing_norms() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 95.0),\n            (1000, 105.0), // Increase here\n            (1500, 90.0),\n            (2000, 85.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let err = verify_weight_norm_decay(&weight_norms, None, &config)\n            .expect_err(\"expected weight norm decay verification to fail\");\n        assert!(err.contains(\"weight norm increased\"));\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_allows_small_noise() {\n        // Weight decay with small numerical noise (within 1% tolerance)\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 95.0),\n            (1000, 90.5),  // Small increase within tolerance\n            (1500, 90.0),\n            (2000, 85.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let report = verify_weight_norm_decay(&weight_norms, None, &config)\n            .expect(\"expected weight norm decay with small noise to pass\");\n\n        assert_eq!(report.initial_norm, 100.0);\n        assert_eq!(report.final_norm, 85.0);\n        assert!((report.relative_decrease - 0.15).abs() < 0.01);\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_finds_grok_step_norm() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (1000, 80.0),\n            (1800, 68.0),  // Within window of grok step (2200 - 500 = 1700)\n            (2000, 65.0),\n            (2200, 62.0),  // Around grok step\n            (3000, 55.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let report = verify_weight_norm_decay(&weight_norms, Some(2200), &config)\n            .expect(\"expected weight norm decay verification to pass\");\n\n        assert!(report.grok_step_norm.is_some());\n        let grok_norm = report.grok_step_norm.unwrap();\n        // Should find the first norm in the window [1700, 2700], which is 68.0 at step 1800\n        assert!((grok_norm - 68.0).abs() < 0.01);\n    }\n}\n","structuredPatch":[{"oldStart":1124,"oldLines":6,"newStart":1124,"newLines":7,"lines":["     })"," }"," ","+#[allow(dead_code)]"," fn dominant_frequency_ratio(signal: &[f64]) -> Result<(usize, f64), String> {","     if signal.len() < 2 {","         return Err(\"signal too short for FFT\".to_string());"]}],"userModified":false,"replaceAll":false}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01PbdZvUdYWHiDxZu9fg7xPr","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01Lc6DGQZokkAYCicPfR3NHW","name":"Edit","input":{"replace_all":false,"file_path":"/Users/admin/MechInt/src/verify.rs","old_string":"fn best_rectified_sine_correlation(signal: &[f64], frequency: usize, phase_steps: usize) -> f64 {","new_string":"#[allow(dead_code)]\nfn best_rectified_sine_correlation(signal: &[f64], frequency: usize, phase_steps: usize) -> f64 {"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":357,"cache_read_input_tokens":64430,"cache_creation":{"ephemeral_5m_input_tokens":357,"ephemeral_1h_input_tokens":0},"output_tokens":198,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"3a357f04-c568-4863-b638-0adab83cdf82"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01Lc6DGQZokkAYCicPfR3NHW","type":"tool_result","content":"The file /Users/admin/MechInt/src/verify.rs has been updated. Here's the result of running `cat -n` on a snippet of the edited file:\n  1167â†’\n  1168â†’    Ok((dominant_idx, ratio))\n  1169â†’}\n  1170â†’\n  1171â†’#[allow(dead_code)]\n  1172â†’fn best_rectified_sine_correlation(signal: &[f64], frequency: usize, phase_steps: usize) -> f64 {\n  1173â†’    if signal.is_empty() || frequency == 0 || phase_steps == 0 {\n  1174â†’        return 0.0;\n  1175â†’    }\n  1176â†’"}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"99e593d5-1721-4b51-b667-92ee01f21b5f","tool_use_result":{"filePath":"/Users/admin/MechInt/src/verify.rs","oldString":"fn best_rectified_sine_correlation(signal: &[f64], frequency: usize, phase_steps: usize) -> f64 {","newString":"#[allow(dead_code)]\nfn best_rectified_sine_correlation(signal: &[f64], frequency: usize, phase_steps: usize) -> f64 {","originalFile":"use burn::{\n    data::dataset::Dataset,\n    tensor::{backend::Backend, Int, Tensor},\n};\nuse rustfft::{num_complex::Complex, FftPlanner};\n\nuse crate::analysis::{AccuracyHistory, LossHistory};\nuse crate::data::ModularAdditionDataset;\nuse crate::model::Transformer;\n\n#[derive(Debug, Clone)]\npub struct GrokkingVerificationConfig {\n    pub early_train_acc_threshold: f32,\n    pub early_step_max: usize,\n    pub plateau_min_step: usize,\n    pub generalization_window: usize,\n    pub chance_accuracy: f32,\n    pub chance_tolerance: f32,\n    pub target_val_acc_threshold: f32,\n    pub accuracy_window: usize,\n    pub loss_window: usize,\n    pub loss_drop_fraction: f64,\n}\n\nimpl GrokkingVerificationConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        Self {\n            early_train_acc_threshold: 0.99,\n            early_step_max: 1000,\n            plateau_min_step: 1000,\n            generalization_window: 10_000,\n            chance_accuracy: 1.0 / modulus as f32,\n            chance_tolerance: 0.02,\n            target_val_acc_threshold: 0.99,\n            accuracy_window: 5,\n            loss_window: 5,\n            loss_drop_fraction: 0.5,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct GrokkingPhaseReport {\n    pub early_train_step: usize,\n    pub transition_step: usize,\n    pub plateau_max_val_acc: f32,\n    pub loss_drop_ratio: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossVerificationConfig {\n    pub plateau_min_step: usize,\n    pub drop_window: usize,\n    pub drop_fraction: f64,\n    pub min_step_lead: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossReport {\n    pub restricted_drop_step: usize,\n    pub full_drop_step: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossDropConfig {\n    pub min_relative_decrease: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossDropReport {\n    pub drop_step: usize,\n    pub drop_loss: f64,\n    pub baseline_loss: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct ExcludedLossSpikeConfig {\n    pub min_relative_increase: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct ExcludedLossSpikeReport {\n    pub spike_step: usize,\n    pub spike_loss: f64,\n    pub baseline_loss: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct MlpWaveVerificationConfig {\n    pub max_negative_fraction: f64,\n    pub negative_tolerance: f64,\n    pub min_dominant_ratio: f64,\n    pub min_correlation: f64,\n    pub phase_steps: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct MlpWaveNeuronReport {\n    pub neuron_index: usize,\n    pub dominant_frequency: usize,\n    pub dominant_ratio: f64,\n    pub negative_fraction: f64,\n    pub best_correlation: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct MlpWaveReport {\n    pub neuron_reports: Vec<MlpWaveNeuronReport>,\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldConfig {\n    pub fixed_y: usize,\n    pub neuron_indices: Vec<usize>,\n    pub min_post_score: f64,\n    pub max_pre_score: f64,\n    pub max_axis_ratio: f64,\n    pub min_pairs_passing: usize,\n}\n\nimpl PairwiseManifoldConfig {\n    pub fn default_for_modulus(_modulus: usize) -> Self {\n        Self {\n            fixed_y: 0,\n            neuron_indices: (0..7).collect(),\n            min_post_score: 0.75,\n            max_pre_score: 0.55,\n            max_axis_ratio: 8.0,\n            min_pairs_passing: 10,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldPairReport {\n    pub neuron_a: usize,\n    pub neuron_b: usize,\n    pub circularity_score: f64,\n    pub radius_cv: f64,\n    pub axis_ratio: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldReport {\n    pub pair_reports: Vec<PairwiseManifoldPairReport>,\n    pub average_score: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldTransitionReport {\n    pub pre_average_score: f64,\n    pub post_average_score: f64,\n    pub passing_pairs: usize,\n    pub total_pairs: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct DiagonalRidgeConfig {\n    pub neuron_index: usize,\n    pub max_lag: usize,\n    pub min_diagonal_ratio: f64,\n    pub min_diagonal_score: f64,\n}\n\nimpl DiagonalRidgeConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        let max_lag = (modulus / 10).max(4).min(20);\n        Self {\n            neuron_index: 10,\n            max_lag,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct DiagonalRidgeReport {\n    pub neuron_index: usize,\n    pub diagonal_score: f64,\n    pub axis_score: f64,\n    pub ratio: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct SnakeCurveConfig {\n    pub early_step_max: usize,\n    pub train_convergence_threshold: f32,\n    pub val_plateau_min_step: usize,\n    pub val_plateau_max_acc: f32,\n    pub val_final_threshold: f32,\n}\n\nimpl SnakeCurveConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        Self {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 1000,\n            val_plateau_max_acc: 1.0 / modulus as f32 + 0.05,\n            val_final_threshold: 0.90,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct WeightNormDecayConfig {\n    pub min_relative_decrease: f64,\n    pub plateau_min_step: usize,\n    pub grok_step_window: usize,\n}\n\nimpl WeightNormDecayConfig {\n    pub fn default_for_grokking() -> Self {\n        Self {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct WeightNormDecayReport {\n    pub initial_norm: f64,\n    pub final_norm: f64,\n    pub relative_decrease: f64,\n    pub grok_step_norm: Option<f64>,\n}\n\n#[derive(Debug, Clone)]\npub struct SnakeCurveReport {\n    pub train_converged_by_step: usize,\n    pub val_plateau_max_acc: f32,\n    pub val_grok_step: usize,\n    pub curve_shape_valid: bool,\n}\n\npub fn verify_grokking_phase_transition(\n    loss_history: &LossHistory,\n    accuracy_history: &AccuracyHistory,\n    config: &GrokkingVerificationConfig,\n) -> Result<GrokkingPhaseReport, String> {\n    let (early_train_step, _) = accuracy_history\n        .train_snapshots\n        .iter()\n        .find(|(_, acc)| *acc >= config.early_train_acc_threshold)\n        .ok_or_else(|| \"train accuracy never reaches threshold\".to_string())?;\n\n    if *early_train_step > config.early_step_max {\n        return Err(format!(\n            \"train accuracy reaches {:.2} after step {}, expected before {}\",\n            config.early_train_acc_threshold, early_train_step, config.early_step_max\n        ));\n    }\n\n    let plateau_max_val_acc = accuracy_history\n        .val_snapshots\n        .iter()\n        .filter(|(step, _)| *step <= config.plateau_min_step)\n        .map(|(_, acc)| *acc)\n        .fold(None::<f32>, |accum, value| {\n            Some(accum.map_or(value, |max_val| max_val.max(value)))\n        })\n        .ok_or_else(|| \"no validation accuracy samples before plateau_min_step\".to_string())?;\n\n    if plateau_max_val_acc > config.chance_accuracy + config.chance_tolerance {\n        return Err(format!(\n            \"validation accuracy before step {} exceeds chance (max {:.4})\",\n            config.plateau_min_step, plateau_max_val_acc\n        ));\n    }\n\n    let (transition_step, _) = accuracy_history\n        .val_snapshots\n        .iter()\n        .find(|(step, acc)| {\n            *step >= config.plateau_min_step && *acc >= config.target_val_acc_threshold\n        })\n        .ok_or_else(|| \"validation accuracy never reaches target threshold\".to_string())?;\n\n    let latest_allowed = config.plateau_min_step + config.generalization_window;\n    if *transition_step > latest_allowed {\n        return Err(format!(\n            \"validation accuracy reaches {:.2} at step {}, expected before {}\",\n            config.target_val_acc_threshold, transition_step, latest_allowed\n        ));\n    }\n\n    verify_accuracy_jump(\n        &accuracy_history.val_snapshots,\n        *transition_step,\n        config.accuracy_window,\n        config.chance_accuracy,\n        config.chance_tolerance,\n        config.target_val_acc_threshold,\n    )?;\n\n    let loss_drop_ratio = verify_loss_drop(\n        &loss_history.val_snapshots,\n        *transition_step,\n        config.loss_window,\n        config.loss_drop_fraction,\n    )?;\n\n    Ok(GrokkingPhaseReport {\n        early_train_step: *early_train_step,\n        transition_step: *transition_step,\n        plateau_max_val_acc,\n        loss_drop_ratio,\n    })\n}\n\nfn verify_accuracy_jump(\n    val_accuracy: &[(usize, f32)],\n    transition_step: usize,\n    window: usize,\n    chance_accuracy: f32,\n    chance_tolerance: f32,\n    target_val_acc_threshold: f32,\n) -> Result<(), String> {\n    let before_values: Vec<f32> = val_accuracy\n        .iter()\n        .filter(|(step, _)| *step < transition_step)\n        .map(|(_, acc)| *acc)\n        .collect();\n\n    let after_values: Vec<f32> = val_accuracy\n        .iter()\n        .filter(|(step, _)| *step >= transition_step)\n        .map(|(_, acc)| *acc)\n        .collect();\n\n    if before_values.len() < window || after_values.len() < window {\n        return Err(\"insufficient accuracy samples around transition\".to_string());\n    }\n\n    let before_avg: f32 =\n        before_values[before_values.len() - window..].iter().sum::<f32>() / window as f32;\n    let after_avg: f32 = after_values[..window].iter().sum::<f32>() / window as f32;\n\n    if before_avg > chance_accuracy + chance_tolerance {\n        return Err(format!(\n            \"validation accuracy plateau too high before transition (avg {:.4})\",\n            before_avg\n        ));\n    }\n\n    if after_avg < target_val_acc_threshold {\n        return Err(format!(\n            \"validation accuracy after transition too low (avg {:.4})\",\n            after_avg\n        ));\n    }\n\n    Ok(())\n}\n\nfn verify_loss_drop(\n    val_losses: &[(usize, f64)],\n    transition_step: usize,\n    loss_window: usize,\n    loss_drop_fraction: f64,\n) -> Result<f64, String> {\n    let before_losses: Vec<f64> = val_losses\n        .iter()\n        .filter(|(step, _)| *step < transition_step)\n        .map(|(_, loss)| *loss)\n        .collect();\n\n    let after_losses: Vec<f64> = val_losses\n        .iter()\n        .filter(|(step, _)| *step >= transition_step)\n        .map(|(_, loss)| *loss)\n        .collect();\n\n    if before_losses.len() < loss_window || after_losses.len() < loss_window {\n        return Err(\"insufficient loss samples around transition\".to_string());\n    }\n\n    let before_avg: f64 =\n        before_losses[before_losses.len() - loss_window..].iter().sum::<f64>()\n            / loss_window as f64;\n    let after_avg: f64 = after_losses[..loss_window].iter().sum::<f64>() / loss_window as f64;\n\n    if after_avg > before_avg * (1.0 - loss_drop_fraction) {\n        return Err(format!(\n            \"validation loss drop too small (before {:.4}, after {:.4})\",\n            before_avg, after_avg\n        ));\n    }\n\n    Ok(after_avg / before_avg)\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_restricted_loss_early_drop(\n    full_losses: &[(usize, f64)],\n    restricted_losses: &[(usize, f64)],\n    config: &RestrictedLossVerificationConfig,\n) -> Result<RestrictedLossReport, String> {\n    let restricted_baseline =\n        baseline_loss(restricted_losses, config.plateau_min_step, config.drop_window)?;\n    let full_baseline = baseline_loss(full_losses, config.plateau_min_step, config.drop_window)?;\n\n    let restricted_drop_step = find_drop_step(\n        restricted_losses,\n        config.plateau_min_step,\n        restricted_baseline,\n        config.drop_fraction,\n    )?;\n    let full_drop_step = find_drop_step(\n        full_losses,\n        config.plateau_min_step,\n        full_baseline,\n        config.drop_fraction,\n    )?;\n\n    if restricted_drop_step + config.min_step_lead > full_drop_step {\n        return Err(format!(\n            \"restricted loss drops at step {}, expected at least {} steps before full loss (step {})\",\n            restricted_drop_step, config.min_step_lead, full_drop_step\n        ));\n    }\n\n    Ok(RestrictedLossReport {\n        restricted_drop_step,\n        full_drop_step,\n    })\n}\n\npub fn verify_restricted_loss_drop(\n    restricted_losses: &[(usize, f64)],\n    config: &RestrictedLossDropConfig,\n) -> Result<RestrictedLossDropReport, String> {\n    if restricted_losses.len() < 3 {\n        return Err(\"restricted loss history requires at least 3 points\".to_string());\n    }\n\n    let first = restricted_losses.first().ok_or_else(|| {\n        \"restricted loss history is empty\".to_string()\n    })?;\n    let baseline_loss = first.1;\n\n    let (min_idx, (min_step, min_loss)) = restricted_losses\n        .iter()\n        .enumerate()\n        .min_by(|(_, a), (_, b)| {\n            a.1.partial_cmp(&b.1).unwrap_or(std::cmp::Ordering::Equal)\n        })\n        .ok_or_else(|| \"restricted loss history is empty\".to_string())?;\n\n    if min_idx == 0 {\n        return Err(\"restricted loss minimum occurs at start (no drop detected)\".to_string());\n    }\n\n    let relative_decrease = (baseline_loss - min_loss) / baseline_loss;\n    if relative_decrease < config.min_relative_decrease {\n        return Err(format!(\n            \"restricted loss decrease too small (baseline {:.4}, min {:.4}, decrease {:.1}%)\",\n            baseline_loss, min_loss, relative_decrease * 100.0\n        ));\n    }\n\n    Ok(RestrictedLossDropReport {\n        drop_step: *min_step,\n        drop_loss: *min_loss,\n        baseline_loss,\n    })\n}\n\npub fn verify_excluded_loss_spike(\n    excluded_losses: &[(usize, f64)],\n    config: &ExcludedLossSpikeConfig,\n) -> Result<ExcludedLossSpikeReport, String> {\n    if excluded_losses.len() < 3 {\n        return Err(\"excluded loss history requires at least 3 points\".to_string());\n    }\n\n    let first = excluded_losses.first().ok_or_else(|| {\n        \"excluded loss history is empty\".to_string()\n    })?;\n    let last = excluded_losses.last().ok_or_else(|| {\n        \"excluded loss history is empty\".to_string()\n    })?;\n    let baseline_loss = (first.1 + last.1) / 2.0;\n\n    let (peak_idx, (peak_step, peak_loss)) = excluded_losses\n        .iter()\n        .enumerate()\n        .max_by(|(_, a), (_, b)| {\n            a.1.partial_cmp(&b.1).unwrap_or(std::cmp::Ordering::Equal)\n        })\n        .ok_or_else(|| \"excluded loss history is empty\".to_string())?;\n\n    if peak_idx == 0 || peak_idx + 1 == excluded_losses.len() {\n        return Err(\"excluded loss spike occurs at boundary\".to_string());\n    }\n\n    if peak_loss < &(baseline_loss * (1.0 + config.min_relative_increase)) {\n        return Err(format!(\n            \"excluded loss spike too small (baseline {:.4}, peak {:.4})\",\n            baseline_loss, peak_loss\n        ));\n    }\n\n    Ok(ExcludedLossSpikeReport {\n        spike_step: *peak_step,\n        spike_loss: *peak_loss,\n        baseline_loss,\n    })\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_mlp_activation_waves<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n    fixed_y: usize,\n    neuron_indices: &[usize],\n    config: &MlpWaveVerificationConfig,\n) -> Result<MlpWaveReport, String> {\n    let activations = collect_mlp_post_relu_activations(model, device, fixed_y)?;\n    if neuron_indices.is_empty() {\n        return Err(\"neuron indices list is empty\".to_string());\n    }\n\n    let d_ff = activations.len();\n    for &idx in neuron_indices {\n        if idx >= d_ff {\n            return Err(format!(\n                \"neuron index {} out of range (d_ff={})\",\n                idx, d_ff\n            ));\n        }\n    }\n\n    let signals: Vec<(usize, Vec<f64>)> = neuron_indices\n        .iter()\n        .map(|&idx| (idx, activations[idx].clone()))\n        .collect();\n    verify_mlp_activation_waves_from_signals(&signals, config)\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_mlp_activation_waves_from_signals(\n    signals: &[(usize, Vec<f64>)],\n    config: &MlpWaveVerificationConfig,\n) -> Result<MlpWaveReport, String> {\n    if signals.is_empty() {\n        return Err(\"no activation signals provided\".to_string());\n    }\n\n    let mut reports = Vec::with_capacity(signals.len());\n    for (neuron_index, signal) in signals {\n        let report = analyze_mlp_activation_wave(*neuron_index, signal, config)?;\n        reports.push(report);\n    }\n\n    Ok(MlpWaveReport {\n        neuron_reports: reports,\n    })\n}\n\npub fn verify_mlp_pairwise_manifold_transition<B: Backend>(\n    pre_model: &Transformer<B>,\n    post_model: &Transformer<B>,\n    device: &B::Device,\n    config: &PairwiseManifoldConfig,\n) -> Result<PairwiseManifoldTransitionReport, String> {\n    let modulus = ModularAdditionDataset::modulus();\n    if config.fixed_y >= modulus {\n        return Err(format!(\n            \"fixed_y {} out of range (modulus {})\",\n            config.fixed_y, modulus\n        ));\n    }\n\n    let pre_activations = collect_mlp_post_relu_activations(pre_model, device, config.fixed_y)?;\n    let post_activations = collect_mlp_post_relu_activations(post_model, device, config.fixed_y)?;\n\n    let d_ff = pre_activations.len();\n    if post_activations.len() != d_ff {\n        return Err(\"post activations shape mismatch\".to_string());\n    }\n\n    if config.neuron_indices.len() < 2 {\n        return Err(\"need at least two neuron indices\".to_string());\n    }\n\n    for &idx in &config.neuron_indices {\n        if idx >= d_ff {\n            return Err(format!(\n                \"neuron index {} out of range (d_ff={})\",\n                idx, d_ff\n            ));\n        }\n    }\n\n    let pre_signals: Vec<(usize, Vec<f64>)> = config\n        .neuron_indices\n        .iter()\n        .map(|&idx| (idx, pre_activations[idx].clone()))\n        .collect();\n    let post_signals: Vec<(usize, Vec<f64>)> = config\n        .neuron_indices\n        .iter()\n        .map(|&idx| (idx, post_activations[idx].clone()))\n        .collect();\n\n    verify_pairwise_manifold_transition_from_signals(&pre_signals, &post_signals, config)\n}\n\npub fn collect_mlp_activation_surface<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n    neuron_index: usize,\n) -> Result<Vec<Vec<f64>>, String> {\n    let modulus = ModularAdditionDataset::modulus();\n    let equals_token = ModularAdditionDataset::equals_token();\n\n    let mut inputs_vec = Vec::with_capacity(modulus * modulus * 3);\n    for x in 0..modulus {\n        for y in 0..modulus {\n            inputs_vec.push(x as i32);\n            inputs_vec.push(y as i32);\n            inputs_vec.push(equals_token as i32);\n        }\n    }\n\n    let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n        .reshape([modulus * modulus, 3]);\n    let (_, mlp_acts) = model.forward_with_mlp_activations(inputs);\n    let [batch_size, d_ff] = mlp_acts.dims();\n    if batch_size != modulus * modulus {\n        return Err(format!(\n            \"unexpected activation batch size {} (expected {})\",\n            batch_size,\n            modulus * modulus\n        ));\n    }\n    if neuron_index >= d_ff {\n        return Err(format!(\n            \"neuron index {} out of range (d_ff={})\",\n            neuron_index, d_ff\n        ));\n    }\n\n    let data: Vec<f32> = mlp_acts.into_data().to_vec().unwrap();\n    let mut surface = vec![vec![0.0f64; modulus]; modulus];\n    for x in 0..modulus {\n        for y in 0..modulus {\n            let batch = x * modulus + y;\n            let idx = batch * d_ff + neuron_index;\n            surface[x][y] = data[idx] as f64;\n        }\n    }\n\n    Ok(surface)\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_constructive_interference_surface_from_grid(\n    surface: &[Vec<f64>],\n    config: &DiagonalRidgeConfig,\n) -> Result<DiagonalRidgeReport, String> {\n    let (diag_main, diag_anti, axis_x, axis_y) =\n        surface_autocorrelation_scores(surface, config.max_lag)?;\n\n    let diagonal_score = diag_main.max(diag_anti);\n    let axis_score = axis_x.max(axis_y);\n    let ratio = if axis_score > 0.0 {\n        diagonal_score / axis_score\n    } else {\n        f64::INFINITY\n    };\n\n    if diagonal_score < config.min_diagonal_score {\n        return Err(format!(\n            \"diagonal autocorrelation {:.3} below threshold {:.3}\",\n            diagonal_score, config.min_diagonal_score\n        ));\n    }\n\n    if ratio < config.min_diagonal_ratio {\n        return Err(format!(\n            \"diagonal/axis ratio {:.3} below threshold {:.3}\",\n            ratio, config.min_diagonal_ratio\n        ));\n    }\n\n    Ok(DiagonalRidgeReport {\n        neuron_index: config.neuron_index,\n        diagonal_score,\n        axis_score,\n        ratio,\n    })\n}\n\npub fn verify_pairwise_manifold_transition_from_signals(\n    pre_signals: &[(usize, Vec<f64>)],\n    post_signals: &[(usize, Vec<f64>)],\n    config: &PairwiseManifoldConfig,\n) -> Result<PairwiseManifoldTransitionReport, String> {\n    let pre_report = evaluate_pairwise_manifolds_from_signals(pre_signals)?;\n    let post_report = evaluate_pairwise_manifolds_from_signals(post_signals)?;\n\n    if pre_report.average_score > config.max_pre_score {\n        return Err(format!(\n            \"pre-grok average circularity {:.3} exceeds threshold {:.3}\",\n            pre_report.average_score, config.max_pre_score\n        ));\n    }\n\n    let passing_pairs = post_report\n        .pair_reports\n        .iter()\n        .filter(|pair| {\n            pair.circularity_score >= config.min_post_score\n                && pair.axis_ratio <= config.max_axis_ratio\n        })\n        .count();\n    if passing_pairs < config.min_pairs_passing {\n        return Err(format!(\n            \"post-grok only {} pairs meet circularity threshold (need {})\",\n            passing_pairs, config.min_pairs_passing\n        ));\n    }\n\n    Ok(PairwiseManifoldTransitionReport {\n        pre_average_score: pre_report.average_score,\n        post_average_score: post_report.average_score,\n        passing_pairs,\n        total_pairs: post_report.pair_reports.len(),\n    })\n}\n\nfn evaluate_pairwise_manifolds_from_signals(\n    signals: &[(usize, Vec<f64>)],\n) -> Result<PairwiseManifoldReport, String> {\n    if signals.len() < 2 {\n        return Err(\"need at least two activation signals\".to_string());\n    }\n\n    let signal_len = signals[0].1.len();\n    if signal_len < 3 {\n        return Err(\"activation signal too short\".to_string());\n    }\n\n    for (idx, signal) in signals {\n        if signal.len() != signal_len {\n            return Err(format!(\n                \"signal length mismatch for neuron {}\",\n                idx\n            ));\n        }\n    }\n\n    let mut reports = Vec::new();\n    for i in 0..signals.len() {\n        for j in (i + 1)..signals.len() {\n            let (neuron_a, signal_a) = &signals[i];\n            let (neuron_b, signal_b) = &signals[j];\n            let points: Vec<(f64, f64)> = signal_a\n                .iter()\n                .zip(signal_b.iter())\n                .map(|(x, y)| (*x, *y))\n                .collect();\n            let (score, radius_cv, axis_ratio) =\n                match pairwise_circularity_metrics(&points) {\n                    Ok(metrics) => metrics,\n                    Err(_) => (0.0, 1.0, f64::INFINITY),\n                };\n            reports.push(PairwiseManifoldPairReport {\n                neuron_a: *neuron_a,\n                neuron_b: *neuron_b,\n                circularity_score: score,\n                radius_cv,\n                axis_ratio,\n            });\n        }\n    }\n\n    let average_score = reports\n        .iter()\n        .map(|pair| pair.circularity_score)\n        .sum::<f64>()\n        / reports.len() as f64;\n\n    Ok(PairwiseManifoldReport {\n        pair_reports: reports,\n        average_score,\n    })\n}\n\nfn pairwise_circularity_metrics(points: &[(f64, f64)]) -> Result<(f64, f64, f64), String> {\n    if points.len() < 3 {\n        return Err(\"not enough points for circularity\".to_string());\n    }\n\n    let n = points.len() as f64;\n    let mean_x = points.iter().map(|(x, _)| *x).sum::<f64>() / n;\n    let mean_y = points.iter().map(|(_, y)| *y).sum::<f64>() / n;\n\n    let mut var_x = 0.0;\n    let mut var_y = 0.0;\n    let mut cov_xy = 0.0;\n    for (x, y) in points {\n        let dx = x - mean_x;\n        let dy = y - mean_y;\n        var_x += dx * dx;\n        var_y += dy * dy;\n        cov_xy += dx * dy;\n    }\n    var_x /= n;\n    var_y /= n;\n    cov_xy /= n;\n\n    let trace = var_x + var_y;\n    let det = var_x * var_y - cov_xy * cov_xy;\n    if det <= 1.0e-12 || trace <= 0.0 {\n        return Err(\"degenerate covariance\".to_string());\n    }\n\n    let half_trace = trace * 0.5;\n    let disc = (half_trace * half_trace - det).max(0.0).sqrt();\n    let lambda1 = half_trace + disc;\n    let lambda2 = half_trace - disc;\n    if lambda2 <= 0.0 {\n        return Err(\"degenerate eigenvalues\".to_string());\n    }\n\n    let axis_ratio = (lambda1 / lambda2).sqrt();\n\n    let mut v1_x = cov_xy;\n    let mut v1_y = lambda1 - var_x;\n    if v1_x.abs() < 1.0e-12 && v1_y.abs() < 1.0e-12 {\n        v1_x = 1.0;\n        v1_y = 0.0;\n    }\n    let norm = (v1_x * v1_x + v1_y * v1_y).sqrt();\n    v1_x /= norm;\n    v1_y /= norm;\n    let v2_x = -v1_y;\n    let v2_y = v1_x;\n\n    let mut radii = Vec::with_capacity(points.len());\n    for (x, y) in points {\n        let dx = x - mean_x;\n        let dy = y - mean_y;\n        let p1 = dx * v1_x + dy * v1_y;\n        let p2 = dx * v2_x + dy * v2_y;\n        let w1 = p1 / lambda1.sqrt();\n        let w2 = p2 / lambda2.sqrt();\n        radii.push((w1 * w1 + w2 * w2).sqrt());\n    }\n\n    let mean_radius = radii.iter().sum::<f64>() / radii.len() as f64;\n    if mean_radius == 0.0 {\n        return Err(\"zero mean radius\".to_string());\n    }\n\n    let mut variance = 0.0;\n    for r in &radii {\n        let diff = r - mean_radius;\n        variance += diff * diff;\n    }\n    let std = (variance / radii.len() as f64).sqrt();\n    let radius_cv = std / mean_radius;\n    let score = 1.0 / (1.0 + radius_cv);\n\n    Ok((score, radius_cv, axis_ratio))\n}\n\n// Helper functions for verification - reserved for future integration\n#[allow(dead_code)]\nfn baseline_loss(\n    losses: &[(usize, f64)],\n    plateau_min_step: usize,\n    drop_window: usize,\n) -> Result<f64, String> {\n    let before: Vec<f64> = losses\n        .iter()\n        .filter(|(step, _)| *step <= plateau_min_step)\n        .map(|(_, loss)| *loss)\n        .collect();\n\n    if before.len() < drop_window || drop_window == 0 {\n        return Err(\"insufficient loss samples for baseline\".to_string());\n    }\n\n    let window = &before[before.len() - drop_window..];\n    Ok(window.iter().sum::<f64>() / drop_window as f64)\n}\n\n#[allow(dead_code)]\nfn find_drop_step(\n    losses: &[(usize, f64)],\n    plateau_min_step: usize,\n    baseline: f64,\n    drop_fraction: f64,\n) -> Result<usize, String> {\n    losses\n        .iter()\n        .filter(|(step, _)| *step >= plateau_min_step)\n        .find(|(_, loss)| *loss <= baseline * (1.0 - drop_fraction))\n        .map(|(step, _)| *step)\n        .ok_or_else(|| \"loss never drops below threshold\".to_string())\n}\n\nfn collect_mlp_post_relu_activations<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n    fixed_y: usize,\n) -> Result<Vec<Vec<f64>>, String> {\n    let modulus = ModularAdditionDataset::modulus();\n    let equals_token = ModularAdditionDataset::equals_token();\n\n    let mut inputs_vec = Vec::with_capacity(modulus * 3);\n    for x in 0..modulus {\n        inputs_vec.push(x as i32);\n        inputs_vec.push(fixed_y as i32);\n        inputs_vec.push(equals_token as i32);\n    }\n\n    let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n        .reshape([modulus, 3]);\n    let (_, mlp_acts) = model.forward_with_mlp_activations(inputs);\n    let [batch_size, d_ff] = mlp_acts.dims();\n\n    if batch_size != modulus {\n        return Err(format!(\n            \"unexpected activation batch size {} (expected {})\",\n            batch_size, modulus\n        ));\n    }\n\n    let data: Vec<f32> = mlp_acts.into_data().to_vec().unwrap();\n    let mut per_neuron = vec![Vec::with_capacity(batch_size); d_ff];\n    for batch in 0..batch_size {\n        for neuron in 0..d_ff {\n            let idx = batch * d_ff + neuron;\n            per_neuron[neuron].push(data[idx] as f64);\n        }\n    }\n\n    Ok(per_neuron)\n}\n\n#[allow(dead_code)]\nfn surface_autocorrelation_scores(\n    surface: &[Vec<f64>],\n    max_lag: usize,\n) -> Result<(f64, f64, f64, f64), String> {\n    let (height, width) = surface_dimensions(surface)?;\n    if height < 2 || width < 2 {\n        return Err(\"surface grid too small\".to_string());\n    }\n\n    let (mean, var) = surface_mean_variance(surface)?;\n    if var == 0.0 {\n        return Err(\"surface variance is zero\".to_string());\n    }\n\n    let max_lag = max_lag.min(height.saturating_sub(1)).min(width.saturating_sub(1));\n    if max_lag == 0 {\n        return Err(\"max_lag too small for surface\".to_string());\n    }\n\n    let diag_main = average_autocorrelation(surface, mean, var, max_lag, 1, 1)?;\n    let diag_anti = average_autocorrelation(surface, mean, var, max_lag, 1, -1)?;\n    let axis_x = average_autocorrelation(surface, mean, var, max_lag, 1, 0)?;\n    let axis_y = average_autocorrelation(surface, mean, var, max_lag, 0, 1)?;\n\n    Ok((diag_main, diag_anti, axis_x, axis_y))\n}\n\n#[allow(dead_code)]\nfn surface_dimensions(surface: &[Vec<f64>]) -> Result<(usize, usize), String> {\n    let height = surface.len();\n    if height == 0 {\n        return Err(\"surface is empty\".to_string());\n    }\n    let width = surface[0].len();\n    if width == 0 {\n        return Err(\"surface has empty rows\".to_string());\n    }\n    for (idx, row) in surface.iter().enumerate() {\n        if row.len() != width {\n            return Err(format!(\n                \"surface row {} has length {}, expected {}\",\n                idx,\n                row.len(),\n                width\n            ));\n        }\n    }\n    Ok((height, width))\n}\n\n#[allow(dead_code)]\nfn surface_mean_variance(surface: &[Vec<f64>]) -> Result<(f64, f64), String> {\n    let (height, width) = surface_dimensions(surface)?;\n    let mut sum = 0.0;\n    for row in surface {\n        for value in row {\n            sum += *value;\n        }\n    }\n    let count = (height * width) as f64;\n    let mean = sum / count;\n    let mut variance = 0.0;\n    for row in surface {\n        for value in row {\n            let diff = *value - mean;\n            variance += diff * diff;\n        }\n    }\n    variance /= count;\n    Ok((mean, variance))\n}\n\n#[allow(dead_code)]\nfn average_autocorrelation(\n    surface: &[Vec<f64>],\n    mean: f64,\n    variance: f64,\n    max_lag: usize,\n    dx: i32,\n    dy: i32,\n) -> Result<f64, String> {\n    let (height, width) = surface_dimensions(surface)?;\n    let mut total = 0.0;\n    let mut lag_count = 0usize;\n\n    for lag in 1..=max_lag {\n        let mut sum = 0.0;\n        let mut count = 0usize;\n        let lag = lag as i32;\n        for x in 0..height {\n            for y in 0..width {\n                let x2 = x as i32 + dx * lag;\n                let y2 = y as i32 + dy * lag;\n                if x2 < 0 || y2 < 0 {\n                    continue;\n                }\n                let x2 = x2 as usize;\n                let y2 = y2 as usize;\n                if x2 >= height || y2 >= width {\n                    continue;\n                }\n                let a = surface[x][y] - mean;\n                let b = surface[x2][y2] - mean;\n                sum += a * b;\n                count += 1;\n            }\n        }\n        if count == 0 {\n            continue;\n        }\n        let corr = sum / (count as f64 * variance);\n        total += corr.abs();\n        lag_count += 1;\n    }\n\n    if lag_count == 0 {\n        return Err(\"no valid lags for autocorrelation\".to_string());\n    }\n\n    Ok(total / lag_count as f64)\n}\n\n#[allow(dead_code)]\nfn analyze_mlp_activation_wave(\n    neuron_index: usize,\n    signal: &[f64],\n    config: &MlpWaveVerificationConfig,\n) -> Result<MlpWaveNeuronReport, String> {\n    if signal.len() < 3 {\n        return Err(format!(\n            \"activation signal for neuron {} is too short\",\n            neuron_index\n        ));\n    }\n\n    let negative_count = signal\n        .iter()\n        .filter(|value| **value < -config.negative_tolerance)\n        .count();\n    let negative_fraction = negative_count as f64 / signal.len() as f64;\n    if negative_fraction > config.max_negative_fraction {\n        return Err(format!(\n            \"neuron {} has {:.3} negative activations\",\n            neuron_index, negative_fraction\n        ));\n    }\n\n    let (dominant_frequency, dominant_ratio) = dominant_frequency_ratio(signal)?;\n    if dominant_ratio < config.min_dominant_ratio {\n        return Err(format!(\n            \"neuron {} dominant frequency ratio {:.3} below threshold {:.3}\",\n            neuron_index, dominant_ratio, config.min_dominant_ratio\n        ));\n    }\n\n    let best_correlation =\n        best_rectified_sine_correlation(signal, dominant_frequency, config.phase_steps);\n    if best_correlation < config.min_correlation {\n        return Err(format!(\n            \"neuron {} rectified sine correlation {:.3} below threshold {:.3}\",\n            neuron_index, best_correlation, config.min_correlation\n        ));\n    }\n\n    Ok(MlpWaveNeuronReport {\n        neuron_index,\n        dominant_frequency,\n        dominant_ratio,\n        negative_fraction,\n        best_correlation,\n    })\n}\n\n#[allow(dead_code)]\nfn dominant_frequency_ratio(signal: &[f64]) -> Result<(usize, f64), String> {\n    if signal.len() < 2 {\n        return Err(\"signal too short for FFT\".to_string());\n    }\n\n    let mut planner = FftPlanner::new();\n    let fft = planner.plan_fft_forward(signal.len());\n    let mut buffer: Vec<Complex<f64>> = signal\n        .iter()\n        .map(|&x| Complex::new(x, 0.0))\n        .collect();\n    fft.process(&mut buffer);\n    let magnitudes: Vec<f64> = buffer.iter().map(|c| c.norm()).collect();\n\n    if magnitudes.len() < 2 {\n        return Err(\"insufficient FFT bins\".to_string());\n    }\n\n    let mut dominant_idx = 1usize;\n    let mut dominant_mag = magnitudes[1];\n    for (idx, &mag) in magnitudes.iter().enumerate().skip(2) {\n        if mag > dominant_mag {\n            dominant_mag = mag;\n            dominant_idx = idx;\n        }\n    }\n\n    if dominant_idx == 0 {\n        return Err(\"dominant frequency is zero\".to_string());\n    }\n\n    let mut nonzero = magnitudes[1..].to_vec();\n    nonzero.sort_by(|a, b| a.partial_cmp(b).unwrap_or(std::cmp::Ordering::Equal));\n    let median_mag = nonzero[nonzero.len() / 2];\n    let ratio = if median_mag == 0.0 {\n        dominant_mag\n    } else {\n        dominant_mag / median_mag\n    };\n\n    Ok((dominant_idx, ratio))\n}\n\nfn best_rectified_sine_correlation(signal: &[f64], frequency: usize, phase_steps: usize) -> f64 {\n    if signal.is_empty() || frequency == 0 || phase_steps == 0 {\n        return 0.0;\n    }\n\n    let tau = std::f64::consts::PI * 2.0;\n    let mut best = -1.0;\n    for step in 0..phase_steps {\n        let phase = tau * step as f64 / phase_steps as f64;\n        let wave: Vec<f64> = (0..signal.len())\n            .map(|idx| {\n                let angle = tau * frequency as f64 * idx as f64 / signal.len() as f64 + phase;\n                angle.sin().max(0.0)\n            })\n            .collect();\n        let corr = pearson_correlation(signal, &wave);\n        if corr > best {\n            best = corr;\n        }\n    }\n\n    best\n}\n\nfn pearson_correlation(a: &[f64], b: &[f64]) -> f64 {\n    if a.len() != b.len() || a.is_empty() {\n        return 0.0;\n    }\n\n    let mean_a = a.iter().sum::<f64>() / a.len() as f64;\n    let mean_b = b.iter().sum::<f64>() / b.len() as f64;\n\n    let mut num = 0.0;\n    let mut denom_a = 0.0;\n    let mut denom_b = 0.0;\n    for (x, y) in a.iter().zip(b.iter()) {\n        let da = x - mean_a;\n        let db = y - mean_b;\n        num += da * db;\n        denom_a += da * da;\n        denom_b += db * db;\n    }\n\n    if denom_a == 0.0 || denom_b == 0.0 {\n        return 0.0;\n    }\n\n    num / (denom_a.sqrt() * denom_b.sqrt())\n}\n\n/// Verify that the model actually learned modular addition\n/// by testing it on ALL possible examples systematically\npub fn verify_full_accuracy<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n) -> (f32, f32) {\n    println!(\"ðŸ” Verifying model on COMPLETE dataset (no sampling)...\");\n\n    let train_dataset = ModularAdditionDataset::new(true, 42);\n    let val_dataset = ModularAdditionDataset::new(false, 42);\n\n    let train_acc = test_all_examples(model, &train_dataset, device);\n    let val_acc = test_all_examples(model, &val_dataset, device);\n\n    println!(\"ðŸ“Š FULL Dataset Results:\");\n    println!(\"   Training: {}/{} = {:.2}%\",\n        (train_acc * train_dataset.len() as f32) as usize,\n        train_dataset.len(),\n        train_acc * 100.0);\n    println!(\"   Validation: {}/{} = {:.2}%\",\n        (val_acc * val_dataset.len() as f32) as usize,\n        val_dataset.len(),\n        val_acc * 100.0);\n\n    (train_acc, val_acc)\n}\n\n/// Test model on ALL examples in dataset (no random sampling)\nfn test_all_examples<B: Backend>(\n    model: &Transformer<B>,\n    dataset: &ModularAdditionDataset,\n    device: &B::Device,\n) -> f32 {\n    let mut total_correct = 0;\n    let total = dataset.len();\n\n    // Test in batches to avoid memory issues\n    let batch_size = 100;\n    for batch_start in (0..total).step_by(batch_size) {\n        let batch_end = (batch_start + batch_size).min(total);\n        let batch_len = batch_end - batch_start;\n\n        let mut inputs_vec = Vec::new();\n        let mut targets_vec = Vec::new();\n\n        for idx in batch_start..batch_end {\n            let (input, target) = dataset.get(idx).unwrap();\n            inputs_vec.extend(input);\n            targets_vec.push(target as i32);\n        }\n\n        let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n            .reshape([batch_len, 3]);\n\n        let logits = model.forward(inputs);\n        let predictions = logits.argmax(1).squeeze::<1>();\n        let predictions_vec: Vec<i32> = predictions.into_data().to_vec().unwrap();\n\n        let correct = predictions_vec\n            .iter()\n            .zip(targets_vec.iter())\n            .filter(|(p, t)| p == t)\n            .count();\n\n        total_correct += correct;\n    }\n\n    total_correct as f32 / total as f32\n}\n\n/// Verify that weight norms decrease over time with weight decay\n/// and show correlation with the grokking transition\npub fn verify_weight_norm_decay(\n    weight_norm_history: &[(usize, f64)],\n    grok_step: Option<usize>,\n    config: &WeightNormDecayConfig,\n) -> Result<WeightNormDecayReport, String> {\n    if weight_norm_history.len() < 2 {\n        return Err(\"weight norm history requires at least 2 snapshots\".to_string());\n    }\n\n    let initial_norm = weight_norm_history\n        .first()\n        .ok_or_else(|| \"weight norm history is empty\".to_string())?\n        .1;\n    let final_norm = weight_norm_history\n        .last()\n        .ok_or_else(|| \"weight norm history is empty\".to_string())?\n        .1;\n\n    if initial_norm <= 0.0 {\n        return Err(\"initial weight norm must be positive\".to_string());\n    }\n\n    // Check that norms are generally decreasing (with weight decay)\n    let relative_decrease = (initial_norm - final_norm) / initial_norm;\n    if relative_decrease < config.min_relative_decrease {\n        return Err(format!(\n            \"weight norm decrease {:.4} below threshold {:.4} (initial {:.4}, final {:.4})\",\n            relative_decrease, config.min_relative_decrease, initial_norm, final_norm\n        ));\n    }\n\n    // Check that norms are not increasing (weight decay should monotonically decrease norms)\n    for window in weight_norm_history.windows(2) {\n        let prev_norm = window[0].1;\n        let next_norm = window[1].1;\n        if next_norm > prev_norm * 1.01 {\n            // Allow 1% tolerance for numerical noise\n            return Err(format!(\n                \"weight norm increased from {:.4} to {:.4} at step {}\",\n                prev_norm, next_norm, window[1].0\n            ));\n        }\n    }\n\n    // If grok step is provided, check that norms show a shift around grokking\n    let grok_step_norm = if let Some(step) = grok_step {\n        weight_norm_history\n            .iter()\n            .filter(|(s, _)| {\n                *s >= step.saturating_sub(config.grok_step_window)\n                    && *s <= step + config.grok_step_window\n            })\n            .map(|(_, norm)| *norm)\n            .next()\n    } else {\n        None\n    };\n\n    Ok(WeightNormDecayReport {\n        initial_norm,\n        final_norm,\n        relative_decrease,\n        grok_step_norm,\n    })\n}\n\n/// Verify the characteristic \"snake curve\" shape of grokking phenomenon\n/// on log-scale plots: train accuracy rises early, validation stays flat then jumps\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_snake_curve_shape(\n    train_acc: &[(usize, f32)],\n    val_acc: &[(usize, f32)],\n    config: &SnakeCurveConfig,\n) -> Result<SnakeCurveReport, String> {\n    if train_acc.is_empty() || val_acc.is_empty() {\n        return Err(\"empty accuracy history\".to_string());\n    }\n\n    // Phase 1: Verify early train convergence (immediate drop on log scale)\n    let train_converged_step = train_acc\n        .iter()\n        .find(|(_, acc)| *acc >= config.train_convergence_threshold)\n        .map(|(step, _)| *step)\n        .ok_or_else(|| {\n            format!(\n                \"train accuracy never reaches {} threshold\",\n                config.train_convergence_threshold\n            )\n        })?;\n\n    if train_converged_step > config.early_step_max {\n        return Err(format!(\n            \"train accuracy converges at step {}, expected before {}\",\n            train_converged_step, config.early_step_max\n        ));\n    }\n\n    // Phase 2: Verify validation plateau (flat on log scale)\n    let plateau_samples: Vec<f32> = val_acc\n        .iter()\n        .filter(|(step, _)| *step >= 1 && *step <= config.val_plateau_min_step)\n        .map(|(_, acc)| *acc)\n        .collect();\n\n    if plateau_samples.is_empty() {\n        return Err(\"no validation samples in plateau region\".to_string());\n    }\n\n    let val_plateau_max = plateau_samples\n        .iter()\n        .fold(0.0f32, |max_val, &acc| max_val.max(acc));\n\n    if val_plateau_max > config.val_plateau_max_acc {\n        return Err(format!(\n            \"validation accuracy in plateau region too high: max {:.4} exceeds {:.4}\",\n            val_plateau_max, config.val_plateau_max_acc\n        ));\n    }\n\n    // Phase 3: Verify delayed validation jump (crash up on log scale)\n    let val_grok_step = val_acc\n        .iter()\n        .filter(|(step, _)| *step > config.val_plateau_min_step)\n        .find(|(_, acc)| *acc >= config.val_final_threshold)\n        .map(|(step, _)| *step)\n        .ok_or_else(|| {\n            format!(\n                \"validation accuracy never reaches {} after plateau\",\n                config.val_final_threshold\n            )\n        })?;\n\n    // Phase 4: Verify phase ordering (snake shape: early train, late val)\n    let curve_shape_valid = train_converged_step < config.val_plateau_min_step\n        && val_grok_step > config.val_plateau_min_step\n        && train_converged_step < val_grok_step;\n\n    if !curve_shape_valid {\n        return Err(format!(\n            \"invalid curve shape: train converged at {}, val plateau until {}, val grokked at {}\",\n            train_converged_step, config.val_plateau_min_step, val_grok_step\n        ));\n    }\n\n    Ok(SnakeCurveReport {\n        train_converged_by_step: train_converged_step,\n        val_plateau_max_acc: val_plateau_max,\n        val_grok_step,\n        curve_shape_valid,\n    })\n}\n\n/// Test specific examples to see if model actually computes mod p\npub fn test_specific_examples<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n) {\n    println!();\n    println!(\"ðŸ§ª Testing specific examples:\");\n\n    let modulus = ModularAdditionDataset::modulus();\n    let equals_token = ModularAdditionDataset::equals_token();\n    let test_cases = vec![\n        (0, 0, 0),\n        (1, 1, 2),\n        (50, 50, (50 + 50) % modulus),\n        (112, 1, (112 + 1) % modulus),\n        (56, 57, (56 + 57) % modulus),\n        (112, 112, (112 + 112) % modulus),\n        (10, 20, 30),\n        (60, 60, (60 + 60) % modulus),\n    ];\n\n    for (a, b, expected) in test_cases {\n        let input_vec = vec![a as i32, b as i32, equals_token as i32];\n        let input = Tensor::<B, 1, Int>::from_ints(input_vec.as_slice(), device)\n            .reshape([1, 3]);\n\n        let logits = model.forward(input);\n        let prediction = logits.argmax(1).squeeze::<1>();\n        let pred_value: i32 = prediction.into_data().to_vec().unwrap()[0];\n\n        let correct = if pred_value == expected as i32 { \"âœ“\" } else { \"âœ—\" };\n        println!(\n            \"   {} + {} mod {} = {} | Model: {} {}\",\n            a, b, modulus, expected, pred_value, correct\n        );\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    fn make_histories(\n        train_acc: &[(usize, f32)],\n        val_acc: &[(usize, f32)],\n        val_loss: &[(usize, f64)],\n    ) -> (LossHistory, AccuracyHistory) {\n        let mut loss_history = LossHistory::new();\n        loss_history.val_snapshots = val_loss.to_vec();\n        let mut accuracy_history = AccuracyHistory::new();\n        accuracy_history.train_snapshots = train_acc.to_vec();\n        accuracy_history.val_snapshots = val_acc.to_vec();\n        (loss_history, accuracy_history)\n    }\n\n    #[test]\n    fn grokking_phase_verification_passes_on_synthetic_signal() {\n        let train_acc = vec![(0, 0.2), (500, 0.995), (1000, 0.999)];\n        let val_acc = vec![\n            (0, 0.01),\n            (500, 0.01),\n            (1500, 0.01),\n            (2200, 0.995),\n            (2500, 0.999),\n        ];\n        let val_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 2.0),\n            (2200, 0.2),\n            (2500, 0.15),\n        ];\n\n        let (loss_history, accuracy_history) = make_histories(&train_acc, &val_acc, &val_loss);\n        let config = GrokkingVerificationConfig {\n            early_step_max: 1000,\n            plateau_min_step: 1500,\n            generalization_window: 1500,\n            accuracy_window: 2,\n            loss_window: 2,\n            ..GrokkingVerificationConfig::default_for_modulus(ModularAdditionDataset::modulus())\n        };\n\n        let report = verify_grokking_phase_transition(&loss_history, &accuracy_history, &config)\n            .expect(\"expected grokking verification to pass\");\n        assert!(report.transition_step >= config.plateau_min_step);\n        assert!(report.early_train_step <= config.early_step_max);\n    }\n\n    #[test]\n    fn grokking_phase_verification_fails_without_transition() {\n        let train_acc = vec![(0, 0.2), (500, 0.995)];\n        let val_acc = vec![(0, 0.01), (500, 0.01), (1500, 0.02)];\n        let val_loss = vec![(0, 2.0), (500, 2.0), (1500, 1.9)];\n\n        let (loss_history, accuracy_history) = make_histories(&train_acc, &val_acc, &val_loss);\n        let config = GrokkingVerificationConfig {\n            early_step_max: 1000,\n            plateau_min_step: 1000,\n            generalization_window: 1000,\n            accuracy_window: 2,\n            loss_window: 2,\n            ..GrokkingVerificationConfig::default_for_modulus(ModularAdditionDataset::modulus())\n        };\n\n        let err = verify_grokking_phase_transition(&loss_history, &accuracy_history, &config)\n            .expect_err(\"expected grokking verification to fail\");\n        assert!(err.contains(\"validation accuracy never reaches target\"));\n    }\n\n    #[test]\n    fn grokking_phase_verification_fails_with_slow_accuracy_rise() {\n        let train_acc = vec![(0, 0.2), (500, 0.995), (1000, 0.999)];\n        let val_acc = vec![\n            (0, 0.01),\n            (500, 0.01),\n            (1500, 0.5),\n            (2000, 0.6),\n            (2200, 0.995),\n            (2500, 0.999),\n        ];\n        let val_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 2.0),\n            (2200, 0.2),\n            (2500, 0.15),\n        ];\n\n        let (loss_history, accuracy_history) = make_histories(&train_acc, &val_acc, &val_loss);\n        let config = GrokkingVerificationConfig {\n            early_step_max: 1000,\n            plateau_min_step: 1000,\n            generalization_window: 2000,\n            accuracy_window: 2,\n            loss_window: 2,\n            ..GrokkingVerificationConfig::default_for_modulus(ModularAdditionDataset::modulus())\n        };\n\n        let err = verify_grokking_phase_transition(&loss_history, &accuracy_history, &config)\n            .expect_err(\"expected grokking verification to fail\");\n        assert!(err.contains(\"plateau too high\"));\n    }\n\n    #[test]\n    fn restricted_loss_detection_passes_with_early_drop() {\n        let full_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 2.0),\n            (2500, 2.0),\n            (3500, 0.3),\n        ];\n        let restricted_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 0.8),\n            (2500, 0.4),\n            (3500, 0.3),\n        ];\n\n        let config = RestrictedLossVerificationConfig {\n            plateau_min_step: 1000,\n            drop_window: 2,\n            drop_fraction: 0.5,\n            min_step_lead: 500,\n        };\n\n        let report = verify_restricted_loss_early_drop(&full_loss, &restricted_loss, &config)\n            .expect(\"expected restricted loss verification to pass\");\n        assert!(report.restricted_drop_step < report.full_drop_step);\n    }\n\n    #[test]\n    fn restricted_loss_detection_fails_when_full_drops_first() {\n        let full_loss = vec![(0, 2.0), (1000, 1.0), (1500, 0.8)];\n        let restricted_loss = vec![(0, 2.0), (1000, 2.0), (1500, 0.8)];\n\n        let config = RestrictedLossVerificationConfig {\n            plateau_min_step: 500,\n            drop_window: 1,\n            drop_fraction: 0.3,\n            min_step_lead: 200,\n        };\n\n        let err = verify_restricted_loss_early_drop(&full_loss, &restricted_loss, &config)\n            .expect_err(\"expected restricted loss verification to fail\");\n        assert!(err.contains(\"restricted loss drops\"));\n    }\n\n    #[test]\n    fn excluded_loss_spike_verification_passes() {\n        let excluded_loss = vec![(0, 1.0), (1500, 2.2), (3000, 1.1)];\n        let config = ExcludedLossSpikeConfig {\n            min_relative_increase: 0.5,\n        };\n\n        let report = verify_excluded_loss_spike(&excluded_loss, &config)\n            .expect(\"expected excluded loss spike verification to pass\");\n        assert_eq!(report.spike_step, 1500);\n    }\n\n    #[test]\n    fn excluded_loss_spike_verification_fails_without_spike() {\n        let excluded_loss = vec![(0, 1.0), (1500, 1.1), (3000, 1.0)];\n        let config = ExcludedLossSpikeConfig {\n            min_relative_increase: 0.5,\n        };\n\n        let err = verify_excluded_loss_spike(&excluded_loss, &config)\n            .expect_err(\"expected excluded loss spike verification to fail\");\n        assert!(err.contains(\"excluded loss spike too small\"));\n    }\n\n    #[test]\n    fn restricted_loss_drop_verification_passes() {\n        let restricted_loss = vec![\n            (0, 2.0),\n            (1000, 1.8),\n            (5000, 1.2),\n            (7000, 0.8),\n            (10000, 0.9),\n        ];\n        let config = RestrictedLossDropConfig {\n            min_relative_decrease: 0.3,\n        };\n\n        let report = verify_restricted_loss_drop(&restricted_loss, &config)\n            .expect(\"expected restricted loss drop verification to pass\");\n        assert_eq!(report.drop_step, 7000);\n        assert!(report.baseline_loss > report.drop_loss);\n        assert!((report.baseline_loss - report.drop_loss) / report.baseline_loss >= 0.3);\n    }\n\n    #[test]\n    fn restricted_loss_drop_verification_fails_without_drop() {\n        let restricted_loss = vec![\n            (0, 2.0),\n            (1000, 2.1),\n            (5000, 1.9),\n            (7000, 2.0),\n            (10000, 2.05),\n        ];\n        let config = RestrictedLossDropConfig {\n            min_relative_decrease: 0.3,\n        };\n\n        let err = verify_restricted_loss_drop(&restricted_loss, &config)\n            .expect_err(\"expected restricted loss drop verification to fail\");\n        assert!(err.contains(\"restricted loss decrease too small\"));\n    }\n\n    #[test]\n    fn mlp_activation_wave_verification_passes_on_rectified_sine() {\n        let n = ModularAdditionDataset::modulus();\n        let freq = 5.0;\n        let mut signal = Vec::with_capacity(n);\n        for idx in 0..n {\n            let angle = 2.0 * std::f64::consts::PI * freq * idx as f64 / n as f64;\n            let rectified = angle.sin().max(0.0);\n            let wobble = (2.0 * std::f64::consts::PI * 2.0 * idx as f64 / n as f64)\n                .sin()\n                .abs()\n                * 0.02;\n            signal.push(rectified + wobble);\n        }\n\n        let config = MlpWaveVerificationConfig {\n            max_negative_fraction: 0.0,\n            negative_tolerance: 1e-6,\n            min_dominant_ratio: 3.0,\n            min_correlation: 0.7,\n            phase_steps: 32,\n        };\n\n        let report =\n            verify_mlp_activation_waves_from_signals(&[(0, signal)], &config)\n                .expect(\"expected rectified sine to pass\");\n        assert_eq!(report.neuron_reports.len(), 1);\n    }\n\n    #[test]\n    fn mlp_activation_wave_verification_fails_on_noise() {\n        let n = ModularAdditionDataset::modulus();\n        let signal: Vec<f64> = (0..n).map(|idx| idx as f64 / n as f64).collect();\n\n        let config = MlpWaveVerificationConfig {\n            max_negative_fraction: 0.0,\n            negative_tolerance: 1e-6,\n            min_dominant_ratio: 4.0,\n            min_correlation: 0.8,\n            phase_steps: 16,\n        };\n\n        let err =\n            verify_mlp_activation_waves_from_signals(&[(0, signal)], &config)\n                .expect_err(\"expected linear signal to fail\");\n        assert!(err.contains(\"dominant frequency ratio\") || err.contains(\"correlation\"));\n    }\n\n    #[test]\n    fn pairwise_manifold_transition_passes_on_elliptic_signals() {\n        let n = ModularAdditionDataset::modulus();\n        let freq = 5.0;\n        let mut post_signals = Vec::new();\n        for neuron_idx in 0..7usize {\n            let phase = 2.0 * std::f64::consts::PI * neuron_idx as f64 / 7.0;\n            let signal: Vec<f64> = (0..n)\n                .map(|idx| {\n                    let angle = 2.0 * std::f64::consts::PI * freq * idx as f64 / n as f64 + phase;\n                    angle.sin()\n                })\n                .collect();\n            post_signals.push((neuron_idx, signal));\n        }\n\n        let mut pre_signals = Vec::new();\n        for neuron_idx in 0..7usize {\n            let signal: Vec<f64> = (0..n)\n                .map(|idx| idx as f64 / n as f64 + neuron_idx as f64 * 0.01)\n                .collect();\n            pre_signals.push((neuron_idx, signal));\n        }\n\n        let config = PairwiseManifoldConfig {\n            fixed_y: 0,\n            neuron_indices: (0..7).collect(),\n            min_post_score: 0.75,\n            max_pre_score: 0.55,\n            max_axis_ratio: 8.0,\n            min_pairs_passing: 8,\n        };\n\n        let report = verify_pairwise_manifold_transition_from_signals(\n            &pre_signals,\n            &post_signals,\n            &config,\n        )\n        .expect(\"expected pairwise manifold transition to pass\");\n        assert!(report.passing_pairs >= config.min_pairs_passing);\n    }\n\n    #[test]\n    fn pairwise_manifold_transition_fails_without_rings() {\n        let n = ModularAdditionDataset::modulus();\n        let mut signals = Vec::new();\n        for neuron_idx in 0..7usize {\n            let signal: Vec<f64> = (0..n)\n                .map(|idx| idx as f64 / n as f64 + neuron_idx as f64 * 0.02)\n                .collect();\n            signals.push((neuron_idx, signal));\n        }\n\n        let config = PairwiseManifoldConfig {\n            fixed_y: 0,\n            neuron_indices: (0..7).collect(),\n            min_post_score: 0.8,\n            max_pre_score: 0.55,\n            max_axis_ratio: 4.0,\n            min_pairs_passing: 5,\n        };\n\n        let err = verify_pairwise_manifold_transition_from_signals(\n            &signals,\n            &signals,\n            &config,\n        )\n        .expect_err(\"expected pairwise manifold transition to fail\");\n        assert!(err.contains(\"post-grok\") || err.contains(\"pre-grok\"));\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_passes_on_diagonal_corrugation() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create diagonal ridges: f(x,y) = sin(2Ï€ * freq * (x+y) / n)\n        let freq = 3.0;\n        for x in 0..n {\n            for y in 0..n {\n                let angle = 2.0 * std::f64::consts::PI * freq * (x + y) as f64 / n as f64;\n                surface[x][y] = angle.sin();\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let report = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect(\"expected diagonal ridge verification to pass\");\n        assert!(report.diagonal_score > report.axis_score);\n        assert!(report.ratio >= config.min_diagonal_ratio);\n        assert!(report.diagonal_score >= config.min_diagonal_score);\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_passes_on_anti_diagonal_pattern() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create anti-diagonal ridges: f(x,y) = sin(2Ï€ * freq * (x-y) / n)\n        let freq = 3.0;\n        for x in 0..n {\n            for y in 0..n {\n                let angle = 2.0 * std::f64::consts::PI * freq * (x as i32 - y as i32) as f64 / n as f64;\n                surface[x][y] = angle.sin();\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let report = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect(\"expected anti-diagonal ridge verification to pass\");\n        assert!(report.diagonal_score > report.axis_score);\n        assert!(report.ratio >= config.min_diagonal_ratio);\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_fails_on_axis_aligned_pattern() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create axis-aligned ridges: f(x,y) = sin(2Ï€ * freq * x / n)\n        let freq = 3.0;\n        for x in 0..n {\n            for y in 0..n {\n                let angle = 2.0 * std::f64::consts::PI * freq * x as f64 / n as f64;\n                surface[x][y] = angle.sin();\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let err = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect_err(\"expected axis-aligned pattern to fail diagonal verification\");\n        assert!(err.contains(\"diagonal/axis ratio\") || err.contains(\"diagonal autocorrelation\"));\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_fails_on_flat_surface() {\n        let n = 50;\n        let surface = vec![vec![1.5; n]; n];\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let err = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect_err(\"expected flat surface to fail\");\n        assert!(err.contains(\"surface variance is zero\"));\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_fails_on_random_noise() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create random noise (low autocorrelation everywhere)\n        for x in 0..n {\n            for y in 0..n {\n                surface[x][y] = ((x * 17 + y * 31) % 100) as f64 / 100.0;\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let err = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect_err(\"expected random noise to fail diagonal verification\");\n        assert!(err.contains(\"diagonal/axis ratio\") || err.contains(\"diagonal autocorrelation\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_passes_on_valid_grokking_curve() {\n        // Simulate typical grokking: train converges early, val plateaus then jumps\n        let train_acc = vec![\n            (1, 0.10),\n            (10, 0.50),\n            (100, 0.80),\n            (500, 0.98),\n            (1000, 0.99),\n            (2000, 0.995),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (10, 0.009),\n            (100, 0.01),\n            (500, 0.011),\n            (1000, 0.009),\n            (1500, 0.01),\n            (2000, 0.92),\n            (3000, 0.98),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 1500,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let report = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect(\"expected snake curve verification to pass\");\n\n        assert_eq!(report.train_converged_by_step, 500);\n        assert!(report.val_plateau_max_acc < config.val_plateau_max_acc);\n        assert_eq!(report.val_grok_step, 2000);\n        assert!(report.curve_shape_valid);\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_on_simultaneous_convergence() {\n        // Both train and val converge at the same time (no grokking)\n        let train_acc = vec![\n            (1, 0.10),\n            (100, 0.98),\n            (500, 0.99),\n        ];\n        let val_acc = vec![\n            (1, 0.10),\n            (100, 0.97),\n            (500, 0.98),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 200,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let err = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect_err(\"expected snake curve verification to fail\");\n        assert!(err.contains(\"plateau region too high\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_on_slow_train_convergence() {\n        // Train takes too long to converge (not the typical snake curve)\n        let train_acc = vec![\n            (1, 0.10),\n            (500, 0.50),\n            (1000, 0.70),\n            (2000, 0.98),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (500, 0.01),\n            (1000, 0.01),\n            (2000, 0.92),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 1500,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let err = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect_err(\"expected snake curve verification to fail\");\n        assert!(err.contains(\"converges at step 2000\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_without_val_jump() {\n        // Validation never jumps up (no generalization)\n        let train_acc = vec![\n            (1, 0.10),\n            (100, 0.98),\n            (500, 0.99),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (100, 0.01),\n            (500, 0.01),\n            (2000, 0.02),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 200,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let err = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect_err(\"expected snake curve verification to fail\");\n        assert!(err.contains(\"validation accuracy never reaches\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_on_wrong_phase_ordering() {\n        // Val jumps before train converges (backwards snake)\n        let train_acc = vec![\n            (1, 0.10),\n            (500, 0.50),\n            (2000, 0.98),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (100, 0.95),\n            (500, 0.98),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 200,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let result = verify_snake_curve_shape(&train_acc, &val_acc, &config);\n        // Should fail because val jumps before train converges (not a proper snake curve)\n        // This can fail for multiple reasons, but should definitely fail\n        assert!(result.is_err(), \"Expected snake curve verification to fail but it passed\");\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_passes_on_decreasing_norms() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 85.0),\n            (1000, 72.0),\n            (1500, 63.0),\n            (2000, 58.0),\n            (2500, 55.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let report = verify_weight_norm_decay(&weight_norms, Some(2000), &config)\n            .expect(\"expected weight norm decay verification to pass\");\n\n        assert_eq!(report.initial_norm, 100.0);\n        assert_eq!(report.final_norm, 55.0);\n        assert!((report.relative_decrease - 0.45).abs() < 0.01);\n        assert!(report.grok_step_norm.is_some());\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_fails_on_flat_norms() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 99.0),\n            (1000, 98.5),\n            (1500, 98.0),\n            (2000, 97.5),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let err = verify_weight_norm_decay(&weight_norms, None, &config)\n            .expect_err(\"expected weight norm decay verification to fail\");\n        assert!(err.contains(\"weight norm decrease\"));\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_fails_on_increasing_norms() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 95.0),\n            (1000, 105.0), // Increase here\n            (1500, 90.0),\n            (2000, 85.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let err = verify_weight_norm_decay(&weight_norms, None, &config)\n            .expect_err(\"expected weight norm decay verification to fail\");\n        assert!(err.contains(\"weight norm increased\"));\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_allows_small_noise() {\n        // Weight decay with small numerical noise (within 1% tolerance)\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 95.0),\n            (1000, 90.5),  // Small increase within tolerance\n            (1500, 90.0),\n            (2000, 85.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let report = verify_weight_norm_decay(&weight_norms, None, &config)\n            .expect(\"expected weight norm decay with small noise to pass\");\n\n        assert_eq!(report.initial_norm, 100.0);\n        assert_eq!(report.final_norm, 85.0);\n        assert!((report.relative_decrease - 0.15).abs() < 0.01);\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_finds_grok_step_norm() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (1000, 80.0),\n            (1800, 68.0),  // Within window of grok step (2200 - 500 = 1700)\n            (2000, 65.0),\n            (2200, 62.0),  // Around grok step\n            (3000, 55.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let report = verify_weight_norm_decay(&weight_norms, Some(2200), &config)\n            .expect(\"expected weight norm decay verification to pass\");\n\n        assert!(report.grok_step_norm.is_some());\n        let grok_norm = report.grok_step_norm.unwrap();\n        // Should find the first norm in the window [1700, 2700], which is 68.0 at step 1800\n        assert!((grok_norm - 68.0).abs() < 0.01);\n    }\n}\n","structuredPatch":[{"oldStart":1168,"oldLines":6,"newStart":1168,"newLines":7,"lines":["     Ok((dominant_idx, ratio))"," }"," ","+#[allow(dead_code)]"," fn best_rectified_sine_correlation(signal: &[f64], frequency: usize, phase_steps: usize) -> f64 {","     if signal.is_empty() || frequency == 0 || phase_steps == 0 {","         return 0.0;"]}],"userModified":false,"replaceAll":false}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01XhQAu9KuQ8aWhkPnqUrKuX","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01Y11atKSJGvnV7pdhMECd9T","name":"Edit","input":{"replace_all":false,"file_path":"/Users/admin/MechInt/src/verify.rs","old_string":"fn pearson_correlation(a: &[f64], b: &[f64]) -> f64 {","new_string":"#[allow(dead_code)]\nfn pearson_correlation(a: &[f64], b: &[f64]) -> f64 {"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":383,"cache_read_input_tokens":64787,"cache_creation":{"ephemeral_5m_input_tokens":383,"ephemeral_1h_input_tokens":0},"output_tokens":180,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"6f946ff9-e327-4c9c-ad06-434db12da963"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01Y11atKSJGvnV7pdhMECd9T","type":"tool_result","content":"The file /Users/admin/MechInt/src/verify.rs has been updated. Here's the result of running `cat -n` on a snippet of the edited file:\n  1192â†’\n  1193â†’    best\n  1194â†’}\n  1195â†’\n  1196â†’#[allow(dead_code)]\n  1197â†’fn pearson_correlation(a: &[f64], b: &[f64]) -> f64 {\n  1198â†’    if a.len() != b.len() || a.is_empty() {\n  1199â†’        return 0.0;\n  1200â†’    }\n  1201â†’"}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"0ec58171-eddc-4237-a731-c0858bf8a5d1","tool_use_result":{"filePath":"/Users/admin/MechInt/src/verify.rs","oldString":"fn pearson_correlation(a: &[f64], b: &[f64]) -> f64 {","newString":"#[allow(dead_code)]\nfn pearson_correlation(a: &[f64], b: &[f64]) -> f64 {","originalFile":"use burn::{\n    data::dataset::Dataset,\n    tensor::{backend::Backend, Int, Tensor},\n};\nuse rustfft::{num_complex::Complex, FftPlanner};\n\nuse crate::analysis::{AccuracyHistory, LossHistory};\nuse crate::data::ModularAdditionDataset;\nuse crate::model::Transformer;\n\n#[derive(Debug, Clone)]\npub struct GrokkingVerificationConfig {\n    pub early_train_acc_threshold: f32,\n    pub early_step_max: usize,\n    pub plateau_min_step: usize,\n    pub generalization_window: usize,\n    pub chance_accuracy: f32,\n    pub chance_tolerance: f32,\n    pub target_val_acc_threshold: f32,\n    pub accuracy_window: usize,\n    pub loss_window: usize,\n    pub loss_drop_fraction: f64,\n}\n\nimpl GrokkingVerificationConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        Self {\n            early_train_acc_threshold: 0.99,\n            early_step_max: 1000,\n            plateau_min_step: 1000,\n            generalization_window: 10_000,\n            chance_accuracy: 1.0 / modulus as f32,\n            chance_tolerance: 0.02,\n            target_val_acc_threshold: 0.99,\n            accuracy_window: 5,\n            loss_window: 5,\n            loss_drop_fraction: 0.5,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct GrokkingPhaseReport {\n    pub early_train_step: usize,\n    pub transition_step: usize,\n    pub plateau_max_val_acc: f32,\n    pub loss_drop_ratio: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossVerificationConfig {\n    pub plateau_min_step: usize,\n    pub drop_window: usize,\n    pub drop_fraction: f64,\n    pub min_step_lead: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossReport {\n    pub restricted_drop_step: usize,\n    pub full_drop_step: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossDropConfig {\n    pub min_relative_decrease: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossDropReport {\n    pub drop_step: usize,\n    pub drop_loss: f64,\n    pub baseline_loss: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct ExcludedLossSpikeConfig {\n    pub min_relative_increase: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct ExcludedLossSpikeReport {\n    pub spike_step: usize,\n    pub spike_loss: f64,\n    pub baseline_loss: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct MlpWaveVerificationConfig {\n    pub max_negative_fraction: f64,\n    pub negative_tolerance: f64,\n    pub min_dominant_ratio: f64,\n    pub min_correlation: f64,\n    pub phase_steps: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct MlpWaveNeuronReport {\n    pub neuron_index: usize,\n    pub dominant_frequency: usize,\n    pub dominant_ratio: f64,\n    pub negative_fraction: f64,\n    pub best_correlation: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct MlpWaveReport {\n    pub neuron_reports: Vec<MlpWaveNeuronReport>,\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldConfig {\n    pub fixed_y: usize,\n    pub neuron_indices: Vec<usize>,\n    pub min_post_score: f64,\n    pub max_pre_score: f64,\n    pub max_axis_ratio: f64,\n    pub min_pairs_passing: usize,\n}\n\nimpl PairwiseManifoldConfig {\n    pub fn default_for_modulus(_modulus: usize) -> Self {\n        Self {\n            fixed_y: 0,\n            neuron_indices: (0..7).collect(),\n            min_post_score: 0.75,\n            max_pre_score: 0.55,\n            max_axis_ratio: 8.0,\n            min_pairs_passing: 10,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldPairReport {\n    pub neuron_a: usize,\n    pub neuron_b: usize,\n    pub circularity_score: f64,\n    pub radius_cv: f64,\n    pub axis_ratio: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldReport {\n    pub pair_reports: Vec<PairwiseManifoldPairReport>,\n    pub average_score: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldTransitionReport {\n    pub pre_average_score: f64,\n    pub post_average_score: f64,\n    pub passing_pairs: usize,\n    pub total_pairs: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct DiagonalRidgeConfig {\n    pub neuron_index: usize,\n    pub max_lag: usize,\n    pub min_diagonal_ratio: f64,\n    pub min_diagonal_score: f64,\n}\n\nimpl DiagonalRidgeConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        let max_lag = (modulus / 10).max(4).min(20);\n        Self {\n            neuron_index: 10,\n            max_lag,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct DiagonalRidgeReport {\n    pub neuron_index: usize,\n    pub diagonal_score: f64,\n    pub axis_score: f64,\n    pub ratio: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct SnakeCurveConfig {\n    pub early_step_max: usize,\n    pub train_convergence_threshold: f32,\n    pub val_plateau_min_step: usize,\n    pub val_plateau_max_acc: f32,\n    pub val_final_threshold: f32,\n}\n\nimpl SnakeCurveConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        Self {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 1000,\n            val_plateau_max_acc: 1.0 / modulus as f32 + 0.05,\n            val_final_threshold: 0.90,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct WeightNormDecayConfig {\n    pub min_relative_decrease: f64,\n    pub plateau_min_step: usize,\n    pub grok_step_window: usize,\n}\n\nimpl WeightNormDecayConfig {\n    pub fn default_for_grokking() -> Self {\n        Self {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct WeightNormDecayReport {\n    pub initial_norm: f64,\n    pub final_norm: f64,\n    pub relative_decrease: f64,\n    pub grok_step_norm: Option<f64>,\n}\n\n#[derive(Debug, Clone)]\npub struct SnakeCurveReport {\n    pub train_converged_by_step: usize,\n    pub val_plateau_max_acc: f32,\n    pub val_grok_step: usize,\n    pub curve_shape_valid: bool,\n}\n\npub fn verify_grokking_phase_transition(\n    loss_history: &LossHistory,\n    accuracy_history: &AccuracyHistory,\n    config: &GrokkingVerificationConfig,\n) -> Result<GrokkingPhaseReport, String> {\n    let (early_train_step, _) = accuracy_history\n        .train_snapshots\n        .iter()\n        .find(|(_, acc)| *acc >= config.early_train_acc_threshold)\n        .ok_or_else(|| \"train accuracy never reaches threshold\".to_string())?;\n\n    if *early_train_step > config.early_step_max {\n        return Err(format!(\n            \"train accuracy reaches {:.2} after step {}, expected before {}\",\n            config.early_train_acc_threshold, early_train_step, config.early_step_max\n        ));\n    }\n\n    let plateau_max_val_acc = accuracy_history\n        .val_snapshots\n        .iter()\n        .filter(|(step, _)| *step <= config.plateau_min_step)\n        .map(|(_, acc)| *acc)\n        .fold(None::<f32>, |accum, value| {\n            Some(accum.map_or(value, |max_val| max_val.max(value)))\n        })\n        .ok_or_else(|| \"no validation accuracy samples before plateau_min_step\".to_string())?;\n\n    if plateau_max_val_acc > config.chance_accuracy + config.chance_tolerance {\n        return Err(format!(\n            \"validation accuracy before step {} exceeds chance (max {:.4})\",\n            config.plateau_min_step, plateau_max_val_acc\n        ));\n    }\n\n    let (transition_step, _) = accuracy_history\n        .val_snapshots\n        .iter()\n        .find(|(step, acc)| {\n            *step >= config.plateau_min_step && *acc >= config.target_val_acc_threshold\n        })\n        .ok_or_else(|| \"validation accuracy never reaches target threshold\".to_string())?;\n\n    let latest_allowed = config.plateau_min_step + config.generalization_window;\n    if *transition_step > latest_allowed {\n        return Err(format!(\n            \"validation accuracy reaches {:.2} at step {}, expected before {}\",\n            config.target_val_acc_threshold, transition_step, latest_allowed\n        ));\n    }\n\n    verify_accuracy_jump(\n        &accuracy_history.val_snapshots,\n        *transition_step,\n        config.accuracy_window,\n        config.chance_accuracy,\n        config.chance_tolerance,\n        config.target_val_acc_threshold,\n    )?;\n\n    let loss_drop_ratio = verify_loss_drop(\n        &loss_history.val_snapshots,\n        *transition_step,\n        config.loss_window,\n        config.loss_drop_fraction,\n    )?;\n\n    Ok(GrokkingPhaseReport {\n        early_train_step: *early_train_step,\n        transition_step: *transition_step,\n        plateau_max_val_acc,\n        loss_drop_ratio,\n    })\n}\n\nfn verify_accuracy_jump(\n    val_accuracy: &[(usize, f32)],\n    transition_step: usize,\n    window: usize,\n    chance_accuracy: f32,\n    chance_tolerance: f32,\n    target_val_acc_threshold: f32,\n) -> Result<(), String> {\n    let before_values: Vec<f32> = val_accuracy\n        .iter()\n        .filter(|(step, _)| *step < transition_step)\n        .map(|(_, acc)| *acc)\n        .collect();\n\n    let after_values: Vec<f32> = val_accuracy\n        .iter()\n        .filter(|(step, _)| *step >= transition_step)\n        .map(|(_, acc)| *acc)\n        .collect();\n\n    if before_values.len() < window || after_values.len() < window {\n        return Err(\"insufficient accuracy samples around transition\".to_string());\n    }\n\n    let before_avg: f32 =\n        before_values[before_values.len() - window..].iter().sum::<f32>() / window as f32;\n    let after_avg: f32 = after_values[..window].iter().sum::<f32>() / window as f32;\n\n    if before_avg > chance_accuracy + chance_tolerance {\n        return Err(format!(\n            \"validation accuracy plateau too high before transition (avg {:.4})\",\n            before_avg\n        ));\n    }\n\n    if after_avg < target_val_acc_threshold {\n        return Err(format!(\n            \"validation accuracy after transition too low (avg {:.4})\",\n            after_avg\n        ));\n    }\n\n    Ok(())\n}\n\nfn verify_loss_drop(\n    val_losses: &[(usize, f64)],\n    transition_step: usize,\n    loss_window: usize,\n    loss_drop_fraction: f64,\n) -> Result<f64, String> {\n    let before_losses: Vec<f64> = val_losses\n        .iter()\n        .filter(|(step, _)| *step < transition_step)\n        .map(|(_, loss)| *loss)\n        .collect();\n\n    let after_losses: Vec<f64> = val_losses\n        .iter()\n        .filter(|(step, _)| *step >= transition_step)\n        .map(|(_, loss)| *loss)\n        .collect();\n\n    if before_losses.len() < loss_window || after_losses.len() < loss_window {\n        return Err(\"insufficient loss samples around transition\".to_string());\n    }\n\n    let before_avg: f64 =\n        before_losses[before_losses.len() - loss_window..].iter().sum::<f64>()\n            / loss_window as f64;\n    let after_avg: f64 = after_losses[..loss_window].iter().sum::<f64>() / loss_window as f64;\n\n    if after_avg > before_avg * (1.0 - loss_drop_fraction) {\n        return Err(format!(\n            \"validation loss drop too small (before {:.4}, after {:.4})\",\n            before_avg, after_avg\n        ));\n    }\n\n    Ok(after_avg / before_avg)\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_restricted_loss_early_drop(\n    full_losses: &[(usize, f64)],\n    restricted_losses: &[(usize, f64)],\n    config: &RestrictedLossVerificationConfig,\n) -> Result<RestrictedLossReport, String> {\n    let restricted_baseline =\n        baseline_loss(restricted_losses, config.plateau_min_step, config.drop_window)?;\n    let full_baseline = baseline_loss(full_losses, config.plateau_min_step, config.drop_window)?;\n\n    let restricted_drop_step = find_drop_step(\n        restricted_losses,\n        config.plateau_min_step,\n        restricted_baseline,\n        config.drop_fraction,\n    )?;\n    let full_drop_step = find_drop_step(\n        full_losses,\n        config.plateau_min_step,\n        full_baseline,\n        config.drop_fraction,\n    )?;\n\n    if restricted_drop_step + config.min_step_lead > full_drop_step {\n        return Err(format!(\n            \"restricted loss drops at step {}, expected at least {} steps before full loss (step {})\",\n            restricted_drop_step, config.min_step_lead, full_drop_step\n        ));\n    }\n\n    Ok(RestrictedLossReport {\n        restricted_drop_step,\n        full_drop_step,\n    })\n}\n\npub fn verify_restricted_loss_drop(\n    restricted_losses: &[(usize, f64)],\n    config: &RestrictedLossDropConfig,\n) -> Result<RestrictedLossDropReport, String> {\n    if restricted_losses.len() < 3 {\n        return Err(\"restricted loss history requires at least 3 points\".to_string());\n    }\n\n    let first = restricted_losses.first().ok_or_else(|| {\n        \"restricted loss history is empty\".to_string()\n    })?;\n    let baseline_loss = first.1;\n\n    let (min_idx, (min_step, min_loss)) = restricted_losses\n        .iter()\n        .enumerate()\n        .min_by(|(_, a), (_, b)| {\n            a.1.partial_cmp(&b.1).unwrap_or(std::cmp::Ordering::Equal)\n        })\n        .ok_or_else(|| \"restricted loss history is empty\".to_string())?;\n\n    if min_idx == 0 {\n        return Err(\"restricted loss minimum occurs at start (no drop detected)\".to_string());\n    }\n\n    let relative_decrease = (baseline_loss - min_loss) / baseline_loss;\n    if relative_decrease < config.min_relative_decrease {\n        return Err(format!(\n            \"restricted loss decrease too small (baseline {:.4}, min {:.4}, decrease {:.1}%)\",\n            baseline_loss, min_loss, relative_decrease * 100.0\n        ));\n    }\n\n    Ok(RestrictedLossDropReport {\n        drop_step: *min_step,\n        drop_loss: *min_loss,\n        baseline_loss,\n    })\n}\n\npub fn verify_excluded_loss_spike(\n    excluded_losses: &[(usize, f64)],\n    config: &ExcludedLossSpikeConfig,\n) -> Result<ExcludedLossSpikeReport, String> {\n    if excluded_losses.len() < 3 {\n        return Err(\"excluded loss history requires at least 3 points\".to_string());\n    }\n\n    let first = excluded_losses.first().ok_or_else(|| {\n        \"excluded loss history is empty\".to_string()\n    })?;\n    let last = excluded_losses.last().ok_or_else(|| {\n        \"excluded loss history is empty\".to_string()\n    })?;\n    let baseline_loss = (first.1 + last.1) / 2.0;\n\n    let (peak_idx, (peak_step, peak_loss)) = excluded_losses\n        .iter()\n        .enumerate()\n        .max_by(|(_, a), (_, b)| {\n            a.1.partial_cmp(&b.1).unwrap_or(std::cmp::Ordering::Equal)\n        })\n        .ok_or_else(|| \"excluded loss history is empty\".to_string())?;\n\n    if peak_idx == 0 || peak_idx + 1 == excluded_losses.len() {\n        return Err(\"excluded loss spike occurs at boundary\".to_string());\n    }\n\n    if peak_loss < &(baseline_loss * (1.0 + config.min_relative_increase)) {\n        return Err(format!(\n            \"excluded loss spike too small (baseline {:.4}, peak {:.4})\",\n            baseline_loss, peak_loss\n        ));\n    }\n\n    Ok(ExcludedLossSpikeReport {\n        spike_step: *peak_step,\n        spike_loss: *peak_loss,\n        baseline_loss,\n    })\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_mlp_activation_waves<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n    fixed_y: usize,\n    neuron_indices: &[usize],\n    config: &MlpWaveVerificationConfig,\n) -> Result<MlpWaveReport, String> {\n    let activations = collect_mlp_post_relu_activations(model, device, fixed_y)?;\n    if neuron_indices.is_empty() {\n        return Err(\"neuron indices list is empty\".to_string());\n    }\n\n    let d_ff = activations.len();\n    for &idx in neuron_indices {\n        if idx >= d_ff {\n            return Err(format!(\n                \"neuron index {} out of range (d_ff={})\",\n                idx, d_ff\n            ));\n        }\n    }\n\n    let signals: Vec<(usize, Vec<f64>)> = neuron_indices\n        .iter()\n        .map(|&idx| (idx, activations[idx].clone()))\n        .collect();\n    verify_mlp_activation_waves_from_signals(&signals, config)\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_mlp_activation_waves_from_signals(\n    signals: &[(usize, Vec<f64>)],\n    config: &MlpWaveVerificationConfig,\n) -> Result<MlpWaveReport, String> {\n    if signals.is_empty() {\n        return Err(\"no activation signals provided\".to_string());\n    }\n\n    let mut reports = Vec::with_capacity(signals.len());\n    for (neuron_index, signal) in signals {\n        let report = analyze_mlp_activation_wave(*neuron_index, signal, config)?;\n        reports.push(report);\n    }\n\n    Ok(MlpWaveReport {\n        neuron_reports: reports,\n    })\n}\n\npub fn verify_mlp_pairwise_manifold_transition<B: Backend>(\n    pre_model: &Transformer<B>,\n    post_model: &Transformer<B>,\n    device: &B::Device,\n    config: &PairwiseManifoldConfig,\n) -> Result<PairwiseManifoldTransitionReport, String> {\n    let modulus = ModularAdditionDataset::modulus();\n    if config.fixed_y >= modulus {\n        return Err(format!(\n            \"fixed_y {} out of range (modulus {})\",\n            config.fixed_y, modulus\n        ));\n    }\n\n    let pre_activations = collect_mlp_post_relu_activations(pre_model, device, config.fixed_y)?;\n    let post_activations = collect_mlp_post_relu_activations(post_model, device, config.fixed_y)?;\n\n    let d_ff = pre_activations.len();\n    if post_activations.len() != d_ff {\n        return Err(\"post activations shape mismatch\".to_string());\n    }\n\n    if config.neuron_indices.len() < 2 {\n        return Err(\"need at least two neuron indices\".to_string());\n    }\n\n    for &idx in &config.neuron_indices {\n        if idx >= d_ff {\n            return Err(format!(\n                \"neuron index {} out of range (d_ff={})\",\n                idx, d_ff\n            ));\n        }\n    }\n\n    let pre_signals: Vec<(usize, Vec<f64>)> = config\n        .neuron_indices\n        .iter()\n        .map(|&idx| (idx, pre_activations[idx].clone()))\n        .collect();\n    let post_signals: Vec<(usize, Vec<f64>)> = config\n        .neuron_indices\n        .iter()\n        .map(|&idx| (idx, post_activations[idx].clone()))\n        .collect();\n\n    verify_pairwise_manifold_transition_from_signals(&pre_signals, &post_signals, config)\n}\n\npub fn collect_mlp_activation_surface<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n    neuron_index: usize,\n) -> Result<Vec<Vec<f64>>, String> {\n    let modulus = ModularAdditionDataset::modulus();\n    let equals_token = ModularAdditionDataset::equals_token();\n\n    let mut inputs_vec = Vec::with_capacity(modulus * modulus * 3);\n    for x in 0..modulus {\n        for y in 0..modulus {\n            inputs_vec.push(x as i32);\n            inputs_vec.push(y as i32);\n            inputs_vec.push(equals_token as i32);\n        }\n    }\n\n    let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n        .reshape([modulus * modulus, 3]);\n    let (_, mlp_acts) = model.forward_with_mlp_activations(inputs);\n    let [batch_size, d_ff] = mlp_acts.dims();\n    if batch_size != modulus * modulus {\n        return Err(format!(\n            \"unexpected activation batch size {} (expected {})\",\n            batch_size,\n            modulus * modulus\n        ));\n    }\n    if neuron_index >= d_ff {\n        return Err(format!(\n            \"neuron index {} out of range (d_ff={})\",\n            neuron_index, d_ff\n        ));\n    }\n\n    let data: Vec<f32> = mlp_acts.into_data().to_vec().unwrap();\n    let mut surface = vec![vec![0.0f64; modulus]; modulus];\n    for x in 0..modulus {\n        for y in 0..modulus {\n            let batch = x * modulus + y;\n            let idx = batch * d_ff + neuron_index;\n            surface[x][y] = data[idx] as f64;\n        }\n    }\n\n    Ok(surface)\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_constructive_interference_surface_from_grid(\n    surface: &[Vec<f64>],\n    config: &DiagonalRidgeConfig,\n) -> Result<DiagonalRidgeReport, String> {\n    let (diag_main, diag_anti, axis_x, axis_y) =\n        surface_autocorrelation_scores(surface, config.max_lag)?;\n\n    let diagonal_score = diag_main.max(diag_anti);\n    let axis_score = axis_x.max(axis_y);\n    let ratio = if axis_score > 0.0 {\n        diagonal_score / axis_score\n    } else {\n        f64::INFINITY\n    };\n\n    if diagonal_score < config.min_diagonal_score {\n        return Err(format!(\n            \"diagonal autocorrelation {:.3} below threshold {:.3}\",\n            diagonal_score, config.min_diagonal_score\n        ));\n    }\n\n    if ratio < config.min_diagonal_ratio {\n        return Err(format!(\n            \"diagonal/axis ratio {:.3} below threshold {:.3}\",\n            ratio, config.min_diagonal_ratio\n        ));\n    }\n\n    Ok(DiagonalRidgeReport {\n        neuron_index: config.neuron_index,\n        diagonal_score,\n        axis_score,\n        ratio,\n    })\n}\n\npub fn verify_pairwise_manifold_transition_from_signals(\n    pre_signals: &[(usize, Vec<f64>)],\n    post_signals: &[(usize, Vec<f64>)],\n    config: &PairwiseManifoldConfig,\n) -> Result<PairwiseManifoldTransitionReport, String> {\n    let pre_report = evaluate_pairwise_manifolds_from_signals(pre_signals)?;\n    let post_report = evaluate_pairwise_manifolds_from_signals(post_signals)?;\n\n    if pre_report.average_score > config.max_pre_score {\n        return Err(format!(\n            \"pre-grok average circularity {:.3} exceeds threshold {:.3}\",\n            pre_report.average_score, config.max_pre_score\n        ));\n    }\n\n    let passing_pairs = post_report\n        .pair_reports\n        .iter()\n        .filter(|pair| {\n            pair.circularity_score >= config.min_post_score\n                && pair.axis_ratio <= config.max_axis_ratio\n        })\n        .count();\n    if passing_pairs < config.min_pairs_passing {\n        return Err(format!(\n            \"post-grok only {} pairs meet circularity threshold (need {})\",\n            passing_pairs, config.min_pairs_passing\n        ));\n    }\n\n    Ok(PairwiseManifoldTransitionReport {\n        pre_average_score: pre_report.average_score,\n        post_average_score: post_report.average_score,\n        passing_pairs,\n        total_pairs: post_report.pair_reports.len(),\n    })\n}\n\nfn evaluate_pairwise_manifolds_from_signals(\n    signals: &[(usize, Vec<f64>)],\n) -> Result<PairwiseManifoldReport, String> {\n    if signals.len() < 2 {\n        return Err(\"need at least two activation signals\".to_string());\n    }\n\n    let signal_len = signals[0].1.len();\n    if signal_len < 3 {\n        return Err(\"activation signal too short\".to_string());\n    }\n\n    for (idx, signal) in signals {\n        if signal.len() != signal_len {\n            return Err(format!(\n                \"signal length mismatch for neuron {}\",\n                idx\n            ));\n        }\n    }\n\n    let mut reports = Vec::new();\n    for i in 0..signals.len() {\n        for j in (i + 1)..signals.len() {\n            let (neuron_a, signal_a) = &signals[i];\n            let (neuron_b, signal_b) = &signals[j];\n            let points: Vec<(f64, f64)> = signal_a\n                .iter()\n                .zip(signal_b.iter())\n                .map(|(x, y)| (*x, *y))\n                .collect();\n            let (score, radius_cv, axis_ratio) =\n                match pairwise_circularity_metrics(&points) {\n                    Ok(metrics) => metrics,\n                    Err(_) => (0.0, 1.0, f64::INFINITY),\n                };\n            reports.push(PairwiseManifoldPairReport {\n                neuron_a: *neuron_a,\n                neuron_b: *neuron_b,\n                circularity_score: score,\n                radius_cv,\n                axis_ratio,\n            });\n        }\n    }\n\n    let average_score = reports\n        .iter()\n        .map(|pair| pair.circularity_score)\n        .sum::<f64>()\n        / reports.len() as f64;\n\n    Ok(PairwiseManifoldReport {\n        pair_reports: reports,\n        average_score,\n    })\n}\n\nfn pairwise_circularity_metrics(points: &[(f64, f64)]) -> Result<(f64, f64, f64), String> {\n    if points.len() < 3 {\n        return Err(\"not enough points for circularity\".to_string());\n    }\n\n    let n = points.len() as f64;\n    let mean_x = points.iter().map(|(x, _)| *x).sum::<f64>() / n;\n    let mean_y = points.iter().map(|(_, y)| *y).sum::<f64>() / n;\n\n    let mut var_x = 0.0;\n    let mut var_y = 0.0;\n    let mut cov_xy = 0.0;\n    for (x, y) in points {\n        let dx = x - mean_x;\n        let dy = y - mean_y;\n        var_x += dx * dx;\n        var_y += dy * dy;\n        cov_xy += dx * dy;\n    }\n    var_x /= n;\n    var_y /= n;\n    cov_xy /= n;\n\n    let trace = var_x + var_y;\n    let det = var_x * var_y - cov_xy * cov_xy;\n    if det <= 1.0e-12 || trace <= 0.0 {\n        return Err(\"degenerate covariance\".to_string());\n    }\n\n    let half_trace = trace * 0.5;\n    let disc = (half_trace * half_trace - det).max(0.0).sqrt();\n    let lambda1 = half_trace + disc;\n    let lambda2 = half_trace - disc;\n    if lambda2 <= 0.0 {\n        return Err(\"degenerate eigenvalues\".to_string());\n    }\n\n    let axis_ratio = (lambda1 / lambda2).sqrt();\n\n    let mut v1_x = cov_xy;\n    let mut v1_y = lambda1 - var_x;\n    if v1_x.abs() < 1.0e-12 && v1_y.abs() < 1.0e-12 {\n        v1_x = 1.0;\n        v1_y = 0.0;\n    }\n    let norm = (v1_x * v1_x + v1_y * v1_y).sqrt();\n    v1_x /= norm;\n    v1_y /= norm;\n    let v2_x = -v1_y;\n    let v2_y = v1_x;\n\n    let mut radii = Vec::with_capacity(points.len());\n    for (x, y) in points {\n        let dx = x - mean_x;\n        let dy = y - mean_y;\n        let p1 = dx * v1_x + dy * v1_y;\n        let p2 = dx * v2_x + dy * v2_y;\n        let w1 = p1 / lambda1.sqrt();\n        let w2 = p2 / lambda2.sqrt();\n        radii.push((w1 * w1 + w2 * w2).sqrt());\n    }\n\n    let mean_radius = radii.iter().sum::<f64>() / radii.len() as f64;\n    if mean_radius == 0.0 {\n        return Err(\"zero mean radius\".to_string());\n    }\n\n    let mut variance = 0.0;\n    for r in &radii {\n        let diff = r - mean_radius;\n        variance += diff * diff;\n    }\n    let std = (variance / radii.len() as f64).sqrt();\n    let radius_cv = std / mean_radius;\n    let score = 1.0 / (1.0 + radius_cv);\n\n    Ok((score, radius_cv, axis_ratio))\n}\n\n// Helper functions for verification - reserved for future integration\n#[allow(dead_code)]\nfn baseline_loss(\n    losses: &[(usize, f64)],\n    plateau_min_step: usize,\n    drop_window: usize,\n) -> Result<f64, String> {\n    let before: Vec<f64> = losses\n        .iter()\n        .filter(|(step, _)| *step <= plateau_min_step)\n        .map(|(_, loss)| *loss)\n        .collect();\n\n    if before.len() < drop_window || drop_window == 0 {\n        return Err(\"insufficient loss samples for baseline\".to_string());\n    }\n\n    let window = &before[before.len() - drop_window..];\n    Ok(window.iter().sum::<f64>() / drop_window as f64)\n}\n\n#[allow(dead_code)]\nfn find_drop_step(\n    losses: &[(usize, f64)],\n    plateau_min_step: usize,\n    baseline: f64,\n    drop_fraction: f64,\n) -> Result<usize, String> {\n    losses\n        .iter()\n        .filter(|(step, _)| *step >= plateau_min_step)\n        .find(|(_, loss)| *loss <= baseline * (1.0 - drop_fraction))\n        .map(|(step, _)| *step)\n        .ok_or_else(|| \"loss never drops below threshold\".to_string())\n}\n\nfn collect_mlp_post_relu_activations<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n    fixed_y: usize,\n) -> Result<Vec<Vec<f64>>, String> {\n    let modulus = ModularAdditionDataset::modulus();\n    let equals_token = ModularAdditionDataset::equals_token();\n\n    let mut inputs_vec = Vec::with_capacity(modulus * 3);\n    for x in 0..modulus {\n        inputs_vec.push(x as i32);\n        inputs_vec.push(fixed_y as i32);\n        inputs_vec.push(equals_token as i32);\n    }\n\n    let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n        .reshape([modulus, 3]);\n    let (_, mlp_acts) = model.forward_with_mlp_activations(inputs);\n    let [batch_size, d_ff] = mlp_acts.dims();\n\n    if batch_size != modulus {\n        return Err(format!(\n            \"unexpected activation batch size {} (expected {})\",\n            batch_size, modulus\n        ));\n    }\n\n    let data: Vec<f32> = mlp_acts.into_data().to_vec().unwrap();\n    let mut per_neuron = vec![Vec::with_capacity(batch_size); d_ff];\n    for batch in 0..batch_size {\n        for neuron in 0..d_ff {\n            let idx = batch * d_ff + neuron;\n            per_neuron[neuron].push(data[idx] as f64);\n        }\n    }\n\n    Ok(per_neuron)\n}\n\n#[allow(dead_code)]\nfn surface_autocorrelation_scores(\n    surface: &[Vec<f64>],\n    max_lag: usize,\n) -> Result<(f64, f64, f64, f64), String> {\n    let (height, width) = surface_dimensions(surface)?;\n    if height < 2 || width < 2 {\n        return Err(\"surface grid too small\".to_string());\n    }\n\n    let (mean, var) = surface_mean_variance(surface)?;\n    if var == 0.0 {\n        return Err(\"surface variance is zero\".to_string());\n    }\n\n    let max_lag = max_lag.min(height.saturating_sub(1)).min(width.saturating_sub(1));\n    if max_lag == 0 {\n        return Err(\"max_lag too small for surface\".to_string());\n    }\n\n    let diag_main = average_autocorrelation(surface, mean, var, max_lag, 1, 1)?;\n    let diag_anti = average_autocorrelation(surface, mean, var, max_lag, 1, -1)?;\n    let axis_x = average_autocorrelation(surface, mean, var, max_lag, 1, 0)?;\n    let axis_y = average_autocorrelation(surface, mean, var, max_lag, 0, 1)?;\n\n    Ok((diag_main, diag_anti, axis_x, axis_y))\n}\n\n#[allow(dead_code)]\nfn surface_dimensions(surface: &[Vec<f64>]) -> Result<(usize, usize), String> {\n    let height = surface.len();\n    if height == 0 {\n        return Err(\"surface is empty\".to_string());\n    }\n    let width = surface[0].len();\n    if width == 0 {\n        return Err(\"surface has empty rows\".to_string());\n    }\n    for (idx, row) in surface.iter().enumerate() {\n        if row.len() != width {\n            return Err(format!(\n                \"surface row {} has length {}, expected {}\",\n                idx,\n                row.len(),\n                width\n            ));\n        }\n    }\n    Ok((height, width))\n}\n\n#[allow(dead_code)]\nfn surface_mean_variance(surface: &[Vec<f64>]) -> Result<(f64, f64), String> {\n    let (height, width) = surface_dimensions(surface)?;\n    let mut sum = 0.0;\n    for row in surface {\n        for value in row {\n            sum += *value;\n        }\n    }\n    let count = (height * width) as f64;\n    let mean = sum / count;\n    let mut variance = 0.0;\n    for row in surface {\n        for value in row {\n            let diff = *value - mean;\n            variance += diff * diff;\n        }\n    }\n    variance /= count;\n    Ok((mean, variance))\n}\n\n#[allow(dead_code)]\nfn average_autocorrelation(\n    surface: &[Vec<f64>],\n    mean: f64,\n    variance: f64,\n    max_lag: usize,\n    dx: i32,\n    dy: i32,\n) -> Result<f64, String> {\n    let (height, width) = surface_dimensions(surface)?;\n    let mut total = 0.0;\n    let mut lag_count = 0usize;\n\n    for lag in 1..=max_lag {\n        let mut sum = 0.0;\n        let mut count = 0usize;\n        let lag = lag as i32;\n        for x in 0..height {\n            for y in 0..width {\n                let x2 = x as i32 + dx * lag;\n                let y2 = y as i32 + dy * lag;\n                if x2 < 0 || y2 < 0 {\n                    continue;\n                }\n                let x2 = x2 as usize;\n                let y2 = y2 as usize;\n                if x2 >= height || y2 >= width {\n                    continue;\n                }\n                let a = surface[x][y] - mean;\n                let b = surface[x2][y2] - mean;\n                sum += a * b;\n                count += 1;\n            }\n        }\n        if count == 0 {\n            continue;\n        }\n        let corr = sum / (count as f64 * variance);\n        total += corr.abs();\n        lag_count += 1;\n    }\n\n    if lag_count == 0 {\n        return Err(\"no valid lags for autocorrelation\".to_string());\n    }\n\n    Ok(total / lag_count as f64)\n}\n\n#[allow(dead_code)]\nfn analyze_mlp_activation_wave(\n    neuron_index: usize,\n    signal: &[f64],\n    config: &MlpWaveVerificationConfig,\n) -> Result<MlpWaveNeuronReport, String> {\n    if signal.len() < 3 {\n        return Err(format!(\n            \"activation signal for neuron {} is too short\",\n            neuron_index\n        ));\n    }\n\n    let negative_count = signal\n        .iter()\n        .filter(|value| **value < -config.negative_tolerance)\n        .count();\n    let negative_fraction = negative_count as f64 / signal.len() as f64;\n    if negative_fraction > config.max_negative_fraction {\n        return Err(format!(\n            \"neuron {} has {:.3} negative activations\",\n            neuron_index, negative_fraction\n        ));\n    }\n\n    let (dominant_frequency, dominant_ratio) = dominant_frequency_ratio(signal)?;\n    if dominant_ratio < config.min_dominant_ratio {\n        return Err(format!(\n            \"neuron {} dominant frequency ratio {:.3} below threshold {:.3}\",\n            neuron_index, dominant_ratio, config.min_dominant_ratio\n        ));\n    }\n\n    let best_correlation =\n        best_rectified_sine_correlation(signal, dominant_frequency, config.phase_steps);\n    if best_correlation < config.min_correlation {\n        return Err(format!(\n            \"neuron {} rectified sine correlation {:.3} below threshold {:.3}\",\n            neuron_index, best_correlation, config.min_correlation\n        ));\n    }\n\n    Ok(MlpWaveNeuronReport {\n        neuron_index,\n        dominant_frequency,\n        dominant_ratio,\n        negative_fraction,\n        best_correlation,\n    })\n}\n\n#[allow(dead_code)]\nfn dominant_frequency_ratio(signal: &[f64]) -> Result<(usize, f64), String> {\n    if signal.len() < 2 {\n        return Err(\"signal too short for FFT\".to_string());\n    }\n\n    let mut planner = FftPlanner::new();\n    let fft = planner.plan_fft_forward(signal.len());\n    let mut buffer: Vec<Complex<f64>> = signal\n        .iter()\n        .map(|&x| Complex::new(x, 0.0))\n        .collect();\n    fft.process(&mut buffer);\n    let magnitudes: Vec<f64> = buffer.iter().map(|c| c.norm()).collect();\n\n    if magnitudes.len() < 2 {\n        return Err(\"insufficient FFT bins\".to_string());\n    }\n\n    let mut dominant_idx = 1usize;\n    let mut dominant_mag = magnitudes[1];\n    for (idx, &mag) in magnitudes.iter().enumerate().skip(2) {\n        if mag > dominant_mag {\n            dominant_mag = mag;\n            dominant_idx = idx;\n        }\n    }\n\n    if dominant_idx == 0 {\n        return Err(\"dominant frequency is zero\".to_string());\n    }\n\n    let mut nonzero = magnitudes[1..].to_vec();\n    nonzero.sort_by(|a, b| a.partial_cmp(b).unwrap_or(std::cmp::Ordering::Equal));\n    let median_mag = nonzero[nonzero.len() / 2];\n    let ratio = if median_mag == 0.0 {\n        dominant_mag\n    } else {\n        dominant_mag / median_mag\n    };\n\n    Ok((dominant_idx, ratio))\n}\n\n#[allow(dead_code)]\nfn best_rectified_sine_correlation(signal: &[f64], frequency: usize, phase_steps: usize) -> f64 {\n    if signal.is_empty() || frequency == 0 || phase_steps == 0 {\n        return 0.0;\n    }\n\n    let tau = std::f64::consts::PI * 2.0;\n    let mut best = -1.0;\n    for step in 0..phase_steps {\n        let phase = tau * step as f64 / phase_steps as f64;\n        let wave: Vec<f64> = (0..signal.len())\n            .map(|idx| {\n                let angle = tau * frequency as f64 * idx as f64 / signal.len() as f64 + phase;\n                angle.sin().max(0.0)\n            })\n            .collect();\n        let corr = pearson_correlation(signal, &wave);\n        if corr > best {\n            best = corr;\n        }\n    }\n\n    best\n}\n\nfn pearson_correlation(a: &[f64], b: &[f64]) -> f64 {\n    if a.len() != b.len() || a.is_empty() {\n        return 0.0;\n    }\n\n    let mean_a = a.iter().sum::<f64>() / a.len() as f64;\n    let mean_b = b.iter().sum::<f64>() / b.len() as f64;\n\n    let mut num = 0.0;\n    let mut denom_a = 0.0;\n    let mut denom_b = 0.0;\n    for (x, y) in a.iter().zip(b.iter()) {\n        let da = x - mean_a;\n        let db = y - mean_b;\n        num += da * db;\n        denom_a += da * da;\n        denom_b += db * db;\n    }\n\n    if denom_a == 0.0 || denom_b == 0.0 {\n        return 0.0;\n    }\n\n    num / (denom_a.sqrt() * denom_b.sqrt())\n}\n\n/// Verify that the model actually learned modular addition\n/// by testing it on ALL possible examples systematically\npub fn verify_full_accuracy<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n) -> (f32, f32) {\n    println!(\"ðŸ” Verifying model on COMPLETE dataset (no sampling)...\");\n\n    let train_dataset = ModularAdditionDataset::new(true, 42);\n    let val_dataset = ModularAdditionDataset::new(false, 42);\n\n    let train_acc = test_all_examples(model, &train_dataset, device);\n    let val_acc = test_all_examples(model, &val_dataset, device);\n\n    println!(\"ðŸ“Š FULL Dataset Results:\");\n    println!(\"   Training: {}/{} = {:.2}%\",\n        (train_acc * train_dataset.len() as f32) as usize,\n        train_dataset.len(),\n        train_acc * 100.0);\n    println!(\"   Validation: {}/{} = {:.2}%\",\n        (val_acc * val_dataset.len() as f32) as usize,\n        val_dataset.len(),\n        val_acc * 100.0);\n\n    (train_acc, val_acc)\n}\n\n/// Test model on ALL examples in dataset (no random sampling)\nfn test_all_examples<B: Backend>(\n    model: &Transformer<B>,\n    dataset: &ModularAdditionDataset,\n    device: &B::Device,\n) -> f32 {\n    let mut total_correct = 0;\n    let total = dataset.len();\n\n    // Test in batches to avoid memory issues\n    let batch_size = 100;\n    for batch_start in (0..total).step_by(batch_size) {\n        let batch_end = (batch_start + batch_size).min(total);\n        let batch_len = batch_end - batch_start;\n\n        let mut inputs_vec = Vec::new();\n        let mut targets_vec = Vec::new();\n\n        for idx in batch_start..batch_end {\n            let (input, target) = dataset.get(idx).unwrap();\n            inputs_vec.extend(input);\n            targets_vec.push(target as i32);\n        }\n\n        let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n            .reshape([batch_len, 3]);\n\n        let logits = model.forward(inputs);\n        let predictions = logits.argmax(1).squeeze::<1>();\n        let predictions_vec: Vec<i32> = predictions.into_data().to_vec().unwrap();\n\n        let correct = predictions_vec\n            .iter()\n            .zip(targets_vec.iter())\n            .filter(|(p, t)| p == t)\n            .count();\n\n        total_correct += correct;\n    }\n\n    total_correct as f32 / total as f32\n}\n\n/// Verify that weight norms decrease over time with weight decay\n/// and show correlation with the grokking transition\npub fn verify_weight_norm_decay(\n    weight_norm_history: &[(usize, f64)],\n    grok_step: Option<usize>,\n    config: &WeightNormDecayConfig,\n) -> Result<WeightNormDecayReport, String> {\n    if weight_norm_history.len() < 2 {\n        return Err(\"weight norm history requires at least 2 snapshots\".to_string());\n    }\n\n    let initial_norm = weight_norm_history\n        .first()\n        .ok_or_else(|| \"weight norm history is empty\".to_string())?\n        .1;\n    let final_norm = weight_norm_history\n        .last()\n        .ok_or_else(|| \"weight norm history is empty\".to_string())?\n        .1;\n\n    if initial_norm <= 0.0 {\n        return Err(\"initial weight norm must be positive\".to_string());\n    }\n\n    // Check that norms are generally decreasing (with weight decay)\n    let relative_decrease = (initial_norm - final_norm) / initial_norm;\n    if relative_decrease < config.min_relative_decrease {\n        return Err(format!(\n            \"weight norm decrease {:.4} below threshold {:.4} (initial {:.4}, final {:.4})\",\n            relative_decrease, config.min_relative_decrease, initial_norm, final_norm\n        ));\n    }\n\n    // Check that norms are not increasing (weight decay should monotonically decrease norms)\n    for window in weight_norm_history.windows(2) {\n        let prev_norm = window[0].1;\n        let next_norm = window[1].1;\n        if next_norm > prev_norm * 1.01 {\n            // Allow 1% tolerance for numerical noise\n            return Err(format!(\n                \"weight norm increased from {:.4} to {:.4} at step {}\",\n                prev_norm, next_norm, window[1].0\n            ));\n        }\n    }\n\n    // If grok step is provided, check that norms show a shift around grokking\n    let grok_step_norm = if let Some(step) = grok_step {\n        weight_norm_history\n            .iter()\n            .filter(|(s, _)| {\n                *s >= step.saturating_sub(config.grok_step_window)\n                    && *s <= step + config.grok_step_window\n            })\n            .map(|(_, norm)| *norm)\n            .next()\n    } else {\n        None\n    };\n\n    Ok(WeightNormDecayReport {\n        initial_norm,\n        final_norm,\n        relative_decrease,\n        grok_step_norm,\n    })\n}\n\n/// Verify the characteristic \"snake curve\" shape of grokking phenomenon\n/// on log-scale plots: train accuracy rises early, validation stays flat then jumps\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_snake_curve_shape(\n    train_acc: &[(usize, f32)],\n    val_acc: &[(usize, f32)],\n    config: &SnakeCurveConfig,\n) -> Result<SnakeCurveReport, String> {\n    if train_acc.is_empty() || val_acc.is_empty() {\n        return Err(\"empty accuracy history\".to_string());\n    }\n\n    // Phase 1: Verify early train convergence (immediate drop on log scale)\n    let train_converged_step = train_acc\n        .iter()\n        .find(|(_, acc)| *acc >= config.train_convergence_threshold)\n        .map(|(step, _)| *step)\n        .ok_or_else(|| {\n            format!(\n                \"train accuracy never reaches {} threshold\",\n                config.train_convergence_threshold\n            )\n        })?;\n\n    if train_converged_step > config.early_step_max {\n        return Err(format!(\n            \"train accuracy converges at step {}, expected before {}\",\n            train_converged_step, config.early_step_max\n        ));\n    }\n\n    // Phase 2: Verify validation plateau (flat on log scale)\n    let plateau_samples: Vec<f32> = val_acc\n        .iter()\n        .filter(|(step, _)| *step >= 1 && *step <= config.val_plateau_min_step)\n        .map(|(_, acc)| *acc)\n        .collect();\n\n    if plateau_samples.is_empty() {\n        return Err(\"no validation samples in plateau region\".to_string());\n    }\n\n    let val_plateau_max = plateau_samples\n        .iter()\n        .fold(0.0f32, |max_val, &acc| max_val.max(acc));\n\n    if val_plateau_max > config.val_plateau_max_acc {\n        return Err(format!(\n            \"validation accuracy in plateau region too high: max {:.4} exceeds {:.4}\",\n            val_plateau_max, config.val_plateau_max_acc\n        ));\n    }\n\n    // Phase 3: Verify delayed validation jump (crash up on log scale)\n    let val_grok_step = val_acc\n        .iter()\n        .filter(|(step, _)| *step > config.val_plateau_min_step)\n        .find(|(_, acc)| *acc >= config.val_final_threshold)\n        .map(|(step, _)| *step)\n        .ok_or_else(|| {\n            format!(\n                \"validation accuracy never reaches {} after plateau\",\n                config.val_final_threshold\n            )\n        })?;\n\n    // Phase 4: Verify phase ordering (snake shape: early train, late val)\n    let curve_shape_valid = train_converged_step < config.val_plateau_min_step\n        && val_grok_step > config.val_plateau_min_step\n        && train_converged_step < val_grok_step;\n\n    if !curve_shape_valid {\n        return Err(format!(\n            \"invalid curve shape: train converged at {}, val plateau until {}, val grokked at {}\",\n            train_converged_step, config.val_plateau_min_step, val_grok_step\n        ));\n    }\n\n    Ok(SnakeCurveReport {\n        train_converged_by_step: train_converged_step,\n        val_plateau_max_acc: val_plateau_max,\n        val_grok_step,\n        curve_shape_valid,\n    })\n}\n\n/// Test specific examples to see if model actually computes mod p\npub fn test_specific_examples<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n) {\n    println!();\n    println!(\"ðŸ§ª Testing specific examples:\");\n\n    let modulus = ModularAdditionDataset::modulus();\n    let equals_token = ModularAdditionDataset::equals_token();\n    let test_cases = vec![\n        (0, 0, 0),\n        (1, 1, 2),\n        (50, 50, (50 + 50) % modulus),\n        (112, 1, (112 + 1) % modulus),\n        (56, 57, (56 + 57) % modulus),\n        (112, 112, (112 + 112) % modulus),\n        (10, 20, 30),\n        (60, 60, (60 + 60) % modulus),\n    ];\n\n    for (a, b, expected) in test_cases {\n        let input_vec = vec![a as i32, b as i32, equals_token as i32];\n        let input = Tensor::<B, 1, Int>::from_ints(input_vec.as_slice(), device)\n            .reshape([1, 3]);\n\n        let logits = model.forward(input);\n        let prediction = logits.argmax(1).squeeze::<1>();\n        let pred_value: i32 = prediction.into_data().to_vec().unwrap()[0];\n\n        let correct = if pred_value == expected as i32 { \"âœ“\" } else { \"âœ—\" };\n        println!(\n            \"   {} + {} mod {} = {} | Model: {} {}\",\n            a, b, modulus, expected, pred_value, correct\n        );\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    fn make_histories(\n        train_acc: &[(usize, f32)],\n        val_acc: &[(usize, f32)],\n        val_loss: &[(usize, f64)],\n    ) -> (LossHistory, AccuracyHistory) {\n        let mut loss_history = LossHistory::new();\n        loss_history.val_snapshots = val_loss.to_vec();\n        let mut accuracy_history = AccuracyHistory::new();\n        accuracy_history.train_snapshots = train_acc.to_vec();\n        accuracy_history.val_snapshots = val_acc.to_vec();\n        (loss_history, accuracy_history)\n    }\n\n    #[test]\n    fn grokking_phase_verification_passes_on_synthetic_signal() {\n        let train_acc = vec![(0, 0.2), (500, 0.995), (1000, 0.999)];\n        let val_acc = vec![\n            (0, 0.01),\n            (500, 0.01),\n            (1500, 0.01),\n            (2200, 0.995),\n            (2500, 0.999),\n        ];\n        let val_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 2.0),\n            (2200, 0.2),\n            (2500, 0.15),\n        ];\n\n        let (loss_history, accuracy_history) = make_histories(&train_acc, &val_acc, &val_loss);\n        let config = GrokkingVerificationConfig {\n            early_step_max: 1000,\n            plateau_min_step: 1500,\n            generalization_window: 1500,\n            accuracy_window: 2,\n            loss_window: 2,\n            ..GrokkingVerificationConfig::default_for_modulus(ModularAdditionDataset::modulus())\n        };\n\n        let report = verify_grokking_phase_transition(&loss_history, &accuracy_history, &config)\n            .expect(\"expected grokking verification to pass\");\n        assert!(report.transition_step >= config.plateau_min_step);\n        assert!(report.early_train_step <= config.early_step_max);\n    }\n\n    #[test]\n    fn grokking_phase_verification_fails_without_transition() {\n        let train_acc = vec![(0, 0.2), (500, 0.995)];\n        let val_acc = vec![(0, 0.01), (500, 0.01), (1500, 0.02)];\n        let val_loss = vec![(0, 2.0), (500, 2.0), (1500, 1.9)];\n\n        let (loss_history, accuracy_history) = make_histories(&train_acc, &val_acc, &val_loss);\n        let config = GrokkingVerificationConfig {\n            early_step_max: 1000,\n            plateau_min_step: 1000,\n            generalization_window: 1000,\n            accuracy_window: 2,\n            loss_window: 2,\n            ..GrokkingVerificationConfig::default_for_modulus(ModularAdditionDataset::modulus())\n        };\n\n        let err = verify_grokking_phase_transition(&loss_history, &accuracy_history, &config)\n            .expect_err(\"expected grokking verification to fail\");\n        assert!(err.contains(\"validation accuracy never reaches target\"));\n    }\n\n    #[test]\n    fn grokking_phase_verification_fails_with_slow_accuracy_rise() {\n        let train_acc = vec![(0, 0.2), (500, 0.995), (1000, 0.999)];\n        let val_acc = vec![\n            (0, 0.01),\n            (500, 0.01),\n            (1500, 0.5),\n            (2000, 0.6),\n            (2200, 0.995),\n            (2500, 0.999),\n        ];\n        let val_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 2.0),\n            (2200, 0.2),\n            (2500, 0.15),\n        ];\n\n        let (loss_history, accuracy_history) = make_histories(&train_acc, &val_acc, &val_loss);\n        let config = GrokkingVerificationConfig {\n            early_step_max: 1000,\n            plateau_min_step: 1000,\n            generalization_window: 2000,\n            accuracy_window: 2,\n            loss_window: 2,\n            ..GrokkingVerificationConfig::default_for_modulus(ModularAdditionDataset::modulus())\n        };\n\n        let err = verify_grokking_phase_transition(&loss_history, &accuracy_history, &config)\n            .expect_err(\"expected grokking verification to fail\");\n        assert!(err.contains(\"plateau too high\"));\n    }\n\n    #[test]\n    fn restricted_loss_detection_passes_with_early_drop() {\n        let full_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 2.0),\n            (2500, 2.0),\n            (3500, 0.3),\n        ];\n        let restricted_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 0.8),\n            (2500, 0.4),\n            (3500, 0.3),\n        ];\n\n        let config = RestrictedLossVerificationConfig {\n            plateau_min_step: 1000,\n            drop_window: 2,\n            drop_fraction: 0.5,\n            min_step_lead: 500,\n        };\n\n        let report = verify_restricted_loss_early_drop(&full_loss, &restricted_loss, &config)\n            .expect(\"expected restricted loss verification to pass\");\n        assert!(report.restricted_drop_step < report.full_drop_step);\n    }\n\n    #[test]\n    fn restricted_loss_detection_fails_when_full_drops_first() {\n        let full_loss = vec![(0, 2.0), (1000, 1.0), (1500, 0.8)];\n        let restricted_loss = vec![(0, 2.0), (1000, 2.0), (1500, 0.8)];\n\n        let config = RestrictedLossVerificationConfig {\n            plateau_min_step: 500,\n            drop_window: 1,\n            drop_fraction: 0.3,\n            min_step_lead: 200,\n        };\n\n        let err = verify_restricted_loss_early_drop(&full_loss, &restricted_loss, &config)\n            .expect_err(\"expected restricted loss verification to fail\");\n        assert!(err.contains(\"restricted loss drops\"));\n    }\n\n    #[test]\n    fn excluded_loss_spike_verification_passes() {\n        let excluded_loss = vec![(0, 1.0), (1500, 2.2), (3000, 1.1)];\n        let config = ExcludedLossSpikeConfig {\n            min_relative_increase: 0.5,\n        };\n\n        let report = verify_excluded_loss_spike(&excluded_loss, &config)\n            .expect(\"expected excluded loss spike verification to pass\");\n        assert_eq!(report.spike_step, 1500);\n    }\n\n    #[test]\n    fn excluded_loss_spike_verification_fails_without_spike() {\n        let excluded_loss = vec![(0, 1.0), (1500, 1.1), (3000, 1.0)];\n        let config = ExcludedLossSpikeConfig {\n            min_relative_increase: 0.5,\n        };\n\n        let err = verify_excluded_loss_spike(&excluded_loss, &config)\n            .expect_err(\"expected excluded loss spike verification to fail\");\n        assert!(err.contains(\"excluded loss spike too small\"));\n    }\n\n    #[test]\n    fn restricted_loss_drop_verification_passes() {\n        let restricted_loss = vec![\n            (0, 2.0),\n            (1000, 1.8),\n            (5000, 1.2),\n            (7000, 0.8),\n            (10000, 0.9),\n        ];\n        let config = RestrictedLossDropConfig {\n            min_relative_decrease: 0.3,\n        };\n\n        let report = verify_restricted_loss_drop(&restricted_loss, &config)\n            .expect(\"expected restricted loss drop verification to pass\");\n        assert_eq!(report.drop_step, 7000);\n        assert!(report.baseline_loss > report.drop_loss);\n        assert!((report.baseline_loss - report.drop_loss) / report.baseline_loss >= 0.3);\n    }\n\n    #[test]\n    fn restricted_loss_drop_verification_fails_without_drop() {\n        let restricted_loss = vec![\n            (0, 2.0),\n            (1000, 2.1),\n            (5000, 1.9),\n            (7000, 2.0),\n            (10000, 2.05),\n        ];\n        let config = RestrictedLossDropConfig {\n            min_relative_decrease: 0.3,\n        };\n\n        let err = verify_restricted_loss_drop(&restricted_loss, &config)\n            .expect_err(\"expected restricted loss drop verification to fail\");\n        assert!(err.contains(\"restricted loss decrease too small\"));\n    }\n\n    #[test]\n    fn mlp_activation_wave_verification_passes_on_rectified_sine() {\n        let n = ModularAdditionDataset::modulus();\n        let freq = 5.0;\n        let mut signal = Vec::with_capacity(n);\n        for idx in 0..n {\n            let angle = 2.0 * std::f64::consts::PI * freq * idx as f64 / n as f64;\n            let rectified = angle.sin().max(0.0);\n            let wobble = (2.0 * std::f64::consts::PI * 2.0 * idx as f64 / n as f64)\n                .sin()\n                .abs()\n                * 0.02;\n            signal.push(rectified + wobble);\n        }\n\n        let config = MlpWaveVerificationConfig {\n            max_negative_fraction: 0.0,\n            negative_tolerance: 1e-6,\n            min_dominant_ratio: 3.0,\n            min_correlation: 0.7,\n            phase_steps: 32,\n        };\n\n        let report =\n            verify_mlp_activation_waves_from_signals(&[(0, signal)], &config)\n                .expect(\"expected rectified sine to pass\");\n        assert_eq!(report.neuron_reports.len(), 1);\n    }\n\n    #[test]\n    fn mlp_activation_wave_verification_fails_on_noise() {\n        let n = ModularAdditionDataset::modulus();\n        let signal: Vec<f64> = (0..n).map(|idx| idx as f64 / n as f64).collect();\n\n        let config = MlpWaveVerificationConfig {\n            max_negative_fraction: 0.0,\n            negative_tolerance: 1e-6,\n            min_dominant_ratio: 4.0,\n            min_correlation: 0.8,\n            phase_steps: 16,\n        };\n\n        let err =\n            verify_mlp_activation_waves_from_signals(&[(0, signal)], &config)\n                .expect_err(\"expected linear signal to fail\");\n        assert!(err.contains(\"dominant frequency ratio\") || err.contains(\"correlation\"));\n    }\n\n    #[test]\n    fn pairwise_manifold_transition_passes_on_elliptic_signals() {\n        let n = ModularAdditionDataset::modulus();\n        let freq = 5.0;\n        let mut post_signals = Vec::new();\n        for neuron_idx in 0..7usize {\n            let phase = 2.0 * std::f64::consts::PI * neuron_idx as f64 / 7.0;\n            let signal: Vec<f64> = (0..n)\n                .map(|idx| {\n                    let angle = 2.0 * std::f64::consts::PI * freq * idx as f64 / n as f64 + phase;\n                    angle.sin()\n                })\n                .collect();\n            post_signals.push((neuron_idx, signal));\n        }\n\n        let mut pre_signals = Vec::new();\n        for neuron_idx in 0..7usize {\n            let signal: Vec<f64> = (0..n)\n                .map(|idx| idx as f64 / n as f64 + neuron_idx as f64 * 0.01)\n                .collect();\n            pre_signals.push((neuron_idx, signal));\n        }\n\n        let config = PairwiseManifoldConfig {\n            fixed_y: 0,\n            neuron_indices: (0..7).collect(),\n            min_post_score: 0.75,\n            max_pre_score: 0.55,\n            max_axis_ratio: 8.0,\n            min_pairs_passing: 8,\n        };\n\n        let report = verify_pairwise_manifold_transition_from_signals(\n            &pre_signals,\n            &post_signals,\n            &config,\n        )\n        .expect(\"expected pairwise manifold transition to pass\");\n        assert!(report.passing_pairs >= config.min_pairs_passing);\n    }\n\n    #[test]\n    fn pairwise_manifold_transition_fails_without_rings() {\n        let n = ModularAdditionDataset::modulus();\n        let mut signals = Vec::new();\n        for neuron_idx in 0..7usize {\n            let signal: Vec<f64> = (0..n)\n                .map(|idx| idx as f64 / n as f64 + neuron_idx as f64 * 0.02)\n                .collect();\n            signals.push((neuron_idx, signal));\n        }\n\n        let config = PairwiseManifoldConfig {\n            fixed_y: 0,\n            neuron_indices: (0..7).collect(),\n            min_post_score: 0.8,\n            max_pre_score: 0.55,\n            max_axis_ratio: 4.0,\n            min_pairs_passing: 5,\n        };\n\n        let err = verify_pairwise_manifold_transition_from_signals(\n            &signals,\n            &signals,\n            &config,\n        )\n        .expect_err(\"expected pairwise manifold transition to fail\");\n        assert!(err.contains(\"post-grok\") || err.contains(\"pre-grok\"));\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_passes_on_diagonal_corrugation() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create diagonal ridges: f(x,y) = sin(2Ï€ * freq * (x+y) / n)\n        let freq = 3.0;\n        for x in 0..n {\n            for y in 0..n {\n                let angle = 2.0 * std::f64::consts::PI * freq * (x + y) as f64 / n as f64;\n                surface[x][y] = angle.sin();\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let report = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect(\"expected diagonal ridge verification to pass\");\n        assert!(report.diagonal_score > report.axis_score);\n        assert!(report.ratio >= config.min_diagonal_ratio);\n        assert!(report.diagonal_score >= config.min_diagonal_score);\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_passes_on_anti_diagonal_pattern() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create anti-diagonal ridges: f(x,y) = sin(2Ï€ * freq * (x-y) / n)\n        let freq = 3.0;\n        for x in 0..n {\n            for y in 0..n {\n                let angle = 2.0 * std::f64::consts::PI * freq * (x as i32 - y as i32) as f64 / n as f64;\n                surface[x][y] = angle.sin();\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let report = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect(\"expected anti-diagonal ridge verification to pass\");\n        assert!(report.diagonal_score > report.axis_score);\n        assert!(report.ratio >= config.min_diagonal_ratio);\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_fails_on_axis_aligned_pattern() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create axis-aligned ridges: f(x,y) = sin(2Ï€ * freq * x / n)\n        let freq = 3.0;\n        for x in 0..n {\n            for y in 0..n {\n                let angle = 2.0 * std::f64::consts::PI * freq * x as f64 / n as f64;\n                surface[x][y] = angle.sin();\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let err = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect_err(\"expected axis-aligned pattern to fail diagonal verification\");\n        assert!(err.contains(\"diagonal/axis ratio\") || err.contains(\"diagonal autocorrelation\"));\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_fails_on_flat_surface() {\n        let n = 50;\n        let surface = vec![vec![1.5; n]; n];\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let err = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect_err(\"expected flat surface to fail\");\n        assert!(err.contains(\"surface variance is zero\"));\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_fails_on_random_noise() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create random noise (low autocorrelation everywhere)\n        for x in 0..n {\n            for y in 0..n {\n                surface[x][y] = ((x * 17 + y * 31) % 100) as f64 / 100.0;\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let err = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect_err(\"expected random noise to fail diagonal verification\");\n        assert!(err.contains(\"diagonal/axis ratio\") || err.contains(\"diagonal autocorrelation\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_passes_on_valid_grokking_curve() {\n        // Simulate typical grokking: train converges early, val plateaus then jumps\n        let train_acc = vec![\n            (1, 0.10),\n            (10, 0.50),\n            (100, 0.80),\n            (500, 0.98),\n            (1000, 0.99),\n            (2000, 0.995),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (10, 0.009),\n            (100, 0.01),\n            (500, 0.011),\n            (1000, 0.009),\n            (1500, 0.01),\n            (2000, 0.92),\n            (3000, 0.98),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 1500,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let report = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect(\"expected snake curve verification to pass\");\n\n        assert_eq!(report.train_converged_by_step, 500);\n        assert!(report.val_plateau_max_acc < config.val_plateau_max_acc);\n        assert_eq!(report.val_grok_step, 2000);\n        assert!(report.curve_shape_valid);\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_on_simultaneous_convergence() {\n        // Both train and val converge at the same time (no grokking)\n        let train_acc = vec![\n            (1, 0.10),\n            (100, 0.98),\n            (500, 0.99),\n        ];\n        let val_acc = vec![\n            (1, 0.10),\n            (100, 0.97),\n            (500, 0.98),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 200,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let err = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect_err(\"expected snake curve verification to fail\");\n        assert!(err.contains(\"plateau region too high\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_on_slow_train_convergence() {\n        // Train takes too long to converge (not the typical snake curve)\n        let train_acc = vec![\n            (1, 0.10),\n            (500, 0.50),\n            (1000, 0.70),\n            (2000, 0.98),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (500, 0.01),\n            (1000, 0.01),\n            (2000, 0.92),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 1500,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let err = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect_err(\"expected snake curve verification to fail\");\n        assert!(err.contains(\"converges at step 2000\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_without_val_jump() {\n        // Validation never jumps up (no generalization)\n        let train_acc = vec![\n            (1, 0.10),\n            (100, 0.98),\n            (500, 0.99),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (100, 0.01),\n            (500, 0.01),\n            (2000, 0.02),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 200,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let err = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect_err(\"expected snake curve verification to fail\");\n        assert!(err.contains(\"validation accuracy never reaches\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_on_wrong_phase_ordering() {\n        // Val jumps before train converges (backwards snake)\n        let train_acc = vec![\n            (1, 0.10),\n            (500, 0.50),\n            (2000, 0.98),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (100, 0.95),\n            (500, 0.98),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 200,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let result = verify_snake_curve_shape(&train_acc, &val_acc, &config);\n        // Should fail because val jumps before train converges (not a proper snake curve)\n        // This can fail for multiple reasons, but should definitely fail\n        assert!(result.is_err(), \"Expected snake curve verification to fail but it passed\");\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_passes_on_decreasing_norms() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 85.0),\n            (1000, 72.0),\n            (1500, 63.0),\n            (2000, 58.0),\n            (2500, 55.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let report = verify_weight_norm_decay(&weight_norms, Some(2000), &config)\n            .expect(\"expected weight norm decay verification to pass\");\n\n        assert_eq!(report.initial_norm, 100.0);\n        assert_eq!(report.final_norm, 55.0);\n        assert!((report.relative_decrease - 0.45).abs() < 0.01);\n        assert!(report.grok_step_norm.is_some());\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_fails_on_flat_norms() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 99.0),\n            (1000, 98.5),\n            (1500, 98.0),\n            (2000, 97.5),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let err = verify_weight_norm_decay(&weight_norms, None, &config)\n            .expect_err(\"expected weight norm decay verification to fail\");\n        assert!(err.contains(\"weight norm decrease\"));\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_fails_on_increasing_norms() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 95.0),\n            (1000, 105.0), // Increase here\n            (1500, 90.0),\n            (2000, 85.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let err = verify_weight_norm_decay(&weight_norms, None, &config)\n            .expect_err(\"expected weight norm decay verification to fail\");\n        assert!(err.contains(\"weight norm increased\"));\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_allows_small_noise() {\n        // Weight decay with small numerical noise (within 1% tolerance)\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 95.0),\n            (1000, 90.5),  // Small increase within tolerance\n            (1500, 90.0),\n            (2000, 85.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let report = verify_weight_norm_decay(&weight_norms, None, &config)\n            .expect(\"expected weight norm decay with small noise to pass\");\n\n        assert_eq!(report.initial_norm, 100.0);\n        assert_eq!(report.final_norm, 85.0);\n        assert!((report.relative_decrease - 0.15).abs() < 0.01);\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_finds_grok_step_norm() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (1000, 80.0),\n            (1800, 68.0),  // Within window of grok step (2200 - 500 = 1700)\n            (2000, 65.0),\n            (2200, 62.0),  // Around grok step\n            (3000, 55.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let report = verify_weight_norm_decay(&weight_norms, Some(2200), &config)\n            .expect(\"expected weight norm decay verification to pass\");\n\n        assert!(report.grok_step_norm.is_some());\n        let grok_norm = report.grok_step_norm.unwrap();\n        // Should find the first norm in the window [1700, 2700], which is 68.0 at step 1800\n        assert!((grok_norm - 68.0).abs() < 0.01);\n    }\n}\n","structuredPatch":[{"oldStart":1193,"oldLines":6,"newStart":1193,"newLines":7,"lines":["     best"," }"," ","+#[allow(dead_code)]"," fn pearson_correlation(a: &[f64], b: &[f64]) -> f64 {","     if a.len() != b.len() || a.is_empty() {","         return 0.0;"]}],"userModified":false,"replaceAll":false}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01Edk1Rs36PL67sPXE5J7j8a","type":"message","role":"assistant","content":[{"type":"text","text":"Now let me check the plotting module and checkpoint module:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":345,"cache_read_input_tokens":65170,"cache_creation":{"ephemeral_5m_input_tokens":345,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"26a0e838-fdee-4f55-b561-455ab1f9bd70"}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01Edk1Rs36PL67sPXE5J7j8a","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01HtR6LXkuo1ixUZQjsWeCs7","name":"Bash","input":{"command":"grep -n \"pub fn plot_loss_history\\|pub fn plot_embedding_grid_fast\\|pub fn render_dimension_plot\\|pub fn render_scatter_plot\\|fn viridis_to_rgb\\|fn token_to_rgb\" src/plotting.rs | head -10","description":"Find unused plotting functions"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":345,"cache_read_input_tokens":65170,"cache_creation":{"ephemeral_5m_input_tokens":345,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"fd9900bf-0646-4fd5-8cc2-94b7222f01ad"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01HtR6LXkuo1ixUZQjsWeCs7","type":"tool_result","content":"30:pub fn plot_loss_history_dual(\n105:pub fn plot_loss_history(\n451:pub fn plot_loss_history_dual_logscale(\n873:fn viridis_to_rgb(t: f64) -> Rgb<u8> {\n879:fn token_to_rgb(token_value: usize, p: usize) -> Rgb<u8> {\n885:pub fn plot_embedding_grid_fast(","is_error":false}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"129dca6e-ed08-440c-8c71-a292a391dc94","tool_use_result":{"stdout":"30:pub fn plot_loss_history_dual(\n105:pub fn plot_loss_history(\n451:pub fn plot_loss_history_dual_logscale(\n873:fn viridis_to_rgb(t: f64) -> Rgb<u8> {\n879:fn token_to_rgb(token_value: usize, p: usize) -> Rgb<u8> {\n885:pub fn plot_embedding_grid_fast(","stderr":"","interrupted":false,"isImage":false}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_015pVbgap7wzJNLAkS8AFL9o","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01UCbhTmWsYRQHvtnKg6CXej","name":"Edit","input":{"replace_all":false,"file_path":"/Users/admin/MechInt/src/plotting.rs","old_string":"pub fn plot_loss_history(","new_string":"/// NOTE: Superseded by plot_loss_history_dual - kept for reference\n#[allow(dead_code)]\npub fn plot_loss_history("}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":374,"cache_read_input_tokens":65515,"cache_creation":{"ephemeral_5m_input_tokens":374,"ephemeral_1h_input_tokens":0},"output_tokens":165,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"300bcafb-c410-429c-a44d-e3ddcc93c9e1"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01UCbhTmWsYRQHvtnKg6CXej","type":"tool_result","content":"The file /Users/admin/MechInt/src/plotting.rs has been updated. Here's the result of running `cat -n` on a snippet of the edited file:\n   101â†’    Ok(())\n   102â†’}\n   103â†’\n   104â†’/// Plot training loss over time (legacy, single loss)\n   105â†’/// NOTE: Superseded by plot_loss_history_dual - kept for reference\n   106â†’#[allow(dead_code)]\n   107â†’pub fn plot_loss_history(\n   108â†’    snapshots: &[(usize, f64)], // (step, loss)\n   109â†’    output_path: &str,\n   110â†’) -> Result<(), Box<dyn std::error::Error>> {\n   111â†’    let root = BitMapBackend::new(output_path, (1200, 800)).into_drawing_area();"}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"21d748f9-9846-444e-bb36-f9f56659d0bd","tool_use_result":{"filePath":"/Users/admin/MechInt/src/plotting.rs","oldString":"pub fn plot_loss_history(","newString":"/// NOTE: Superseded by plot_loss_history_dual - kept for reference\n#[allow(dead_code)]\npub fn plot_loss_history(","originalFile":"use image::{ImageBuffer, Rgb, RgbImage};\nuse imageproc::drawing::{draw_filled_circle_mut, draw_line_segment_mut};\nuse plotters::prelude::*;\nuse std::collections::HashMap;\n\nuse crate::data::ModularAdditionDataset;\n\n/// Viridis colormap - maps value in [0, 1] to RGB\nfn viridis_color(t: f64) -> RGBColor {\n    // Viridis colormap approximation (purple â†’ cyan â†’ yellow)\n    // Based on matplotlib's viridis\n    let r = (0.267 + t * (0.329 - 0.267 + t * (0.984 - 0.329))) * 255.0;\n    let g = (0.005 + t * (0.569 - 0.005 + t * (0.906 - 0.569))) * 255.0;\n    let b = (0.329 + t * (0.758 - 0.329 - t * (0.758 - 0.121))) * 255.0;\n\n    RGBColor(\n        r.min(255.0).max(0.0) as u8,\n        g.min(255.0).max(0.0) as u8,\n        b.min(255.0).max(0.0) as u8,\n    )\n}\n\n/// Map token value (0 to p-1) to viridis color\nfn token_color(token_value: usize, p: usize) -> RGBColor {\n    let t = token_value as f64 / (p - 1) as f64;\n    viridis_color(t)\n}\n\n/// Plot training and validation loss over time\npub fn plot_loss_history_dual(\n    train_loss: &[(usize, f64)], // (step, loss)\n    val_loss: &[(usize, f64)],\n    output_path: &str,\n) -> Result<(), Box<dyn std::error::Error>> {\n    let root = BitMapBackend::new(output_path, (1200, 800)).into_drawing_area();\n    root.fill(&WHITE)?;\n\n    let max_step = train_loss\n        .iter()\n        .chain(val_loss.iter())\n        .map(|(s, _)| *s)\n        .max()\n        .unwrap_or(1);\n\n    let max_loss = train_loss\n        .iter()\n        .chain(val_loss.iter())\n        .map(|(_, l)| *l)\n        .fold(0.0f64, f64::max)\n        .max(1.0);\n\n    let mut chart = ChartBuilder::on(&root)\n        .caption(\"Loss Evolution: Train vs Validation\", (\"sans-serif\", 50).into_font())\n        .margin(20)\n        .x_label_area_size(50)\n        .y_label_area_size(70)\n        .build_cartesian_2d(0usize..max_step, 0f64..max_loss)?;\n\n    chart\n        .configure_mesh()\n        .x_desc(\"Training Step\")\n        .y_desc(\"Loss\")\n        .draw()?;\n\n    // Plot training loss\n    chart\n        .draw_series(LineSeries::new(\n            train_loss.iter().map(|(s, l)| (*s, *l)),\n            &RED,\n        ))?\n        .label(\"Train Loss\")\n        .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &RED));\n\n    // Plot validation loss\n    chart\n        .draw_series(LineSeries::new(\n            val_loss.iter().map(|(s, l)| (*s, *l)),\n            &GREEN,\n        ))?\n        .label(\"Val Loss\")\n        .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &GREEN));\n\n    let modulus = ModularAdditionDataset::modulus() as f64;\n    let chance_loss = -(1.0 / modulus).ln();\n    chart\n        .draw_series(LineSeries::new(\n            vec![(0, chance_loss), (max_step, chance_loss)],\n            &BLUE.mix(0.5),\n        ))?\n        .label(\"Chance Level\")\n        .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &BLUE.mix(0.5)));\n\n    chart\n        .configure_series_labels()\n        .background_style(&WHITE.mix(0.8))\n        .border_style(&BLACK)\n        .draw()?;\n\n    root.present()?;\n    println!(\"ðŸ“Š Loss plot saved to: {}\", output_path);\n    Ok(())\n}\n\n/// Plot training loss over time (legacy, single loss)\npub fn plot_loss_history(\n    snapshots: &[(usize, f64)], // (step, loss)\n    output_path: &str,\n) -> Result<(), Box<dyn std::error::Error>> {\n    let root = BitMapBackend::new(output_path, (1200, 800)).into_drawing_area();\n    root.fill(&WHITE)?;\n\n    let max_step = snapshots.iter().map(|(s, _)| *s).max().unwrap_or(1);\n    let max_loss = snapshots\n        .iter()\n        .map(|(_, l)| *l)\n        .fold(0.0f64, f64::max)\n        .max(1.0);\n\n    let mut chart = ChartBuilder::on(&root)\n        .caption(\"Training Loss Evolution\", (\"sans-serif\", 50).into_font())\n        .margin(20)\n        .x_label_area_size(50)\n        .y_label_area_size(70)\n        .build_cartesian_2d(0usize..max_step, 0f64..max_loss)?;\n\n    chart\n        .configure_mesh()\n        .x_desc(\"Training Step\")\n        .y_desc(\"Loss\")\n        .draw()?;\n\n    chart.draw_series(LineSeries::new(\n        snapshots.iter().map(|(s, l)| (*s, *l)),\n        &BLUE,\n    ))?;\n\n    root.present()?;\n    println!(\"ðŸ“Š Loss plot saved to: {}\", output_path);\n    Ok(())\n}\n\n/// Plot training and validation accuracy over time (combined)\npub fn plot_accuracy_history(\n    train_acc: &[(usize, f32)], // (step, accuracy)\n    val_acc: &[(usize, f32)],\n    output_path: &str,\n) -> Result<(), Box<dyn std::error::Error>> {\n    let root = BitMapBackend::new(output_path, (1200, 800)).into_drawing_area();\n    root.fill(&WHITE)?;\n\n    let max_step = train_acc\n        .iter()\n        .chain(val_acc.iter())\n        .map(|(s, _)| *s)\n        .max()\n        .unwrap_or(1);\n\n    let mut chart = ChartBuilder::on(&root)\n        .caption(\n            \"Grokking Phenomenon: Train vs Validation Accuracy\",\n            (\"sans-serif\", 50).into_font(),\n        )\n        .margin(20)\n        .x_label_area_size(50)\n        .y_label_area_size(70)\n        .build_cartesian_2d(0usize..max_step, 0f32..105f32)?;\n\n    chart\n        .configure_mesh()\n        .x_desc(\"Training Step\")\n        .y_desc(\"Accuracy (%)\")\n        .draw()?;\n\n    // Plot training accuracy\n    chart\n        .draw_series(LineSeries::new(\n            train_acc.iter().map(|(s, a)| (*s, a * 100.0)),\n            &BLUE,\n        ))?\n        .label(\"Training Accuracy\")\n        .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &BLUE));\n\n    // Plot validation accuracy\n    chart\n        .draw_series(LineSeries::new(\n            val_acc.iter().map(|(s, a)| (*s, a * 100.0)),\n            &RED,\n        ))?\n        .label(\"Validation Accuracy\")\n        .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &RED));\n\n    // Add reference line at 90% (grokking threshold)\n    chart.draw_series(LineSeries::new(\n        vec![(0, 90.0), (max_step, 90.0)],\n        &GREEN.mix(0.5),\n    ))?;\n\n    chart\n        .configure_series_labels()\n        .background_style(&WHITE.mix(0.8))\n        .border_style(&BLACK)\n        .draw()?;\n\n    root.present()?;\n    println!(\"ðŸ“Š Accuracy plot saved to: {}\", output_path);\n    Ok(())\n}\n\n/// Plot FFT dominant frequency distribution\npub fn plot_fft_frequency_distribution(\n    dominant_frequencies: &[(usize, usize, f64)], // (token_id, frequency, magnitude)\n    output_path: &str,\n) -> Result<(), Box<dyn std::error::Error>> {\n    let root = BitMapBackend::new(output_path, (1200, 800)).into_drawing_area();\n    root.fill(&WHITE)?;\n\n    // Count frequency occurrences\n    let mut freq_counts: HashMap<usize, usize> = HashMap::new();\n    for (_, freq, _) in dominant_frequencies {\n        *freq_counts.entry(*freq).or_default() += 1;\n    }\n\n    let mut freq_vec: Vec<(usize, usize)> = freq_counts.into_iter().collect();\n    freq_vec.sort_by_key(|(f, _)| *f);\n\n    let max_freq = freq_vec.iter().map(|(f, _)| *f).max().unwrap_or(1);\n    let max_count = freq_vec.iter().map(|(_, c)| *c).max().unwrap_or(1);\n\n    let mut chart = ChartBuilder::on(&root)\n        .caption(\n            \"FFT Dominant Frequency Distribution\",\n            (\"sans-serif\", 50).into_font(),\n        )\n        .margin(20)\n        .x_label_area_size(50)\n        .y_label_area_size(70)\n        .build_cartesian_2d(0usize..max_freq, 0usize..max_count + 1)?;\n\n    chart\n        .configure_mesh()\n        .x_desc(\"Frequency\")\n        .y_desc(\"Number of Tokens\")\n        .draw()?;\n\n    chart.draw_series(\n        freq_vec\n            .iter()\n            .map(|(freq, count)| {\n                let color = if *freq == ModularAdditionDataset::modulus() {\n                    RED.filled() // Highlight frequency matching modulus\n                } else {\n                    BLUE.filled()\n                };\n                Rectangle::new([(*freq, 0), (*freq, *count)], color)\n            }),\n    )?;\n\n    root.present()?;\n    println!(\"ðŸ“Š FFT frequency distribution plot saved to: {}\", output_path);\n    Ok(())\n}\n\n/// Plot FFT magnitude spectrum for selected tokens\npub fn plot_fft_spectra(\n    token_spectra: &[(usize, Vec<f64>)], // (token_id, magnitude_spectrum)\n    output_path: &str,\n) -> Result<(), Box<dyn std::error::Error>> {\n    let root = BitMapBackend::new(output_path, (1200, 800)).into_drawing_area();\n    root.fill(&WHITE)?;\n\n    let max_freq = token_spectra\n        .first()\n        .map(|(_, spec)| spec.len())\n        .unwrap_or(1);\n    let max_magnitude = token_spectra\n        .iter()\n        .flat_map(|(_, spec)| spec.iter())\n        .copied()\n        .fold(0.0f64, f64::max)\n        .max(1.0);\n\n    let mut chart = ChartBuilder::on(&root)\n        .caption(\n            \"FFT Magnitude Spectra (Selected Tokens)\",\n            (\"sans-serif\", 50).into_font(),\n        )\n        .margin(20)\n        .x_label_area_size(50)\n        .y_label_area_size(70)\n        .build_cartesian_2d(0usize..max_freq, 0f64..max_magnitude)?;\n\n    chart\n        .configure_mesh()\n        .x_desc(\"Frequency\")\n        .y_desc(\"Magnitude\")\n        .draw()?;\n\n    let colors = [&BLUE, &RED, &GREEN, &MAGENTA, &CYAN, &YELLOW];\n\n    for (idx, (token_id, spectrum)) in token_spectra.iter().enumerate() {\n        let color = colors[idx % colors.len()];\n        chart\n            .draw_series(LineSeries::new(\n                spectrum.iter().enumerate().map(|(f, m)| (f, *m)),\n                color,\n            ))?\n            .label(format!(\"Token {}\", token_id))\n            .legend(move |(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], color));\n    }\n\n    chart\n        .configure_series_labels()\n        .background_style(&WHITE.mix(0.8))\n        .border_style(&BLACK)\n        .draw()?;\n\n    root.present()?;\n    println!(\"ðŸ“Š FFT spectra plot saved to: {}\", output_path);\n    Ok(())\n}\n\n/// Plot combined grokking visualization (loss + accuracy on same timeline)\npub fn plot_grokking_combined(\n    train_loss: &[(usize, f64)],\n    val_loss: &[(usize, f64)],\n    train_acc: &[(usize, f32)],\n    val_acc: &[(usize, f32)],\n    output_path: &str,\n) -> Result<(), Box<dyn std::error::Error>> {\n    let root = BitMapBackend::new(output_path, (1400, 900)).into_drawing_area();\n    root.fill(&WHITE)?;\n\n    let (upper, lower) = root.split_vertically(450);\n\n    // Upper plot: Accuracy\n    {\n        let max_step = train_acc\n            .iter()\n            .chain(val_acc.iter())\n            .map(|(s, _)| *s)\n            .max()\n            .unwrap_or(1);\n\n        let mut chart = ChartBuilder::on(&upper)\n            .caption(\n                \"Grokking: Accuracy Evolution\",\n                (\"sans-serif\", 40).into_font(),\n            )\n            .margin(15)\n            .x_label_area_size(40)\n            .y_label_area_size(60)\n            .build_cartesian_2d(0usize..max_step, 0f32..105f32)?;\n\n        chart\n            .configure_mesh()\n            .x_desc(\"Training Step\")\n            .y_desc(\"Accuracy (%)\")\n            .draw()?;\n\n        chart\n            .draw_series(LineSeries::new(\n                train_acc.iter().map(|(s, a)| (*s, a * 100.0)),\n                &BLUE,\n            ))?\n            .label(\"Training\")\n            .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &BLUE));\n\n        chart\n            .draw_series(LineSeries::new(\n                val_acc.iter().map(|(s, a)| (*s, a * 100.0)),\n                &RED,\n            ))?\n            .label(\"Validation\")\n            .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &RED));\n\n        chart\n            .configure_series_labels()\n            .background_style(&WHITE.mix(0.8))\n            .border_style(&BLACK)\n            .draw()?;\n    }\n\n    // Lower plot: Loss (train and val)\n    {\n        let max_step = train_loss\n            .iter()\n            .chain(val_loss.iter())\n            .map(|(s, _)| *s)\n            .max()\n            .unwrap_or(1);\n\n        let max_loss = train_loss\n            .iter()\n            .chain(val_loss.iter())\n            .map(|(_, l)| *l)\n            .fold(0.0f64, f64::max)\n            .max(1.0);\n\n        let mut chart = ChartBuilder::on(&lower)\n            .caption(\"Loss Evolution\", (\"sans-serif\", 40).into_font())\n            .margin(15)\n            .x_label_area_size(40)\n            .y_label_area_size(60)\n            .build_cartesian_2d(0usize..max_step, 0f64..max_loss)?;\n\n        chart\n            .configure_mesh()\n            .x_desc(\"Training Step\")\n            .y_desc(\"Loss\")\n            .draw()?;\n\n        // Train loss\n        chart\n            .draw_series(LineSeries::new(\n                train_loss.iter().map(|(s, l)| (*s, *l)),\n                &RED,\n            ))?\n            .label(\"Train\")\n            .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &RED));\n\n        // Val loss\n        chart\n            .draw_series(LineSeries::new(\n                val_loss.iter().map(|(s, l)| (*s, *l)),\n                &GREEN,\n            ))?\n            .label(\"Val\")\n            .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &GREEN));\n\n        // Chance level\n        let modulus = ModularAdditionDataset::modulus() as f64;\n        let chance_loss = -(1.0 / modulus).ln();\n        chart.draw_series(LineSeries::new(\n            vec![(0, chance_loss), (max_step, chance_loss)],\n            &BLUE.mix(0.5),\n        ))?;\n\n        chart\n            .configure_series_labels()\n            .background_style(&WHITE.mix(0.8))\n            .border_style(&BLACK)\n            .draw()?;\n    }\n\n    root.present()?;\n    println!(\"ðŸ“Š Combined grokking plot saved to: {}\", output_path);\n    Ok(())\n}\n\n/// Plot training and validation loss over time with LOG-SCALE x-axis\npub fn plot_loss_history_dual_logscale(\n    train_loss: &[(usize, f64)], // (step, loss)\n    val_loss: &[(usize, f64)],\n    output_path: &str,\n) -> Result<(), Box<dyn std::error::Error>> {\n    let root = BitMapBackend::new(output_path, (1200, 800)).into_drawing_area();\n    root.fill(&WHITE)?;\n\n    let max_step = train_loss\n        .iter()\n        .chain(val_loss.iter())\n        .map(|(s, _)| *s)\n        .max()\n        .unwrap_or(1);\n\n    let max_loss = train_loss\n        .iter()\n        .chain(val_loss.iter())\n        .map(|(_, l)| *l)\n        .fold(0.0f64, f64::max)\n        .max(1.0);\n\n    let mut chart = ChartBuilder::on(&root)\n        .caption(\"Loss Evolution (Log Scale): Train vs Validation\", (\"sans-serif\", 50).into_font())\n        .margin(20)\n        .x_label_area_size(50)\n        .y_label_area_size(70)\n        .build_cartesian_2d((1.0..max_step as f64).log_scale(), 0f64..max_loss)?;\n\n    chart\n        .configure_mesh()\n        .x_desc(\"Training Step (log scale)\")\n        .y_desc(\"Loss\")\n        .draw()?;\n\n    // Plot training loss\n    chart\n        .draw_series(LineSeries::new(\n            train_loss.iter().map(|(s, l)| (*s as f64, *l)),\n            &RED,\n        ))?\n        .label(\"Train Loss\")\n        .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &RED));\n\n    // Plot validation loss\n    chart\n        .draw_series(LineSeries::new(\n            val_loss.iter().map(|(s, l)| (*s as f64, *l)),\n            &GREEN,\n        ))?\n        .label(\"Val Loss\")\n        .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &GREEN));\n\n    let modulus = ModularAdditionDataset::modulus() as f64;\n    let chance_loss = -(1.0 / modulus).ln();\n    chart\n        .draw_series(LineSeries::new(\n            vec![(1.0, chance_loss), (max_step as f64, chance_loss)],\n            &BLUE.mix(0.5),\n        ))?\n        .label(\"Chance Level\")\n        .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &BLUE.mix(0.5)));\n\n    chart\n        .configure_series_labels()\n        .background_style(&WHITE.mix(0.8))\n        .border_style(&BLACK)\n        .draw()?;\n\n    root.present()?;\n    println!(\"ðŸ“Š Loss plot (log scale) saved to: {}\", output_path);\n    Ok(())\n}\n\n/// Plot training and validation accuracy over time with LOG-SCALE x-axis\npub fn plot_accuracy_history_logscale(\n    train_acc: &[(usize, f32)], // (step, accuracy)\n    val_acc: &[(usize, f32)],\n    output_path: &str,\n) -> Result<(), Box<dyn std::error::Error>> {\n    let root = BitMapBackend::new(output_path, (1200, 800)).into_drawing_area();\n    root.fill(&WHITE)?;\n\n    let max_step = train_acc\n        .iter()\n        .chain(val_acc.iter())\n        .map(|(s, _)| *s)\n        .max()\n        .unwrap_or(1);\n\n    let mut chart = ChartBuilder::on(&root)\n        .caption(\n            \"Grokking Phenomenon (Log Scale): Train vs Validation Accuracy\",\n            (\"sans-serif\", 50).into_font(),\n        )\n        .margin(20)\n        .x_label_area_size(50)\n        .y_label_area_size(70)\n        .build_cartesian_2d((1.0..max_step as f64).log_scale(), 0f32..105f32)?;\n\n    chart\n        .configure_mesh()\n        .x_desc(\"Training Step (log scale)\")\n        .y_desc(\"Accuracy (%)\")\n        .draw()?;\n\n    // Plot training accuracy\n    chart\n        .draw_series(LineSeries::new(\n            train_acc.iter().map(|(s, a)| (*s as f64, a * 100.0)),\n            &BLUE,\n        ))?\n        .label(\"Training Accuracy\")\n        .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &BLUE));\n\n    // Plot validation accuracy\n    chart\n        .draw_series(LineSeries::new(\n            val_acc.iter().map(|(s, a)| (*s as f64, a * 100.0)),\n            &RED,\n        ))?\n        .label(\"Validation Accuracy\")\n        .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &RED));\n\n    // Add reference line at 90% (grokking threshold)\n    chart.draw_series(LineSeries::new(\n        vec![(1.0, 90.0), (max_step as f64, 90.0)],\n        &GREEN.mix(0.5),\n    ))?;\n\n    chart\n        .configure_series_labels()\n        .background_style(&WHITE.mix(0.8))\n        .border_style(&BLACK)\n        .draw()?;\n\n    root.present()?;\n    println!(\"ðŸ“Š Accuracy plot (log scale) saved to: {}\", output_path);\n    Ok(())\n}\n\n/// Plot combined grokking visualization with LOG-SCALE x-axis (loss + accuracy)\npub fn plot_grokking_combined_logscale(\n    train_loss: &[(usize, f64)],\n    val_loss: &[(usize, f64)],\n    train_acc: &[(usize, f32)],\n    val_acc: &[(usize, f32)],\n    output_path: &str,\n) -> Result<(), Box<dyn std::error::Error>> {\n    let root = BitMapBackend::new(output_path, (1400, 900)).into_drawing_area();\n    root.fill(&WHITE)?;\n\n    let (upper, lower) = root.split_vertically(450);\n\n    // Upper plot: Accuracy (log scale)\n    {\n        let max_step = train_acc\n            .iter()\n            .chain(val_acc.iter())\n            .map(|(s, _)| *s)\n            .max()\n            .unwrap_or(1);\n\n        let mut chart = ChartBuilder::on(&upper)\n            .caption(\n                \"Grokking: Accuracy Evolution (Log Scale)\",\n                (\"sans-serif\", 40).into_font(),\n            )\n            .margin(15)\n            .x_label_area_size(40)\n            .y_label_area_size(60)\n            .build_cartesian_2d((1.0..max_step as f64).log_scale(), 0f32..105f32)?;\n\n        chart\n            .configure_mesh()\n            .x_desc(\"Training Step (log scale)\")\n            .y_desc(\"Accuracy (%)\")\n            .draw()?;\n\n        chart\n            .draw_series(LineSeries::new(\n                train_acc.iter().map(|(s, a)| (*s as f64, a * 100.0)),\n                &BLUE,\n            ))?\n            .label(\"Training\")\n            .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &BLUE));\n\n        chart\n            .draw_series(LineSeries::new(\n                val_acc.iter().map(|(s, a)| (*s as f64, a * 100.0)),\n                &RED,\n            ))?\n            .label(\"Validation\")\n            .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &RED));\n\n        chart\n            .configure_series_labels()\n            .background_style(&WHITE.mix(0.8))\n            .border_style(&BLACK)\n            .draw()?;\n    }\n\n    // Lower plot: Loss (log scale)\n    {\n        let max_step = train_loss\n            .iter()\n            .chain(val_loss.iter())\n            .map(|(s, _)| *s)\n            .max()\n            .unwrap_or(1);\n\n        let max_loss = train_loss\n            .iter()\n            .chain(val_loss.iter())\n            .map(|(_, l)| *l)\n            .fold(0.0f64, f64::max)\n            .max(1.0);\n\n        let mut chart = ChartBuilder::on(&lower)\n            .caption(\"Loss Evolution (Log Scale)\", (\"sans-serif\", 40).into_font())\n            .margin(15)\n            .x_label_area_size(40)\n            .y_label_area_size(60)\n            .build_cartesian_2d((1.0..max_step as f64).log_scale(), 0f64..max_loss)?;\n\n        chart\n            .configure_mesh()\n            .x_desc(\"Training Step (log scale)\")\n            .y_desc(\"Loss\")\n            .draw()?;\n\n        // Train loss\n        chart\n            .draw_series(LineSeries::new(\n                train_loss.iter().map(|(s, l)| (*s as f64, *l)),\n                &RED,\n            ))?\n            .label(\"Train\")\n            .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &RED));\n\n        // Val loss\n        chart\n            .draw_series(LineSeries::new(\n                val_loss.iter().map(|(s, l)| (*s as f64, *l)),\n                &GREEN,\n            ))?\n            .label(\"Val\")\n            .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &GREEN));\n\n        // Chance level\n        let modulus = ModularAdditionDataset::modulus() as f64;\n        let chance_loss = -(1.0 / modulus).ln();\n        chart.draw_series(LineSeries::new(\n            vec![(1.0, chance_loss), (max_step as f64, chance_loss)],\n            &BLUE.mix(0.5),\n        ))?;\n\n        chart\n            .configure_series_labels()\n            .background_style(&WHITE.mix(0.8))\n            .border_style(&BLACK)\n            .draw()?;\n    }\n\n    root.present()?;\n    println!(\"ðŸ“Š Combined grokking plot (log scale) saved to: {}\", output_path);\n    Ok(())\n}\n\n/// Plot 3Ã—3 embedding analysis grid (faster version)\n/// embeddings: [p, d] matrix where p is the number of tokens, d=embedding_dim\n/// dimensions: indices of 3 embedding dimensions to visualize\npub fn plot_embedding_grid_3x3(\n    embeddings: &[Vec<f64>],\n    dimensions: &[usize; 3],\n    output_path: &str,\n    title: &str,\n) -> Result<(), Box<dyn std::error::Error>> {\n    let p = embeddings.len(); // number of tokens\n    let point_size = 3; // slightly larger for visibility\n\n    // Create canvas for 3Ã—3 grid\n    let root = BitMapBackend::new(output_path, (1200, 1200)).into_drawing_area();\n    root.fill(&BLACK)?; // Black background\n\n    // Split into 3Ã—3 grid\n    let cell_areas = root.split_evenly((3, 3));\n\n    println!(\"   Rendering 3Ã—3 grid ({} cells)...\", 9);\n    for row in 0..3 {\n        for col in 0..3 {\n            let idx = row * 3 + col;\n            print!(\".\");\n            use std::io::Write;\n            std::io::stdout().flush().unwrap();\n            let area = &cell_areas[idx];\n\n            if col == 0 {\n                // First column: embedding dimension vs token index\n                let dim = dimensions[row];\n\n                // Get dimension values for all tokens\n                let values: Vec<(usize, f64)> = (0..p)\n                    .map(|token| (token, embeddings[token][dim]))\n                    .collect();\n\n                // Find min/max for y-axis\n                let min_val = values.iter().map(|(_, v)| *v).fold(f64::INFINITY, f64::min);\n                let max_val = values.iter().map(|(_, v)| *v).fold(f64::NEG_INFINITY, f64::max);\n                let margin = (max_val - min_val) * 0.1;\n\n                let mut chart = ChartBuilder::on(area)\n                    .margin(5)\n                    .build_cartesian_2d(\n                        0..p,\n                        (min_val - margin)..(max_val + margin),\n                    )?;\n\n                // Subtle gray crosshairs at origin\n                chart.configure_mesh()\n                    .disable_x_mesh()\n                    .disable_y_mesh()\n                    .x_labels(0)\n                    .y_labels(0)\n                    .axis_style(RGBAColor(50, 50, 50, 0.3))\n                    .draw()?;\n\n                // Plot points colored by token value\n                for (token, value) in values.iter() {\n                    let color = token_color(*token, p);\n                    chart.draw_series(std::iter::once(Circle::new(\n                        (*token, *value),\n                        point_size,\n                        color.filled(),\n                    )))?;\n                }\n            } else {\n                // Pairwise scatter plot\n                let dim_x = dimensions[col];\n                let dim_y = dimensions[row];\n\n                // Extract x and y values\n                let points: Vec<(f64, f64)> = (0..p)\n                    .map(|token| (embeddings[token][dim_x], embeddings[token][dim_y]))\n                    .collect();\n\n                // Find min/max for axes\n                let min_x = points.iter().map(|(x, _)| *x).fold(f64::INFINITY, f64::min);\n                let max_x = points.iter().map(|(x, _)| *x).fold(f64::NEG_INFINITY, f64::max);\n                let min_y = points.iter().map(|(_, y)| *y).fold(f64::INFINITY, f64::min);\n                let max_y = points.iter().map(|(_, y)| *y).fold(f64::NEG_INFINITY, f64::max);\n\n                let margin_x = (max_x - min_x) * 0.1;\n                let margin_y = (max_y - min_y) * 0.1;\n\n                let mut chart = ChartBuilder::on(area)\n                    .margin(5)\n                    .build_cartesian_2d(\n                        (min_x - margin_x)..(max_x + margin_x),\n                        (min_y - margin_y)..(max_y + margin_y),\n                    )?;\n\n                // Subtle gray crosshairs at origin\n                chart.configure_mesh()\n                    .disable_x_mesh()\n                    .disable_y_mesh()\n                    .x_labels(0)\n                    .y_labels(0)\n                    .axis_style(RGBAColor(50, 50, 50, 0.3))\n                    .draw()?;\n\n                // Plot points colored by token value\n                for token in 0..p {\n                    let color = token_color(token, p);\n                    let (x, y) = points[token];\n                    chart.draw_series(std::iter::once(Circle::new(\n                        (x, y),\n                        point_size,\n                        color.filled(),\n                    )))?;\n                }\n            }\n        }\n    }\n\n    println!(); // newline after progress dots\n    root.present()?;\n    println!(\"   ðŸ“Š Embedding grid plot saved to: {}\", output_path);\n    println!(\"      Title: {}\", title);\n    Ok(())\n}\n\n/// Select 7 interesting dimensions from embeddings based on variance\npub fn select_interesting_dimensions(embeddings: &[Vec<f64>], n: usize) -> Vec<usize> {\n    let d = embeddings[0].len(); // embedding dimension\n    let p = embeddings.len(); // number of tokens\n\n    // Compute variance for each dimension\n    let mut variances: Vec<(usize, f64)> = Vec::with_capacity(d);\n\n    for dim in 0..d {\n        // Compute mean\n        let mean: f64 = embeddings.iter().map(|emb| emb[dim]).sum::<f64>() / p as f64;\n\n        // Compute variance\n        let variance: f64 = embeddings\n            .iter()\n            .map(|emb| (emb[dim] - mean).powi(2))\n            .sum::<f64>()\n            / p as f64;\n\n        variances.push((dim, variance));\n    }\n\n    // Sort by variance (descending)\n    variances.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap_or(std::cmp::Ordering::Equal));\n\n    // Take top n dimensions\n    variances.iter().take(n).map(|(dim, _)| *dim).collect()\n}\n\n/// Convert viridis color to image::Rgb\nfn viridis_to_rgb(t: f64) -> Rgb<u8> {\n    let color = viridis_color(t);\n    Rgb([color.0, color.1, color.2])\n}\n\n/// Map token value to RGB color\nfn token_to_rgb(token_value: usize, p: usize) -> Rgb<u8> {\n    let t = token_value as f64 / (p - 1) as f64;\n    viridis_to_rgb(t)\n}\n\n/// Fast 7Ã—7 embedding grid using direct pixel manipulation\npub fn plot_embedding_grid_fast(\n    embeddings: &[Vec<f64>],\n    dimensions: &[usize; 7],\n    output_path: &str,\n    title: &str,\n) -> Result<(), Box<dyn std::error::Error>> {\n    let p = embeddings.len(); // number of tokens\n    let cell_size: usize = 300; // pixels per cell\n    let grid_size: usize = 7;\n    let total_size = cell_size * grid_size;\n\n    println!(\"   ðŸŽ¨ Creating {}Ã—{} pixel canvas...\", total_size, total_size);\n    let mut img: RgbImage = ImageBuffer::from_pixel(total_size as u32, total_size as u32, Rgb([0, 0, 0]));\n\n    println!(\"   ðŸ–Œï¸  Rendering {} cells...\", grid_size * grid_size);\n\n    for row in 0..grid_size as usize {\n        for col in 0..grid_size as usize {\n            let cell_idx = row * grid_size as usize + col;\n            if cell_idx % 10 == 0 {\n                print!(\".\");\n                use std::io::Write;\n                std::io::stdout().flush().unwrap();\n            }\n\n            let x_offset = col * cell_size as usize;\n            let y_offset = row * cell_size as usize;\n\n            if col == 0 {\n                // First column: dimension vs token index\n                let dim = dimensions[row];\n                render_dimension_plot(\n                    &mut img,\n                    embeddings,\n                    dim,\n                    p,\n                    x_offset,\n                    y_offset,\n                    cell_size,\n                );\n            } else {\n                // Pairwise scatter plot\n                let dim_x = dimensions[col];\n                let dim_y = dimensions[row];\n                render_scatter_plot(\n                    &mut img,\n                    embeddings,\n                    dim_x,\n                    dim_y,\n                    p,\n                    x_offset,\n                    y_offset,\n                    cell_size,\n                );\n            }\n        }\n    }\n\n    println!(); // newline after progress dots\n    println!(\"   ðŸ’¾ Saving image to {}...\", output_path);\n    img.save(output_path)?;\n    println!(\"   âœ… Embedding grid saved!\");\n    println!(\"      Title: {}\", title);\n    Ok(())\n}\n\n/// Render dimension vs index plot into a cell\nfn render_dimension_plot(\n    img: &mut RgbImage,\n    embeddings: &[Vec<f64>],\n    dim: usize,\n    p: usize,\n    x_offset: usize,\n    y_offset: usize,\n    cell_size: usize,\n) {\n    // Find min/max for this dimension\n    let values: Vec<f64> = (0..p).map(|token| embeddings[token][dim]).collect();\n    let min_val = values.iter().copied().fold(f64::INFINITY, f64::min);\n    let max_val = values.iter().copied().fold(f64::NEG_INFINITY, f64::max);\n    let range = max_val - min_val;\n\n    if range == 0.0 {\n        return; // Skip if no variation\n    }\n\n    let margin = 20;\n    let plot_width = cell_size - 2 * margin;\n    let plot_height = cell_size - 2 * margin;\n\n    // Draw subtle crosshairs\n    let mid_x = (x_offset + cell_size / 2) as f32;\n    let mid_y = (y_offset + cell_size / 2) as f32;\n    let gray = Rgb([30, 30, 30]);\n    draw_line_segment_mut(\n        img,\n        ((x_offset + margin) as f32, mid_y),\n        ((x_offset + cell_size - margin) as f32, mid_y),\n        gray,\n    );\n    draw_line_segment_mut(\n        img,\n        (mid_x, (y_offset + margin) as f32),\n        (mid_x, (y_offset + cell_size - margin) as f32),\n        gray,\n    );\n\n    // Plot points\n    for token in 0..p {\n        let x = x_offset + margin + (token * plot_width / p);\n        let normalized = (values[token] - min_val) / range;\n        let y = y_offset + cell_size - margin - (normalized * plot_height as f64) as usize;\n\n        let color = token_to_rgb(token, p);\n        draw_filled_circle_mut(img, (x as i32, y as i32), 2, color);\n    }\n}\n\n/// Render scatter plot into a cell\nfn render_scatter_plot(\n    img: &mut RgbImage,\n    embeddings: &[Vec<f64>],\n    dim_x: usize,\n    dim_y: usize,\n    p: usize,\n    x_offset: usize,\n    y_offset: usize,\n    cell_size: usize,\n) {\n    // Find min/max for both dimensions\n    let values_x: Vec<f64> = (0..p).map(|token| embeddings[token][dim_x]).collect();\n    let values_y: Vec<f64> = (0..p).map(|token| embeddings[token][dim_y]).collect();\n\n    let min_x = values_x.iter().copied().fold(f64::INFINITY, f64::min);\n    let max_x = values_x.iter().copied().fold(f64::NEG_INFINITY, f64::max);\n    let min_y = values_y.iter().copied().fold(f64::INFINITY, f64::min);\n    let max_y = values_y.iter().copied().fold(f64::NEG_INFINITY, f64::max);\n\n    let range_x = max_x - min_x;\n    let range_y = max_y - min_y;\n\n    if range_x == 0.0 || range_y == 0.0 {\n        return; // Skip if no variation\n    }\n\n    let margin = 20;\n    let plot_width = cell_size - 2 * margin;\n    let plot_height = cell_size - 2 * margin;\n\n    // Draw subtle crosshairs at origin\n    let mid_x = (x_offset + cell_size / 2) as f32;\n    let mid_y = (y_offset + cell_size / 2) as f32;\n    let gray = Rgb([30, 30, 30]);\n    draw_line_segment_mut(\n        img,\n        ((x_offset + margin) as f32, mid_y),\n        ((x_offset + cell_size - margin) as f32, mid_y),\n        gray,\n    );\n    draw_line_segment_mut(\n        img,\n        (mid_x, (y_offset + margin) as f32),\n        (mid_x, (y_offset + cell_size - margin) as f32),\n        gray,\n    );\n\n    // Plot points\n    for token in 0..p {\n        let norm_x = (values_x[token] - min_x) / range_x;\n        let norm_y = (values_y[token] - min_y) / range_y;\n\n        let x = x_offset + margin + (norm_x * plot_width as f64) as usize;\n        let y = y_offset + cell_size - margin - (norm_y * plot_height as f64) as usize;\n\n        let color = token_to_rgb(token, p);\n        draw_filled_circle_mut(img, (x as i32, y as i32), 2, color);\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn logscale_plotting_functions_exist() {\n        // Verify that log-scale plotting functions exist and have the correct signature\n        // This is a compile-time check that ensures the API is correct\n        let _: fn(&[(usize, f64)], &[(usize, f64)], &str) -> Result<(), Box<dyn std::error::Error>> =\n            plot_loss_history_dual_logscale;\n        let _: fn(&[(usize, f32)], &[(usize, f32)], &str) -> Result<(), Box<dyn std::error::Error>> =\n            plot_accuracy_history_logscale;\n        let _: fn(&[(usize, f64)], &[(usize, f64)], &[(usize, f32)], &[(usize, f32)], &str) -> Result<(), Box<dyn std::error::Error>> =\n            plot_grokking_combined_logscale;\n    }\n\n    #[test]\n    fn snake_curve_can_be_verified_with_plotting_data() {\n        use crate::verify::{verify_snake_curve_shape, SnakeCurveConfig};\n\n        // Simulate grokking data that would produce a snake curve on log-scale plot\n        let train_acc = vec![\n            (1, 0.10),\n            (10, 0.50),\n            (100, 0.90),\n            (500, 0.99),\n            (1000, 0.995),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (10, 0.01),\n            (100, 0.01),\n            (500, 0.01),\n            (1000, 0.01),\n            (2000, 0.95),\n        ];\n\n        let config = SnakeCurveConfig::default_for_modulus(113);\n        let report = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect(\"snake curve verification should pass on grokking data\");\n\n        // Verify the snake curve characteristics are present\n        assert!(report.train_converged_by_step < 1000, \"train should converge early\");\n        assert!(report.val_grok_step > 1000, \"val should grok late\");\n        assert!(report.curve_shape_valid, \"curve shape should be valid\");\n    }\n}\n","structuredPatch":[{"oldStart":102,"oldLines":6,"newStart":102,"newLines":8,"lines":[" }"," "," /// Plot training loss over time (legacy, single loss)","+/// NOTE: Superseded by plot_loss_history_dual - kept for reference","+#[allow(dead_code)]"," pub fn plot_loss_history(","     snapshots: &[(usize, f64)], // (step, loss)","     output_path: &str,"]}],"userModified":false,"replaceAll":false}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01MB3yssrATu2B3xSjkpi8hd","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_012uiRYuBy7PisKuShNdL74H","name":"Edit","input":{"replace_all":false,"file_path":"/Users/admin/MechInt/src/plotting.rs","old_string":"fn viridis_to_rgb(t: f64) -> Rgb<u8> {","new_string":"// Helper functions for alternative visualizations - reserved for future use\n#[allow(dead_code)]\nfn viridis_to_rgb(t: f64) -> Rgb<u8> {"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":382,"cache_read_input_tokens":65889,"cache_creation":{"ephemeral_5m_input_tokens":382,"ephemeral_1h_input_tokens":0},"output_tokens":187,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"26bc1bf7-1264-46f7-89fa-4f6747762a6d"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_012uiRYuBy7PisKuShNdL74H","type":"tool_result","content":"The file /Users/admin/MechInt/src/plotting.rs has been updated. Here's the result of running `cat -n` on a snippet of the edited file:\n   871â†’    variances.iter().take(n).map(|(dim, _)| *dim).collect()\n   872â†’}\n   873â†’\n   874â†’/// Convert viridis color to image::Rgb\n   875â†’// Helper functions for alternative visualizations - reserved for future use\n   876â†’#[allow(dead_code)]\n   877â†’fn viridis_to_rgb(t: f64) -> Rgb<u8> {\n   878â†’    let color = viridis_color(t);\n   879â†’    Rgb([color.0, color.1, color.2])\n   880â†’}\n   881â†’"}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"39917084-a7ca-4cfc-89c0-d1d4222fbf9a","tool_use_result":{"filePath":"/Users/admin/MechInt/src/plotting.rs","oldString":"fn viridis_to_rgb(t: f64) -> Rgb<u8> {","newString":"// Helper functions for alternative visualizations - reserved for future use\n#[allow(dead_code)]\nfn viridis_to_rgb(t: f64) -> Rgb<u8> {","originalFile":"use image::{ImageBuffer, Rgb, RgbImage};\nuse imageproc::drawing::{draw_filled_circle_mut, draw_line_segment_mut};\nuse plotters::prelude::*;\nuse std::collections::HashMap;\n\nuse crate::data::ModularAdditionDataset;\n\n/// Viridis colormap - maps value in [0, 1] to RGB\nfn viridis_color(t: f64) -> RGBColor {\n    // Viridis colormap approximation (purple â†’ cyan â†’ yellow)\n    // Based on matplotlib's viridis\n    let r = (0.267 + t * (0.329 - 0.267 + t * (0.984 - 0.329))) * 255.0;\n    let g = (0.005 + t * (0.569 - 0.005 + t * (0.906 - 0.569))) * 255.0;\n    let b = (0.329 + t * (0.758 - 0.329 - t * (0.758 - 0.121))) * 255.0;\n\n    RGBColor(\n        r.min(255.0).max(0.0) as u8,\n        g.min(255.0).max(0.0) as u8,\n        b.min(255.0).max(0.0) as u8,\n    )\n}\n\n/// Map token value (0 to p-1) to viridis color\nfn token_color(token_value: usize, p: usize) -> RGBColor {\n    let t = token_value as f64 / (p - 1) as f64;\n    viridis_color(t)\n}\n\n/// Plot training and validation loss over time\npub fn plot_loss_history_dual(\n    train_loss: &[(usize, f64)], // (step, loss)\n    val_loss: &[(usize, f64)],\n    output_path: &str,\n) -> Result<(), Box<dyn std::error::Error>> {\n    let root = BitMapBackend::new(output_path, (1200, 800)).into_drawing_area();\n    root.fill(&WHITE)?;\n\n    let max_step = train_loss\n        .iter()\n        .chain(val_loss.iter())\n        .map(|(s, _)| *s)\n        .max()\n        .unwrap_or(1);\n\n    let max_loss = train_loss\n        .iter()\n        .chain(val_loss.iter())\n        .map(|(_, l)| *l)\n        .fold(0.0f64, f64::max)\n        .max(1.0);\n\n    let mut chart = ChartBuilder::on(&root)\n        .caption(\"Loss Evolution: Train vs Validation\", (\"sans-serif\", 50).into_font())\n        .margin(20)\n        .x_label_area_size(50)\n        .y_label_area_size(70)\n        .build_cartesian_2d(0usize..max_step, 0f64..max_loss)?;\n\n    chart\n        .configure_mesh()\n        .x_desc(\"Training Step\")\n        .y_desc(\"Loss\")\n        .draw()?;\n\n    // Plot training loss\n    chart\n        .draw_series(LineSeries::new(\n            train_loss.iter().map(|(s, l)| (*s, *l)),\n            &RED,\n        ))?\n        .label(\"Train Loss\")\n        .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &RED));\n\n    // Plot validation loss\n    chart\n        .draw_series(LineSeries::new(\n            val_loss.iter().map(|(s, l)| (*s, *l)),\n            &GREEN,\n        ))?\n        .label(\"Val Loss\")\n        .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &GREEN));\n\n    let modulus = ModularAdditionDataset::modulus() as f64;\n    let chance_loss = -(1.0 / modulus).ln();\n    chart\n        .draw_series(LineSeries::new(\n            vec![(0, chance_loss), (max_step, chance_loss)],\n            &BLUE.mix(0.5),\n        ))?\n        .label(\"Chance Level\")\n        .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &BLUE.mix(0.5)));\n\n    chart\n        .configure_series_labels()\n        .background_style(&WHITE.mix(0.8))\n        .border_style(&BLACK)\n        .draw()?;\n\n    root.present()?;\n    println!(\"ðŸ“Š Loss plot saved to: {}\", output_path);\n    Ok(())\n}\n\n/// Plot training loss over time (legacy, single loss)\n/// NOTE: Superseded by plot_loss_history_dual - kept for reference\n#[allow(dead_code)]\npub fn plot_loss_history(\n    snapshots: &[(usize, f64)], // (step, loss)\n    output_path: &str,\n) -> Result<(), Box<dyn std::error::Error>> {\n    let root = BitMapBackend::new(output_path, (1200, 800)).into_drawing_area();\n    root.fill(&WHITE)?;\n\n    let max_step = snapshots.iter().map(|(s, _)| *s).max().unwrap_or(1);\n    let max_loss = snapshots\n        .iter()\n        .map(|(_, l)| *l)\n        .fold(0.0f64, f64::max)\n        .max(1.0);\n\n    let mut chart = ChartBuilder::on(&root)\n        .caption(\"Training Loss Evolution\", (\"sans-serif\", 50).into_font())\n        .margin(20)\n        .x_label_area_size(50)\n        .y_label_area_size(70)\n        .build_cartesian_2d(0usize..max_step, 0f64..max_loss)?;\n\n    chart\n        .configure_mesh()\n        .x_desc(\"Training Step\")\n        .y_desc(\"Loss\")\n        .draw()?;\n\n    chart.draw_series(LineSeries::new(\n        snapshots.iter().map(|(s, l)| (*s, *l)),\n        &BLUE,\n    ))?;\n\n    root.present()?;\n    println!(\"ðŸ“Š Loss plot saved to: {}\", output_path);\n    Ok(())\n}\n\n/// Plot training and validation accuracy over time (combined)\npub fn plot_accuracy_history(\n    train_acc: &[(usize, f32)], // (step, accuracy)\n    val_acc: &[(usize, f32)],\n    output_path: &str,\n) -> Result<(), Box<dyn std::error::Error>> {\n    let root = BitMapBackend::new(output_path, (1200, 800)).into_drawing_area();\n    root.fill(&WHITE)?;\n\n    let max_step = train_acc\n        .iter()\n        .chain(val_acc.iter())\n        .map(|(s, _)| *s)\n        .max()\n        .unwrap_or(1);\n\n    let mut chart = ChartBuilder::on(&root)\n        .caption(\n            \"Grokking Phenomenon: Train vs Validation Accuracy\",\n            (\"sans-serif\", 50).into_font(),\n        )\n        .margin(20)\n        .x_label_area_size(50)\n        .y_label_area_size(70)\n        .build_cartesian_2d(0usize..max_step, 0f32..105f32)?;\n\n    chart\n        .configure_mesh()\n        .x_desc(\"Training Step\")\n        .y_desc(\"Accuracy (%)\")\n        .draw()?;\n\n    // Plot training accuracy\n    chart\n        .draw_series(LineSeries::new(\n            train_acc.iter().map(|(s, a)| (*s, a * 100.0)),\n            &BLUE,\n        ))?\n        .label(\"Training Accuracy\")\n        .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &BLUE));\n\n    // Plot validation accuracy\n    chart\n        .draw_series(LineSeries::new(\n            val_acc.iter().map(|(s, a)| (*s, a * 100.0)),\n            &RED,\n        ))?\n        .label(\"Validation Accuracy\")\n        .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &RED));\n\n    // Add reference line at 90% (grokking threshold)\n    chart.draw_series(LineSeries::new(\n        vec![(0, 90.0), (max_step, 90.0)],\n        &GREEN.mix(0.5),\n    ))?;\n\n    chart\n        .configure_series_labels()\n        .background_style(&WHITE.mix(0.8))\n        .border_style(&BLACK)\n        .draw()?;\n\n    root.present()?;\n    println!(\"ðŸ“Š Accuracy plot saved to: {}\", output_path);\n    Ok(())\n}\n\n/// Plot FFT dominant frequency distribution\npub fn plot_fft_frequency_distribution(\n    dominant_frequencies: &[(usize, usize, f64)], // (token_id, frequency, magnitude)\n    output_path: &str,\n) -> Result<(), Box<dyn std::error::Error>> {\n    let root = BitMapBackend::new(output_path, (1200, 800)).into_drawing_area();\n    root.fill(&WHITE)?;\n\n    // Count frequency occurrences\n    let mut freq_counts: HashMap<usize, usize> = HashMap::new();\n    for (_, freq, _) in dominant_frequencies {\n        *freq_counts.entry(*freq).or_default() += 1;\n    }\n\n    let mut freq_vec: Vec<(usize, usize)> = freq_counts.into_iter().collect();\n    freq_vec.sort_by_key(|(f, _)| *f);\n\n    let max_freq = freq_vec.iter().map(|(f, _)| *f).max().unwrap_or(1);\n    let max_count = freq_vec.iter().map(|(_, c)| *c).max().unwrap_or(1);\n\n    let mut chart = ChartBuilder::on(&root)\n        .caption(\n            \"FFT Dominant Frequency Distribution\",\n            (\"sans-serif\", 50).into_font(),\n        )\n        .margin(20)\n        .x_label_area_size(50)\n        .y_label_area_size(70)\n        .build_cartesian_2d(0usize..max_freq, 0usize..max_count + 1)?;\n\n    chart\n        .configure_mesh()\n        .x_desc(\"Frequency\")\n        .y_desc(\"Number of Tokens\")\n        .draw()?;\n\n    chart.draw_series(\n        freq_vec\n            .iter()\n            .map(|(freq, count)| {\n                let color = if *freq == ModularAdditionDataset::modulus() {\n                    RED.filled() // Highlight frequency matching modulus\n                } else {\n                    BLUE.filled()\n                };\n                Rectangle::new([(*freq, 0), (*freq, *count)], color)\n            }),\n    )?;\n\n    root.present()?;\n    println!(\"ðŸ“Š FFT frequency distribution plot saved to: {}\", output_path);\n    Ok(())\n}\n\n/// Plot FFT magnitude spectrum for selected tokens\npub fn plot_fft_spectra(\n    token_spectra: &[(usize, Vec<f64>)], // (token_id, magnitude_spectrum)\n    output_path: &str,\n) -> Result<(), Box<dyn std::error::Error>> {\n    let root = BitMapBackend::new(output_path, (1200, 800)).into_drawing_area();\n    root.fill(&WHITE)?;\n\n    let max_freq = token_spectra\n        .first()\n        .map(|(_, spec)| spec.len())\n        .unwrap_or(1);\n    let max_magnitude = token_spectra\n        .iter()\n        .flat_map(|(_, spec)| spec.iter())\n        .copied()\n        .fold(0.0f64, f64::max)\n        .max(1.0);\n\n    let mut chart = ChartBuilder::on(&root)\n        .caption(\n            \"FFT Magnitude Spectra (Selected Tokens)\",\n            (\"sans-serif\", 50).into_font(),\n        )\n        .margin(20)\n        .x_label_area_size(50)\n        .y_label_area_size(70)\n        .build_cartesian_2d(0usize..max_freq, 0f64..max_magnitude)?;\n\n    chart\n        .configure_mesh()\n        .x_desc(\"Frequency\")\n        .y_desc(\"Magnitude\")\n        .draw()?;\n\n    let colors = [&BLUE, &RED, &GREEN, &MAGENTA, &CYAN, &YELLOW];\n\n    for (idx, (token_id, spectrum)) in token_spectra.iter().enumerate() {\n        let color = colors[idx % colors.len()];\n        chart\n            .draw_series(LineSeries::new(\n                spectrum.iter().enumerate().map(|(f, m)| (f, *m)),\n                color,\n            ))?\n            .label(format!(\"Token {}\", token_id))\n            .legend(move |(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], color));\n    }\n\n    chart\n        .configure_series_labels()\n        .background_style(&WHITE.mix(0.8))\n        .border_style(&BLACK)\n        .draw()?;\n\n    root.present()?;\n    println!(\"ðŸ“Š FFT spectra plot saved to: {}\", output_path);\n    Ok(())\n}\n\n/// Plot combined grokking visualization (loss + accuracy on same timeline)\npub fn plot_grokking_combined(\n    train_loss: &[(usize, f64)],\n    val_loss: &[(usize, f64)],\n    train_acc: &[(usize, f32)],\n    val_acc: &[(usize, f32)],\n    output_path: &str,\n) -> Result<(), Box<dyn std::error::Error>> {\n    let root = BitMapBackend::new(output_path, (1400, 900)).into_drawing_area();\n    root.fill(&WHITE)?;\n\n    let (upper, lower) = root.split_vertically(450);\n\n    // Upper plot: Accuracy\n    {\n        let max_step = train_acc\n            .iter()\n            .chain(val_acc.iter())\n            .map(|(s, _)| *s)\n            .max()\n            .unwrap_or(1);\n\n        let mut chart = ChartBuilder::on(&upper)\n            .caption(\n                \"Grokking: Accuracy Evolution\",\n                (\"sans-serif\", 40).into_font(),\n            )\n            .margin(15)\n            .x_label_area_size(40)\n            .y_label_area_size(60)\n            .build_cartesian_2d(0usize..max_step, 0f32..105f32)?;\n\n        chart\n            .configure_mesh()\n            .x_desc(\"Training Step\")\n            .y_desc(\"Accuracy (%)\")\n            .draw()?;\n\n        chart\n            .draw_series(LineSeries::new(\n                train_acc.iter().map(|(s, a)| (*s, a * 100.0)),\n                &BLUE,\n            ))?\n            .label(\"Training\")\n            .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &BLUE));\n\n        chart\n            .draw_series(LineSeries::new(\n                val_acc.iter().map(|(s, a)| (*s, a * 100.0)),\n                &RED,\n            ))?\n            .label(\"Validation\")\n            .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &RED));\n\n        chart\n            .configure_series_labels()\n            .background_style(&WHITE.mix(0.8))\n            .border_style(&BLACK)\n            .draw()?;\n    }\n\n    // Lower plot: Loss (train and val)\n    {\n        let max_step = train_loss\n            .iter()\n            .chain(val_loss.iter())\n            .map(|(s, _)| *s)\n            .max()\n            .unwrap_or(1);\n\n        let max_loss = train_loss\n            .iter()\n            .chain(val_loss.iter())\n            .map(|(_, l)| *l)\n            .fold(0.0f64, f64::max)\n            .max(1.0);\n\n        let mut chart = ChartBuilder::on(&lower)\n            .caption(\"Loss Evolution\", (\"sans-serif\", 40).into_font())\n            .margin(15)\n            .x_label_area_size(40)\n            .y_label_area_size(60)\n            .build_cartesian_2d(0usize..max_step, 0f64..max_loss)?;\n\n        chart\n            .configure_mesh()\n            .x_desc(\"Training Step\")\n            .y_desc(\"Loss\")\n            .draw()?;\n\n        // Train loss\n        chart\n            .draw_series(LineSeries::new(\n                train_loss.iter().map(|(s, l)| (*s, *l)),\n                &RED,\n            ))?\n            .label(\"Train\")\n            .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &RED));\n\n        // Val loss\n        chart\n            .draw_series(LineSeries::new(\n                val_loss.iter().map(|(s, l)| (*s, *l)),\n                &GREEN,\n            ))?\n            .label(\"Val\")\n            .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &GREEN));\n\n        // Chance level\n        let modulus = ModularAdditionDataset::modulus() as f64;\n        let chance_loss = -(1.0 / modulus).ln();\n        chart.draw_series(LineSeries::new(\n            vec![(0, chance_loss), (max_step, chance_loss)],\n            &BLUE.mix(0.5),\n        ))?;\n\n        chart\n            .configure_series_labels()\n            .background_style(&WHITE.mix(0.8))\n            .border_style(&BLACK)\n            .draw()?;\n    }\n\n    root.present()?;\n    println!(\"ðŸ“Š Combined grokking plot saved to: {}\", output_path);\n    Ok(())\n}\n\n/// Plot training and validation loss over time with LOG-SCALE x-axis\npub fn plot_loss_history_dual_logscale(\n    train_loss: &[(usize, f64)], // (step, loss)\n    val_loss: &[(usize, f64)],\n    output_path: &str,\n) -> Result<(), Box<dyn std::error::Error>> {\n    let root = BitMapBackend::new(output_path, (1200, 800)).into_drawing_area();\n    root.fill(&WHITE)?;\n\n    let max_step = train_loss\n        .iter()\n        .chain(val_loss.iter())\n        .map(|(s, _)| *s)\n        .max()\n        .unwrap_or(1);\n\n    let max_loss = train_loss\n        .iter()\n        .chain(val_loss.iter())\n        .map(|(_, l)| *l)\n        .fold(0.0f64, f64::max)\n        .max(1.0);\n\n    let mut chart = ChartBuilder::on(&root)\n        .caption(\"Loss Evolution (Log Scale): Train vs Validation\", (\"sans-serif\", 50).into_font())\n        .margin(20)\n        .x_label_area_size(50)\n        .y_label_area_size(70)\n        .build_cartesian_2d((1.0..max_step as f64).log_scale(), 0f64..max_loss)?;\n\n    chart\n        .configure_mesh()\n        .x_desc(\"Training Step (log scale)\")\n        .y_desc(\"Loss\")\n        .draw()?;\n\n    // Plot training loss\n    chart\n        .draw_series(LineSeries::new(\n            train_loss.iter().map(|(s, l)| (*s as f64, *l)),\n            &RED,\n        ))?\n        .label(\"Train Loss\")\n        .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &RED));\n\n    // Plot validation loss\n    chart\n        .draw_series(LineSeries::new(\n            val_loss.iter().map(|(s, l)| (*s as f64, *l)),\n            &GREEN,\n        ))?\n        .label(\"Val Loss\")\n        .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &GREEN));\n\n    let modulus = ModularAdditionDataset::modulus() as f64;\n    let chance_loss = -(1.0 / modulus).ln();\n    chart\n        .draw_series(LineSeries::new(\n            vec![(1.0, chance_loss), (max_step as f64, chance_loss)],\n            &BLUE.mix(0.5),\n        ))?\n        .label(\"Chance Level\")\n        .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &BLUE.mix(0.5)));\n\n    chart\n        .configure_series_labels()\n        .background_style(&WHITE.mix(0.8))\n        .border_style(&BLACK)\n        .draw()?;\n\n    root.present()?;\n    println!(\"ðŸ“Š Loss plot (log scale) saved to: {}\", output_path);\n    Ok(())\n}\n\n/// Plot training and validation accuracy over time with LOG-SCALE x-axis\npub fn plot_accuracy_history_logscale(\n    train_acc: &[(usize, f32)], // (step, accuracy)\n    val_acc: &[(usize, f32)],\n    output_path: &str,\n) -> Result<(), Box<dyn std::error::Error>> {\n    let root = BitMapBackend::new(output_path, (1200, 800)).into_drawing_area();\n    root.fill(&WHITE)?;\n\n    let max_step = train_acc\n        .iter()\n        .chain(val_acc.iter())\n        .map(|(s, _)| *s)\n        .max()\n        .unwrap_or(1);\n\n    let mut chart = ChartBuilder::on(&root)\n        .caption(\n            \"Grokking Phenomenon (Log Scale): Train vs Validation Accuracy\",\n            (\"sans-serif\", 50).into_font(),\n        )\n        .margin(20)\n        .x_label_area_size(50)\n        .y_label_area_size(70)\n        .build_cartesian_2d((1.0..max_step as f64).log_scale(), 0f32..105f32)?;\n\n    chart\n        .configure_mesh()\n        .x_desc(\"Training Step (log scale)\")\n        .y_desc(\"Accuracy (%)\")\n        .draw()?;\n\n    // Plot training accuracy\n    chart\n        .draw_series(LineSeries::new(\n            train_acc.iter().map(|(s, a)| (*s as f64, a * 100.0)),\n            &BLUE,\n        ))?\n        .label(\"Training Accuracy\")\n        .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &BLUE));\n\n    // Plot validation accuracy\n    chart\n        .draw_series(LineSeries::new(\n            val_acc.iter().map(|(s, a)| (*s as f64, a * 100.0)),\n            &RED,\n        ))?\n        .label(\"Validation Accuracy\")\n        .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &RED));\n\n    // Add reference line at 90% (grokking threshold)\n    chart.draw_series(LineSeries::new(\n        vec![(1.0, 90.0), (max_step as f64, 90.0)],\n        &GREEN.mix(0.5),\n    ))?;\n\n    chart\n        .configure_series_labels()\n        .background_style(&WHITE.mix(0.8))\n        .border_style(&BLACK)\n        .draw()?;\n\n    root.present()?;\n    println!(\"ðŸ“Š Accuracy plot (log scale) saved to: {}\", output_path);\n    Ok(())\n}\n\n/// Plot combined grokking visualization with LOG-SCALE x-axis (loss + accuracy)\npub fn plot_grokking_combined_logscale(\n    train_loss: &[(usize, f64)],\n    val_loss: &[(usize, f64)],\n    train_acc: &[(usize, f32)],\n    val_acc: &[(usize, f32)],\n    output_path: &str,\n) -> Result<(), Box<dyn std::error::Error>> {\n    let root = BitMapBackend::new(output_path, (1400, 900)).into_drawing_area();\n    root.fill(&WHITE)?;\n\n    let (upper, lower) = root.split_vertically(450);\n\n    // Upper plot: Accuracy (log scale)\n    {\n        let max_step = train_acc\n            .iter()\n            .chain(val_acc.iter())\n            .map(|(s, _)| *s)\n            .max()\n            .unwrap_or(1);\n\n        let mut chart = ChartBuilder::on(&upper)\n            .caption(\n                \"Grokking: Accuracy Evolution (Log Scale)\",\n                (\"sans-serif\", 40).into_font(),\n            )\n            .margin(15)\n            .x_label_area_size(40)\n            .y_label_area_size(60)\n            .build_cartesian_2d((1.0..max_step as f64).log_scale(), 0f32..105f32)?;\n\n        chart\n            .configure_mesh()\n            .x_desc(\"Training Step (log scale)\")\n            .y_desc(\"Accuracy (%)\")\n            .draw()?;\n\n        chart\n            .draw_series(LineSeries::new(\n                train_acc.iter().map(|(s, a)| (*s as f64, a * 100.0)),\n                &BLUE,\n            ))?\n            .label(\"Training\")\n            .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &BLUE));\n\n        chart\n            .draw_series(LineSeries::new(\n                val_acc.iter().map(|(s, a)| (*s as f64, a * 100.0)),\n                &RED,\n            ))?\n            .label(\"Validation\")\n            .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &RED));\n\n        chart\n            .configure_series_labels()\n            .background_style(&WHITE.mix(0.8))\n            .border_style(&BLACK)\n            .draw()?;\n    }\n\n    // Lower plot: Loss (log scale)\n    {\n        let max_step = train_loss\n            .iter()\n            .chain(val_loss.iter())\n            .map(|(s, _)| *s)\n            .max()\n            .unwrap_or(1);\n\n        let max_loss = train_loss\n            .iter()\n            .chain(val_loss.iter())\n            .map(|(_, l)| *l)\n            .fold(0.0f64, f64::max)\n            .max(1.0);\n\n        let mut chart = ChartBuilder::on(&lower)\n            .caption(\"Loss Evolution (Log Scale)\", (\"sans-serif\", 40).into_font())\n            .margin(15)\n            .x_label_area_size(40)\n            .y_label_area_size(60)\n            .build_cartesian_2d((1.0..max_step as f64).log_scale(), 0f64..max_loss)?;\n\n        chart\n            .configure_mesh()\n            .x_desc(\"Training Step (log scale)\")\n            .y_desc(\"Loss\")\n            .draw()?;\n\n        // Train loss\n        chart\n            .draw_series(LineSeries::new(\n                train_loss.iter().map(|(s, l)| (*s as f64, *l)),\n                &RED,\n            ))?\n            .label(\"Train\")\n            .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &RED));\n\n        // Val loss\n        chart\n            .draw_series(LineSeries::new(\n                val_loss.iter().map(|(s, l)| (*s as f64, *l)),\n                &GREEN,\n            ))?\n            .label(\"Val\")\n            .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &GREEN));\n\n        // Chance level\n        let modulus = ModularAdditionDataset::modulus() as f64;\n        let chance_loss = -(1.0 / modulus).ln();\n        chart.draw_series(LineSeries::new(\n            vec![(1.0, chance_loss), (max_step as f64, chance_loss)],\n            &BLUE.mix(0.5),\n        ))?;\n\n        chart\n            .configure_series_labels()\n            .background_style(&WHITE.mix(0.8))\n            .border_style(&BLACK)\n            .draw()?;\n    }\n\n    root.present()?;\n    println!(\"ðŸ“Š Combined grokking plot (log scale) saved to: {}\", output_path);\n    Ok(())\n}\n\n/// Plot 3Ã—3 embedding analysis grid (faster version)\n/// embeddings: [p, d] matrix where p is the number of tokens, d=embedding_dim\n/// dimensions: indices of 3 embedding dimensions to visualize\npub fn plot_embedding_grid_3x3(\n    embeddings: &[Vec<f64>],\n    dimensions: &[usize; 3],\n    output_path: &str,\n    title: &str,\n) -> Result<(), Box<dyn std::error::Error>> {\n    let p = embeddings.len(); // number of tokens\n    let point_size = 3; // slightly larger for visibility\n\n    // Create canvas for 3Ã—3 grid\n    let root = BitMapBackend::new(output_path, (1200, 1200)).into_drawing_area();\n    root.fill(&BLACK)?; // Black background\n\n    // Split into 3Ã—3 grid\n    let cell_areas = root.split_evenly((3, 3));\n\n    println!(\"   Rendering 3Ã—3 grid ({} cells)...\", 9);\n    for row in 0..3 {\n        for col in 0..3 {\n            let idx = row * 3 + col;\n            print!(\".\");\n            use std::io::Write;\n            std::io::stdout().flush().unwrap();\n            let area = &cell_areas[idx];\n\n            if col == 0 {\n                // First column: embedding dimension vs token index\n                let dim = dimensions[row];\n\n                // Get dimension values for all tokens\n                let values: Vec<(usize, f64)> = (0..p)\n                    .map(|token| (token, embeddings[token][dim]))\n                    .collect();\n\n                // Find min/max for y-axis\n                let min_val = values.iter().map(|(_, v)| *v).fold(f64::INFINITY, f64::min);\n                let max_val = values.iter().map(|(_, v)| *v).fold(f64::NEG_INFINITY, f64::max);\n                let margin = (max_val - min_val) * 0.1;\n\n                let mut chart = ChartBuilder::on(area)\n                    .margin(5)\n                    .build_cartesian_2d(\n                        0..p,\n                        (min_val - margin)..(max_val + margin),\n                    )?;\n\n                // Subtle gray crosshairs at origin\n                chart.configure_mesh()\n                    .disable_x_mesh()\n                    .disable_y_mesh()\n                    .x_labels(0)\n                    .y_labels(0)\n                    .axis_style(RGBAColor(50, 50, 50, 0.3))\n                    .draw()?;\n\n                // Plot points colored by token value\n                for (token, value) in values.iter() {\n                    let color = token_color(*token, p);\n                    chart.draw_series(std::iter::once(Circle::new(\n                        (*token, *value),\n                        point_size,\n                        color.filled(),\n                    )))?;\n                }\n            } else {\n                // Pairwise scatter plot\n                let dim_x = dimensions[col];\n                let dim_y = dimensions[row];\n\n                // Extract x and y values\n                let points: Vec<(f64, f64)> = (0..p)\n                    .map(|token| (embeddings[token][dim_x], embeddings[token][dim_y]))\n                    .collect();\n\n                // Find min/max for axes\n                let min_x = points.iter().map(|(x, _)| *x).fold(f64::INFINITY, f64::min);\n                let max_x = points.iter().map(|(x, _)| *x).fold(f64::NEG_INFINITY, f64::max);\n                let min_y = points.iter().map(|(_, y)| *y).fold(f64::INFINITY, f64::min);\n                let max_y = points.iter().map(|(_, y)| *y).fold(f64::NEG_INFINITY, f64::max);\n\n                let margin_x = (max_x - min_x) * 0.1;\n                let margin_y = (max_y - min_y) * 0.1;\n\n                let mut chart = ChartBuilder::on(area)\n                    .margin(5)\n                    .build_cartesian_2d(\n                        (min_x - margin_x)..(max_x + margin_x),\n                        (min_y - margin_y)..(max_y + margin_y),\n                    )?;\n\n                // Subtle gray crosshairs at origin\n                chart.configure_mesh()\n                    .disable_x_mesh()\n                    .disable_y_mesh()\n                    .x_labels(0)\n                    .y_labels(0)\n                    .axis_style(RGBAColor(50, 50, 50, 0.3))\n                    .draw()?;\n\n                // Plot points colored by token value\n                for token in 0..p {\n                    let color = token_color(token, p);\n                    let (x, y) = points[token];\n                    chart.draw_series(std::iter::once(Circle::new(\n                        (x, y),\n                        point_size,\n                        color.filled(),\n                    )))?;\n                }\n            }\n        }\n    }\n\n    println!(); // newline after progress dots\n    root.present()?;\n    println!(\"   ðŸ“Š Embedding grid plot saved to: {}\", output_path);\n    println!(\"      Title: {}\", title);\n    Ok(())\n}\n\n/// Select 7 interesting dimensions from embeddings based on variance\npub fn select_interesting_dimensions(embeddings: &[Vec<f64>], n: usize) -> Vec<usize> {\n    let d = embeddings[0].len(); // embedding dimension\n    let p = embeddings.len(); // number of tokens\n\n    // Compute variance for each dimension\n    let mut variances: Vec<(usize, f64)> = Vec::with_capacity(d);\n\n    for dim in 0..d {\n        // Compute mean\n        let mean: f64 = embeddings.iter().map(|emb| emb[dim]).sum::<f64>() / p as f64;\n\n        // Compute variance\n        let variance: f64 = embeddings\n            .iter()\n            .map(|emb| (emb[dim] - mean).powi(2))\n            .sum::<f64>()\n            / p as f64;\n\n        variances.push((dim, variance));\n    }\n\n    // Sort by variance (descending)\n    variances.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap_or(std::cmp::Ordering::Equal));\n\n    // Take top n dimensions\n    variances.iter().take(n).map(|(dim, _)| *dim).collect()\n}\n\n/// Convert viridis color to image::Rgb\nfn viridis_to_rgb(t: f64) -> Rgb<u8> {\n    let color = viridis_color(t);\n    Rgb([color.0, color.1, color.2])\n}\n\n/// Map token value to RGB color\nfn token_to_rgb(token_value: usize, p: usize) -> Rgb<u8> {\n    let t = token_value as f64 / (p - 1) as f64;\n    viridis_to_rgb(t)\n}\n\n/// Fast 7Ã—7 embedding grid using direct pixel manipulation\npub fn plot_embedding_grid_fast(\n    embeddings: &[Vec<f64>],\n    dimensions: &[usize; 7],\n    output_path: &str,\n    title: &str,\n) -> Result<(), Box<dyn std::error::Error>> {\n    let p = embeddings.len(); // number of tokens\n    let cell_size: usize = 300; // pixels per cell\n    let grid_size: usize = 7;\n    let total_size = cell_size * grid_size;\n\n    println!(\"   ðŸŽ¨ Creating {}Ã—{} pixel canvas...\", total_size, total_size);\n    let mut img: RgbImage = ImageBuffer::from_pixel(total_size as u32, total_size as u32, Rgb([0, 0, 0]));\n\n    println!(\"   ðŸ–Œï¸  Rendering {} cells...\", grid_size * grid_size);\n\n    for row in 0..grid_size as usize {\n        for col in 0..grid_size as usize {\n            let cell_idx = row * grid_size as usize + col;\n            if cell_idx % 10 == 0 {\n                print!(\".\");\n                use std::io::Write;\n                std::io::stdout().flush().unwrap();\n            }\n\n            let x_offset = col * cell_size as usize;\n            let y_offset = row * cell_size as usize;\n\n            if col == 0 {\n                // First column: dimension vs token index\n                let dim = dimensions[row];\n                render_dimension_plot(\n                    &mut img,\n                    embeddings,\n                    dim,\n                    p,\n                    x_offset,\n                    y_offset,\n                    cell_size,\n                );\n            } else {\n                // Pairwise scatter plot\n                let dim_x = dimensions[col];\n                let dim_y = dimensions[row];\n                render_scatter_plot(\n                    &mut img,\n                    embeddings,\n                    dim_x,\n                    dim_y,\n                    p,\n                    x_offset,\n                    y_offset,\n                    cell_size,\n                );\n            }\n        }\n    }\n\n    println!(); // newline after progress dots\n    println!(\"   ðŸ’¾ Saving image to {}...\", output_path);\n    img.save(output_path)?;\n    println!(\"   âœ… Embedding grid saved!\");\n    println!(\"      Title: {}\", title);\n    Ok(())\n}\n\n/// Render dimension vs index plot into a cell\nfn render_dimension_plot(\n    img: &mut RgbImage,\n    embeddings: &[Vec<f64>],\n    dim: usize,\n    p: usize,\n    x_offset: usize,\n    y_offset: usize,\n    cell_size: usize,\n) {\n    // Find min/max for this dimension\n    let values: Vec<f64> = (0..p).map(|token| embeddings[token][dim]).collect();\n    let min_val = values.iter().copied().fold(f64::INFINITY, f64::min);\n    let max_val = values.iter().copied().fold(f64::NEG_INFINITY, f64::max);\n    let range = max_val - min_val;\n\n    if range == 0.0 {\n        return; // Skip if no variation\n    }\n\n    let margin = 20;\n    let plot_width = cell_size - 2 * margin;\n    let plot_height = cell_size - 2 * margin;\n\n    // Draw subtle crosshairs\n    let mid_x = (x_offset + cell_size / 2) as f32;\n    let mid_y = (y_offset + cell_size / 2) as f32;\n    let gray = Rgb([30, 30, 30]);\n    draw_line_segment_mut(\n        img,\n        ((x_offset + margin) as f32, mid_y),\n        ((x_offset + cell_size - margin) as f32, mid_y),\n        gray,\n    );\n    draw_line_segment_mut(\n        img,\n        (mid_x, (y_offset + margin) as f32),\n        (mid_x, (y_offset + cell_size - margin) as f32),\n        gray,\n    );\n\n    // Plot points\n    for token in 0..p {\n        let x = x_offset + margin + (token * plot_width / p);\n        let normalized = (values[token] - min_val) / range;\n        let y = y_offset + cell_size - margin - (normalized * plot_height as f64) as usize;\n\n        let color = token_to_rgb(token, p);\n        draw_filled_circle_mut(img, (x as i32, y as i32), 2, color);\n    }\n}\n\n/// Render scatter plot into a cell\nfn render_scatter_plot(\n    img: &mut RgbImage,\n    embeddings: &[Vec<f64>],\n    dim_x: usize,\n    dim_y: usize,\n    p: usize,\n    x_offset: usize,\n    y_offset: usize,\n    cell_size: usize,\n) {\n    // Find min/max for both dimensions\n    let values_x: Vec<f64> = (0..p).map(|token| embeddings[token][dim_x]).collect();\n    let values_y: Vec<f64> = (0..p).map(|token| embeddings[token][dim_y]).collect();\n\n    let min_x = values_x.iter().copied().fold(f64::INFINITY, f64::min);\n    let max_x = values_x.iter().copied().fold(f64::NEG_INFINITY, f64::max);\n    let min_y = values_y.iter().copied().fold(f64::INFINITY, f64::min);\n    let max_y = values_y.iter().copied().fold(f64::NEG_INFINITY, f64::max);\n\n    let range_x = max_x - min_x;\n    let range_y = max_y - min_y;\n\n    if range_x == 0.0 || range_y == 0.0 {\n        return; // Skip if no variation\n    }\n\n    let margin = 20;\n    let plot_width = cell_size - 2 * margin;\n    let plot_height = cell_size - 2 * margin;\n\n    // Draw subtle crosshairs at origin\n    let mid_x = (x_offset + cell_size / 2) as f32;\n    let mid_y = (y_offset + cell_size / 2) as f32;\n    let gray = Rgb([30, 30, 30]);\n    draw_line_segment_mut(\n        img,\n        ((x_offset + margin) as f32, mid_y),\n        ((x_offset + cell_size - margin) as f32, mid_y),\n        gray,\n    );\n    draw_line_segment_mut(\n        img,\n        (mid_x, (y_offset + margin) as f32),\n        (mid_x, (y_offset + cell_size - margin) as f32),\n        gray,\n    );\n\n    // Plot points\n    for token in 0..p {\n        let norm_x = (values_x[token] - min_x) / range_x;\n        let norm_y = (values_y[token] - min_y) / range_y;\n\n        let x = x_offset + margin + (norm_x * plot_width as f64) as usize;\n        let y = y_offset + cell_size - margin - (norm_y * plot_height as f64) as usize;\n\n        let color = token_to_rgb(token, p);\n        draw_filled_circle_mut(img, (x as i32, y as i32), 2, color);\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn logscale_plotting_functions_exist() {\n        // Verify that log-scale plotting functions exist and have the correct signature\n        // This is a compile-time check that ensures the API is correct\n        let _: fn(&[(usize, f64)], &[(usize, f64)], &str) -> Result<(), Box<dyn std::error::Error>> =\n            plot_loss_history_dual_logscale;\n        let _: fn(&[(usize, f32)], &[(usize, f32)], &str) -> Result<(), Box<dyn std::error::Error>> =\n            plot_accuracy_history_logscale;\n        let _: fn(&[(usize, f64)], &[(usize, f64)], &[(usize, f32)], &[(usize, f32)], &str) -> Result<(), Box<dyn std::error::Error>> =\n            plot_grokking_combined_logscale;\n    }\n\n    #[test]\n    fn snake_curve_can_be_verified_with_plotting_data() {\n        use crate::verify::{verify_snake_curve_shape, SnakeCurveConfig};\n\n        // Simulate grokking data that would produce a snake curve on log-scale plot\n        let train_acc = vec![\n            (1, 0.10),\n            (10, 0.50),\n            (100, 0.90),\n            (500, 0.99),\n            (1000, 0.995),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (10, 0.01),\n            (100, 0.01),\n            (500, 0.01),\n            (1000, 0.01),\n            (2000, 0.95),\n        ];\n\n        let config = SnakeCurveConfig::default_for_modulus(113);\n        let report = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect(\"snake curve verification should pass on grokking data\");\n\n        // Verify the snake curve characteristics are present\n        assert!(report.train_converged_by_step < 1000, \"train should converge early\");\n        assert!(report.val_grok_step > 1000, \"val should grok late\");\n        assert!(report.curve_shape_valid, \"curve shape should be valid\");\n    }\n}\n","structuredPatch":[{"oldStart":872,"oldLines":6,"newStart":872,"newLines":8,"lines":[" }"," "," /// Convert viridis color to image::Rgb","+// Helper functions for alternative visualizations - reserved for future use","+#[allow(dead_code)]"," fn viridis_to_rgb(t: f64) -> Rgb<u8> {","     let color = viridis_color(t);","     Rgb([color.0, color.1, color.2])"]}],"userModified":false,"replaceAll":false}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01BDEikz5vko9XF4a723CHGm","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_015y4BYZnLpeweDJsAYqCQb1","name":"Edit","input":{"replace_all":false,"file_path":"/Users/admin/MechInt/src/plotting.rs","old_string":"fn token_to_rgb(token_value: usize, p: usize) -> Rgb<u8> {","new_string":"#[allow(dead_code)]\nfn token_to_rgb(token_value: usize, p: usize) -> Rgb<u8> {"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":599,"cache_read_input_tokens":66271,"cache_creation":{"ephemeral_5m_input_tokens":599,"ephemeral_1h_input_tokens":0},"output_tokens":24,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"64183749-0e63-43aa-8dd6-d9d752ff92ba"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_015y4BYZnLpeweDJsAYqCQb1","type":"tool_result","content":"The file /Users/admin/MechInt/src/plotting.rs has been updated. Here's the result of running `cat -n` on a snippet of the edited file:\n   879â†’    Rgb([color.0, color.1, color.2])\n   880â†’}\n   881â†’\n   882â†’/// Map token value to RGB color\n   883â†’#[allow(dead_code)]\n   884â†’fn token_to_rgb(token_value: usize, p: usize) -> Rgb<u8> {\n   885â†’    let t = token_value as f64 / (p - 1) as f64;\n   886â†’    viridis_to_rgb(t)\n   887â†’}\n   888â†’"}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"7a809b07-90da-4deb-9080-b9db5c02158d","tool_use_result":{"filePath":"/Users/admin/MechInt/src/plotting.rs","oldString":"fn token_to_rgb(token_value: usize, p: usize) -> Rgb<u8> {","newString":"#[allow(dead_code)]\nfn token_to_rgb(token_value: usize, p: usize) -> Rgb<u8> {","originalFile":"use image::{ImageBuffer, Rgb, RgbImage};\nuse imageproc::drawing::{draw_filled_circle_mut, draw_line_segment_mut};\nuse plotters::prelude::*;\nuse std::collections::HashMap;\n\nuse crate::data::ModularAdditionDataset;\n\n/// Viridis colormap - maps value in [0, 1] to RGB\nfn viridis_color(t: f64) -> RGBColor {\n    // Viridis colormap approximation (purple â†’ cyan â†’ yellow)\n    // Based on matplotlib's viridis\n    let r = (0.267 + t * (0.329 - 0.267 + t * (0.984 - 0.329))) * 255.0;\n    let g = (0.005 + t * (0.569 - 0.005 + t * (0.906 - 0.569))) * 255.0;\n    let b = (0.329 + t * (0.758 - 0.329 - t * (0.758 - 0.121))) * 255.0;\n\n    RGBColor(\n        r.min(255.0).max(0.0) as u8,\n        g.min(255.0).max(0.0) as u8,\n        b.min(255.0).max(0.0) as u8,\n    )\n}\n\n/// Map token value (0 to p-1) to viridis color\nfn token_color(token_value: usize, p: usize) -> RGBColor {\n    let t = token_value as f64 / (p - 1) as f64;\n    viridis_color(t)\n}\n\n/// Plot training and validation loss over time\npub fn plot_loss_history_dual(\n    train_loss: &[(usize, f64)], // (step, loss)\n    val_loss: &[(usize, f64)],\n    output_path: &str,\n) -> Result<(), Box<dyn std::error::Error>> {\n    let root = BitMapBackend::new(output_path, (1200, 800)).into_drawing_area();\n    root.fill(&WHITE)?;\n\n    let max_step = train_loss\n        .iter()\n        .chain(val_loss.iter())\n        .map(|(s, _)| *s)\n        .max()\n        .unwrap_or(1);\n\n    let max_loss = train_loss\n        .iter()\n        .chain(val_loss.iter())\n        .map(|(_, l)| *l)\n        .fold(0.0f64, f64::max)\n        .max(1.0);\n\n    let mut chart = ChartBuilder::on(&root)\n        .caption(\"Loss Evolution: Train vs Validation\", (\"sans-serif\", 50).into_font())\n        .margin(20)\n        .x_label_area_size(50)\n        .y_label_area_size(70)\n        .build_cartesian_2d(0usize..max_step, 0f64..max_loss)?;\n\n    chart\n        .configure_mesh()\n        .x_desc(\"Training Step\")\n        .y_desc(\"Loss\")\n        .draw()?;\n\n    // Plot training loss\n    chart\n        .draw_series(LineSeries::new(\n            train_loss.iter().map(|(s, l)| (*s, *l)),\n            &RED,\n        ))?\n        .label(\"Train Loss\")\n        .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &RED));\n\n    // Plot validation loss\n    chart\n        .draw_series(LineSeries::new(\n            val_loss.iter().map(|(s, l)| (*s, *l)),\n            &GREEN,\n        ))?\n        .label(\"Val Loss\")\n        .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &GREEN));\n\n    let modulus = ModularAdditionDataset::modulus() as f64;\n    let chance_loss = -(1.0 / modulus).ln();\n    chart\n        .draw_series(LineSeries::new(\n            vec![(0, chance_loss), (max_step, chance_loss)],\n            &BLUE.mix(0.5),\n        ))?\n        .label(\"Chance Level\")\n        .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &BLUE.mix(0.5)));\n\n    chart\n        .configure_series_labels()\n        .background_style(&WHITE.mix(0.8))\n        .border_style(&BLACK)\n        .draw()?;\n\n    root.present()?;\n    println!(\"ðŸ“Š Loss plot saved to: {}\", output_path);\n    Ok(())\n}\n\n/// Plot training loss over time (legacy, single loss)\n/// NOTE: Superseded by plot_loss_history_dual - kept for reference\n#[allow(dead_code)]\npub fn plot_loss_history(\n    snapshots: &[(usize, f64)], // (step, loss)\n    output_path: &str,\n) -> Result<(), Box<dyn std::error::Error>> {\n    let root = BitMapBackend::new(output_path, (1200, 800)).into_drawing_area();\n    root.fill(&WHITE)?;\n\n    let max_step = snapshots.iter().map(|(s, _)| *s).max().unwrap_or(1);\n    let max_loss = snapshots\n        .iter()\n        .map(|(_, l)| *l)\n        .fold(0.0f64, f64::max)\n        .max(1.0);\n\n    let mut chart = ChartBuilder::on(&root)\n        .caption(\"Training Loss Evolution\", (\"sans-serif\", 50).into_font())\n        .margin(20)\n        .x_label_area_size(50)\n        .y_label_area_size(70)\n        .build_cartesian_2d(0usize..max_step, 0f64..max_loss)?;\n\n    chart\n        .configure_mesh()\n        .x_desc(\"Training Step\")\n        .y_desc(\"Loss\")\n        .draw()?;\n\n    chart.draw_series(LineSeries::new(\n        snapshots.iter().map(|(s, l)| (*s, *l)),\n        &BLUE,\n    ))?;\n\n    root.present()?;\n    println!(\"ðŸ“Š Loss plot saved to: {}\", output_path);\n    Ok(())\n}\n\n/// Plot training and validation accuracy over time (combined)\npub fn plot_accuracy_history(\n    train_acc: &[(usize, f32)], // (step, accuracy)\n    val_acc: &[(usize, f32)],\n    output_path: &str,\n) -> Result<(), Box<dyn std::error::Error>> {\n    let root = BitMapBackend::new(output_path, (1200, 800)).into_drawing_area();\n    root.fill(&WHITE)?;\n\n    let max_step = train_acc\n        .iter()\n        .chain(val_acc.iter())\n        .map(|(s, _)| *s)\n        .max()\n        .unwrap_or(1);\n\n    let mut chart = ChartBuilder::on(&root)\n        .caption(\n            \"Grokking Phenomenon: Train vs Validation Accuracy\",\n            (\"sans-serif\", 50).into_font(),\n        )\n        .margin(20)\n        .x_label_area_size(50)\n        .y_label_area_size(70)\n        .build_cartesian_2d(0usize..max_step, 0f32..105f32)?;\n\n    chart\n        .configure_mesh()\n        .x_desc(\"Training Step\")\n        .y_desc(\"Accuracy (%)\")\n        .draw()?;\n\n    // Plot training accuracy\n    chart\n        .draw_series(LineSeries::new(\n            train_acc.iter().map(|(s, a)| (*s, a * 100.0)),\n            &BLUE,\n        ))?\n        .label(\"Training Accuracy\")\n        .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &BLUE));\n\n    // Plot validation accuracy\n    chart\n        .draw_series(LineSeries::new(\n            val_acc.iter().map(|(s, a)| (*s, a * 100.0)),\n            &RED,\n        ))?\n        .label(\"Validation Accuracy\")\n        .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &RED));\n\n    // Add reference line at 90% (grokking threshold)\n    chart.draw_series(LineSeries::new(\n        vec![(0, 90.0), (max_step, 90.0)],\n        &GREEN.mix(0.5),\n    ))?;\n\n    chart\n        .configure_series_labels()\n        .background_style(&WHITE.mix(0.8))\n        .border_style(&BLACK)\n        .draw()?;\n\n    root.present()?;\n    println!(\"ðŸ“Š Accuracy plot saved to: {}\", output_path);\n    Ok(())\n}\n\n/// Plot FFT dominant frequency distribution\npub fn plot_fft_frequency_distribution(\n    dominant_frequencies: &[(usize, usize, f64)], // (token_id, frequency, magnitude)\n    output_path: &str,\n) -> Result<(), Box<dyn std::error::Error>> {\n    let root = BitMapBackend::new(output_path, (1200, 800)).into_drawing_area();\n    root.fill(&WHITE)?;\n\n    // Count frequency occurrences\n    let mut freq_counts: HashMap<usize, usize> = HashMap::new();\n    for (_, freq, _) in dominant_frequencies {\n        *freq_counts.entry(*freq).or_default() += 1;\n    }\n\n    let mut freq_vec: Vec<(usize, usize)> = freq_counts.into_iter().collect();\n    freq_vec.sort_by_key(|(f, _)| *f);\n\n    let max_freq = freq_vec.iter().map(|(f, _)| *f).max().unwrap_or(1);\n    let max_count = freq_vec.iter().map(|(_, c)| *c).max().unwrap_or(1);\n\n    let mut chart = ChartBuilder::on(&root)\n        .caption(\n            \"FFT Dominant Frequency Distribution\",\n            (\"sans-serif\", 50).into_font(),\n        )\n        .margin(20)\n        .x_label_area_size(50)\n        .y_label_area_size(70)\n        .build_cartesian_2d(0usize..max_freq, 0usize..max_count + 1)?;\n\n    chart\n        .configure_mesh()\n        .x_desc(\"Frequency\")\n        .y_desc(\"Number of Tokens\")\n        .draw()?;\n\n    chart.draw_series(\n        freq_vec\n            .iter()\n            .map(|(freq, count)| {\n                let color = if *freq == ModularAdditionDataset::modulus() {\n                    RED.filled() // Highlight frequency matching modulus\n                } else {\n                    BLUE.filled()\n                };\n                Rectangle::new([(*freq, 0), (*freq, *count)], color)\n            }),\n    )?;\n\n    root.present()?;\n    println!(\"ðŸ“Š FFT frequency distribution plot saved to: {}\", output_path);\n    Ok(())\n}\n\n/// Plot FFT magnitude spectrum for selected tokens\npub fn plot_fft_spectra(\n    token_spectra: &[(usize, Vec<f64>)], // (token_id, magnitude_spectrum)\n    output_path: &str,\n) -> Result<(), Box<dyn std::error::Error>> {\n    let root = BitMapBackend::new(output_path, (1200, 800)).into_drawing_area();\n    root.fill(&WHITE)?;\n\n    let max_freq = token_spectra\n        .first()\n        .map(|(_, spec)| spec.len())\n        .unwrap_or(1);\n    let max_magnitude = token_spectra\n        .iter()\n        .flat_map(|(_, spec)| spec.iter())\n        .copied()\n        .fold(0.0f64, f64::max)\n        .max(1.0);\n\n    let mut chart = ChartBuilder::on(&root)\n        .caption(\n            \"FFT Magnitude Spectra (Selected Tokens)\",\n            (\"sans-serif\", 50).into_font(),\n        )\n        .margin(20)\n        .x_label_area_size(50)\n        .y_label_area_size(70)\n        .build_cartesian_2d(0usize..max_freq, 0f64..max_magnitude)?;\n\n    chart\n        .configure_mesh()\n        .x_desc(\"Frequency\")\n        .y_desc(\"Magnitude\")\n        .draw()?;\n\n    let colors = [&BLUE, &RED, &GREEN, &MAGENTA, &CYAN, &YELLOW];\n\n    for (idx, (token_id, spectrum)) in token_spectra.iter().enumerate() {\n        let color = colors[idx % colors.len()];\n        chart\n            .draw_series(LineSeries::new(\n                spectrum.iter().enumerate().map(|(f, m)| (f, *m)),\n                color,\n            ))?\n            .label(format!(\"Token {}\", token_id))\n            .legend(move |(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], color));\n    }\n\n    chart\n        .configure_series_labels()\n        .background_style(&WHITE.mix(0.8))\n        .border_style(&BLACK)\n        .draw()?;\n\n    root.present()?;\n    println!(\"ðŸ“Š FFT spectra plot saved to: {}\", output_path);\n    Ok(())\n}\n\n/// Plot combined grokking visualization (loss + accuracy on same timeline)\npub fn plot_grokking_combined(\n    train_loss: &[(usize, f64)],\n    val_loss: &[(usize, f64)],\n    train_acc: &[(usize, f32)],\n    val_acc: &[(usize, f32)],\n    output_path: &str,\n) -> Result<(), Box<dyn std::error::Error>> {\n    let root = BitMapBackend::new(output_path, (1400, 900)).into_drawing_area();\n    root.fill(&WHITE)?;\n\n    let (upper, lower) = root.split_vertically(450);\n\n    // Upper plot: Accuracy\n    {\n        let max_step = train_acc\n            .iter()\n            .chain(val_acc.iter())\n            .map(|(s, _)| *s)\n            .max()\n            .unwrap_or(1);\n\n        let mut chart = ChartBuilder::on(&upper)\n            .caption(\n                \"Grokking: Accuracy Evolution\",\n                (\"sans-serif\", 40).into_font(),\n            )\n            .margin(15)\n            .x_label_area_size(40)\n            .y_label_area_size(60)\n            .build_cartesian_2d(0usize..max_step, 0f32..105f32)?;\n\n        chart\n            .configure_mesh()\n            .x_desc(\"Training Step\")\n            .y_desc(\"Accuracy (%)\")\n            .draw()?;\n\n        chart\n            .draw_series(LineSeries::new(\n                train_acc.iter().map(|(s, a)| (*s, a * 100.0)),\n                &BLUE,\n            ))?\n            .label(\"Training\")\n            .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &BLUE));\n\n        chart\n            .draw_series(LineSeries::new(\n                val_acc.iter().map(|(s, a)| (*s, a * 100.0)),\n                &RED,\n            ))?\n            .label(\"Validation\")\n            .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &RED));\n\n        chart\n            .configure_series_labels()\n            .background_style(&WHITE.mix(0.8))\n            .border_style(&BLACK)\n            .draw()?;\n    }\n\n    // Lower plot: Loss (train and val)\n    {\n        let max_step = train_loss\n            .iter()\n            .chain(val_loss.iter())\n            .map(|(s, _)| *s)\n            .max()\n            .unwrap_or(1);\n\n        let max_loss = train_loss\n            .iter()\n            .chain(val_loss.iter())\n            .map(|(_, l)| *l)\n            .fold(0.0f64, f64::max)\n            .max(1.0);\n\n        let mut chart = ChartBuilder::on(&lower)\n            .caption(\"Loss Evolution\", (\"sans-serif\", 40).into_font())\n            .margin(15)\n            .x_label_area_size(40)\n            .y_label_area_size(60)\n            .build_cartesian_2d(0usize..max_step, 0f64..max_loss)?;\n\n        chart\n            .configure_mesh()\n            .x_desc(\"Training Step\")\n            .y_desc(\"Loss\")\n            .draw()?;\n\n        // Train loss\n        chart\n            .draw_series(LineSeries::new(\n                train_loss.iter().map(|(s, l)| (*s, *l)),\n                &RED,\n            ))?\n            .label(\"Train\")\n            .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &RED));\n\n        // Val loss\n        chart\n            .draw_series(LineSeries::new(\n                val_loss.iter().map(|(s, l)| (*s, *l)),\n                &GREEN,\n            ))?\n            .label(\"Val\")\n            .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &GREEN));\n\n        // Chance level\n        let modulus = ModularAdditionDataset::modulus() as f64;\n        let chance_loss = -(1.0 / modulus).ln();\n        chart.draw_series(LineSeries::new(\n            vec![(0, chance_loss), (max_step, chance_loss)],\n            &BLUE.mix(0.5),\n        ))?;\n\n        chart\n            .configure_series_labels()\n            .background_style(&WHITE.mix(0.8))\n            .border_style(&BLACK)\n            .draw()?;\n    }\n\n    root.present()?;\n    println!(\"ðŸ“Š Combined grokking plot saved to: {}\", output_path);\n    Ok(())\n}\n\n/// Plot training and validation loss over time with LOG-SCALE x-axis\npub fn plot_loss_history_dual_logscale(\n    train_loss: &[(usize, f64)], // (step, loss)\n    val_loss: &[(usize, f64)],\n    output_path: &str,\n) -> Result<(), Box<dyn std::error::Error>> {\n    let root = BitMapBackend::new(output_path, (1200, 800)).into_drawing_area();\n    root.fill(&WHITE)?;\n\n    let max_step = train_loss\n        .iter()\n        .chain(val_loss.iter())\n        .map(|(s, _)| *s)\n        .max()\n        .unwrap_or(1);\n\n    let max_loss = train_loss\n        .iter()\n        .chain(val_loss.iter())\n        .map(|(_, l)| *l)\n        .fold(0.0f64, f64::max)\n        .max(1.0);\n\n    let mut chart = ChartBuilder::on(&root)\n        .caption(\"Loss Evolution (Log Scale): Train vs Validation\", (\"sans-serif\", 50).into_font())\n        .margin(20)\n        .x_label_area_size(50)\n        .y_label_area_size(70)\n        .build_cartesian_2d((1.0..max_step as f64).log_scale(), 0f64..max_loss)?;\n\n    chart\n        .configure_mesh()\n        .x_desc(\"Training Step (log scale)\")\n        .y_desc(\"Loss\")\n        .draw()?;\n\n    // Plot training loss\n    chart\n        .draw_series(LineSeries::new(\n            train_loss.iter().map(|(s, l)| (*s as f64, *l)),\n            &RED,\n        ))?\n        .label(\"Train Loss\")\n        .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &RED));\n\n    // Plot validation loss\n    chart\n        .draw_series(LineSeries::new(\n            val_loss.iter().map(|(s, l)| (*s as f64, *l)),\n            &GREEN,\n        ))?\n        .label(\"Val Loss\")\n        .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &GREEN));\n\n    let modulus = ModularAdditionDataset::modulus() as f64;\n    let chance_loss = -(1.0 / modulus).ln();\n    chart\n        .draw_series(LineSeries::new(\n            vec![(1.0, chance_loss), (max_step as f64, chance_loss)],\n            &BLUE.mix(0.5),\n        ))?\n        .label(\"Chance Level\")\n        .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &BLUE.mix(0.5)));\n\n    chart\n        .configure_series_labels()\n        .background_style(&WHITE.mix(0.8))\n        .border_style(&BLACK)\n        .draw()?;\n\n    root.present()?;\n    println!(\"ðŸ“Š Loss plot (log scale) saved to: {}\", output_path);\n    Ok(())\n}\n\n/// Plot training and validation accuracy over time with LOG-SCALE x-axis\npub fn plot_accuracy_history_logscale(\n    train_acc: &[(usize, f32)], // (step, accuracy)\n    val_acc: &[(usize, f32)],\n    output_path: &str,\n) -> Result<(), Box<dyn std::error::Error>> {\n    let root = BitMapBackend::new(output_path, (1200, 800)).into_drawing_area();\n    root.fill(&WHITE)?;\n\n    let max_step = train_acc\n        .iter()\n        .chain(val_acc.iter())\n        .map(|(s, _)| *s)\n        .max()\n        .unwrap_or(1);\n\n    let mut chart = ChartBuilder::on(&root)\n        .caption(\n            \"Grokking Phenomenon (Log Scale): Train vs Validation Accuracy\",\n            (\"sans-serif\", 50).into_font(),\n        )\n        .margin(20)\n        .x_label_area_size(50)\n        .y_label_area_size(70)\n        .build_cartesian_2d((1.0..max_step as f64).log_scale(), 0f32..105f32)?;\n\n    chart\n        .configure_mesh()\n        .x_desc(\"Training Step (log scale)\")\n        .y_desc(\"Accuracy (%)\")\n        .draw()?;\n\n    // Plot training accuracy\n    chart\n        .draw_series(LineSeries::new(\n            train_acc.iter().map(|(s, a)| (*s as f64, a * 100.0)),\n            &BLUE,\n        ))?\n        .label(\"Training Accuracy\")\n        .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &BLUE));\n\n    // Plot validation accuracy\n    chart\n        .draw_series(LineSeries::new(\n            val_acc.iter().map(|(s, a)| (*s as f64, a * 100.0)),\n            &RED,\n        ))?\n        .label(\"Validation Accuracy\")\n        .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &RED));\n\n    // Add reference line at 90% (grokking threshold)\n    chart.draw_series(LineSeries::new(\n        vec![(1.0, 90.0), (max_step as f64, 90.0)],\n        &GREEN.mix(0.5),\n    ))?;\n\n    chart\n        .configure_series_labels()\n        .background_style(&WHITE.mix(0.8))\n        .border_style(&BLACK)\n        .draw()?;\n\n    root.present()?;\n    println!(\"ðŸ“Š Accuracy plot (log scale) saved to: {}\", output_path);\n    Ok(())\n}\n\n/// Plot combined grokking visualization with LOG-SCALE x-axis (loss + accuracy)\npub fn plot_grokking_combined_logscale(\n    train_loss: &[(usize, f64)],\n    val_loss: &[(usize, f64)],\n    train_acc: &[(usize, f32)],\n    val_acc: &[(usize, f32)],\n    output_path: &str,\n) -> Result<(), Box<dyn std::error::Error>> {\n    let root = BitMapBackend::new(output_path, (1400, 900)).into_drawing_area();\n    root.fill(&WHITE)?;\n\n    let (upper, lower) = root.split_vertically(450);\n\n    // Upper plot: Accuracy (log scale)\n    {\n        let max_step = train_acc\n            .iter()\n            .chain(val_acc.iter())\n            .map(|(s, _)| *s)\n            .max()\n            .unwrap_or(1);\n\n        let mut chart = ChartBuilder::on(&upper)\n            .caption(\n                \"Grokking: Accuracy Evolution (Log Scale)\",\n                (\"sans-serif\", 40).into_font(),\n            )\n            .margin(15)\n            .x_label_area_size(40)\n            .y_label_area_size(60)\n            .build_cartesian_2d((1.0..max_step as f64).log_scale(), 0f32..105f32)?;\n\n        chart\n            .configure_mesh()\n            .x_desc(\"Training Step (log scale)\")\n            .y_desc(\"Accuracy (%)\")\n            .draw()?;\n\n        chart\n            .draw_series(LineSeries::new(\n                train_acc.iter().map(|(s, a)| (*s as f64, a * 100.0)),\n                &BLUE,\n            ))?\n            .label(\"Training\")\n            .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &BLUE));\n\n        chart\n            .draw_series(LineSeries::new(\n                val_acc.iter().map(|(s, a)| (*s as f64, a * 100.0)),\n                &RED,\n            ))?\n            .label(\"Validation\")\n            .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &RED));\n\n        chart\n            .configure_series_labels()\n            .background_style(&WHITE.mix(0.8))\n            .border_style(&BLACK)\n            .draw()?;\n    }\n\n    // Lower plot: Loss (log scale)\n    {\n        let max_step = train_loss\n            .iter()\n            .chain(val_loss.iter())\n            .map(|(s, _)| *s)\n            .max()\n            .unwrap_or(1);\n\n        let max_loss = train_loss\n            .iter()\n            .chain(val_loss.iter())\n            .map(|(_, l)| *l)\n            .fold(0.0f64, f64::max)\n            .max(1.0);\n\n        let mut chart = ChartBuilder::on(&lower)\n            .caption(\"Loss Evolution (Log Scale)\", (\"sans-serif\", 40).into_font())\n            .margin(15)\n            .x_label_area_size(40)\n            .y_label_area_size(60)\n            .build_cartesian_2d((1.0..max_step as f64).log_scale(), 0f64..max_loss)?;\n\n        chart\n            .configure_mesh()\n            .x_desc(\"Training Step (log scale)\")\n            .y_desc(\"Loss\")\n            .draw()?;\n\n        // Train loss\n        chart\n            .draw_series(LineSeries::new(\n                train_loss.iter().map(|(s, l)| (*s as f64, *l)),\n                &RED,\n            ))?\n            .label(\"Train\")\n            .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &RED));\n\n        // Val loss\n        chart\n            .draw_series(LineSeries::new(\n                val_loss.iter().map(|(s, l)| (*s as f64, *l)),\n                &GREEN,\n            ))?\n            .label(\"Val\")\n            .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &GREEN));\n\n        // Chance level\n        let modulus = ModularAdditionDataset::modulus() as f64;\n        let chance_loss = -(1.0 / modulus).ln();\n        chart.draw_series(LineSeries::new(\n            vec![(1.0, chance_loss), (max_step as f64, chance_loss)],\n            &BLUE.mix(0.5),\n        ))?;\n\n        chart\n            .configure_series_labels()\n            .background_style(&WHITE.mix(0.8))\n            .border_style(&BLACK)\n            .draw()?;\n    }\n\n    root.present()?;\n    println!(\"ðŸ“Š Combined grokking plot (log scale) saved to: {}\", output_path);\n    Ok(())\n}\n\n/// Plot 3Ã—3 embedding analysis grid (faster version)\n/// embeddings: [p, d] matrix where p is the number of tokens, d=embedding_dim\n/// dimensions: indices of 3 embedding dimensions to visualize\npub fn plot_embedding_grid_3x3(\n    embeddings: &[Vec<f64>],\n    dimensions: &[usize; 3],\n    output_path: &str,\n    title: &str,\n) -> Result<(), Box<dyn std::error::Error>> {\n    let p = embeddings.len(); // number of tokens\n    let point_size = 3; // slightly larger for visibility\n\n    // Create canvas for 3Ã—3 grid\n    let root = BitMapBackend::new(output_path, (1200, 1200)).into_drawing_area();\n    root.fill(&BLACK)?; // Black background\n\n    // Split into 3Ã—3 grid\n    let cell_areas = root.split_evenly((3, 3));\n\n    println!(\"   Rendering 3Ã—3 grid ({} cells)...\", 9);\n    for row in 0..3 {\n        for col in 0..3 {\n            let idx = row * 3 + col;\n            print!(\".\");\n            use std::io::Write;\n            std::io::stdout().flush().unwrap();\n            let area = &cell_areas[idx];\n\n            if col == 0 {\n                // First column: embedding dimension vs token index\n                let dim = dimensions[row];\n\n                // Get dimension values for all tokens\n                let values: Vec<(usize, f64)> = (0..p)\n                    .map(|token| (token, embeddings[token][dim]))\n                    .collect();\n\n                // Find min/max for y-axis\n                let min_val = values.iter().map(|(_, v)| *v).fold(f64::INFINITY, f64::min);\n                let max_val = values.iter().map(|(_, v)| *v).fold(f64::NEG_INFINITY, f64::max);\n                let margin = (max_val - min_val) * 0.1;\n\n                let mut chart = ChartBuilder::on(area)\n                    .margin(5)\n                    .build_cartesian_2d(\n                        0..p,\n                        (min_val - margin)..(max_val + margin),\n                    )?;\n\n                // Subtle gray crosshairs at origin\n                chart.configure_mesh()\n                    .disable_x_mesh()\n                    .disable_y_mesh()\n                    .x_labels(0)\n                    .y_labels(0)\n                    .axis_style(RGBAColor(50, 50, 50, 0.3))\n                    .draw()?;\n\n                // Plot points colored by token value\n                for (token, value) in values.iter() {\n                    let color = token_color(*token, p);\n                    chart.draw_series(std::iter::once(Circle::new(\n                        (*token, *value),\n                        point_size,\n                        color.filled(),\n                    )))?;\n                }\n            } else {\n                // Pairwise scatter plot\n                let dim_x = dimensions[col];\n                let dim_y = dimensions[row];\n\n                // Extract x and y values\n                let points: Vec<(f64, f64)> = (0..p)\n                    .map(|token| (embeddings[token][dim_x], embeddings[token][dim_y]))\n                    .collect();\n\n                // Find min/max for axes\n                let min_x = points.iter().map(|(x, _)| *x).fold(f64::INFINITY, f64::min);\n                let max_x = points.iter().map(|(x, _)| *x).fold(f64::NEG_INFINITY, f64::max);\n                let min_y = points.iter().map(|(_, y)| *y).fold(f64::INFINITY, f64::min);\n                let max_y = points.iter().map(|(_, y)| *y).fold(f64::NEG_INFINITY, f64::max);\n\n                let margin_x = (max_x - min_x) * 0.1;\n                let margin_y = (max_y - min_y) * 0.1;\n\n                let mut chart = ChartBuilder::on(area)\n                    .margin(5)\n                    .build_cartesian_2d(\n                        (min_x - margin_x)..(max_x + margin_x),\n                        (min_y - margin_y)..(max_y + margin_y),\n                    )?;\n\n                // Subtle gray crosshairs at origin\n                chart.configure_mesh()\n                    .disable_x_mesh()\n                    .disable_y_mesh()\n                    .x_labels(0)\n                    .y_labels(0)\n                    .axis_style(RGBAColor(50, 50, 50, 0.3))\n                    .draw()?;\n\n                // Plot points colored by token value\n                for token in 0..p {\n                    let color = token_color(token, p);\n                    let (x, y) = points[token];\n                    chart.draw_series(std::iter::once(Circle::new(\n                        (x, y),\n                        point_size,\n                        color.filled(),\n                    )))?;\n                }\n            }\n        }\n    }\n\n    println!(); // newline after progress dots\n    root.present()?;\n    println!(\"   ðŸ“Š Embedding grid plot saved to: {}\", output_path);\n    println!(\"      Title: {}\", title);\n    Ok(())\n}\n\n/// Select 7 interesting dimensions from embeddings based on variance\npub fn select_interesting_dimensions(embeddings: &[Vec<f64>], n: usize) -> Vec<usize> {\n    let d = embeddings[0].len(); // embedding dimension\n    let p = embeddings.len(); // number of tokens\n\n    // Compute variance for each dimension\n    let mut variances: Vec<(usize, f64)> = Vec::with_capacity(d);\n\n    for dim in 0..d {\n        // Compute mean\n        let mean: f64 = embeddings.iter().map(|emb| emb[dim]).sum::<f64>() / p as f64;\n\n        // Compute variance\n        let variance: f64 = embeddings\n            .iter()\n            .map(|emb| (emb[dim] - mean).powi(2))\n            .sum::<f64>()\n            / p as f64;\n\n        variances.push((dim, variance));\n    }\n\n    // Sort by variance (descending)\n    variances.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap_or(std::cmp::Ordering::Equal));\n\n    // Take top n dimensions\n    variances.iter().take(n).map(|(dim, _)| *dim).collect()\n}\n\n/// Convert viridis color to image::Rgb\n// Helper functions for alternative visualizations - reserved for future use\n#[allow(dead_code)]\nfn viridis_to_rgb(t: f64) -> Rgb<u8> {\n    let color = viridis_color(t);\n    Rgb([color.0, color.1, color.2])\n}\n\n/// Map token value to RGB color\nfn token_to_rgb(token_value: usize, p: usize) -> Rgb<u8> {\n    let t = token_value as f64 / (p - 1) as f64;\n    viridis_to_rgb(t)\n}\n\n/// Fast 7Ã—7 embedding grid using direct pixel manipulation\npub fn plot_embedding_grid_fast(\n    embeddings: &[Vec<f64>],\n    dimensions: &[usize; 7],\n    output_path: &str,\n    title: &str,\n) -> Result<(), Box<dyn std::error::Error>> {\n    let p = embeddings.len(); // number of tokens\n    let cell_size: usize = 300; // pixels per cell\n    let grid_size: usize = 7;\n    let total_size = cell_size * grid_size;\n\n    println!(\"   ðŸŽ¨ Creating {}Ã—{} pixel canvas...\", total_size, total_size);\n    let mut img: RgbImage = ImageBuffer::from_pixel(total_size as u32, total_size as u32, Rgb([0, 0, 0]));\n\n    println!(\"   ðŸ–Œï¸  Rendering {} cells...\", grid_size * grid_size);\n\n    for row in 0..grid_size as usize {\n        for col in 0..grid_size as usize {\n            let cell_idx = row * grid_size as usize + col;\n            if cell_idx % 10 == 0 {\n                print!(\".\");\n                use std::io::Write;\n                std::io::stdout().flush().unwrap();\n            }\n\n            let x_offset = col * cell_size as usize;\n            let y_offset = row * cell_size as usize;\n\n            if col == 0 {\n                // First column: dimension vs token index\n                let dim = dimensions[row];\n                render_dimension_plot(\n                    &mut img,\n                    embeddings,\n                    dim,\n                    p,\n                    x_offset,\n                    y_offset,\n                    cell_size,\n                );\n            } else {\n                // Pairwise scatter plot\n                let dim_x = dimensions[col];\n                let dim_y = dimensions[row];\n                render_scatter_plot(\n                    &mut img,\n                    embeddings,\n                    dim_x,\n                    dim_y,\n                    p,\n                    x_offset,\n                    y_offset,\n                    cell_size,\n                );\n            }\n        }\n    }\n\n    println!(); // newline after progress dots\n    println!(\"   ðŸ’¾ Saving image to {}...\", output_path);\n    img.save(output_path)?;\n    println!(\"   âœ… Embedding grid saved!\");\n    println!(\"      Title: {}\", title);\n    Ok(())\n}\n\n/// Render dimension vs index plot into a cell\nfn render_dimension_plot(\n    img: &mut RgbImage,\n    embeddings: &[Vec<f64>],\n    dim: usize,\n    p: usize,\n    x_offset: usize,\n    y_offset: usize,\n    cell_size: usize,\n) {\n    // Find min/max for this dimension\n    let values: Vec<f64> = (0..p).map(|token| embeddings[token][dim]).collect();\n    let min_val = values.iter().copied().fold(f64::INFINITY, f64::min);\n    let max_val = values.iter().copied().fold(f64::NEG_INFINITY, f64::max);\n    let range = max_val - min_val;\n\n    if range == 0.0 {\n        return; // Skip if no variation\n    }\n\n    let margin = 20;\n    let plot_width = cell_size - 2 * margin;\n    let plot_height = cell_size - 2 * margin;\n\n    // Draw subtle crosshairs\n    let mid_x = (x_offset + cell_size / 2) as f32;\n    let mid_y = (y_offset + cell_size / 2) as f32;\n    let gray = Rgb([30, 30, 30]);\n    draw_line_segment_mut(\n        img,\n        ((x_offset + margin) as f32, mid_y),\n        ((x_offset + cell_size - margin) as f32, mid_y),\n        gray,\n    );\n    draw_line_segment_mut(\n        img,\n        (mid_x, (y_offset + margin) as f32),\n        (mid_x, (y_offset + cell_size - margin) as f32),\n        gray,\n    );\n\n    // Plot points\n    for token in 0..p {\n        let x = x_offset + margin + (token * plot_width / p);\n        let normalized = (values[token] - min_val) / range;\n        let y = y_offset + cell_size - margin - (normalized * plot_height as f64) as usize;\n\n        let color = token_to_rgb(token, p);\n        draw_filled_circle_mut(img, (x as i32, y as i32), 2, color);\n    }\n}\n\n/// Render scatter plot into a cell\nfn render_scatter_plot(\n    img: &mut RgbImage,\n    embeddings: &[Vec<f64>],\n    dim_x: usize,\n    dim_y: usize,\n    p: usize,\n    x_offset: usize,\n    y_offset: usize,\n    cell_size: usize,\n) {\n    // Find min/max for both dimensions\n    let values_x: Vec<f64> = (0..p).map(|token| embeddings[token][dim_x]).collect();\n    let values_y: Vec<f64> = (0..p).map(|token| embeddings[token][dim_y]).collect();\n\n    let min_x = values_x.iter().copied().fold(f64::INFINITY, f64::min);\n    let max_x = values_x.iter().copied().fold(f64::NEG_INFINITY, f64::max);\n    let min_y = values_y.iter().copied().fold(f64::INFINITY, f64::min);\n    let max_y = values_y.iter().copied().fold(f64::NEG_INFINITY, f64::max);\n\n    let range_x = max_x - min_x;\n    let range_y = max_y - min_y;\n\n    if range_x == 0.0 || range_y == 0.0 {\n        return; // Skip if no variation\n    }\n\n    let margin = 20;\n    let plot_width = cell_size - 2 * margin;\n    let plot_height = cell_size - 2 * margin;\n\n    // Draw subtle crosshairs at origin\n    let mid_x = (x_offset + cell_size / 2) as f32;\n    let mid_y = (y_offset + cell_size / 2) as f32;\n    let gray = Rgb([30, 30, 30]);\n    draw_line_segment_mut(\n        img,\n        ((x_offset + margin) as f32, mid_y),\n        ((x_offset + cell_size - margin) as f32, mid_y),\n        gray,\n    );\n    draw_line_segment_mut(\n        img,\n        (mid_x, (y_offset + margin) as f32),\n        (mid_x, (y_offset + cell_size - margin) as f32),\n        gray,\n    );\n\n    // Plot points\n    for token in 0..p {\n        let norm_x = (values_x[token] - min_x) / range_x;\n        let norm_y = (values_y[token] - min_y) / range_y;\n\n        let x = x_offset + margin + (norm_x * plot_width as f64) as usize;\n        let y = y_offset + cell_size - margin - (norm_y * plot_height as f64) as usize;\n\n        let color = token_to_rgb(token, p);\n        draw_filled_circle_mut(img, (x as i32, y as i32), 2, color);\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn logscale_plotting_functions_exist() {\n        // Verify that log-scale plotting functions exist and have the correct signature\n        // This is a compile-time check that ensures the API is correct\n        let _: fn(&[(usize, f64)], &[(usize, f64)], &str) -> Result<(), Box<dyn std::error::Error>> =\n            plot_loss_history_dual_logscale;\n        let _: fn(&[(usize, f32)], &[(usize, f32)], &str) -> Result<(), Box<dyn std::error::Error>> =\n            plot_accuracy_history_logscale;\n        let _: fn(&[(usize, f64)], &[(usize, f64)], &[(usize, f32)], &[(usize, f32)], &str) -> Result<(), Box<dyn std::error::Error>> =\n            plot_grokking_combined_logscale;\n    }\n\n    #[test]\n    fn snake_curve_can_be_verified_with_plotting_data() {\n        use crate::verify::{verify_snake_curve_shape, SnakeCurveConfig};\n\n        // Simulate grokking data that would produce a snake curve on log-scale plot\n        let train_acc = vec![\n            (1, 0.10),\n            (10, 0.50),\n            (100, 0.90),\n            (500, 0.99),\n            (1000, 0.995),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (10, 0.01),\n            (100, 0.01),\n            (500, 0.01),\n            (1000, 0.01),\n            (2000, 0.95),\n        ];\n\n        let config = SnakeCurveConfig::default_for_modulus(113);\n        let report = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect(\"snake curve verification should pass on grokking data\");\n\n        // Verify the snake curve characteristics are present\n        assert!(report.train_converged_by_step < 1000, \"train should converge early\");\n        assert!(report.val_grok_step > 1000, \"val should grok late\");\n        assert!(report.curve_shape_valid, \"curve shape should be valid\");\n    }\n}\n","structuredPatch":[{"oldStart":880,"oldLines":6,"newStart":880,"newLines":7,"lines":[" }"," "," /// Map token value to RGB color","+#[allow(dead_code)]"," fn token_to_rgb(token_value: usize, p: usize) -> Rgb<u8> {","     let t = token_value as f64 / (p - 1) as f64;","     viridis_to_rgb(t)"]}],"userModified":false,"replaceAll":false}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01Wmr2sjNMSFx2DhocAcwoST","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01PWCzWeDwNb2Z8Nng8kjbof","name":"Edit","input":{"replace_all":false,"file_path":"/Users/admin/MechInt/src/plotting.rs","old_string":"pub fn plot_embedding_grid_fast(","new_string":"/// NOTE: Alternative visualization approach - reserved for future use\n#[allow(dead_code)]\npub fn plot_embedding_grid_fast("}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":543,"cache_read_input_tokens":66870,"cache_creation":{"ephemeral_5m_input_tokens":543,"ephemeral_1h_input_tokens":0},"output_tokens":162,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"99b7ea26-a41b-48f8-9e46-a59831876368"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01PWCzWeDwNb2Z8Nng8kjbof","type":"tool_result","content":"The file /Users/admin/MechInt/src/plotting.rs has been updated. Here's the result of running `cat -n` on a snippet of the edited file:\n   886â†’    viridis_to_rgb(t)\n   887â†’}\n   888â†’\n   889â†’/// Fast 7Ã—7 embedding grid using direct pixel manipulation\n   890â†’/// NOTE: Alternative visualization approach - reserved for future use\n   891â†’#[allow(dead_code)]\n   892â†’pub fn plot_embedding_grid_fast(\n   893â†’    embeddings: &[Vec<f64>],\n   894â†’    dimensions: &[usize; 7],\n   895â†’    output_path: &str,\n   896â†’    title: &str,"}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"72d51f56-7bf6-4acc-ad04-ae194aa51d4c","tool_use_result":{"filePath":"/Users/admin/MechInt/src/plotting.rs","oldString":"pub fn plot_embedding_grid_fast(","newString":"/// NOTE: Alternative visualization approach - reserved for future use\n#[allow(dead_code)]\npub fn plot_embedding_grid_fast(","originalFile":"use image::{ImageBuffer, Rgb, RgbImage};\nuse imageproc::drawing::{draw_filled_circle_mut, draw_line_segment_mut};\nuse plotters::prelude::*;\nuse std::collections::HashMap;\n\nuse crate::data::ModularAdditionDataset;\n\n/// Viridis colormap - maps value in [0, 1] to RGB\nfn viridis_color(t: f64) -> RGBColor {\n    // Viridis colormap approximation (purple â†’ cyan â†’ yellow)\n    // Based on matplotlib's viridis\n    let r = (0.267 + t * (0.329 - 0.267 + t * (0.984 - 0.329))) * 255.0;\n    let g = (0.005 + t * (0.569 - 0.005 + t * (0.906 - 0.569))) * 255.0;\n    let b = (0.329 + t * (0.758 - 0.329 - t * (0.758 - 0.121))) * 255.0;\n\n    RGBColor(\n        r.min(255.0).max(0.0) as u8,\n        g.min(255.0).max(0.0) as u8,\n        b.min(255.0).max(0.0) as u8,\n    )\n}\n\n/// Map token value (0 to p-1) to viridis color\nfn token_color(token_value: usize, p: usize) -> RGBColor {\n    let t = token_value as f64 / (p - 1) as f64;\n    viridis_color(t)\n}\n\n/// Plot training and validation loss over time\npub fn plot_loss_history_dual(\n    train_loss: &[(usize, f64)], // (step, loss)\n    val_loss: &[(usize, f64)],\n    output_path: &str,\n) -> Result<(), Box<dyn std::error::Error>> {\n    let root = BitMapBackend::new(output_path, (1200, 800)).into_drawing_area();\n    root.fill(&WHITE)?;\n\n    let max_step = train_loss\n        .iter()\n        .chain(val_loss.iter())\n        .map(|(s, _)| *s)\n        .max()\n        .unwrap_or(1);\n\n    let max_loss = train_loss\n        .iter()\n        .chain(val_loss.iter())\n        .map(|(_, l)| *l)\n        .fold(0.0f64, f64::max)\n        .max(1.0);\n\n    let mut chart = ChartBuilder::on(&root)\n        .caption(\"Loss Evolution: Train vs Validation\", (\"sans-serif\", 50).into_font())\n        .margin(20)\n        .x_label_area_size(50)\n        .y_label_area_size(70)\n        .build_cartesian_2d(0usize..max_step, 0f64..max_loss)?;\n\n    chart\n        .configure_mesh()\n        .x_desc(\"Training Step\")\n        .y_desc(\"Loss\")\n        .draw()?;\n\n    // Plot training loss\n    chart\n        .draw_series(LineSeries::new(\n            train_loss.iter().map(|(s, l)| (*s, *l)),\n            &RED,\n        ))?\n        .label(\"Train Loss\")\n        .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &RED));\n\n    // Plot validation loss\n    chart\n        .draw_series(LineSeries::new(\n            val_loss.iter().map(|(s, l)| (*s, *l)),\n            &GREEN,\n        ))?\n        .label(\"Val Loss\")\n        .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &GREEN));\n\n    let modulus = ModularAdditionDataset::modulus() as f64;\n    let chance_loss = -(1.0 / modulus).ln();\n    chart\n        .draw_series(LineSeries::new(\n            vec![(0, chance_loss), (max_step, chance_loss)],\n            &BLUE.mix(0.5),\n        ))?\n        .label(\"Chance Level\")\n        .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &BLUE.mix(0.5)));\n\n    chart\n        .configure_series_labels()\n        .background_style(&WHITE.mix(0.8))\n        .border_style(&BLACK)\n        .draw()?;\n\n    root.present()?;\n    println!(\"ðŸ“Š Loss plot saved to: {}\", output_path);\n    Ok(())\n}\n\n/// Plot training loss over time (legacy, single loss)\n/// NOTE: Superseded by plot_loss_history_dual - kept for reference\n#[allow(dead_code)]\npub fn plot_loss_history(\n    snapshots: &[(usize, f64)], // (step, loss)\n    output_path: &str,\n) -> Result<(), Box<dyn std::error::Error>> {\n    let root = BitMapBackend::new(output_path, (1200, 800)).into_drawing_area();\n    root.fill(&WHITE)?;\n\n    let max_step = snapshots.iter().map(|(s, _)| *s).max().unwrap_or(1);\n    let max_loss = snapshots\n        .iter()\n        .map(|(_, l)| *l)\n        .fold(0.0f64, f64::max)\n        .max(1.0);\n\n    let mut chart = ChartBuilder::on(&root)\n        .caption(\"Training Loss Evolution\", (\"sans-serif\", 50).into_font())\n        .margin(20)\n        .x_label_area_size(50)\n        .y_label_area_size(70)\n        .build_cartesian_2d(0usize..max_step, 0f64..max_loss)?;\n\n    chart\n        .configure_mesh()\n        .x_desc(\"Training Step\")\n        .y_desc(\"Loss\")\n        .draw()?;\n\n    chart.draw_series(LineSeries::new(\n        snapshots.iter().map(|(s, l)| (*s, *l)),\n        &BLUE,\n    ))?;\n\n    root.present()?;\n    println!(\"ðŸ“Š Loss plot saved to: {}\", output_path);\n    Ok(())\n}\n\n/// Plot training and validation accuracy over time (combined)\npub fn plot_accuracy_history(\n    train_acc: &[(usize, f32)], // (step, accuracy)\n    val_acc: &[(usize, f32)],\n    output_path: &str,\n) -> Result<(), Box<dyn std::error::Error>> {\n    let root = BitMapBackend::new(output_path, (1200, 800)).into_drawing_area();\n    root.fill(&WHITE)?;\n\n    let max_step = train_acc\n        .iter()\n        .chain(val_acc.iter())\n        .map(|(s, _)| *s)\n        .max()\n        .unwrap_or(1);\n\n    let mut chart = ChartBuilder::on(&root)\n        .caption(\n            \"Grokking Phenomenon: Train vs Validation Accuracy\",\n            (\"sans-serif\", 50).into_font(),\n        )\n        .margin(20)\n        .x_label_area_size(50)\n        .y_label_area_size(70)\n        .build_cartesian_2d(0usize..max_step, 0f32..105f32)?;\n\n    chart\n        .configure_mesh()\n        .x_desc(\"Training Step\")\n        .y_desc(\"Accuracy (%)\")\n        .draw()?;\n\n    // Plot training accuracy\n    chart\n        .draw_series(LineSeries::new(\n            train_acc.iter().map(|(s, a)| (*s, a * 100.0)),\n            &BLUE,\n        ))?\n        .label(\"Training Accuracy\")\n        .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &BLUE));\n\n    // Plot validation accuracy\n    chart\n        .draw_series(LineSeries::new(\n            val_acc.iter().map(|(s, a)| (*s, a * 100.0)),\n            &RED,\n        ))?\n        .label(\"Validation Accuracy\")\n        .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &RED));\n\n    // Add reference line at 90% (grokking threshold)\n    chart.draw_series(LineSeries::new(\n        vec![(0, 90.0), (max_step, 90.0)],\n        &GREEN.mix(0.5),\n    ))?;\n\n    chart\n        .configure_series_labels()\n        .background_style(&WHITE.mix(0.8))\n        .border_style(&BLACK)\n        .draw()?;\n\n    root.present()?;\n    println!(\"ðŸ“Š Accuracy plot saved to: {}\", output_path);\n    Ok(())\n}\n\n/// Plot FFT dominant frequency distribution\npub fn plot_fft_frequency_distribution(\n    dominant_frequencies: &[(usize, usize, f64)], // (token_id, frequency, magnitude)\n    output_path: &str,\n) -> Result<(), Box<dyn std::error::Error>> {\n    let root = BitMapBackend::new(output_path, (1200, 800)).into_drawing_area();\n    root.fill(&WHITE)?;\n\n    // Count frequency occurrences\n    let mut freq_counts: HashMap<usize, usize> = HashMap::new();\n    for (_, freq, _) in dominant_frequencies {\n        *freq_counts.entry(*freq).or_default() += 1;\n    }\n\n    let mut freq_vec: Vec<(usize, usize)> = freq_counts.into_iter().collect();\n    freq_vec.sort_by_key(|(f, _)| *f);\n\n    let max_freq = freq_vec.iter().map(|(f, _)| *f).max().unwrap_or(1);\n    let max_count = freq_vec.iter().map(|(_, c)| *c).max().unwrap_or(1);\n\n    let mut chart = ChartBuilder::on(&root)\n        .caption(\n            \"FFT Dominant Frequency Distribution\",\n            (\"sans-serif\", 50).into_font(),\n        )\n        .margin(20)\n        .x_label_area_size(50)\n        .y_label_area_size(70)\n        .build_cartesian_2d(0usize..max_freq, 0usize..max_count + 1)?;\n\n    chart\n        .configure_mesh()\n        .x_desc(\"Frequency\")\n        .y_desc(\"Number of Tokens\")\n        .draw()?;\n\n    chart.draw_series(\n        freq_vec\n            .iter()\n            .map(|(freq, count)| {\n                let color = if *freq == ModularAdditionDataset::modulus() {\n                    RED.filled() // Highlight frequency matching modulus\n                } else {\n                    BLUE.filled()\n                };\n                Rectangle::new([(*freq, 0), (*freq, *count)], color)\n            }),\n    )?;\n\n    root.present()?;\n    println!(\"ðŸ“Š FFT frequency distribution plot saved to: {}\", output_path);\n    Ok(())\n}\n\n/// Plot FFT magnitude spectrum for selected tokens\npub fn plot_fft_spectra(\n    token_spectra: &[(usize, Vec<f64>)], // (token_id, magnitude_spectrum)\n    output_path: &str,\n) -> Result<(), Box<dyn std::error::Error>> {\n    let root = BitMapBackend::new(output_path, (1200, 800)).into_drawing_area();\n    root.fill(&WHITE)?;\n\n    let max_freq = token_spectra\n        .first()\n        .map(|(_, spec)| spec.len())\n        .unwrap_or(1);\n    let max_magnitude = token_spectra\n        .iter()\n        .flat_map(|(_, spec)| spec.iter())\n        .copied()\n        .fold(0.0f64, f64::max)\n        .max(1.0);\n\n    let mut chart = ChartBuilder::on(&root)\n        .caption(\n            \"FFT Magnitude Spectra (Selected Tokens)\",\n            (\"sans-serif\", 50).into_font(),\n        )\n        .margin(20)\n        .x_label_area_size(50)\n        .y_label_area_size(70)\n        .build_cartesian_2d(0usize..max_freq, 0f64..max_magnitude)?;\n\n    chart\n        .configure_mesh()\n        .x_desc(\"Frequency\")\n        .y_desc(\"Magnitude\")\n        .draw()?;\n\n    let colors = [&BLUE, &RED, &GREEN, &MAGENTA, &CYAN, &YELLOW];\n\n    for (idx, (token_id, spectrum)) in token_spectra.iter().enumerate() {\n        let color = colors[idx % colors.len()];\n        chart\n            .draw_series(LineSeries::new(\n                spectrum.iter().enumerate().map(|(f, m)| (f, *m)),\n                color,\n            ))?\n            .label(format!(\"Token {}\", token_id))\n            .legend(move |(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], color));\n    }\n\n    chart\n        .configure_series_labels()\n        .background_style(&WHITE.mix(0.8))\n        .border_style(&BLACK)\n        .draw()?;\n\n    root.present()?;\n    println!(\"ðŸ“Š FFT spectra plot saved to: {}\", output_path);\n    Ok(())\n}\n\n/// Plot combined grokking visualization (loss + accuracy on same timeline)\npub fn plot_grokking_combined(\n    train_loss: &[(usize, f64)],\n    val_loss: &[(usize, f64)],\n    train_acc: &[(usize, f32)],\n    val_acc: &[(usize, f32)],\n    output_path: &str,\n) -> Result<(), Box<dyn std::error::Error>> {\n    let root = BitMapBackend::new(output_path, (1400, 900)).into_drawing_area();\n    root.fill(&WHITE)?;\n\n    let (upper, lower) = root.split_vertically(450);\n\n    // Upper plot: Accuracy\n    {\n        let max_step = train_acc\n            .iter()\n            .chain(val_acc.iter())\n            .map(|(s, _)| *s)\n            .max()\n            .unwrap_or(1);\n\n        let mut chart = ChartBuilder::on(&upper)\n            .caption(\n                \"Grokking: Accuracy Evolution\",\n                (\"sans-serif\", 40).into_font(),\n            )\n            .margin(15)\n            .x_label_area_size(40)\n            .y_label_area_size(60)\n            .build_cartesian_2d(0usize..max_step, 0f32..105f32)?;\n\n        chart\n            .configure_mesh()\n            .x_desc(\"Training Step\")\n            .y_desc(\"Accuracy (%)\")\n            .draw()?;\n\n        chart\n            .draw_series(LineSeries::new(\n                train_acc.iter().map(|(s, a)| (*s, a * 100.0)),\n                &BLUE,\n            ))?\n            .label(\"Training\")\n            .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &BLUE));\n\n        chart\n            .draw_series(LineSeries::new(\n                val_acc.iter().map(|(s, a)| (*s, a * 100.0)),\n                &RED,\n            ))?\n            .label(\"Validation\")\n            .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &RED));\n\n        chart\n            .configure_series_labels()\n            .background_style(&WHITE.mix(0.8))\n            .border_style(&BLACK)\n            .draw()?;\n    }\n\n    // Lower plot: Loss (train and val)\n    {\n        let max_step = train_loss\n            .iter()\n            .chain(val_loss.iter())\n            .map(|(s, _)| *s)\n            .max()\n            .unwrap_or(1);\n\n        let max_loss = train_loss\n            .iter()\n            .chain(val_loss.iter())\n            .map(|(_, l)| *l)\n            .fold(0.0f64, f64::max)\n            .max(1.0);\n\n        let mut chart = ChartBuilder::on(&lower)\n            .caption(\"Loss Evolution\", (\"sans-serif\", 40).into_font())\n            .margin(15)\n            .x_label_area_size(40)\n            .y_label_area_size(60)\n            .build_cartesian_2d(0usize..max_step, 0f64..max_loss)?;\n\n        chart\n            .configure_mesh()\n            .x_desc(\"Training Step\")\n            .y_desc(\"Loss\")\n            .draw()?;\n\n        // Train loss\n        chart\n            .draw_series(LineSeries::new(\n                train_loss.iter().map(|(s, l)| (*s, *l)),\n                &RED,\n            ))?\n            .label(\"Train\")\n            .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &RED));\n\n        // Val loss\n        chart\n            .draw_series(LineSeries::new(\n                val_loss.iter().map(|(s, l)| (*s, *l)),\n                &GREEN,\n            ))?\n            .label(\"Val\")\n            .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &GREEN));\n\n        // Chance level\n        let modulus = ModularAdditionDataset::modulus() as f64;\n        let chance_loss = -(1.0 / modulus).ln();\n        chart.draw_series(LineSeries::new(\n            vec![(0, chance_loss), (max_step, chance_loss)],\n            &BLUE.mix(0.5),\n        ))?;\n\n        chart\n            .configure_series_labels()\n            .background_style(&WHITE.mix(0.8))\n            .border_style(&BLACK)\n            .draw()?;\n    }\n\n    root.present()?;\n    println!(\"ðŸ“Š Combined grokking plot saved to: {}\", output_path);\n    Ok(())\n}\n\n/// Plot training and validation loss over time with LOG-SCALE x-axis\npub fn plot_loss_history_dual_logscale(\n    train_loss: &[(usize, f64)], // (step, loss)\n    val_loss: &[(usize, f64)],\n    output_path: &str,\n) -> Result<(), Box<dyn std::error::Error>> {\n    let root = BitMapBackend::new(output_path, (1200, 800)).into_drawing_area();\n    root.fill(&WHITE)?;\n\n    let max_step = train_loss\n        .iter()\n        .chain(val_loss.iter())\n        .map(|(s, _)| *s)\n        .max()\n        .unwrap_or(1);\n\n    let max_loss = train_loss\n        .iter()\n        .chain(val_loss.iter())\n        .map(|(_, l)| *l)\n        .fold(0.0f64, f64::max)\n        .max(1.0);\n\n    let mut chart = ChartBuilder::on(&root)\n        .caption(\"Loss Evolution (Log Scale): Train vs Validation\", (\"sans-serif\", 50).into_font())\n        .margin(20)\n        .x_label_area_size(50)\n        .y_label_area_size(70)\n        .build_cartesian_2d((1.0..max_step as f64).log_scale(), 0f64..max_loss)?;\n\n    chart\n        .configure_mesh()\n        .x_desc(\"Training Step (log scale)\")\n        .y_desc(\"Loss\")\n        .draw()?;\n\n    // Plot training loss\n    chart\n        .draw_series(LineSeries::new(\n            train_loss.iter().map(|(s, l)| (*s as f64, *l)),\n            &RED,\n        ))?\n        .label(\"Train Loss\")\n        .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &RED));\n\n    // Plot validation loss\n    chart\n        .draw_series(LineSeries::new(\n            val_loss.iter().map(|(s, l)| (*s as f64, *l)),\n            &GREEN,\n        ))?\n        .label(\"Val Loss\")\n        .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &GREEN));\n\n    let modulus = ModularAdditionDataset::modulus() as f64;\n    let chance_loss = -(1.0 / modulus).ln();\n    chart\n        .draw_series(LineSeries::new(\n            vec![(1.0, chance_loss), (max_step as f64, chance_loss)],\n            &BLUE.mix(0.5),\n        ))?\n        .label(\"Chance Level\")\n        .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &BLUE.mix(0.5)));\n\n    chart\n        .configure_series_labels()\n        .background_style(&WHITE.mix(0.8))\n        .border_style(&BLACK)\n        .draw()?;\n\n    root.present()?;\n    println!(\"ðŸ“Š Loss plot (log scale) saved to: {}\", output_path);\n    Ok(())\n}\n\n/// Plot training and validation accuracy over time with LOG-SCALE x-axis\npub fn plot_accuracy_history_logscale(\n    train_acc: &[(usize, f32)], // (step, accuracy)\n    val_acc: &[(usize, f32)],\n    output_path: &str,\n) -> Result<(), Box<dyn std::error::Error>> {\n    let root = BitMapBackend::new(output_path, (1200, 800)).into_drawing_area();\n    root.fill(&WHITE)?;\n\n    let max_step = train_acc\n        .iter()\n        .chain(val_acc.iter())\n        .map(|(s, _)| *s)\n        .max()\n        .unwrap_or(1);\n\n    let mut chart = ChartBuilder::on(&root)\n        .caption(\n            \"Grokking Phenomenon (Log Scale): Train vs Validation Accuracy\",\n            (\"sans-serif\", 50).into_font(),\n        )\n        .margin(20)\n        .x_label_area_size(50)\n        .y_label_area_size(70)\n        .build_cartesian_2d((1.0..max_step as f64).log_scale(), 0f32..105f32)?;\n\n    chart\n        .configure_mesh()\n        .x_desc(\"Training Step (log scale)\")\n        .y_desc(\"Accuracy (%)\")\n        .draw()?;\n\n    // Plot training accuracy\n    chart\n        .draw_series(LineSeries::new(\n            train_acc.iter().map(|(s, a)| (*s as f64, a * 100.0)),\n            &BLUE,\n        ))?\n        .label(\"Training Accuracy\")\n        .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &BLUE));\n\n    // Plot validation accuracy\n    chart\n        .draw_series(LineSeries::new(\n            val_acc.iter().map(|(s, a)| (*s as f64, a * 100.0)),\n            &RED,\n        ))?\n        .label(\"Validation Accuracy\")\n        .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &RED));\n\n    // Add reference line at 90% (grokking threshold)\n    chart.draw_series(LineSeries::new(\n        vec![(1.0, 90.0), (max_step as f64, 90.0)],\n        &GREEN.mix(0.5),\n    ))?;\n\n    chart\n        .configure_series_labels()\n        .background_style(&WHITE.mix(0.8))\n        .border_style(&BLACK)\n        .draw()?;\n\n    root.present()?;\n    println!(\"ðŸ“Š Accuracy plot (log scale) saved to: {}\", output_path);\n    Ok(())\n}\n\n/// Plot combined grokking visualization with LOG-SCALE x-axis (loss + accuracy)\npub fn plot_grokking_combined_logscale(\n    train_loss: &[(usize, f64)],\n    val_loss: &[(usize, f64)],\n    train_acc: &[(usize, f32)],\n    val_acc: &[(usize, f32)],\n    output_path: &str,\n) -> Result<(), Box<dyn std::error::Error>> {\n    let root = BitMapBackend::new(output_path, (1400, 900)).into_drawing_area();\n    root.fill(&WHITE)?;\n\n    let (upper, lower) = root.split_vertically(450);\n\n    // Upper plot: Accuracy (log scale)\n    {\n        let max_step = train_acc\n            .iter()\n            .chain(val_acc.iter())\n            .map(|(s, _)| *s)\n            .max()\n            .unwrap_or(1);\n\n        let mut chart = ChartBuilder::on(&upper)\n            .caption(\n                \"Grokking: Accuracy Evolution (Log Scale)\",\n                (\"sans-serif\", 40).into_font(),\n            )\n            .margin(15)\n            .x_label_area_size(40)\n            .y_label_area_size(60)\n            .build_cartesian_2d((1.0..max_step as f64).log_scale(), 0f32..105f32)?;\n\n        chart\n            .configure_mesh()\n            .x_desc(\"Training Step (log scale)\")\n            .y_desc(\"Accuracy (%)\")\n            .draw()?;\n\n        chart\n            .draw_series(LineSeries::new(\n                train_acc.iter().map(|(s, a)| (*s as f64, a * 100.0)),\n                &BLUE,\n            ))?\n            .label(\"Training\")\n            .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &BLUE));\n\n        chart\n            .draw_series(LineSeries::new(\n                val_acc.iter().map(|(s, a)| (*s as f64, a * 100.0)),\n                &RED,\n            ))?\n            .label(\"Validation\")\n            .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &RED));\n\n        chart\n            .configure_series_labels()\n            .background_style(&WHITE.mix(0.8))\n            .border_style(&BLACK)\n            .draw()?;\n    }\n\n    // Lower plot: Loss (log scale)\n    {\n        let max_step = train_loss\n            .iter()\n            .chain(val_loss.iter())\n            .map(|(s, _)| *s)\n            .max()\n            .unwrap_or(1);\n\n        let max_loss = train_loss\n            .iter()\n            .chain(val_loss.iter())\n            .map(|(_, l)| *l)\n            .fold(0.0f64, f64::max)\n            .max(1.0);\n\n        let mut chart = ChartBuilder::on(&lower)\n            .caption(\"Loss Evolution (Log Scale)\", (\"sans-serif\", 40).into_font())\n            .margin(15)\n            .x_label_area_size(40)\n            .y_label_area_size(60)\n            .build_cartesian_2d((1.0..max_step as f64).log_scale(), 0f64..max_loss)?;\n\n        chart\n            .configure_mesh()\n            .x_desc(\"Training Step (log scale)\")\n            .y_desc(\"Loss\")\n            .draw()?;\n\n        // Train loss\n        chart\n            .draw_series(LineSeries::new(\n                train_loss.iter().map(|(s, l)| (*s as f64, *l)),\n                &RED,\n            ))?\n            .label(\"Train\")\n            .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &RED));\n\n        // Val loss\n        chart\n            .draw_series(LineSeries::new(\n                val_loss.iter().map(|(s, l)| (*s as f64, *l)),\n                &GREEN,\n            ))?\n            .label(\"Val\")\n            .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &GREEN));\n\n        // Chance level\n        let modulus = ModularAdditionDataset::modulus() as f64;\n        let chance_loss = -(1.0 / modulus).ln();\n        chart.draw_series(LineSeries::new(\n            vec![(1.0, chance_loss), (max_step as f64, chance_loss)],\n            &BLUE.mix(0.5),\n        ))?;\n\n        chart\n            .configure_series_labels()\n            .background_style(&WHITE.mix(0.8))\n            .border_style(&BLACK)\n            .draw()?;\n    }\n\n    root.present()?;\n    println!(\"ðŸ“Š Combined grokking plot (log scale) saved to: {}\", output_path);\n    Ok(())\n}\n\n/// Plot 3Ã—3 embedding analysis grid (faster version)\n/// embeddings: [p, d] matrix where p is the number of tokens, d=embedding_dim\n/// dimensions: indices of 3 embedding dimensions to visualize\npub fn plot_embedding_grid_3x3(\n    embeddings: &[Vec<f64>],\n    dimensions: &[usize; 3],\n    output_path: &str,\n    title: &str,\n) -> Result<(), Box<dyn std::error::Error>> {\n    let p = embeddings.len(); // number of tokens\n    let point_size = 3; // slightly larger for visibility\n\n    // Create canvas for 3Ã—3 grid\n    let root = BitMapBackend::new(output_path, (1200, 1200)).into_drawing_area();\n    root.fill(&BLACK)?; // Black background\n\n    // Split into 3Ã—3 grid\n    let cell_areas = root.split_evenly((3, 3));\n\n    println!(\"   Rendering 3Ã—3 grid ({} cells)...\", 9);\n    for row in 0..3 {\n        for col in 0..3 {\n            let idx = row * 3 + col;\n            print!(\".\");\n            use std::io::Write;\n            std::io::stdout().flush().unwrap();\n            let area = &cell_areas[idx];\n\n            if col == 0 {\n                // First column: embedding dimension vs token index\n                let dim = dimensions[row];\n\n                // Get dimension values for all tokens\n                let values: Vec<(usize, f64)> = (0..p)\n                    .map(|token| (token, embeddings[token][dim]))\n                    .collect();\n\n                // Find min/max for y-axis\n                let min_val = values.iter().map(|(_, v)| *v).fold(f64::INFINITY, f64::min);\n                let max_val = values.iter().map(|(_, v)| *v).fold(f64::NEG_INFINITY, f64::max);\n                let margin = (max_val - min_val) * 0.1;\n\n                let mut chart = ChartBuilder::on(area)\n                    .margin(5)\n                    .build_cartesian_2d(\n                        0..p,\n                        (min_val - margin)..(max_val + margin),\n                    )?;\n\n                // Subtle gray crosshairs at origin\n                chart.configure_mesh()\n                    .disable_x_mesh()\n                    .disable_y_mesh()\n                    .x_labels(0)\n                    .y_labels(0)\n                    .axis_style(RGBAColor(50, 50, 50, 0.3))\n                    .draw()?;\n\n                // Plot points colored by token value\n                for (token, value) in values.iter() {\n                    let color = token_color(*token, p);\n                    chart.draw_series(std::iter::once(Circle::new(\n                        (*token, *value),\n                        point_size,\n                        color.filled(),\n                    )))?;\n                }\n            } else {\n                // Pairwise scatter plot\n                let dim_x = dimensions[col];\n                let dim_y = dimensions[row];\n\n                // Extract x and y values\n                let points: Vec<(f64, f64)> = (0..p)\n                    .map(|token| (embeddings[token][dim_x], embeddings[token][dim_y]))\n                    .collect();\n\n                // Find min/max for axes\n                let min_x = points.iter().map(|(x, _)| *x).fold(f64::INFINITY, f64::min);\n                let max_x = points.iter().map(|(x, _)| *x).fold(f64::NEG_INFINITY, f64::max);\n                let min_y = points.iter().map(|(_, y)| *y).fold(f64::INFINITY, f64::min);\n                let max_y = points.iter().map(|(_, y)| *y).fold(f64::NEG_INFINITY, f64::max);\n\n                let margin_x = (max_x - min_x) * 0.1;\n                let margin_y = (max_y - min_y) * 0.1;\n\n                let mut chart = ChartBuilder::on(area)\n                    .margin(5)\n                    .build_cartesian_2d(\n                        (min_x - margin_x)..(max_x + margin_x),\n                        (min_y - margin_y)..(max_y + margin_y),\n                    )?;\n\n                // Subtle gray crosshairs at origin\n                chart.configure_mesh()\n                    .disable_x_mesh()\n                    .disable_y_mesh()\n                    .x_labels(0)\n                    .y_labels(0)\n                    .axis_style(RGBAColor(50, 50, 50, 0.3))\n                    .draw()?;\n\n                // Plot points colored by token value\n                for token in 0..p {\n                    let color = token_color(token, p);\n                    let (x, y) = points[token];\n                    chart.draw_series(std::iter::once(Circle::new(\n                        (x, y),\n                        point_size,\n                        color.filled(),\n                    )))?;\n                }\n            }\n        }\n    }\n\n    println!(); // newline after progress dots\n    root.present()?;\n    println!(\"   ðŸ“Š Embedding grid plot saved to: {}\", output_path);\n    println!(\"      Title: {}\", title);\n    Ok(())\n}\n\n/// Select 7 interesting dimensions from embeddings based on variance\npub fn select_interesting_dimensions(embeddings: &[Vec<f64>], n: usize) -> Vec<usize> {\n    let d = embeddings[0].len(); // embedding dimension\n    let p = embeddings.len(); // number of tokens\n\n    // Compute variance for each dimension\n    let mut variances: Vec<(usize, f64)> = Vec::with_capacity(d);\n\n    for dim in 0..d {\n        // Compute mean\n        let mean: f64 = embeddings.iter().map(|emb| emb[dim]).sum::<f64>() / p as f64;\n\n        // Compute variance\n        let variance: f64 = embeddings\n            .iter()\n            .map(|emb| (emb[dim] - mean).powi(2))\n            .sum::<f64>()\n            / p as f64;\n\n        variances.push((dim, variance));\n    }\n\n    // Sort by variance (descending)\n    variances.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap_or(std::cmp::Ordering::Equal));\n\n    // Take top n dimensions\n    variances.iter().take(n).map(|(dim, _)| *dim).collect()\n}\n\n/// Convert viridis color to image::Rgb\n// Helper functions for alternative visualizations - reserved for future use\n#[allow(dead_code)]\nfn viridis_to_rgb(t: f64) -> Rgb<u8> {\n    let color = viridis_color(t);\n    Rgb([color.0, color.1, color.2])\n}\n\n/// Map token value to RGB color\n#[allow(dead_code)]\nfn token_to_rgb(token_value: usize, p: usize) -> Rgb<u8> {\n    let t = token_value as f64 / (p - 1) as f64;\n    viridis_to_rgb(t)\n}\n\n/// Fast 7Ã—7 embedding grid using direct pixel manipulation\npub fn plot_embedding_grid_fast(\n    embeddings: &[Vec<f64>],\n    dimensions: &[usize; 7],\n    output_path: &str,\n    title: &str,\n) -> Result<(), Box<dyn std::error::Error>> {\n    let p = embeddings.len(); // number of tokens\n    let cell_size: usize = 300; // pixels per cell\n    let grid_size: usize = 7;\n    let total_size = cell_size * grid_size;\n\n    println!(\"   ðŸŽ¨ Creating {}Ã—{} pixel canvas...\", total_size, total_size);\n    let mut img: RgbImage = ImageBuffer::from_pixel(total_size as u32, total_size as u32, Rgb([0, 0, 0]));\n\n    println!(\"   ðŸ–Œï¸  Rendering {} cells...\", grid_size * grid_size);\n\n    for row in 0..grid_size as usize {\n        for col in 0..grid_size as usize {\n            let cell_idx = row * grid_size as usize + col;\n            if cell_idx % 10 == 0 {\n                print!(\".\");\n                use std::io::Write;\n                std::io::stdout().flush().unwrap();\n            }\n\n            let x_offset = col * cell_size as usize;\n            let y_offset = row * cell_size as usize;\n\n            if col == 0 {\n                // First column: dimension vs token index\n                let dim = dimensions[row];\n                render_dimension_plot(\n                    &mut img,\n                    embeddings,\n                    dim,\n                    p,\n                    x_offset,\n                    y_offset,\n                    cell_size,\n                );\n            } else {\n                // Pairwise scatter plot\n                let dim_x = dimensions[col];\n                let dim_y = dimensions[row];\n                render_scatter_plot(\n                    &mut img,\n                    embeddings,\n                    dim_x,\n                    dim_y,\n                    p,\n                    x_offset,\n                    y_offset,\n                    cell_size,\n                );\n            }\n        }\n    }\n\n    println!(); // newline after progress dots\n    println!(\"   ðŸ’¾ Saving image to {}...\", output_path);\n    img.save(output_path)?;\n    println!(\"   âœ… Embedding grid saved!\");\n    println!(\"      Title: {}\", title);\n    Ok(())\n}\n\n/// Render dimension vs index plot into a cell\nfn render_dimension_plot(\n    img: &mut RgbImage,\n    embeddings: &[Vec<f64>],\n    dim: usize,\n    p: usize,\n    x_offset: usize,\n    y_offset: usize,\n    cell_size: usize,\n) {\n    // Find min/max for this dimension\n    let values: Vec<f64> = (0..p).map(|token| embeddings[token][dim]).collect();\n    let min_val = values.iter().copied().fold(f64::INFINITY, f64::min);\n    let max_val = values.iter().copied().fold(f64::NEG_INFINITY, f64::max);\n    let range = max_val - min_val;\n\n    if range == 0.0 {\n        return; // Skip if no variation\n    }\n\n    let margin = 20;\n    let plot_width = cell_size - 2 * margin;\n    let plot_height = cell_size - 2 * margin;\n\n    // Draw subtle crosshairs\n    let mid_x = (x_offset + cell_size / 2) as f32;\n    let mid_y = (y_offset + cell_size / 2) as f32;\n    let gray = Rgb([30, 30, 30]);\n    draw_line_segment_mut(\n        img,\n        ((x_offset + margin) as f32, mid_y),\n        ((x_offset + cell_size - margin) as f32, mid_y),\n        gray,\n    );\n    draw_line_segment_mut(\n        img,\n        (mid_x, (y_offset + margin) as f32),\n        (mid_x, (y_offset + cell_size - margin) as f32),\n        gray,\n    );\n\n    // Plot points\n    for token in 0..p {\n        let x = x_offset + margin + (token * plot_width / p);\n        let normalized = (values[token] - min_val) / range;\n        let y = y_offset + cell_size - margin - (normalized * plot_height as f64) as usize;\n\n        let color = token_to_rgb(token, p);\n        draw_filled_circle_mut(img, (x as i32, y as i32), 2, color);\n    }\n}\n\n/// Render scatter plot into a cell\nfn render_scatter_plot(\n    img: &mut RgbImage,\n    embeddings: &[Vec<f64>],\n    dim_x: usize,\n    dim_y: usize,\n    p: usize,\n    x_offset: usize,\n    y_offset: usize,\n    cell_size: usize,\n) {\n    // Find min/max for both dimensions\n    let values_x: Vec<f64> = (0..p).map(|token| embeddings[token][dim_x]).collect();\n    let values_y: Vec<f64> = (0..p).map(|token| embeddings[token][dim_y]).collect();\n\n    let min_x = values_x.iter().copied().fold(f64::INFINITY, f64::min);\n    let max_x = values_x.iter().copied().fold(f64::NEG_INFINITY, f64::max);\n    let min_y = values_y.iter().copied().fold(f64::INFINITY, f64::min);\n    let max_y = values_y.iter().copied().fold(f64::NEG_INFINITY, f64::max);\n\n    let range_x = max_x - min_x;\n    let range_y = max_y - min_y;\n\n    if range_x == 0.0 || range_y == 0.0 {\n        return; // Skip if no variation\n    }\n\n    let margin = 20;\n    let plot_width = cell_size - 2 * margin;\n    let plot_height = cell_size - 2 * margin;\n\n    // Draw subtle crosshairs at origin\n    let mid_x = (x_offset + cell_size / 2) as f32;\n    let mid_y = (y_offset + cell_size / 2) as f32;\n    let gray = Rgb([30, 30, 30]);\n    draw_line_segment_mut(\n        img,\n        ((x_offset + margin) as f32, mid_y),\n        ((x_offset + cell_size - margin) as f32, mid_y),\n        gray,\n    );\n    draw_line_segment_mut(\n        img,\n        (mid_x, (y_offset + margin) as f32),\n        (mid_x, (y_offset + cell_size - margin) as f32),\n        gray,\n    );\n\n    // Plot points\n    for token in 0..p {\n        let norm_x = (values_x[token] - min_x) / range_x;\n        let norm_y = (values_y[token] - min_y) / range_y;\n\n        let x = x_offset + margin + (norm_x * plot_width as f64) as usize;\n        let y = y_offset + cell_size - margin - (norm_y * plot_height as f64) as usize;\n\n        let color = token_to_rgb(token, p);\n        draw_filled_circle_mut(img, (x as i32, y as i32), 2, color);\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn logscale_plotting_functions_exist() {\n        // Verify that log-scale plotting functions exist and have the correct signature\n        // This is a compile-time check that ensures the API is correct\n        let _: fn(&[(usize, f64)], &[(usize, f64)], &str) -> Result<(), Box<dyn std::error::Error>> =\n            plot_loss_history_dual_logscale;\n        let _: fn(&[(usize, f32)], &[(usize, f32)], &str) -> Result<(), Box<dyn std::error::Error>> =\n            plot_accuracy_history_logscale;\n        let _: fn(&[(usize, f64)], &[(usize, f64)], &[(usize, f32)], &[(usize, f32)], &str) -> Result<(), Box<dyn std::error::Error>> =\n            plot_grokking_combined_logscale;\n    }\n\n    #[test]\n    fn snake_curve_can_be_verified_with_plotting_data() {\n        use crate::verify::{verify_snake_curve_shape, SnakeCurveConfig};\n\n        // Simulate grokking data that would produce a snake curve on log-scale plot\n        let train_acc = vec![\n            (1, 0.10),\n            (10, 0.50),\n            (100, 0.90),\n            (500, 0.99),\n            (1000, 0.995),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (10, 0.01),\n            (100, 0.01),\n            (500, 0.01),\n            (1000, 0.01),\n            (2000, 0.95),\n        ];\n\n        let config = SnakeCurveConfig::default_for_modulus(113);\n        let report = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect(\"snake curve verification should pass on grokking data\");\n\n        // Verify the snake curve characteristics are present\n        assert!(report.train_converged_by_step < 1000, \"train should converge early\");\n        assert!(report.val_grok_step > 1000, \"val should grok late\");\n        assert!(report.curve_shape_valid, \"curve shape should be valid\");\n    }\n}\n","structuredPatch":[{"oldStart":887,"oldLines":6,"newStart":887,"newLines":8,"lines":[" }"," "," /// Fast 7Ã—7 embedding grid using direct pixel manipulation","+/// NOTE: Alternative visualization approach - reserved for future use","+#[allow(dead_code)]"," pub fn plot_embedding_grid_fast(","     embeddings: &[Vec<f64>],","     dimensions: &[usize; 7],"]}],"userModified":false,"replaceAll":false}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01XUQFTLWqbN3CdA4oKS6Zts","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01WwFpA8HNKjTvUrC6ddZnfV","name":"Bash","input":{"command":"grep -n \"^fn render_dimension_plot\\|^fn render_scatter_plot\" src/plotting.rs","description":"Find render functions"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":489,"cache_read_input_tokens":67413,"cache_creation":{"ephemeral_5m_input_tokens":489,"ephemeral_1h_input_tokens":0},"output_tokens":26,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"a67f9f71-9bab-49a8-87fe-d4c7c0a0eef0"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01WwFpA8HNKjTvUrC6ddZnfV","type":"tool_result","content":"959:fn render_dimension_plot(\n1011:fn render_scatter_plot(","is_error":false}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"f72ace8e-d3f4-4923-9461-40260cfed09d","tool_use_result":{"stdout":"959:fn render_dimension_plot(\n1011:fn render_scatter_plot(","stderr":"","interrupted":false,"isImage":false}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01Npr45wUy93opPcXv8xNVXT","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_011mNEw6oQVUowJzmGGfpVHf","name":"Edit","input":{"replace_all":false,"file_path":"/Users/admin/MechInt/src/plotting.rs","old_string":"fn render_dimension_plot(","new_string":"#[allow(dead_code)]\nfn render_dimension_plot("}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":128,"cache_read_input_tokens":67902,"cache_creation":{"ephemeral_5m_input_tokens":128,"ephemeral_1h_input_tokens":0},"output_tokens":144,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"c9fda6c9-f1a9-4691-8e27-24060f1822fc"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_011mNEw6oQVUowJzmGGfpVHf","type":"tool_result","content":"The file /Users/admin/MechInt/src/plotting.rs has been updated. Here's the result of running `cat -n` on a snippet of the edited file:\n   955â†’    Ok(())\n   956â†’}\n   957â†’\n   958â†’/// Render dimension vs index plot into a cell\n   959â†’#[allow(dead_code)]\n   960â†’fn render_dimension_plot(\n   961â†’    img: &mut RgbImage,\n   962â†’    embeddings: &[Vec<f64>],\n   963â†’    dim: usize,\n   964â†’    p: usize,"}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"45f8cd6f-0438-4845-8292-dfa9d59881aa","tool_use_result":{"filePath":"/Users/admin/MechInt/src/plotting.rs","oldString":"fn render_dimension_plot(","newString":"#[allow(dead_code)]\nfn render_dimension_plot(","originalFile":"use image::{ImageBuffer, Rgb, RgbImage};\nuse imageproc::drawing::{draw_filled_circle_mut, draw_line_segment_mut};\nuse plotters::prelude::*;\nuse std::collections::HashMap;\n\nuse crate::data::ModularAdditionDataset;\n\n/// Viridis colormap - maps value in [0, 1] to RGB\nfn viridis_color(t: f64) -> RGBColor {\n    // Viridis colormap approximation (purple â†’ cyan â†’ yellow)\n    // Based on matplotlib's viridis\n    let r = (0.267 + t * (0.329 - 0.267 + t * (0.984 - 0.329))) * 255.0;\n    let g = (0.005 + t * (0.569 - 0.005 + t * (0.906 - 0.569))) * 255.0;\n    let b = (0.329 + t * (0.758 - 0.329 - t * (0.758 - 0.121))) * 255.0;\n\n    RGBColor(\n        r.min(255.0).max(0.0) as u8,\n        g.min(255.0).max(0.0) as u8,\n        b.min(255.0).max(0.0) as u8,\n    )\n}\n\n/// Map token value (0 to p-1) to viridis color\nfn token_color(token_value: usize, p: usize) -> RGBColor {\n    let t = token_value as f64 / (p - 1) as f64;\n    viridis_color(t)\n}\n\n/// Plot training and validation loss over time\npub fn plot_loss_history_dual(\n    train_loss: &[(usize, f64)], // (step, loss)\n    val_loss: &[(usize, f64)],\n    output_path: &str,\n) -> Result<(), Box<dyn std::error::Error>> {\n    let root = BitMapBackend::new(output_path, (1200, 800)).into_drawing_area();\n    root.fill(&WHITE)?;\n\n    let max_step = train_loss\n        .iter()\n        .chain(val_loss.iter())\n        .map(|(s, _)| *s)\n        .max()\n        .unwrap_or(1);\n\n    let max_loss = train_loss\n        .iter()\n        .chain(val_loss.iter())\n        .map(|(_, l)| *l)\n        .fold(0.0f64, f64::max)\n        .max(1.0);\n\n    let mut chart = ChartBuilder::on(&root)\n        .caption(\"Loss Evolution: Train vs Validation\", (\"sans-serif\", 50).into_font())\n        .margin(20)\n        .x_label_area_size(50)\n        .y_label_area_size(70)\n        .build_cartesian_2d(0usize..max_step, 0f64..max_loss)?;\n\n    chart\n        .configure_mesh()\n        .x_desc(\"Training Step\")\n        .y_desc(\"Loss\")\n        .draw()?;\n\n    // Plot training loss\n    chart\n        .draw_series(LineSeries::new(\n            train_loss.iter().map(|(s, l)| (*s, *l)),\n            &RED,\n        ))?\n        .label(\"Train Loss\")\n        .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &RED));\n\n    // Plot validation loss\n    chart\n        .draw_series(LineSeries::new(\n            val_loss.iter().map(|(s, l)| (*s, *l)),\n            &GREEN,\n        ))?\n        .label(\"Val Loss\")\n        .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &GREEN));\n\n    let modulus = ModularAdditionDataset::modulus() as f64;\n    let chance_loss = -(1.0 / modulus).ln();\n    chart\n        .draw_series(LineSeries::new(\n            vec![(0, chance_loss), (max_step, chance_loss)],\n            &BLUE.mix(0.5),\n        ))?\n        .label(\"Chance Level\")\n        .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &BLUE.mix(0.5)));\n\n    chart\n        .configure_series_labels()\n        .background_style(&WHITE.mix(0.8))\n        .border_style(&BLACK)\n        .draw()?;\n\n    root.present()?;\n    println!(\"ðŸ“Š Loss plot saved to: {}\", output_path);\n    Ok(())\n}\n\n/// Plot training loss over time (legacy, single loss)\n/// NOTE: Superseded by plot_loss_history_dual - kept for reference\n#[allow(dead_code)]\npub fn plot_loss_history(\n    snapshots: &[(usize, f64)], // (step, loss)\n    output_path: &str,\n) -> Result<(), Box<dyn std::error::Error>> {\n    let root = BitMapBackend::new(output_path, (1200, 800)).into_drawing_area();\n    root.fill(&WHITE)?;\n\n    let max_step = snapshots.iter().map(|(s, _)| *s).max().unwrap_or(1);\n    let max_loss = snapshots\n        .iter()\n        .map(|(_, l)| *l)\n        .fold(0.0f64, f64::max)\n        .max(1.0);\n\n    let mut chart = ChartBuilder::on(&root)\n        .caption(\"Training Loss Evolution\", (\"sans-serif\", 50).into_font())\n        .margin(20)\n        .x_label_area_size(50)\n        .y_label_area_size(70)\n        .build_cartesian_2d(0usize..max_step, 0f64..max_loss)?;\n\n    chart\n        .configure_mesh()\n        .x_desc(\"Training Step\")\n        .y_desc(\"Loss\")\n        .draw()?;\n\n    chart.draw_series(LineSeries::new(\n        snapshots.iter().map(|(s, l)| (*s, *l)),\n        &BLUE,\n    ))?;\n\n    root.present()?;\n    println!(\"ðŸ“Š Loss plot saved to: {}\", output_path);\n    Ok(())\n}\n\n/// Plot training and validation accuracy over time (combined)\npub fn plot_accuracy_history(\n    train_acc: &[(usize, f32)], // (step, accuracy)\n    val_acc: &[(usize, f32)],\n    output_path: &str,\n) -> Result<(), Box<dyn std::error::Error>> {\n    let root = BitMapBackend::new(output_path, (1200, 800)).into_drawing_area();\n    root.fill(&WHITE)?;\n\n    let max_step = train_acc\n        .iter()\n        .chain(val_acc.iter())\n        .map(|(s, _)| *s)\n        .max()\n        .unwrap_or(1);\n\n    let mut chart = ChartBuilder::on(&root)\n        .caption(\n            \"Grokking Phenomenon: Train vs Validation Accuracy\",\n            (\"sans-serif\", 50).into_font(),\n        )\n        .margin(20)\n        .x_label_area_size(50)\n        .y_label_area_size(70)\n        .build_cartesian_2d(0usize..max_step, 0f32..105f32)?;\n\n    chart\n        .configure_mesh()\n        .x_desc(\"Training Step\")\n        .y_desc(\"Accuracy (%)\")\n        .draw()?;\n\n    // Plot training accuracy\n    chart\n        .draw_series(LineSeries::new(\n            train_acc.iter().map(|(s, a)| (*s, a * 100.0)),\n            &BLUE,\n        ))?\n        .label(\"Training Accuracy\")\n        .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &BLUE));\n\n    // Plot validation accuracy\n    chart\n        .draw_series(LineSeries::new(\n            val_acc.iter().map(|(s, a)| (*s, a * 100.0)),\n            &RED,\n        ))?\n        .label(\"Validation Accuracy\")\n        .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &RED));\n\n    // Add reference line at 90% (grokking threshold)\n    chart.draw_series(LineSeries::new(\n        vec![(0, 90.0), (max_step, 90.0)],\n        &GREEN.mix(0.5),\n    ))?;\n\n    chart\n        .configure_series_labels()\n        .background_style(&WHITE.mix(0.8))\n        .border_style(&BLACK)\n        .draw()?;\n\n    root.present()?;\n    println!(\"ðŸ“Š Accuracy plot saved to: {}\", output_path);\n    Ok(())\n}\n\n/// Plot FFT dominant frequency distribution\npub fn plot_fft_frequency_distribution(\n    dominant_frequencies: &[(usize, usize, f64)], // (token_id, frequency, magnitude)\n    output_path: &str,\n) -> Result<(), Box<dyn std::error::Error>> {\n    let root = BitMapBackend::new(output_path, (1200, 800)).into_drawing_area();\n    root.fill(&WHITE)?;\n\n    // Count frequency occurrences\n    let mut freq_counts: HashMap<usize, usize> = HashMap::new();\n    for (_, freq, _) in dominant_frequencies {\n        *freq_counts.entry(*freq).or_default() += 1;\n    }\n\n    let mut freq_vec: Vec<(usize, usize)> = freq_counts.into_iter().collect();\n    freq_vec.sort_by_key(|(f, _)| *f);\n\n    let max_freq = freq_vec.iter().map(|(f, _)| *f).max().unwrap_or(1);\n    let max_count = freq_vec.iter().map(|(_, c)| *c).max().unwrap_or(1);\n\n    let mut chart = ChartBuilder::on(&root)\n        .caption(\n            \"FFT Dominant Frequency Distribution\",\n            (\"sans-serif\", 50).into_font(),\n        )\n        .margin(20)\n        .x_label_area_size(50)\n        .y_label_area_size(70)\n        .build_cartesian_2d(0usize..max_freq, 0usize..max_count + 1)?;\n\n    chart\n        .configure_mesh()\n        .x_desc(\"Frequency\")\n        .y_desc(\"Number of Tokens\")\n        .draw()?;\n\n    chart.draw_series(\n        freq_vec\n            .iter()\n            .map(|(freq, count)| {\n                let color = if *freq == ModularAdditionDataset::modulus() {\n                    RED.filled() // Highlight frequency matching modulus\n                } else {\n                    BLUE.filled()\n                };\n                Rectangle::new([(*freq, 0), (*freq, *count)], color)\n            }),\n    )?;\n\n    root.present()?;\n    println!(\"ðŸ“Š FFT frequency distribution plot saved to: {}\", output_path);\n    Ok(())\n}\n\n/// Plot FFT magnitude spectrum for selected tokens\npub fn plot_fft_spectra(\n    token_spectra: &[(usize, Vec<f64>)], // (token_id, magnitude_spectrum)\n    output_path: &str,\n) -> Result<(), Box<dyn std::error::Error>> {\n    let root = BitMapBackend::new(output_path, (1200, 800)).into_drawing_area();\n    root.fill(&WHITE)?;\n\n    let max_freq = token_spectra\n        .first()\n        .map(|(_, spec)| spec.len())\n        .unwrap_or(1);\n    let max_magnitude = token_spectra\n        .iter()\n        .flat_map(|(_, spec)| spec.iter())\n        .copied()\n        .fold(0.0f64, f64::max)\n        .max(1.0);\n\n    let mut chart = ChartBuilder::on(&root)\n        .caption(\n            \"FFT Magnitude Spectra (Selected Tokens)\",\n            (\"sans-serif\", 50).into_font(),\n        )\n        .margin(20)\n        .x_label_area_size(50)\n        .y_label_area_size(70)\n        .build_cartesian_2d(0usize..max_freq, 0f64..max_magnitude)?;\n\n    chart\n        .configure_mesh()\n        .x_desc(\"Frequency\")\n        .y_desc(\"Magnitude\")\n        .draw()?;\n\n    let colors = [&BLUE, &RED, &GREEN, &MAGENTA, &CYAN, &YELLOW];\n\n    for (idx, (token_id, spectrum)) in token_spectra.iter().enumerate() {\n        let color = colors[idx % colors.len()];\n        chart\n            .draw_series(LineSeries::new(\n                spectrum.iter().enumerate().map(|(f, m)| (f, *m)),\n                color,\n            ))?\n            .label(format!(\"Token {}\", token_id))\n            .legend(move |(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], color));\n    }\n\n    chart\n        .configure_series_labels()\n        .background_style(&WHITE.mix(0.8))\n        .border_style(&BLACK)\n        .draw()?;\n\n    root.present()?;\n    println!(\"ðŸ“Š FFT spectra plot saved to: {}\", output_path);\n    Ok(())\n}\n\n/// Plot combined grokking visualization (loss + accuracy on same timeline)\npub fn plot_grokking_combined(\n    train_loss: &[(usize, f64)],\n    val_loss: &[(usize, f64)],\n    train_acc: &[(usize, f32)],\n    val_acc: &[(usize, f32)],\n    output_path: &str,\n) -> Result<(), Box<dyn std::error::Error>> {\n    let root = BitMapBackend::new(output_path, (1400, 900)).into_drawing_area();\n    root.fill(&WHITE)?;\n\n    let (upper, lower) = root.split_vertically(450);\n\n    // Upper plot: Accuracy\n    {\n        let max_step = train_acc\n            .iter()\n            .chain(val_acc.iter())\n            .map(|(s, _)| *s)\n            .max()\n            .unwrap_or(1);\n\n        let mut chart = ChartBuilder::on(&upper)\n            .caption(\n                \"Grokking: Accuracy Evolution\",\n                (\"sans-serif\", 40).into_font(),\n            )\n            .margin(15)\n            .x_label_area_size(40)\n            .y_label_area_size(60)\n            .build_cartesian_2d(0usize..max_step, 0f32..105f32)?;\n\n        chart\n            .configure_mesh()\n            .x_desc(\"Training Step\")\n            .y_desc(\"Accuracy (%)\")\n            .draw()?;\n\n        chart\n            .draw_series(LineSeries::new(\n                train_acc.iter().map(|(s, a)| (*s, a * 100.0)),\n                &BLUE,\n            ))?\n            .label(\"Training\")\n            .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &BLUE));\n\n        chart\n            .draw_series(LineSeries::new(\n                val_acc.iter().map(|(s, a)| (*s, a * 100.0)),\n                &RED,\n            ))?\n            .label(\"Validation\")\n            .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &RED));\n\n        chart\n            .configure_series_labels()\n            .background_style(&WHITE.mix(0.8))\n            .border_style(&BLACK)\n            .draw()?;\n    }\n\n    // Lower plot: Loss (train and val)\n    {\n        let max_step = train_loss\n            .iter()\n            .chain(val_loss.iter())\n            .map(|(s, _)| *s)\n            .max()\n            .unwrap_or(1);\n\n        let max_loss = train_loss\n            .iter()\n            .chain(val_loss.iter())\n            .map(|(_, l)| *l)\n            .fold(0.0f64, f64::max)\n            .max(1.0);\n\n        let mut chart = ChartBuilder::on(&lower)\n            .caption(\"Loss Evolution\", (\"sans-serif\", 40).into_font())\n            .margin(15)\n            .x_label_area_size(40)\n            .y_label_area_size(60)\n            .build_cartesian_2d(0usize..max_step, 0f64..max_loss)?;\n\n        chart\n            .configure_mesh()\n            .x_desc(\"Training Step\")\n            .y_desc(\"Loss\")\n            .draw()?;\n\n        // Train loss\n        chart\n            .draw_series(LineSeries::new(\n                train_loss.iter().map(|(s, l)| (*s, *l)),\n                &RED,\n            ))?\n            .label(\"Train\")\n            .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &RED));\n\n        // Val loss\n        chart\n            .draw_series(LineSeries::new(\n                val_loss.iter().map(|(s, l)| (*s, *l)),\n                &GREEN,\n            ))?\n            .label(\"Val\")\n            .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &GREEN));\n\n        // Chance level\n        let modulus = ModularAdditionDataset::modulus() as f64;\n        let chance_loss = -(1.0 / modulus).ln();\n        chart.draw_series(LineSeries::new(\n            vec![(0, chance_loss), (max_step, chance_loss)],\n            &BLUE.mix(0.5),\n        ))?;\n\n        chart\n            .configure_series_labels()\n            .background_style(&WHITE.mix(0.8))\n            .border_style(&BLACK)\n            .draw()?;\n    }\n\n    root.present()?;\n    println!(\"ðŸ“Š Combined grokking plot saved to: {}\", output_path);\n    Ok(())\n}\n\n/// Plot training and validation loss over time with LOG-SCALE x-axis\npub fn plot_loss_history_dual_logscale(\n    train_loss: &[(usize, f64)], // (step, loss)\n    val_loss: &[(usize, f64)],\n    output_path: &str,\n) -> Result<(), Box<dyn std::error::Error>> {\n    let root = BitMapBackend::new(output_path, (1200, 800)).into_drawing_area();\n    root.fill(&WHITE)?;\n\n    let max_step = train_loss\n        .iter()\n        .chain(val_loss.iter())\n        .map(|(s, _)| *s)\n        .max()\n        .unwrap_or(1);\n\n    let max_loss = train_loss\n        .iter()\n        .chain(val_loss.iter())\n        .map(|(_, l)| *l)\n        .fold(0.0f64, f64::max)\n        .max(1.0);\n\n    let mut chart = ChartBuilder::on(&root)\n        .caption(\"Loss Evolution (Log Scale): Train vs Validation\", (\"sans-serif\", 50).into_font())\n        .margin(20)\n        .x_label_area_size(50)\n        .y_label_area_size(70)\n        .build_cartesian_2d((1.0..max_step as f64).log_scale(), 0f64..max_loss)?;\n\n    chart\n        .configure_mesh()\n        .x_desc(\"Training Step (log scale)\")\n        .y_desc(\"Loss\")\n        .draw()?;\n\n    // Plot training loss\n    chart\n        .draw_series(LineSeries::new(\n            train_loss.iter().map(|(s, l)| (*s as f64, *l)),\n            &RED,\n        ))?\n        .label(\"Train Loss\")\n        .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &RED));\n\n    // Plot validation loss\n    chart\n        .draw_series(LineSeries::new(\n            val_loss.iter().map(|(s, l)| (*s as f64, *l)),\n            &GREEN,\n        ))?\n        .label(\"Val Loss\")\n        .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &GREEN));\n\n    let modulus = ModularAdditionDataset::modulus() as f64;\n    let chance_loss = -(1.0 / modulus).ln();\n    chart\n        .draw_series(LineSeries::new(\n            vec![(1.0, chance_loss), (max_step as f64, chance_loss)],\n            &BLUE.mix(0.5),\n        ))?\n        .label(\"Chance Level\")\n        .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &BLUE.mix(0.5)));\n\n    chart\n        .configure_series_labels()\n        .background_style(&WHITE.mix(0.8))\n        .border_style(&BLACK)\n        .draw()?;\n\n    root.present()?;\n    println!(\"ðŸ“Š Loss plot (log scale) saved to: {}\", output_path);\n    Ok(())\n}\n\n/// Plot training and validation accuracy over time with LOG-SCALE x-axis\npub fn plot_accuracy_history_logscale(\n    train_acc: &[(usize, f32)], // (step, accuracy)\n    val_acc: &[(usize, f32)],\n    output_path: &str,\n) -> Result<(), Box<dyn std::error::Error>> {\n    let root = BitMapBackend::new(output_path, (1200, 800)).into_drawing_area();\n    root.fill(&WHITE)?;\n\n    let max_step = train_acc\n        .iter()\n        .chain(val_acc.iter())\n        .map(|(s, _)| *s)\n        .max()\n        .unwrap_or(1);\n\n    let mut chart = ChartBuilder::on(&root)\n        .caption(\n            \"Grokking Phenomenon (Log Scale): Train vs Validation Accuracy\",\n            (\"sans-serif\", 50).into_font(),\n        )\n        .margin(20)\n        .x_label_area_size(50)\n        .y_label_area_size(70)\n        .build_cartesian_2d((1.0..max_step as f64).log_scale(), 0f32..105f32)?;\n\n    chart\n        .configure_mesh()\n        .x_desc(\"Training Step (log scale)\")\n        .y_desc(\"Accuracy (%)\")\n        .draw()?;\n\n    // Plot training accuracy\n    chart\n        .draw_series(LineSeries::new(\n            train_acc.iter().map(|(s, a)| (*s as f64, a * 100.0)),\n            &BLUE,\n        ))?\n        .label(\"Training Accuracy\")\n        .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &BLUE));\n\n    // Plot validation accuracy\n    chart\n        .draw_series(LineSeries::new(\n            val_acc.iter().map(|(s, a)| (*s as f64, a * 100.0)),\n            &RED,\n        ))?\n        .label(\"Validation Accuracy\")\n        .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &RED));\n\n    // Add reference line at 90% (grokking threshold)\n    chart.draw_series(LineSeries::new(\n        vec![(1.0, 90.0), (max_step as f64, 90.0)],\n        &GREEN.mix(0.5),\n    ))?;\n\n    chart\n        .configure_series_labels()\n        .background_style(&WHITE.mix(0.8))\n        .border_style(&BLACK)\n        .draw()?;\n\n    root.present()?;\n    println!(\"ðŸ“Š Accuracy plot (log scale) saved to: {}\", output_path);\n    Ok(())\n}\n\n/// Plot combined grokking visualization with LOG-SCALE x-axis (loss + accuracy)\npub fn plot_grokking_combined_logscale(\n    train_loss: &[(usize, f64)],\n    val_loss: &[(usize, f64)],\n    train_acc: &[(usize, f32)],\n    val_acc: &[(usize, f32)],\n    output_path: &str,\n) -> Result<(), Box<dyn std::error::Error>> {\n    let root = BitMapBackend::new(output_path, (1400, 900)).into_drawing_area();\n    root.fill(&WHITE)?;\n\n    let (upper, lower) = root.split_vertically(450);\n\n    // Upper plot: Accuracy (log scale)\n    {\n        let max_step = train_acc\n            .iter()\n            .chain(val_acc.iter())\n            .map(|(s, _)| *s)\n            .max()\n            .unwrap_or(1);\n\n        let mut chart = ChartBuilder::on(&upper)\n            .caption(\n                \"Grokking: Accuracy Evolution (Log Scale)\",\n                (\"sans-serif\", 40).into_font(),\n            )\n            .margin(15)\n            .x_label_area_size(40)\n            .y_label_area_size(60)\n            .build_cartesian_2d((1.0..max_step as f64).log_scale(), 0f32..105f32)?;\n\n        chart\n            .configure_mesh()\n            .x_desc(\"Training Step (log scale)\")\n            .y_desc(\"Accuracy (%)\")\n            .draw()?;\n\n        chart\n            .draw_series(LineSeries::new(\n                train_acc.iter().map(|(s, a)| (*s as f64, a * 100.0)),\n                &BLUE,\n            ))?\n            .label(\"Training\")\n            .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &BLUE));\n\n        chart\n            .draw_series(LineSeries::new(\n                val_acc.iter().map(|(s, a)| (*s as f64, a * 100.0)),\n                &RED,\n            ))?\n            .label(\"Validation\")\n            .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &RED));\n\n        chart\n            .configure_series_labels()\n            .background_style(&WHITE.mix(0.8))\n            .border_style(&BLACK)\n            .draw()?;\n    }\n\n    // Lower plot: Loss (log scale)\n    {\n        let max_step = train_loss\n            .iter()\n            .chain(val_loss.iter())\n            .map(|(s, _)| *s)\n            .max()\n            .unwrap_or(1);\n\n        let max_loss = train_loss\n            .iter()\n            .chain(val_loss.iter())\n            .map(|(_, l)| *l)\n            .fold(0.0f64, f64::max)\n            .max(1.0);\n\n        let mut chart = ChartBuilder::on(&lower)\n            .caption(\"Loss Evolution (Log Scale)\", (\"sans-serif\", 40).into_font())\n            .margin(15)\n            .x_label_area_size(40)\n            .y_label_area_size(60)\n            .build_cartesian_2d((1.0..max_step as f64).log_scale(), 0f64..max_loss)?;\n\n        chart\n            .configure_mesh()\n            .x_desc(\"Training Step (log scale)\")\n            .y_desc(\"Loss\")\n            .draw()?;\n\n        // Train loss\n        chart\n            .draw_series(LineSeries::new(\n                train_loss.iter().map(|(s, l)| (*s as f64, *l)),\n                &RED,\n            ))?\n            .label(\"Train\")\n            .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &RED));\n\n        // Val loss\n        chart\n            .draw_series(LineSeries::new(\n                val_loss.iter().map(|(s, l)| (*s as f64, *l)),\n                &GREEN,\n            ))?\n            .label(\"Val\")\n            .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &GREEN));\n\n        // Chance level\n        let modulus = ModularAdditionDataset::modulus() as f64;\n        let chance_loss = -(1.0 / modulus).ln();\n        chart.draw_series(LineSeries::new(\n            vec![(1.0, chance_loss), (max_step as f64, chance_loss)],\n            &BLUE.mix(0.5),\n        ))?;\n\n        chart\n            .configure_series_labels()\n            .background_style(&WHITE.mix(0.8))\n            .border_style(&BLACK)\n            .draw()?;\n    }\n\n    root.present()?;\n    println!(\"ðŸ“Š Combined grokking plot (log scale) saved to: {}\", output_path);\n    Ok(())\n}\n\n/// Plot 3Ã—3 embedding analysis grid (faster version)\n/// embeddings: [p, d] matrix where p is the number of tokens, d=embedding_dim\n/// dimensions: indices of 3 embedding dimensions to visualize\npub fn plot_embedding_grid_3x3(\n    embeddings: &[Vec<f64>],\n    dimensions: &[usize; 3],\n    output_path: &str,\n    title: &str,\n) -> Result<(), Box<dyn std::error::Error>> {\n    let p = embeddings.len(); // number of tokens\n    let point_size = 3; // slightly larger for visibility\n\n    // Create canvas for 3Ã—3 grid\n    let root = BitMapBackend::new(output_path, (1200, 1200)).into_drawing_area();\n    root.fill(&BLACK)?; // Black background\n\n    // Split into 3Ã—3 grid\n    let cell_areas = root.split_evenly((3, 3));\n\n    println!(\"   Rendering 3Ã—3 grid ({} cells)...\", 9);\n    for row in 0..3 {\n        for col in 0..3 {\n            let idx = row * 3 + col;\n            print!(\".\");\n            use std::io::Write;\n            std::io::stdout().flush().unwrap();\n            let area = &cell_areas[idx];\n\n            if col == 0 {\n                // First column: embedding dimension vs token index\n                let dim = dimensions[row];\n\n                // Get dimension values for all tokens\n                let values: Vec<(usize, f64)> = (0..p)\n                    .map(|token| (token, embeddings[token][dim]))\n                    .collect();\n\n                // Find min/max for y-axis\n                let min_val = values.iter().map(|(_, v)| *v).fold(f64::INFINITY, f64::min);\n                let max_val = values.iter().map(|(_, v)| *v).fold(f64::NEG_INFINITY, f64::max);\n                let margin = (max_val - min_val) * 0.1;\n\n                let mut chart = ChartBuilder::on(area)\n                    .margin(5)\n                    .build_cartesian_2d(\n                        0..p,\n                        (min_val - margin)..(max_val + margin),\n                    )?;\n\n                // Subtle gray crosshairs at origin\n                chart.configure_mesh()\n                    .disable_x_mesh()\n                    .disable_y_mesh()\n                    .x_labels(0)\n                    .y_labels(0)\n                    .axis_style(RGBAColor(50, 50, 50, 0.3))\n                    .draw()?;\n\n                // Plot points colored by token value\n                for (token, value) in values.iter() {\n                    let color = token_color(*token, p);\n                    chart.draw_series(std::iter::once(Circle::new(\n                        (*token, *value),\n                        point_size,\n                        color.filled(),\n                    )))?;\n                }\n            } else {\n                // Pairwise scatter plot\n                let dim_x = dimensions[col];\n                let dim_y = dimensions[row];\n\n                // Extract x and y values\n                let points: Vec<(f64, f64)> = (0..p)\n                    .map(|token| (embeddings[token][dim_x], embeddings[token][dim_y]))\n                    .collect();\n\n                // Find min/max for axes\n                let min_x = points.iter().map(|(x, _)| *x).fold(f64::INFINITY, f64::min);\n                let max_x = points.iter().map(|(x, _)| *x).fold(f64::NEG_INFINITY, f64::max);\n                let min_y = points.iter().map(|(_, y)| *y).fold(f64::INFINITY, f64::min);\n                let max_y = points.iter().map(|(_, y)| *y).fold(f64::NEG_INFINITY, f64::max);\n\n                let margin_x = (max_x - min_x) * 0.1;\n                let margin_y = (max_y - min_y) * 0.1;\n\n                let mut chart = ChartBuilder::on(area)\n                    .margin(5)\n                    .build_cartesian_2d(\n                        (min_x - margin_x)..(max_x + margin_x),\n                        (min_y - margin_y)..(max_y + margin_y),\n                    )?;\n\n                // Subtle gray crosshairs at origin\n                chart.configure_mesh()\n                    .disable_x_mesh()\n                    .disable_y_mesh()\n                    .x_labels(0)\n                    .y_labels(0)\n                    .axis_style(RGBAColor(50, 50, 50, 0.3))\n                    .draw()?;\n\n                // Plot points colored by token value\n                for token in 0..p {\n                    let color = token_color(token, p);\n                    let (x, y) = points[token];\n                    chart.draw_series(std::iter::once(Circle::new(\n                        (x, y),\n                        point_size,\n                        color.filled(),\n                    )))?;\n                }\n            }\n        }\n    }\n\n    println!(); // newline after progress dots\n    root.present()?;\n    println!(\"   ðŸ“Š Embedding grid plot saved to: {}\", output_path);\n    println!(\"      Title: {}\", title);\n    Ok(())\n}\n\n/// Select 7 interesting dimensions from embeddings based on variance\npub fn select_interesting_dimensions(embeddings: &[Vec<f64>], n: usize) -> Vec<usize> {\n    let d = embeddings[0].len(); // embedding dimension\n    let p = embeddings.len(); // number of tokens\n\n    // Compute variance for each dimension\n    let mut variances: Vec<(usize, f64)> = Vec::with_capacity(d);\n\n    for dim in 0..d {\n        // Compute mean\n        let mean: f64 = embeddings.iter().map(|emb| emb[dim]).sum::<f64>() / p as f64;\n\n        // Compute variance\n        let variance: f64 = embeddings\n            .iter()\n            .map(|emb| (emb[dim] - mean).powi(2))\n            .sum::<f64>()\n            / p as f64;\n\n        variances.push((dim, variance));\n    }\n\n    // Sort by variance (descending)\n    variances.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap_or(std::cmp::Ordering::Equal));\n\n    // Take top n dimensions\n    variances.iter().take(n).map(|(dim, _)| *dim).collect()\n}\n\n/// Convert viridis color to image::Rgb\n// Helper functions for alternative visualizations - reserved for future use\n#[allow(dead_code)]\nfn viridis_to_rgb(t: f64) -> Rgb<u8> {\n    let color = viridis_color(t);\n    Rgb([color.0, color.1, color.2])\n}\n\n/// Map token value to RGB color\n#[allow(dead_code)]\nfn token_to_rgb(token_value: usize, p: usize) -> Rgb<u8> {\n    let t = token_value as f64 / (p - 1) as f64;\n    viridis_to_rgb(t)\n}\n\n/// Fast 7Ã—7 embedding grid using direct pixel manipulation\n/// NOTE: Alternative visualization approach - reserved for future use\n#[allow(dead_code)]\npub fn plot_embedding_grid_fast(\n    embeddings: &[Vec<f64>],\n    dimensions: &[usize; 7],\n    output_path: &str,\n    title: &str,\n) -> Result<(), Box<dyn std::error::Error>> {\n    let p = embeddings.len(); // number of tokens\n    let cell_size: usize = 300; // pixels per cell\n    let grid_size: usize = 7;\n    let total_size = cell_size * grid_size;\n\n    println!(\"   ðŸŽ¨ Creating {}Ã—{} pixel canvas...\", total_size, total_size);\n    let mut img: RgbImage = ImageBuffer::from_pixel(total_size as u32, total_size as u32, Rgb([0, 0, 0]));\n\n    println!(\"   ðŸ–Œï¸  Rendering {} cells...\", grid_size * grid_size);\n\n    for row in 0..grid_size as usize {\n        for col in 0..grid_size as usize {\n            let cell_idx = row * grid_size as usize + col;\n            if cell_idx % 10 == 0 {\n                print!(\".\");\n                use std::io::Write;\n                std::io::stdout().flush().unwrap();\n            }\n\n            let x_offset = col * cell_size as usize;\n            let y_offset = row * cell_size as usize;\n\n            if col == 0 {\n                // First column: dimension vs token index\n                let dim = dimensions[row];\n                render_dimension_plot(\n                    &mut img,\n                    embeddings,\n                    dim,\n                    p,\n                    x_offset,\n                    y_offset,\n                    cell_size,\n                );\n            } else {\n                // Pairwise scatter plot\n                let dim_x = dimensions[col];\n                let dim_y = dimensions[row];\n                render_scatter_plot(\n                    &mut img,\n                    embeddings,\n                    dim_x,\n                    dim_y,\n                    p,\n                    x_offset,\n                    y_offset,\n                    cell_size,\n                );\n            }\n        }\n    }\n\n    println!(); // newline after progress dots\n    println!(\"   ðŸ’¾ Saving image to {}...\", output_path);\n    img.save(output_path)?;\n    println!(\"   âœ… Embedding grid saved!\");\n    println!(\"      Title: {}\", title);\n    Ok(())\n}\n\n/// Render dimension vs index plot into a cell\nfn render_dimension_plot(\n    img: &mut RgbImage,\n    embeddings: &[Vec<f64>],\n    dim: usize,\n    p: usize,\n    x_offset: usize,\n    y_offset: usize,\n    cell_size: usize,\n) {\n    // Find min/max for this dimension\n    let values: Vec<f64> = (0..p).map(|token| embeddings[token][dim]).collect();\n    let min_val = values.iter().copied().fold(f64::INFINITY, f64::min);\n    let max_val = values.iter().copied().fold(f64::NEG_INFINITY, f64::max);\n    let range = max_val - min_val;\n\n    if range == 0.0 {\n        return; // Skip if no variation\n    }\n\n    let margin = 20;\n    let plot_width = cell_size - 2 * margin;\n    let plot_height = cell_size - 2 * margin;\n\n    // Draw subtle crosshairs\n    let mid_x = (x_offset + cell_size / 2) as f32;\n    let mid_y = (y_offset + cell_size / 2) as f32;\n    let gray = Rgb([30, 30, 30]);\n    draw_line_segment_mut(\n        img,\n        ((x_offset + margin) as f32, mid_y),\n        ((x_offset + cell_size - margin) as f32, mid_y),\n        gray,\n    );\n    draw_line_segment_mut(\n        img,\n        (mid_x, (y_offset + margin) as f32),\n        (mid_x, (y_offset + cell_size - margin) as f32),\n        gray,\n    );\n\n    // Plot points\n    for token in 0..p {\n        let x = x_offset + margin + (token * plot_width / p);\n        let normalized = (values[token] - min_val) / range;\n        let y = y_offset + cell_size - margin - (normalized * plot_height as f64) as usize;\n\n        let color = token_to_rgb(token, p);\n        draw_filled_circle_mut(img, (x as i32, y as i32), 2, color);\n    }\n}\n\n/// Render scatter plot into a cell\nfn render_scatter_plot(\n    img: &mut RgbImage,\n    embeddings: &[Vec<f64>],\n    dim_x: usize,\n    dim_y: usize,\n    p: usize,\n    x_offset: usize,\n    y_offset: usize,\n    cell_size: usize,\n) {\n    // Find min/max for both dimensions\n    let values_x: Vec<f64> = (0..p).map(|token| embeddings[token][dim_x]).collect();\n    let values_y: Vec<f64> = (0..p).map(|token| embeddings[token][dim_y]).collect();\n\n    let min_x = values_x.iter().copied().fold(f64::INFINITY, f64::min);\n    let max_x = values_x.iter().copied().fold(f64::NEG_INFINITY, f64::max);\n    let min_y = values_y.iter().copied().fold(f64::INFINITY, f64::min);\n    let max_y = values_y.iter().copied().fold(f64::NEG_INFINITY, f64::max);\n\n    let range_x = max_x - min_x;\n    let range_y = max_y - min_y;\n\n    if range_x == 0.0 || range_y == 0.0 {\n        return; // Skip if no variation\n    }\n\n    let margin = 20;\n    let plot_width = cell_size - 2 * margin;\n    let plot_height = cell_size - 2 * margin;\n\n    // Draw subtle crosshairs at origin\n    let mid_x = (x_offset + cell_size / 2) as f32;\n    let mid_y = (y_offset + cell_size / 2) as f32;\n    let gray = Rgb([30, 30, 30]);\n    draw_line_segment_mut(\n        img,\n        ((x_offset + margin) as f32, mid_y),\n        ((x_offset + cell_size - margin) as f32, mid_y),\n        gray,\n    );\n    draw_line_segment_mut(\n        img,\n        (mid_x, (y_offset + margin) as f32),\n        (mid_x, (y_offset + cell_size - margin) as f32),\n        gray,\n    );\n\n    // Plot points\n    for token in 0..p {\n        let norm_x = (values_x[token] - min_x) / range_x;\n        let norm_y = (values_y[token] - min_y) / range_y;\n\n        let x = x_offset + margin + (norm_x * plot_width as f64) as usize;\n        let y = y_offset + cell_size - margin - (norm_y * plot_height as f64) as usize;\n\n        let color = token_to_rgb(token, p);\n        draw_filled_circle_mut(img, (x as i32, y as i32), 2, color);\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn logscale_plotting_functions_exist() {\n        // Verify that log-scale plotting functions exist and have the correct signature\n        // This is a compile-time check that ensures the API is correct\n        let _: fn(&[(usize, f64)], &[(usize, f64)], &str) -> Result<(), Box<dyn std::error::Error>> =\n            plot_loss_history_dual_logscale;\n        let _: fn(&[(usize, f32)], &[(usize, f32)], &str) -> Result<(), Box<dyn std::error::Error>> =\n            plot_accuracy_history_logscale;\n        let _: fn(&[(usize, f64)], &[(usize, f64)], &[(usize, f32)], &[(usize, f32)], &str) -> Result<(), Box<dyn std::error::Error>> =\n            plot_grokking_combined_logscale;\n    }\n\n    #[test]\n    fn snake_curve_can_be_verified_with_plotting_data() {\n        use crate::verify::{verify_snake_curve_shape, SnakeCurveConfig};\n\n        // Simulate grokking data that would produce a snake curve on log-scale plot\n        let train_acc = vec![\n            (1, 0.10),\n            (10, 0.50),\n            (100, 0.90),\n            (500, 0.99),\n            (1000, 0.995),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (10, 0.01),\n            (100, 0.01),\n            (500, 0.01),\n            (1000, 0.01),\n            (2000, 0.95),\n        ];\n\n        let config = SnakeCurveConfig::default_for_modulus(113);\n        let report = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect(\"snake curve verification should pass on grokking data\");\n\n        // Verify the snake curve characteristics are present\n        assert!(report.train_converged_by_step < 1000, \"train should converge early\");\n        assert!(report.val_grok_step > 1000, \"val should grok late\");\n        assert!(report.curve_shape_valid, \"curve shape should be valid\");\n    }\n}\n","structuredPatch":[{"oldStart":956,"oldLines":6,"newStart":956,"newLines":7,"lines":[" }"," "," /// Render dimension vs index plot into a cell","+#[allow(dead_code)]"," fn render_dimension_plot(","     img: &mut RgbImage,","     embeddings: &[Vec<f64>],"]}],"userModified":false,"replaceAll":false}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01VpK8MooMoNhCEPsNLZmJa8","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_0151r8do2jPAkMn1LXPeKfjp","name":"Edit","input":{"replace_all":false,"file_path":"/Users/admin/MechInt/src/plotting.rs","old_string":"fn render_scatter_plot(","new_string":"#[allow(dead_code)]\nfn render_scatter_plot("}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":299,"cache_read_input_tokens":68030,"cache_creation":{"ephemeral_5m_input_tokens":299,"ephemeral_1h_input_tokens":0},"output_tokens":25,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"1d95b592-493e-4628-9427-c48080f231b4"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_0151r8do2jPAkMn1LXPeKfjp","type":"tool_result","content":"The file /Users/admin/MechInt/src/plotting.rs has been updated. Here's the result of running `cat -n` on a snippet of the edited file:\n  1008â†’    }\n  1009â†’}\n  1010â†’\n  1011â†’/// Render scatter plot into a cell\n  1012â†’#[allow(dead_code)]\n  1013â†’fn render_scatter_plot(\n  1014â†’    img: &mut RgbImage,\n  1015â†’    embeddings: &[Vec<f64>],\n  1016â†’    dim_x: usize,\n  1017â†’    dim_y: usize,"}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"489fa545-741b-4fc6-a0f5-e02a63365c4d","tool_use_result":{"filePath":"/Users/admin/MechInt/src/plotting.rs","oldString":"fn render_scatter_plot(","newString":"#[allow(dead_code)]\nfn render_scatter_plot(","originalFile":"use image::{ImageBuffer, Rgb, RgbImage};\nuse imageproc::drawing::{draw_filled_circle_mut, draw_line_segment_mut};\nuse plotters::prelude::*;\nuse std::collections::HashMap;\n\nuse crate::data::ModularAdditionDataset;\n\n/// Viridis colormap - maps value in [0, 1] to RGB\nfn viridis_color(t: f64) -> RGBColor {\n    // Viridis colormap approximation (purple â†’ cyan â†’ yellow)\n    // Based on matplotlib's viridis\n    let r = (0.267 + t * (0.329 - 0.267 + t * (0.984 - 0.329))) * 255.0;\n    let g = (0.005 + t * (0.569 - 0.005 + t * (0.906 - 0.569))) * 255.0;\n    let b = (0.329 + t * (0.758 - 0.329 - t * (0.758 - 0.121))) * 255.0;\n\n    RGBColor(\n        r.min(255.0).max(0.0) as u8,\n        g.min(255.0).max(0.0) as u8,\n        b.min(255.0).max(0.0) as u8,\n    )\n}\n\n/// Map token value (0 to p-1) to viridis color\nfn token_color(token_value: usize, p: usize) -> RGBColor {\n    let t = token_value as f64 / (p - 1) as f64;\n    viridis_color(t)\n}\n\n/// Plot training and validation loss over time\npub fn plot_loss_history_dual(\n    train_loss: &[(usize, f64)], // (step, loss)\n    val_loss: &[(usize, f64)],\n    output_path: &str,\n) -> Result<(), Box<dyn std::error::Error>> {\n    let root = BitMapBackend::new(output_path, (1200, 800)).into_drawing_area();\n    root.fill(&WHITE)?;\n\n    let max_step = train_loss\n        .iter()\n        .chain(val_loss.iter())\n        .map(|(s, _)| *s)\n        .max()\n        .unwrap_or(1);\n\n    let max_loss = train_loss\n        .iter()\n        .chain(val_loss.iter())\n        .map(|(_, l)| *l)\n        .fold(0.0f64, f64::max)\n        .max(1.0);\n\n    let mut chart = ChartBuilder::on(&root)\n        .caption(\"Loss Evolution: Train vs Validation\", (\"sans-serif\", 50).into_font())\n        .margin(20)\n        .x_label_area_size(50)\n        .y_label_area_size(70)\n        .build_cartesian_2d(0usize..max_step, 0f64..max_loss)?;\n\n    chart\n        .configure_mesh()\n        .x_desc(\"Training Step\")\n        .y_desc(\"Loss\")\n        .draw()?;\n\n    // Plot training loss\n    chart\n        .draw_series(LineSeries::new(\n            train_loss.iter().map(|(s, l)| (*s, *l)),\n            &RED,\n        ))?\n        .label(\"Train Loss\")\n        .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &RED));\n\n    // Plot validation loss\n    chart\n        .draw_series(LineSeries::new(\n            val_loss.iter().map(|(s, l)| (*s, *l)),\n            &GREEN,\n        ))?\n        .label(\"Val Loss\")\n        .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &GREEN));\n\n    let modulus = ModularAdditionDataset::modulus() as f64;\n    let chance_loss = -(1.0 / modulus).ln();\n    chart\n        .draw_series(LineSeries::new(\n            vec![(0, chance_loss), (max_step, chance_loss)],\n            &BLUE.mix(0.5),\n        ))?\n        .label(\"Chance Level\")\n        .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &BLUE.mix(0.5)));\n\n    chart\n        .configure_series_labels()\n        .background_style(&WHITE.mix(0.8))\n        .border_style(&BLACK)\n        .draw()?;\n\n    root.present()?;\n    println!(\"ðŸ“Š Loss plot saved to: {}\", output_path);\n    Ok(())\n}\n\n/// Plot training loss over time (legacy, single loss)\n/// NOTE: Superseded by plot_loss_history_dual - kept for reference\n#[allow(dead_code)]\npub fn plot_loss_history(\n    snapshots: &[(usize, f64)], // (step, loss)\n    output_path: &str,\n) -> Result<(), Box<dyn std::error::Error>> {\n    let root = BitMapBackend::new(output_path, (1200, 800)).into_drawing_area();\n    root.fill(&WHITE)?;\n\n    let max_step = snapshots.iter().map(|(s, _)| *s).max().unwrap_or(1);\n    let max_loss = snapshots\n        .iter()\n        .map(|(_, l)| *l)\n        .fold(0.0f64, f64::max)\n        .max(1.0);\n\n    let mut chart = ChartBuilder::on(&root)\n        .caption(\"Training Loss Evolution\", (\"sans-serif\", 50).into_font())\n        .margin(20)\n        .x_label_area_size(50)\n        .y_label_area_size(70)\n        .build_cartesian_2d(0usize..max_step, 0f64..max_loss)?;\n\n    chart\n        .configure_mesh()\n        .x_desc(\"Training Step\")\n        .y_desc(\"Loss\")\n        .draw()?;\n\n    chart.draw_series(LineSeries::new(\n        snapshots.iter().map(|(s, l)| (*s, *l)),\n        &BLUE,\n    ))?;\n\n    root.present()?;\n    println!(\"ðŸ“Š Loss plot saved to: {}\", output_path);\n    Ok(())\n}\n\n/// Plot training and validation accuracy over time (combined)\npub fn plot_accuracy_history(\n    train_acc: &[(usize, f32)], // (step, accuracy)\n    val_acc: &[(usize, f32)],\n    output_path: &str,\n) -> Result<(), Box<dyn std::error::Error>> {\n    let root = BitMapBackend::new(output_path, (1200, 800)).into_drawing_area();\n    root.fill(&WHITE)?;\n\n    let max_step = train_acc\n        .iter()\n        .chain(val_acc.iter())\n        .map(|(s, _)| *s)\n        .max()\n        .unwrap_or(1);\n\n    let mut chart = ChartBuilder::on(&root)\n        .caption(\n            \"Grokking Phenomenon: Train vs Validation Accuracy\",\n            (\"sans-serif\", 50).into_font(),\n        )\n        .margin(20)\n        .x_label_area_size(50)\n        .y_label_area_size(70)\n        .build_cartesian_2d(0usize..max_step, 0f32..105f32)?;\n\n    chart\n        .configure_mesh()\n        .x_desc(\"Training Step\")\n        .y_desc(\"Accuracy (%)\")\n        .draw()?;\n\n    // Plot training accuracy\n    chart\n        .draw_series(LineSeries::new(\n            train_acc.iter().map(|(s, a)| (*s, a * 100.0)),\n            &BLUE,\n        ))?\n        .label(\"Training Accuracy\")\n        .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &BLUE));\n\n    // Plot validation accuracy\n    chart\n        .draw_series(LineSeries::new(\n            val_acc.iter().map(|(s, a)| (*s, a * 100.0)),\n            &RED,\n        ))?\n        .label(\"Validation Accuracy\")\n        .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &RED));\n\n    // Add reference line at 90% (grokking threshold)\n    chart.draw_series(LineSeries::new(\n        vec![(0, 90.0), (max_step, 90.0)],\n        &GREEN.mix(0.5),\n    ))?;\n\n    chart\n        .configure_series_labels()\n        .background_style(&WHITE.mix(0.8))\n        .border_style(&BLACK)\n        .draw()?;\n\n    root.present()?;\n    println!(\"ðŸ“Š Accuracy plot saved to: {}\", output_path);\n    Ok(())\n}\n\n/// Plot FFT dominant frequency distribution\npub fn plot_fft_frequency_distribution(\n    dominant_frequencies: &[(usize, usize, f64)], // (token_id, frequency, magnitude)\n    output_path: &str,\n) -> Result<(), Box<dyn std::error::Error>> {\n    let root = BitMapBackend::new(output_path, (1200, 800)).into_drawing_area();\n    root.fill(&WHITE)?;\n\n    // Count frequency occurrences\n    let mut freq_counts: HashMap<usize, usize> = HashMap::new();\n    for (_, freq, _) in dominant_frequencies {\n        *freq_counts.entry(*freq).or_default() += 1;\n    }\n\n    let mut freq_vec: Vec<(usize, usize)> = freq_counts.into_iter().collect();\n    freq_vec.sort_by_key(|(f, _)| *f);\n\n    let max_freq = freq_vec.iter().map(|(f, _)| *f).max().unwrap_or(1);\n    let max_count = freq_vec.iter().map(|(_, c)| *c).max().unwrap_or(1);\n\n    let mut chart = ChartBuilder::on(&root)\n        .caption(\n            \"FFT Dominant Frequency Distribution\",\n            (\"sans-serif\", 50).into_font(),\n        )\n        .margin(20)\n        .x_label_area_size(50)\n        .y_label_area_size(70)\n        .build_cartesian_2d(0usize..max_freq, 0usize..max_count + 1)?;\n\n    chart\n        .configure_mesh()\n        .x_desc(\"Frequency\")\n        .y_desc(\"Number of Tokens\")\n        .draw()?;\n\n    chart.draw_series(\n        freq_vec\n            .iter()\n            .map(|(freq, count)| {\n                let color = if *freq == ModularAdditionDataset::modulus() {\n                    RED.filled() // Highlight frequency matching modulus\n                } else {\n                    BLUE.filled()\n                };\n                Rectangle::new([(*freq, 0), (*freq, *count)], color)\n            }),\n    )?;\n\n    root.present()?;\n    println!(\"ðŸ“Š FFT frequency distribution plot saved to: {}\", output_path);\n    Ok(())\n}\n\n/// Plot FFT magnitude spectrum for selected tokens\npub fn plot_fft_spectra(\n    token_spectra: &[(usize, Vec<f64>)], // (token_id, magnitude_spectrum)\n    output_path: &str,\n) -> Result<(), Box<dyn std::error::Error>> {\n    let root = BitMapBackend::new(output_path, (1200, 800)).into_drawing_area();\n    root.fill(&WHITE)?;\n\n    let max_freq = token_spectra\n        .first()\n        .map(|(_, spec)| spec.len())\n        .unwrap_or(1);\n    let max_magnitude = token_spectra\n        .iter()\n        .flat_map(|(_, spec)| spec.iter())\n        .copied()\n        .fold(0.0f64, f64::max)\n        .max(1.0);\n\n    let mut chart = ChartBuilder::on(&root)\n        .caption(\n            \"FFT Magnitude Spectra (Selected Tokens)\",\n            (\"sans-serif\", 50).into_font(),\n        )\n        .margin(20)\n        .x_label_area_size(50)\n        .y_label_area_size(70)\n        .build_cartesian_2d(0usize..max_freq, 0f64..max_magnitude)?;\n\n    chart\n        .configure_mesh()\n        .x_desc(\"Frequency\")\n        .y_desc(\"Magnitude\")\n        .draw()?;\n\n    let colors = [&BLUE, &RED, &GREEN, &MAGENTA, &CYAN, &YELLOW];\n\n    for (idx, (token_id, spectrum)) in token_spectra.iter().enumerate() {\n        let color = colors[idx % colors.len()];\n        chart\n            .draw_series(LineSeries::new(\n                spectrum.iter().enumerate().map(|(f, m)| (f, *m)),\n                color,\n            ))?\n            .label(format!(\"Token {}\", token_id))\n            .legend(move |(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], color));\n    }\n\n    chart\n        .configure_series_labels()\n        .background_style(&WHITE.mix(0.8))\n        .border_style(&BLACK)\n        .draw()?;\n\n    root.present()?;\n    println!(\"ðŸ“Š FFT spectra plot saved to: {}\", output_path);\n    Ok(())\n}\n\n/// Plot combined grokking visualization (loss + accuracy on same timeline)\npub fn plot_grokking_combined(\n    train_loss: &[(usize, f64)],\n    val_loss: &[(usize, f64)],\n    train_acc: &[(usize, f32)],\n    val_acc: &[(usize, f32)],\n    output_path: &str,\n) -> Result<(), Box<dyn std::error::Error>> {\n    let root = BitMapBackend::new(output_path, (1400, 900)).into_drawing_area();\n    root.fill(&WHITE)?;\n\n    let (upper, lower) = root.split_vertically(450);\n\n    // Upper plot: Accuracy\n    {\n        let max_step = train_acc\n            .iter()\n            .chain(val_acc.iter())\n            .map(|(s, _)| *s)\n            .max()\n            .unwrap_or(1);\n\n        let mut chart = ChartBuilder::on(&upper)\n            .caption(\n                \"Grokking: Accuracy Evolution\",\n                (\"sans-serif\", 40).into_font(),\n            )\n            .margin(15)\n            .x_label_area_size(40)\n            .y_label_area_size(60)\n            .build_cartesian_2d(0usize..max_step, 0f32..105f32)?;\n\n        chart\n            .configure_mesh()\n            .x_desc(\"Training Step\")\n            .y_desc(\"Accuracy (%)\")\n            .draw()?;\n\n        chart\n            .draw_series(LineSeries::new(\n                train_acc.iter().map(|(s, a)| (*s, a * 100.0)),\n                &BLUE,\n            ))?\n            .label(\"Training\")\n            .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &BLUE));\n\n        chart\n            .draw_series(LineSeries::new(\n                val_acc.iter().map(|(s, a)| (*s, a * 100.0)),\n                &RED,\n            ))?\n            .label(\"Validation\")\n            .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &RED));\n\n        chart\n            .configure_series_labels()\n            .background_style(&WHITE.mix(0.8))\n            .border_style(&BLACK)\n            .draw()?;\n    }\n\n    // Lower plot: Loss (train and val)\n    {\n        let max_step = train_loss\n            .iter()\n            .chain(val_loss.iter())\n            .map(|(s, _)| *s)\n            .max()\n            .unwrap_or(1);\n\n        let max_loss = train_loss\n            .iter()\n            .chain(val_loss.iter())\n            .map(|(_, l)| *l)\n            .fold(0.0f64, f64::max)\n            .max(1.0);\n\n        let mut chart = ChartBuilder::on(&lower)\n            .caption(\"Loss Evolution\", (\"sans-serif\", 40).into_font())\n            .margin(15)\n            .x_label_area_size(40)\n            .y_label_area_size(60)\n            .build_cartesian_2d(0usize..max_step, 0f64..max_loss)?;\n\n        chart\n            .configure_mesh()\n            .x_desc(\"Training Step\")\n            .y_desc(\"Loss\")\n            .draw()?;\n\n        // Train loss\n        chart\n            .draw_series(LineSeries::new(\n                train_loss.iter().map(|(s, l)| (*s, *l)),\n                &RED,\n            ))?\n            .label(\"Train\")\n            .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &RED));\n\n        // Val loss\n        chart\n            .draw_series(LineSeries::new(\n                val_loss.iter().map(|(s, l)| (*s, *l)),\n                &GREEN,\n            ))?\n            .label(\"Val\")\n            .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &GREEN));\n\n        // Chance level\n        let modulus = ModularAdditionDataset::modulus() as f64;\n        let chance_loss = -(1.0 / modulus).ln();\n        chart.draw_series(LineSeries::new(\n            vec![(0, chance_loss), (max_step, chance_loss)],\n            &BLUE.mix(0.5),\n        ))?;\n\n        chart\n            .configure_series_labels()\n            .background_style(&WHITE.mix(0.8))\n            .border_style(&BLACK)\n            .draw()?;\n    }\n\n    root.present()?;\n    println!(\"ðŸ“Š Combined grokking plot saved to: {}\", output_path);\n    Ok(())\n}\n\n/// Plot training and validation loss over time with LOG-SCALE x-axis\npub fn plot_loss_history_dual_logscale(\n    train_loss: &[(usize, f64)], // (step, loss)\n    val_loss: &[(usize, f64)],\n    output_path: &str,\n) -> Result<(), Box<dyn std::error::Error>> {\n    let root = BitMapBackend::new(output_path, (1200, 800)).into_drawing_area();\n    root.fill(&WHITE)?;\n\n    let max_step = train_loss\n        .iter()\n        .chain(val_loss.iter())\n        .map(|(s, _)| *s)\n        .max()\n        .unwrap_or(1);\n\n    let max_loss = train_loss\n        .iter()\n        .chain(val_loss.iter())\n        .map(|(_, l)| *l)\n        .fold(0.0f64, f64::max)\n        .max(1.0);\n\n    let mut chart = ChartBuilder::on(&root)\n        .caption(\"Loss Evolution (Log Scale): Train vs Validation\", (\"sans-serif\", 50).into_font())\n        .margin(20)\n        .x_label_area_size(50)\n        .y_label_area_size(70)\n        .build_cartesian_2d((1.0..max_step as f64).log_scale(), 0f64..max_loss)?;\n\n    chart\n        .configure_mesh()\n        .x_desc(\"Training Step (log scale)\")\n        .y_desc(\"Loss\")\n        .draw()?;\n\n    // Plot training loss\n    chart\n        .draw_series(LineSeries::new(\n            train_loss.iter().map(|(s, l)| (*s as f64, *l)),\n            &RED,\n        ))?\n        .label(\"Train Loss\")\n        .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &RED));\n\n    // Plot validation loss\n    chart\n        .draw_series(LineSeries::new(\n            val_loss.iter().map(|(s, l)| (*s as f64, *l)),\n            &GREEN,\n        ))?\n        .label(\"Val Loss\")\n        .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &GREEN));\n\n    let modulus = ModularAdditionDataset::modulus() as f64;\n    let chance_loss = -(1.0 / modulus).ln();\n    chart\n        .draw_series(LineSeries::new(\n            vec![(1.0, chance_loss), (max_step as f64, chance_loss)],\n            &BLUE.mix(0.5),\n        ))?\n        .label(\"Chance Level\")\n        .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &BLUE.mix(0.5)));\n\n    chart\n        .configure_series_labels()\n        .background_style(&WHITE.mix(0.8))\n        .border_style(&BLACK)\n        .draw()?;\n\n    root.present()?;\n    println!(\"ðŸ“Š Loss plot (log scale) saved to: {}\", output_path);\n    Ok(())\n}\n\n/// Plot training and validation accuracy over time with LOG-SCALE x-axis\npub fn plot_accuracy_history_logscale(\n    train_acc: &[(usize, f32)], // (step, accuracy)\n    val_acc: &[(usize, f32)],\n    output_path: &str,\n) -> Result<(), Box<dyn std::error::Error>> {\n    let root = BitMapBackend::new(output_path, (1200, 800)).into_drawing_area();\n    root.fill(&WHITE)?;\n\n    let max_step = train_acc\n        .iter()\n        .chain(val_acc.iter())\n        .map(|(s, _)| *s)\n        .max()\n        .unwrap_or(1);\n\n    let mut chart = ChartBuilder::on(&root)\n        .caption(\n            \"Grokking Phenomenon (Log Scale): Train vs Validation Accuracy\",\n            (\"sans-serif\", 50).into_font(),\n        )\n        .margin(20)\n        .x_label_area_size(50)\n        .y_label_area_size(70)\n        .build_cartesian_2d((1.0..max_step as f64).log_scale(), 0f32..105f32)?;\n\n    chart\n        .configure_mesh()\n        .x_desc(\"Training Step (log scale)\")\n        .y_desc(\"Accuracy (%)\")\n        .draw()?;\n\n    // Plot training accuracy\n    chart\n        .draw_series(LineSeries::new(\n            train_acc.iter().map(|(s, a)| (*s as f64, a * 100.0)),\n            &BLUE,\n        ))?\n        .label(\"Training Accuracy\")\n        .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &BLUE));\n\n    // Plot validation accuracy\n    chart\n        .draw_series(LineSeries::new(\n            val_acc.iter().map(|(s, a)| (*s as f64, a * 100.0)),\n            &RED,\n        ))?\n        .label(\"Validation Accuracy\")\n        .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &RED));\n\n    // Add reference line at 90% (grokking threshold)\n    chart.draw_series(LineSeries::new(\n        vec![(1.0, 90.0), (max_step as f64, 90.0)],\n        &GREEN.mix(0.5),\n    ))?;\n\n    chart\n        .configure_series_labels()\n        .background_style(&WHITE.mix(0.8))\n        .border_style(&BLACK)\n        .draw()?;\n\n    root.present()?;\n    println!(\"ðŸ“Š Accuracy plot (log scale) saved to: {}\", output_path);\n    Ok(())\n}\n\n/// Plot combined grokking visualization with LOG-SCALE x-axis (loss + accuracy)\npub fn plot_grokking_combined_logscale(\n    train_loss: &[(usize, f64)],\n    val_loss: &[(usize, f64)],\n    train_acc: &[(usize, f32)],\n    val_acc: &[(usize, f32)],\n    output_path: &str,\n) -> Result<(), Box<dyn std::error::Error>> {\n    let root = BitMapBackend::new(output_path, (1400, 900)).into_drawing_area();\n    root.fill(&WHITE)?;\n\n    let (upper, lower) = root.split_vertically(450);\n\n    // Upper plot: Accuracy (log scale)\n    {\n        let max_step = train_acc\n            .iter()\n            .chain(val_acc.iter())\n            .map(|(s, _)| *s)\n            .max()\n            .unwrap_or(1);\n\n        let mut chart = ChartBuilder::on(&upper)\n            .caption(\n                \"Grokking: Accuracy Evolution (Log Scale)\",\n                (\"sans-serif\", 40).into_font(),\n            )\n            .margin(15)\n            .x_label_area_size(40)\n            .y_label_area_size(60)\n            .build_cartesian_2d((1.0..max_step as f64).log_scale(), 0f32..105f32)?;\n\n        chart\n            .configure_mesh()\n            .x_desc(\"Training Step (log scale)\")\n            .y_desc(\"Accuracy (%)\")\n            .draw()?;\n\n        chart\n            .draw_series(LineSeries::new(\n                train_acc.iter().map(|(s, a)| (*s as f64, a * 100.0)),\n                &BLUE,\n            ))?\n            .label(\"Training\")\n            .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &BLUE));\n\n        chart\n            .draw_series(LineSeries::new(\n                val_acc.iter().map(|(s, a)| (*s as f64, a * 100.0)),\n                &RED,\n            ))?\n            .label(\"Validation\")\n            .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &RED));\n\n        chart\n            .configure_series_labels()\n            .background_style(&WHITE.mix(0.8))\n            .border_style(&BLACK)\n            .draw()?;\n    }\n\n    // Lower plot: Loss (log scale)\n    {\n        let max_step = train_loss\n            .iter()\n            .chain(val_loss.iter())\n            .map(|(s, _)| *s)\n            .max()\n            .unwrap_or(1);\n\n        let max_loss = train_loss\n            .iter()\n            .chain(val_loss.iter())\n            .map(|(_, l)| *l)\n            .fold(0.0f64, f64::max)\n            .max(1.0);\n\n        let mut chart = ChartBuilder::on(&lower)\n            .caption(\"Loss Evolution (Log Scale)\", (\"sans-serif\", 40).into_font())\n            .margin(15)\n            .x_label_area_size(40)\n            .y_label_area_size(60)\n            .build_cartesian_2d((1.0..max_step as f64).log_scale(), 0f64..max_loss)?;\n\n        chart\n            .configure_mesh()\n            .x_desc(\"Training Step (log scale)\")\n            .y_desc(\"Loss\")\n            .draw()?;\n\n        // Train loss\n        chart\n            .draw_series(LineSeries::new(\n                train_loss.iter().map(|(s, l)| (*s as f64, *l)),\n                &RED,\n            ))?\n            .label(\"Train\")\n            .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &RED));\n\n        // Val loss\n        chart\n            .draw_series(LineSeries::new(\n                val_loss.iter().map(|(s, l)| (*s as f64, *l)),\n                &GREEN,\n            ))?\n            .label(\"Val\")\n            .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &GREEN));\n\n        // Chance level\n        let modulus = ModularAdditionDataset::modulus() as f64;\n        let chance_loss = -(1.0 / modulus).ln();\n        chart.draw_series(LineSeries::new(\n            vec![(1.0, chance_loss), (max_step as f64, chance_loss)],\n            &BLUE.mix(0.5),\n        ))?;\n\n        chart\n            .configure_series_labels()\n            .background_style(&WHITE.mix(0.8))\n            .border_style(&BLACK)\n            .draw()?;\n    }\n\n    root.present()?;\n    println!(\"ðŸ“Š Combined grokking plot (log scale) saved to: {}\", output_path);\n    Ok(())\n}\n\n/// Plot 3Ã—3 embedding analysis grid (faster version)\n/// embeddings: [p, d] matrix where p is the number of tokens, d=embedding_dim\n/// dimensions: indices of 3 embedding dimensions to visualize\npub fn plot_embedding_grid_3x3(\n    embeddings: &[Vec<f64>],\n    dimensions: &[usize; 3],\n    output_path: &str,\n    title: &str,\n) -> Result<(), Box<dyn std::error::Error>> {\n    let p = embeddings.len(); // number of tokens\n    let point_size = 3; // slightly larger for visibility\n\n    // Create canvas for 3Ã—3 grid\n    let root = BitMapBackend::new(output_path, (1200, 1200)).into_drawing_area();\n    root.fill(&BLACK)?; // Black background\n\n    // Split into 3Ã—3 grid\n    let cell_areas = root.split_evenly((3, 3));\n\n    println!(\"   Rendering 3Ã—3 grid ({} cells)...\", 9);\n    for row in 0..3 {\n        for col in 0..3 {\n            let idx = row * 3 + col;\n            print!(\".\");\n            use std::io::Write;\n            std::io::stdout().flush().unwrap();\n            let area = &cell_areas[idx];\n\n            if col == 0 {\n                // First column: embedding dimension vs token index\n                let dim = dimensions[row];\n\n                // Get dimension values for all tokens\n                let values: Vec<(usize, f64)> = (0..p)\n                    .map(|token| (token, embeddings[token][dim]))\n                    .collect();\n\n                // Find min/max for y-axis\n                let min_val = values.iter().map(|(_, v)| *v).fold(f64::INFINITY, f64::min);\n                let max_val = values.iter().map(|(_, v)| *v).fold(f64::NEG_INFINITY, f64::max);\n                let margin = (max_val - min_val) * 0.1;\n\n                let mut chart = ChartBuilder::on(area)\n                    .margin(5)\n                    .build_cartesian_2d(\n                        0..p,\n                        (min_val - margin)..(max_val + margin),\n                    )?;\n\n                // Subtle gray crosshairs at origin\n                chart.configure_mesh()\n                    .disable_x_mesh()\n                    .disable_y_mesh()\n                    .x_labels(0)\n                    .y_labels(0)\n                    .axis_style(RGBAColor(50, 50, 50, 0.3))\n                    .draw()?;\n\n                // Plot points colored by token value\n                for (token, value) in values.iter() {\n                    let color = token_color(*token, p);\n                    chart.draw_series(std::iter::once(Circle::new(\n                        (*token, *value),\n                        point_size,\n                        color.filled(),\n                    )))?;\n                }\n            } else {\n                // Pairwise scatter plot\n                let dim_x = dimensions[col];\n                let dim_y = dimensions[row];\n\n                // Extract x and y values\n                let points: Vec<(f64, f64)> = (0..p)\n                    .map(|token| (embeddings[token][dim_x], embeddings[token][dim_y]))\n                    .collect();\n\n                // Find min/max for axes\n                let min_x = points.iter().map(|(x, _)| *x).fold(f64::INFINITY, f64::min);\n                let max_x = points.iter().map(|(x, _)| *x).fold(f64::NEG_INFINITY, f64::max);\n                let min_y = points.iter().map(|(_, y)| *y).fold(f64::INFINITY, f64::min);\n                let max_y = points.iter().map(|(_, y)| *y).fold(f64::NEG_INFINITY, f64::max);\n\n                let margin_x = (max_x - min_x) * 0.1;\n                let margin_y = (max_y - min_y) * 0.1;\n\n                let mut chart = ChartBuilder::on(area)\n                    .margin(5)\n                    .build_cartesian_2d(\n                        (min_x - margin_x)..(max_x + margin_x),\n                        (min_y - margin_y)..(max_y + margin_y),\n                    )?;\n\n                // Subtle gray crosshairs at origin\n                chart.configure_mesh()\n                    .disable_x_mesh()\n                    .disable_y_mesh()\n                    .x_labels(0)\n                    .y_labels(0)\n                    .axis_style(RGBAColor(50, 50, 50, 0.3))\n                    .draw()?;\n\n                // Plot points colored by token value\n                for token in 0..p {\n                    let color = token_color(token, p);\n                    let (x, y) = points[token];\n                    chart.draw_series(std::iter::once(Circle::new(\n                        (x, y),\n                        point_size,\n                        color.filled(),\n                    )))?;\n                }\n            }\n        }\n    }\n\n    println!(); // newline after progress dots\n    root.present()?;\n    println!(\"   ðŸ“Š Embedding grid plot saved to: {}\", output_path);\n    println!(\"      Title: {}\", title);\n    Ok(())\n}\n\n/// Select 7 interesting dimensions from embeddings based on variance\npub fn select_interesting_dimensions(embeddings: &[Vec<f64>], n: usize) -> Vec<usize> {\n    let d = embeddings[0].len(); // embedding dimension\n    let p = embeddings.len(); // number of tokens\n\n    // Compute variance for each dimension\n    let mut variances: Vec<(usize, f64)> = Vec::with_capacity(d);\n\n    for dim in 0..d {\n        // Compute mean\n        let mean: f64 = embeddings.iter().map(|emb| emb[dim]).sum::<f64>() / p as f64;\n\n        // Compute variance\n        let variance: f64 = embeddings\n            .iter()\n            .map(|emb| (emb[dim] - mean).powi(2))\n            .sum::<f64>()\n            / p as f64;\n\n        variances.push((dim, variance));\n    }\n\n    // Sort by variance (descending)\n    variances.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap_or(std::cmp::Ordering::Equal));\n\n    // Take top n dimensions\n    variances.iter().take(n).map(|(dim, _)| *dim).collect()\n}\n\n/// Convert viridis color to image::Rgb\n// Helper functions for alternative visualizations - reserved for future use\n#[allow(dead_code)]\nfn viridis_to_rgb(t: f64) -> Rgb<u8> {\n    let color = viridis_color(t);\n    Rgb([color.0, color.1, color.2])\n}\n\n/// Map token value to RGB color\n#[allow(dead_code)]\nfn token_to_rgb(token_value: usize, p: usize) -> Rgb<u8> {\n    let t = token_value as f64 / (p - 1) as f64;\n    viridis_to_rgb(t)\n}\n\n/// Fast 7Ã—7 embedding grid using direct pixel manipulation\n/// NOTE: Alternative visualization approach - reserved for future use\n#[allow(dead_code)]\npub fn plot_embedding_grid_fast(\n    embeddings: &[Vec<f64>],\n    dimensions: &[usize; 7],\n    output_path: &str,\n    title: &str,\n) -> Result<(), Box<dyn std::error::Error>> {\n    let p = embeddings.len(); // number of tokens\n    let cell_size: usize = 300; // pixels per cell\n    let grid_size: usize = 7;\n    let total_size = cell_size * grid_size;\n\n    println!(\"   ðŸŽ¨ Creating {}Ã—{} pixel canvas...\", total_size, total_size);\n    let mut img: RgbImage = ImageBuffer::from_pixel(total_size as u32, total_size as u32, Rgb([0, 0, 0]));\n\n    println!(\"   ðŸ–Œï¸  Rendering {} cells...\", grid_size * grid_size);\n\n    for row in 0..grid_size as usize {\n        for col in 0..grid_size as usize {\n            let cell_idx = row * grid_size as usize + col;\n            if cell_idx % 10 == 0 {\n                print!(\".\");\n                use std::io::Write;\n                std::io::stdout().flush().unwrap();\n            }\n\n            let x_offset = col * cell_size as usize;\n            let y_offset = row * cell_size as usize;\n\n            if col == 0 {\n                // First column: dimension vs token index\n                let dim = dimensions[row];\n                render_dimension_plot(\n                    &mut img,\n                    embeddings,\n                    dim,\n                    p,\n                    x_offset,\n                    y_offset,\n                    cell_size,\n                );\n            } else {\n                // Pairwise scatter plot\n                let dim_x = dimensions[col];\n                let dim_y = dimensions[row];\n                render_scatter_plot(\n                    &mut img,\n                    embeddings,\n                    dim_x,\n                    dim_y,\n                    p,\n                    x_offset,\n                    y_offset,\n                    cell_size,\n                );\n            }\n        }\n    }\n\n    println!(); // newline after progress dots\n    println!(\"   ðŸ’¾ Saving image to {}...\", output_path);\n    img.save(output_path)?;\n    println!(\"   âœ… Embedding grid saved!\");\n    println!(\"      Title: {}\", title);\n    Ok(())\n}\n\n/// Render dimension vs index plot into a cell\n#[allow(dead_code)]\nfn render_dimension_plot(\n    img: &mut RgbImage,\n    embeddings: &[Vec<f64>],\n    dim: usize,\n    p: usize,\n    x_offset: usize,\n    y_offset: usize,\n    cell_size: usize,\n) {\n    // Find min/max for this dimension\n    let values: Vec<f64> = (0..p).map(|token| embeddings[token][dim]).collect();\n    let min_val = values.iter().copied().fold(f64::INFINITY, f64::min);\n    let max_val = values.iter().copied().fold(f64::NEG_INFINITY, f64::max);\n    let range = max_val - min_val;\n\n    if range == 0.0 {\n        return; // Skip if no variation\n    }\n\n    let margin = 20;\n    let plot_width = cell_size - 2 * margin;\n    let plot_height = cell_size - 2 * margin;\n\n    // Draw subtle crosshairs\n    let mid_x = (x_offset + cell_size / 2) as f32;\n    let mid_y = (y_offset + cell_size / 2) as f32;\n    let gray = Rgb([30, 30, 30]);\n    draw_line_segment_mut(\n        img,\n        ((x_offset + margin) as f32, mid_y),\n        ((x_offset + cell_size - margin) as f32, mid_y),\n        gray,\n    );\n    draw_line_segment_mut(\n        img,\n        (mid_x, (y_offset + margin) as f32),\n        (mid_x, (y_offset + cell_size - margin) as f32),\n        gray,\n    );\n\n    // Plot points\n    for token in 0..p {\n        let x = x_offset + margin + (token * plot_width / p);\n        let normalized = (values[token] - min_val) / range;\n        let y = y_offset + cell_size - margin - (normalized * plot_height as f64) as usize;\n\n        let color = token_to_rgb(token, p);\n        draw_filled_circle_mut(img, (x as i32, y as i32), 2, color);\n    }\n}\n\n/// Render scatter plot into a cell\nfn render_scatter_plot(\n    img: &mut RgbImage,\n    embeddings: &[Vec<f64>],\n    dim_x: usize,\n    dim_y: usize,\n    p: usize,\n    x_offset: usize,\n    y_offset: usize,\n    cell_size: usize,\n) {\n    // Find min/max for both dimensions\n    let values_x: Vec<f64> = (0..p).map(|token| embeddings[token][dim_x]).collect();\n    let values_y: Vec<f64> = (0..p).map(|token| embeddings[token][dim_y]).collect();\n\n    let min_x = values_x.iter().copied().fold(f64::INFINITY, f64::min);\n    let max_x = values_x.iter().copied().fold(f64::NEG_INFINITY, f64::max);\n    let min_y = values_y.iter().copied().fold(f64::INFINITY, f64::min);\n    let max_y = values_y.iter().copied().fold(f64::NEG_INFINITY, f64::max);\n\n    let range_x = max_x - min_x;\n    let range_y = max_y - min_y;\n\n    if range_x == 0.0 || range_y == 0.0 {\n        return; // Skip if no variation\n    }\n\n    let margin = 20;\n    let plot_width = cell_size - 2 * margin;\n    let plot_height = cell_size - 2 * margin;\n\n    // Draw subtle crosshairs at origin\n    let mid_x = (x_offset + cell_size / 2) as f32;\n    let mid_y = (y_offset + cell_size / 2) as f32;\n    let gray = Rgb([30, 30, 30]);\n    draw_line_segment_mut(\n        img,\n        ((x_offset + margin) as f32, mid_y),\n        ((x_offset + cell_size - margin) as f32, mid_y),\n        gray,\n    );\n    draw_line_segment_mut(\n        img,\n        (mid_x, (y_offset + margin) as f32),\n        (mid_x, (y_offset + cell_size - margin) as f32),\n        gray,\n    );\n\n    // Plot points\n    for token in 0..p {\n        let norm_x = (values_x[token] - min_x) / range_x;\n        let norm_y = (values_y[token] - min_y) / range_y;\n\n        let x = x_offset + margin + (norm_x * plot_width as f64) as usize;\n        let y = y_offset + cell_size - margin - (norm_y * plot_height as f64) as usize;\n\n        let color = token_to_rgb(token, p);\n        draw_filled_circle_mut(img, (x as i32, y as i32), 2, color);\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn logscale_plotting_functions_exist() {\n        // Verify that log-scale plotting functions exist and have the correct signature\n        // This is a compile-time check that ensures the API is correct\n        let _: fn(&[(usize, f64)], &[(usize, f64)], &str) -> Result<(), Box<dyn std::error::Error>> =\n            plot_loss_history_dual_logscale;\n        let _: fn(&[(usize, f32)], &[(usize, f32)], &str) -> Result<(), Box<dyn std::error::Error>> =\n            plot_accuracy_history_logscale;\n        let _: fn(&[(usize, f64)], &[(usize, f64)], &[(usize, f32)], &[(usize, f32)], &str) -> Result<(), Box<dyn std::error::Error>> =\n            plot_grokking_combined_logscale;\n    }\n\n    #[test]\n    fn snake_curve_can_be_verified_with_plotting_data() {\n        use crate::verify::{verify_snake_curve_shape, SnakeCurveConfig};\n\n        // Simulate grokking data that would produce a snake curve on log-scale plot\n        let train_acc = vec![\n            (1, 0.10),\n            (10, 0.50),\n            (100, 0.90),\n            (500, 0.99),\n            (1000, 0.995),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (10, 0.01),\n            (100, 0.01),\n            (500, 0.01),\n            (1000, 0.01),\n            (2000, 0.95),\n        ];\n\n        let config = SnakeCurveConfig::default_for_modulus(113);\n        let report = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect(\"snake curve verification should pass on grokking data\");\n\n        // Verify the snake curve characteristics are present\n        assert!(report.train_converged_by_step < 1000, \"train should converge early\");\n        assert!(report.val_grok_step > 1000, \"val should grok late\");\n        assert!(report.curve_shape_valid, \"curve shape should be valid\");\n    }\n}\n","structuredPatch":[{"oldStart":1009,"oldLines":6,"newStart":1009,"newLines":7,"lines":[" }"," "," /// Render scatter plot into a cell","+#[allow(dead_code)]"," fn render_scatter_plot(","     img: &mut RgbImage,","     embeddings: &[Vec<f64>],"]}],"userModified":false,"replaceAll":false}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01QnxytLn4JLRWpr8N3nNKcL","type":"message","role":"assistant","content":[{"type":"text","text":"Now let me check the checkpoint module:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":309,"cache_read_input_tokens":68329,"cache_creation":{"ephemeral_5m_input_tokens":309,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"0d098e17-1320-491a-acb2-d2e22a88603c"}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01QnxytLn4JLRWpr8N3nNKcL","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_012XzVYJB8KxzZwqpgXJ9N8C","name":"Bash","input":{"command":"grep -n \"pub fn save_checkpoint\\|pub fn save_labeled_checkpoint\\|pub fn list_checkpoints\" src/checkpoint.rs","description":"Find unused checkpoint functions"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":309,"cache_read_input_tokens":68329,"cache_creation":{"ephemeral_5m_input_tokens":309,"ephemeral_1h_input_tokens":0},"output_tokens":112,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"84c12389-8bdb-4ed2-8776-e3acaf61b2b9"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_012XzVYJB8KxzZwqpgXJ9N8C","type":"tool_result","content":"11:pub fn save_checkpoint<B: AutodiffBackend>(\n56:pub fn save_labeled_checkpoint<B: AutodiffBackend>(\n65:pub fn list_checkpoints() -> Result<Vec<String>, Box<dyn std::error::Error>> {","is_error":false}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"4642233b-d996-49de-b6b4-fce2d8172c43","tool_use_result":{"stdout":"11:pub fn save_checkpoint<B: AutodiffBackend>(\n56:pub fn save_labeled_checkpoint<B: AutodiffBackend>(\n65:pub fn list_checkpoints() -> Result<Vec<String>, Box<dyn std::error::Error>> {","stderr":"","interrupted":false,"isImage":false}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01Y1KBDVYy18p5GD6DgRL9ee","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01VjMYsxE7DPCkt1jcVd5Awa","name":"Edit","input":{"replace_all":false,"file_path":"/Users/admin/MechInt/src/checkpoint.rs","old_string":"pub fn save_checkpoint<B: AutodiffBackend>(","new_string":"/// NOTE: Superseded by copy_checkpoint_set - kept for reference\n#[allow(dead_code)]\npub fn save_checkpoint<B: AutodiffBackend>("}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":187,"cache_read_input_tokens":68638,"cache_creation":{"ephemeral_5m_input_tokens":187,"ephemeral_1h_input_tokens":0},"output_tokens":175,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"f9bac79f-fca9-4106-987c-a9d848efe420"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01VjMYsxE7DPCkt1jcVd5Awa","type":"tool_result","content":"The file /Users/admin/MechInt/src/checkpoint.rs has been updated. Here's the result of running `cat -n` on a snippet of the edited file:\n     7â†’\n     8â†’use crate::model::Transformer;\n     9â†’\n    10â†’/// Save model checkpoint to disk\n    11â†’/// NOTE: Superseded by copy_checkpoint_set - kept for reference\n    12â†’#[allow(dead_code)]\n    13â†’pub fn save_checkpoint<B: AutodiffBackend>(\n    14â†’    model: &Transformer<B>,\n    15â†’    path: &str,\n    16â†’) -> Result<(), Box<dyn std::error::Error>> {\n    17â†’    // Create directory if it doesn't exist"}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"dbe1d118-0dc9-4998-bab7-3de6b783ae36","tool_use_result":{"filePath":"/Users/admin/MechInt/src/checkpoint.rs","oldString":"pub fn save_checkpoint<B: AutodiffBackend>(","newString":"/// NOTE: Superseded by copy_checkpoint_set - kept for reference\n#[allow(dead_code)]\npub fn save_checkpoint<B: AutodiffBackend>(","originalFile":"use burn::{\n    module::{AutodiffModule, Module},\n    record::{CompactRecorder, Recorder},\n    tensor::backend::AutodiffBackend,\n};\nuse std::path::Path;\n\nuse crate::model::Transformer;\n\n/// Save model checkpoint to disk\npub fn save_checkpoint<B: AutodiffBackend>(\n    model: &Transformer<B>,\n    path: &str,\n) -> Result<(), Box<dyn std::error::Error>> {\n    // Create directory if it doesn't exist\n    if let Some(parent) = Path::new(path).parent() {\n        std::fs::create_dir_all(parent)?;\n    }\n\n    // Convert model to record (without autodiff wrapper)\n    let record = model.clone().valid().into_record();\n\n    // Save using CompactRecorder (binary format)\n    CompactRecorder::new()\n        .record(record, path.into())\n        .map_err(|e| format!(\"Failed to save checkpoint: {:?}\", e))?;\n\n    println!(\"ðŸ’¾ Model checkpoint saved to: {}\", path);\n    Ok(())\n}\n\n/// Load model checkpoint from disk\npub fn load_checkpoint<B: AutodiffBackend>(\n    path: &str,\n    device: &B::Device,\n) -> Result<Transformer<B>, Box<dyn std::error::Error>> {\n    use crate::model::TransformerConfig;\n\n    // Create a new model with default config\n    let config = TransformerConfig::default();\n    let model = config.init(device);\n\n    // Load the record\n    let record = CompactRecorder::new()\n        .load(path.into(), device)\n        .map_err(|e| format!(\"Failed to load checkpoint: {:?}\", e))?;\n\n    // Load record into model\n    let model = model.load_record(record);\n\n    println!(\"ðŸ“‚ Model checkpoint loaded from: {}\", path);\n    Ok(model)\n}\n\n/// Save a labeled checkpoint (e.g., \"grokking\", \"final\", \"step_1000\")\npub fn save_labeled_checkpoint<B: AutodiffBackend>(\n    model: &Transformer<B>,\n    label: &str,\n) -> Result<(), Box<dyn std::error::Error>> {\n    let path = format!(\"checkpoints/model_{}\", label);\n    save_checkpoint(model, &path)\n}\n\n/// List available checkpoints\npub fn list_checkpoints() -> Result<Vec<String>, Box<dyn std::error::Error>> {\n    let checkpoint_dir = Path::new(\"checkpoints\");\n\n    if !checkpoint_dir.exists() {\n        return Ok(Vec::new());\n    }\n\n    let mut checkpoints = Vec::new();\n\n    for entry in std::fs::read_dir(checkpoint_dir)? {\n        let entry = entry?;\n        let path = entry.path();\n\n        if path.extension().and_then(|s| s.to_str()) == Some(\"mpk\") {\n            if let Some(filename) = path.file_name().and_then(|s| s.to_str()) {\n                checkpoints.push(filename.to_string());\n            }\n        }\n    }\n\n    checkpoints.sort();\n    Ok(checkpoints)\n}\n","structuredPatch":[{"oldStart":8,"oldLines":6,"newStart":8,"newLines":8,"lines":[" use crate::model::Transformer;"," "," /// Save model checkpoint to disk","+/// NOTE: Superseded by copy_checkpoint_set - kept for reference","+#[allow(dead_code)]"," pub fn save_checkpoint<B: AutodiffBackend>(","     model: &Transformer<B>,","     path: &str,"]}],"userModified":false,"replaceAll":false}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01526EuR5rTMLkzp1judb1fy","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01YTCD3mkUCiErQBov9dSGdY","name":"Edit","input":{"replace_all":false,"file_path":"/Users/admin/MechInt/src/checkpoint.rs","old_string":"pub fn save_labeled_checkpoint<B: AutodiffBackend>(","new_string":"/// NOTE: Superseded by copy_checkpoint_set - kept for reference\n#[allow(dead_code)]\npub fn save_labeled_checkpoint<B: AutodiffBackend>("}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":466,"cache_read_input_tokens":68825,"cache_creation":{"ephemeral_5m_input_tokens":466,"ephemeral_1h_input_tokens":0},"output_tokens":179,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"a5b61f32-ae3a-4ea1-8720-ff3523ab5b14"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01YTCD3mkUCiErQBov9dSGdY","type":"tool_result","content":"The file /Users/admin/MechInt/src/checkpoint.rs has been updated. Here's the result of running `cat -n` on a snippet of the edited file:\n    54â†’    Ok(model)\n    55â†’}\n    56â†’\n    57â†’/// Save a labeled checkpoint (e.g., \"grokking\", \"final\", \"step_1000\")\n    58â†’/// NOTE: Superseded by copy_checkpoint_set - kept for reference\n    59â†’#[allow(dead_code)]\n    60â†’pub fn save_labeled_checkpoint<B: AutodiffBackend>(\n    61â†’    model: &Transformer<B>,\n    62â†’    label: &str,\n    63â†’) -> Result<(), Box<dyn std::error::Error>> {\n    64â†’    let path = format!(\"checkpoints/model_{}\", label);"}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"fcde48fc-df8f-4c01-accb-68ca60827362","tool_use_result":{"filePath":"/Users/admin/MechInt/src/checkpoint.rs","oldString":"pub fn save_labeled_checkpoint<B: AutodiffBackend>(","newString":"/// NOTE: Superseded by copy_checkpoint_set - kept for reference\n#[allow(dead_code)]\npub fn save_labeled_checkpoint<B: AutodiffBackend>(","originalFile":"use burn::{\n    module::{AutodiffModule, Module},\n    record::{CompactRecorder, Recorder},\n    tensor::backend::AutodiffBackend,\n};\nuse std::path::Path;\n\nuse crate::model::Transformer;\n\n/// Save model checkpoint to disk\n/// NOTE: Superseded by copy_checkpoint_set - kept for reference\n#[allow(dead_code)]\npub fn save_checkpoint<B: AutodiffBackend>(\n    model: &Transformer<B>,\n    path: &str,\n) -> Result<(), Box<dyn std::error::Error>> {\n    // Create directory if it doesn't exist\n    if let Some(parent) = Path::new(path).parent() {\n        std::fs::create_dir_all(parent)?;\n    }\n\n    // Convert model to record (without autodiff wrapper)\n    let record = model.clone().valid().into_record();\n\n    // Save using CompactRecorder (binary format)\n    CompactRecorder::new()\n        .record(record, path.into())\n        .map_err(|e| format!(\"Failed to save checkpoint: {:?}\", e))?;\n\n    println!(\"ðŸ’¾ Model checkpoint saved to: {}\", path);\n    Ok(())\n}\n\n/// Load model checkpoint from disk\npub fn load_checkpoint<B: AutodiffBackend>(\n    path: &str,\n    device: &B::Device,\n) -> Result<Transformer<B>, Box<dyn std::error::Error>> {\n    use crate::model::TransformerConfig;\n\n    // Create a new model with default config\n    let config = TransformerConfig::default();\n    let model = config.init(device);\n\n    // Load the record\n    let record = CompactRecorder::new()\n        .load(path.into(), device)\n        .map_err(|e| format!(\"Failed to load checkpoint: {:?}\", e))?;\n\n    // Load record into model\n    let model = model.load_record(record);\n\n    println!(\"ðŸ“‚ Model checkpoint loaded from: {}\", path);\n    Ok(model)\n}\n\n/// Save a labeled checkpoint (e.g., \"grokking\", \"final\", \"step_1000\")\npub fn save_labeled_checkpoint<B: AutodiffBackend>(\n    model: &Transformer<B>,\n    label: &str,\n) -> Result<(), Box<dyn std::error::Error>> {\n    let path = format!(\"checkpoints/model_{}\", label);\n    save_checkpoint(model, &path)\n}\n\n/// List available checkpoints\npub fn list_checkpoints() -> Result<Vec<String>, Box<dyn std::error::Error>> {\n    let checkpoint_dir = Path::new(\"checkpoints\");\n\n    if !checkpoint_dir.exists() {\n        return Ok(Vec::new());\n    }\n\n    let mut checkpoints = Vec::new();\n\n    for entry in std::fs::read_dir(checkpoint_dir)? {\n        let entry = entry?;\n        let path = entry.path();\n\n        if path.extension().and_then(|s| s.to_str()) == Some(\"mpk\") {\n            if let Some(filename) = path.file_name().and_then(|s| s.to_str()) {\n                checkpoints.push(filename.to_string());\n            }\n        }\n    }\n\n    checkpoints.sort();\n    Ok(checkpoints)\n}\n","structuredPatch":[{"oldStart":55,"oldLines":6,"newStart":55,"newLines":8,"lines":[" }"," "," /// Save a labeled checkpoint (e.g., \"grokking\", \"final\", \"step_1000\")","+/// NOTE: Superseded by copy_checkpoint_set - kept for reference","+#[allow(dead_code)]"," pub fn save_labeled_checkpoint<B: AutodiffBackend>(","     model: &Transformer<B>,","     label: &str,"]}],"userModified":false,"replaceAll":false}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01Cbo5aVASRP8cNtMMbjGpYQ","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01P8X8Au4eMMMZTixhv8Hv8X","name":"Edit","input":{"replace_all":false,"file_path":"/Users/admin/MechInt/src/checkpoint.rs","old_string":"pub fn list_checkpoints() -> Result<Vec<String>, Box<dyn std::error::Error>> {","new_string":"/// NOTE: Utility function reserved for future use\n#[allow(dead_code)]\npub fn list_checkpoints() -> Result<Vec<String>, Box<dyn std::error::Error>> {"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":390,"cache_read_input_tokens":69291,"cache_creation":{"ephemeral_5m_input_tokens":390,"ephemeral_1h_input_tokens":0},"output_tokens":191,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"f92a0106-3fd1-4178-b97f-f4ff311f7025"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01P8X8Au4eMMMZTixhv8Hv8X","type":"tool_result","content":"The file /Users/admin/MechInt/src/checkpoint.rs has been updated. Here's the result of running `cat -n` on a snippet of the edited file:\n    65â†’    save_checkpoint(model, &path)\n    66â†’}\n    67â†’\n    68â†’/// List available checkpoints\n    69â†’/// NOTE: Utility function reserved for future use\n    70â†’#[allow(dead_code)]\n    71â†’pub fn list_checkpoints() -> Result<Vec<String>, Box<dyn std::error::Error>> {\n    72â†’    let checkpoint_dir = Path::new(\"checkpoints\");\n    73â†’\n    74â†’    if !checkpoint_dir.exists() {\n    75â†’        return Ok(Vec::new());"}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"39a8ff0c-c00d-4998-8bcb-7fd8e87c1ce5","tool_use_result":{"filePath":"/Users/admin/MechInt/src/checkpoint.rs","oldString":"pub fn list_checkpoints() -> Result<Vec<String>, Box<dyn std::error::Error>> {","newString":"/// NOTE: Utility function reserved for future use\n#[allow(dead_code)]\npub fn list_checkpoints() -> Result<Vec<String>, Box<dyn std::error::Error>> {","originalFile":"use burn::{\n    module::{AutodiffModule, Module},\n    record::{CompactRecorder, Recorder},\n    tensor::backend::AutodiffBackend,\n};\nuse std::path::Path;\n\nuse crate::model::Transformer;\n\n/// Save model checkpoint to disk\n/// NOTE: Superseded by copy_checkpoint_set - kept for reference\n#[allow(dead_code)]\npub fn save_checkpoint<B: AutodiffBackend>(\n    model: &Transformer<B>,\n    path: &str,\n) -> Result<(), Box<dyn std::error::Error>> {\n    // Create directory if it doesn't exist\n    if let Some(parent) = Path::new(path).parent() {\n        std::fs::create_dir_all(parent)?;\n    }\n\n    // Convert model to record (without autodiff wrapper)\n    let record = model.clone().valid().into_record();\n\n    // Save using CompactRecorder (binary format)\n    CompactRecorder::new()\n        .record(record, path.into())\n        .map_err(|e| format!(\"Failed to save checkpoint: {:?}\", e))?;\n\n    println!(\"ðŸ’¾ Model checkpoint saved to: {}\", path);\n    Ok(())\n}\n\n/// Load model checkpoint from disk\npub fn load_checkpoint<B: AutodiffBackend>(\n    path: &str,\n    device: &B::Device,\n) -> Result<Transformer<B>, Box<dyn std::error::Error>> {\n    use crate::model::TransformerConfig;\n\n    // Create a new model with default config\n    let config = TransformerConfig::default();\n    let model = config.init(device);\n\n    // Load the record\n    let record = CompactRecorder::new()\n        .load(path.into(), device)\n        .map_err(|e| format!(\"Failed to load checkpoint: {:?}\", e))?;\n\n    // Load record into model\n    let model = model.load_record(record);\n\n    println!(\"ðŸ“‚ Model checkpoint loaded from: {}\", path);\n    Ok(model)\n}\n\n/// Save a labeled checkpoint (e.g., \"grokking\", \"final\", \"step_1000\")\n/// NOTE: Superseded by copy_checkpoint_set - kept for reference\n#[allow(dead_code)]\npub fn save_labeled_checkpoint<B: AutodiffBackend>(\n    model: &Transformer<B>,\n    label: &str,\n) -> Result<(), Box<dyn std::error::Error>> {\n    let path = format!(\"checkpoints/model_{}\", label);\n    save_checkpoint(model, &path)\n}\n\n/// List available checkpoints\npub fn list_checkpoints() -> Result<Vec<String>, Box<dyn std::error::Error>> {\n    let checkpoint_dir = Path::new(\"checkpoints\");\n\n    if !checkpoint_dir.exists() {\n        return Ok(Vec::new());\n    }\n\n    let mut checkpoints = Vec::new();\n\n    for entry in std::fs::read_dir(checkpoint_dir)? {\n        let entry = entry?;\n        let path = entry.path();\n\n        if path.extension().and_then(|s| s.to_str()) == Some(\"mpk\") {\n            if let Some(filename) = path.file_name().and_then(|s| s.to_str()) {\n                checkpoints.push(filename.to_string());\n            }\n        }\n    }\n\n    checkpoints.sort();\n    Ok(checkpoints)\n}\n","structuredPatch":[{"oldStart":66,"oldLines":6,"newStart":66,"newLines":8,"lines":[" }"," "," /// List available checkpoints","+/// NOTE: Utility function reserved for future use","+#[allow(dead_code)]"," pub fn list_checkpoints() -> Result<Vec<String>, Box<dyn std::error::Error>> {","     let checkpoint_dir = Path::new(\"checkpoints\");"," "]}],"userModified":false,"replaceAll":false}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01Ff9awNNGD47JJW4cWD9nvF","type":"message","role":"assistant","content":[{"type":"text","text":"Now let me check the export module:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":445,"cache_read_input_tokens":69681,"cache_creation":{"ephemeral_5m_input_tokens":445,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"6b2b6cb6-ba22-4d7f-8179-59d0fb75dabc"}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01Ff9awNNGD47JJW4cWD9nvF","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01BAv95sbB8ngr6Uv9Fud4gX","name":"Bash","input":{"command":"grep -n \"pub fn load_json\" src/export.rs","description":"Find export module unused functions"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":445,"cache_read_input_tokens":69681,"cache_creation":{"ephemeral_5m_input_tokens":445,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"ca47e5d6-42fb-438e-9786-941edd9c7f60"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01BAv95sbB8ngr6Uv9Fud4gX","type":"tool_result","content":"35:    pub fn load_json<P: AsRef<Path>>(path: P) -> Result<Self, String> {","is_error":false}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"c578d7d8-55b3-4ebd-b051-c8ccf0876abe","tool_use_result":{"stdout":"35:    pub fn load_json<P: AsRef<Path>>(path: P) -> Result<Self, String> {","stderr":"","interrupted":false,"isImage":false}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01YKjBEGU4WQ2moxLsUHsGki","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_019catEF42oaGPtDcPpw1R2J","name":"Edit","input":{"replace_all":false,"file_path":"/Users/admin/MechInt/src/export.rs","old_string":"    pub fn load_json<P: AsRef<Path>>(path: P) -> Result<Self, String> {","new_string":"    /// NOTE: Reserved for loading activation surfaces from saved JSON files\n    #[allow(dead_code)]\n    pub fn load_json<P: AsRef<Path>>(path: P) -> Result<Self, String> {"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":137,"cache_read_input_tokens":70126,"cache_creation":{"ephemeral_5m_input_tokens":137,"ephemeral_1h_input_tokens":0},"output_tokens":199,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"5cfb97c2-fc2b-45ef-b994-c7945dd90bb3"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_019catEF42oaGPtDcPpw1R2J","type":"tool_result","content":"The file /Users/admin/MechInt/src/export.rs has been updated. Here's the result of running `cat -n` on a snippet of the edited file:\n    31â†’        Ok(())\n    32â†’    }\n    33â†’\n    34â†’    /// Load activation surface from JSON file\n    35â†’    /// NOTE: Reserved for loading activation surfaces from saved JSON files\n    36â†’    #[allow(dead_code)]\n    37â†’    pub fn load_json<P: AsRef<Path>>(path: P) -> Result<Self, String> {\n    38â†’        let json = fs::read_to_string(path.as_ref())\n    39â†’            .map_err(|e| format!(\"Failed to read file: {}\", e))?;\n    40â†’        let surface: ActivationSurface = serde_json::from_str(&json)\n    41â†’            .map_err(|e| format!(\"Failed to deserialize: {}\", e))?;"}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"d780be5d-9fbb-4114-a440-5b64d9225f0d","tool_use_result":{"filePath":"/Users/admin/MechInt/src/export.rs","oldString":"    pub fn load_json<P: AsRef<Path>>(path: P) -> Result<Self, String> {","newString":"    /// NOTE: Reserved for loading activation surfaces from saved JSON files\n    #[allow(dead_code)]\n    pub fn load_json<P: AsRef<Path>>(path: P) -> Result<Self, String> {","originalFile":"use serde::{Deserialize, Serialize};\nuse std::fs;\nuse std::path::Path;\n\n/// Represents a 3D surface dataset for visualization\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct ActivationSurface {\n    /// Neuron index in the MLP layer\n    pub neuron_index: usize,\n    /// Modulus value (grid size will be modulus x modulus)\n    pub modulus: usize,\n    /// 2D grid of activation values: surface[x][y] = activation\n    pub surface: Vec<Vec<f64>>,\n}\n\nimpl ActivationSurface {\n    pub fn new(neuron_index: usize, modulus: usize, surface: Vec<Vec<f64>>) -> Self {\n        Self {\n            neuron_index,\n            modulus,\n            surface,\n        }\n    }\n\n    /// Save activation surface to JSON file\n    pub fn save_json<P: AsRef<Path>>(&self, path: P) -> Result<(), String> {\n        let json = serde_json::to_string_pretty(self)\n            .map_err(|e| format!(\"Failed to serialize: {}\", e))?;\n        fs::write(path.as_ref(), json)\n            .map_err(|e| format!(\"Failed to write file: {}\", e))?;\n        Ok(())\n    }\n\n    /// Load activation surface from JSON file\n    pub fn load_json<P: AsRef<Path>>(path: P) -> Result<Self, String> {\n        let json = fs::read_to_string(path.as_ref())\n            .map_err(|e| format!(\"Failed to read file: {}\", e))?;\n        let surface: ActivationSurface = serde_json::from_str(&json)\n            .map_err(|e| format!(\"Failed to deserialize: {}\", e))?;\n        Ok(surface)\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use tempfile::NamedTempFile;\n\n    #[test]\n    fn test_activation_surface_save_load() {\n        let modulus = 5;\n        let mut surface = vec![vec![0.0; modulus]; modulus];\n        for x in 0..modulus {\n            for y in 0..modulus {\n                surface[x][y] = (x * modulus + y) as f64;\n            }\n        }\n\n        let original = ActivationSurface::new(42, modulus, surface);\n\n        let temp_file = NamedTempFile::new().unwrap();\n        let path = temp_file.path();\n\n        original.save_json(path).unwrap();\n        let loaded = ActivationSurface::load_json(path).unwrap();\n\n        assert_eq!(loaded.neuron_index, 42);\n        assert_eq!(loaded.modulus, 5);\n        assert_eq!(loaded.surface.len(), 5);\n        assert_eq!(loaded.surface[0].len(), 5);\n        assert_eq!(loaded.surface[2][3], 13.0);\n    }\n\n    #[test]\n    fn test_activation_surface_json_format() {\n        let surface = vec![\n            vec![1.0, 2.0],\n            vec![3.0, 4.0],\n        ];\n        let data = ActivationSurface::new(10, 2, surface);\n\n        let json = serde_json::to_string_pretty(&data).unwrap();\n        assert!(json.contains(\"\\\"neuron_index\\\": 10\"));\n        assert!(json.contains(\"\\\"modulus\\\": 2\"));\n        assert!(json.contains(\"\\\"surface\\\"\"));\n    }\n}\n","structuredPatch":[{"oldStart":32,"oldLines":6,"newStart":32,"newLines":8,"lines":["     }"," ","     /// Load activation surface from JSON file","+    /// NOTE: Reserved for loading activation surfaces from saved JSON files","+    #[allow(dead_code)]","     pub fn load_json<P: AsRef<Path>>(path: P) -> Result<Self, String> {","         let json = fs::read_to_string(path.as_ref())","             .map_err(|e| format!(\"Failed to read file: {}\", e))?;"]}],"userModified":false,"replaceAll":false}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_011Ff6LV8tnivvx8XvZEAK4N","type":"message","role":"assistant","content":[{"type":"text","text":"Now let me check if there are any field-level warnings for structs. Let me build and see what's left:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":442,"cache_read_input_tokens":70263,"cache_creation":{"ephemeral_5m_input_tokens":442,"ephemeral_1h_input_tokens":0},"output_tokens":5,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"b6d8fa27-6223-4e74-95c2-c306f0133a36"}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_011Ff6LV8tnivvx8XvZEAK4N","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01Qz3YWPGnYh67K1vmtAp4WQ","name":"Bash","input":{"command":"cargo build 2>&1 | grep \"warning:\" | wc -l","description":"Count remaining warnings","timeout":120000}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":442,"cache_read_input_tokens":70263,"cache_creation":{"ephemeral_5m_input_tokens":442,"ephemeral_1h_input_tokens":0},"output_tokens":5,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"a8ae01e8-50e8-492e-a29d-2c96ba80517d"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01Qz3YWPGnYh67K1vmtAp4WQ","type":"tool_result","content":"      15","is_error":false}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"73e347be-a963-4a9e-a331-ece0d8005291","tool_use_result":{"stdout":"      15","stderr":"","interrupted":false,"isImage":false}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01ThBYELUpWJLuvhkZ1g2VcB","type":"message","role":"assistant","content":[{"type":"text","text":"Great progress! Down from 59 to 15. Let me see what's left:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":145,"cache_read_input_tokens":70705,"cache_creation":{"ephemeral_5m_input_tokens":145,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"3e746bb3-5595-4cbe-9daa-9c1c3a80875b"}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01ThBYELUpWJLuvhkZ1g2VcB","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01QqdYBT1fFAjcHGQZinxNq2","name":"Bash","input":{"command":"cargo build 2>&1 | grep \"warning:\" | head -20","description":"See remaining warnings","timeout":120000}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":145,"cache_read_input_tokens":70705,"cache_creation":{"ephemeral_5m_input_tokens":145,"ephemeral_1h_input_tokens":0},"output_tokens":124,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"d3424e52-5a24-4168-9201-a8cd3cff951c"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01QqdYBT1fFAjcHGQZinxNq2","type":"tool_result","content":"warning: method `add_snapshot` is never used\nwarning: fields `modulus_frequency_fraction`, `spectrum_entropy`, `peak_to_median_ratio`, and `top_frequencies` are never read\nwarning: fields `mean_radius`, `radius_std`, `angular_coverage`, `mean_angle_error`, and `order_fraction` are never read\nwarning: fields `plateau_max_val_acc` and `loss_drop_ratio` are never read\nwarning: fields `restricted_drop_step` and `full_drop_step` are never read\nwarning: fields `neuron_index`, `dominant_frequency`, `dominant_ratio`, `negative_fraction`, and `best_correlation` are never read\nwarning: field `neuron_reports` is never read\nwarning: fields `neuron_a`, `neuron_b`, and `radius_cv` are never read\nwarning: associated function `default_for_modulus` is never used\nwarning: fields `neuron_index`, `diagonal_score`, `axis_score`, and `ratio` are never read\nwarning: associated function `default_for_modulus` is never used\nwarning: field `plateau_min_step` is never read\nwarning: fields `relative_decrease` and `grok_step_norm` are never read\nwarning: fields `train_converged_by_step`, `val_plateau_max_acc`, `val_grok_step`, and `curve_shape_valid` are never read\nwarning: `grokking` (bin \"grokking\") generated 14 warnings","is_error":false}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"3754851a-b9e9-4c2a-877d-cdf01e4fdb9b","tool_use_result":{"stdout":"warning: method `add_snapshot` is never used\nwarning: fields `modulus_frequency_fraction`, `spectrum_entropy`, `peak_to_median_ratio`, and `top_frequencies` are never read\nwarning: fields `mean_radius`, `radius_std`, `angular_coverage`, `mean_angle_error`, and `order_fraction` are never read\nwarning: fields `plateau_max_val_acc` and `loss_drop_ratio` are never read\nwarning: fields `restricted_drop_step` and `full_drop_step` are never read\nwarning: fields `neuron_index`, `dominant_frequency`, `dominant_ratio`, `negative_fraction`, and `best_correlation` are never read\nwarning: field `neuron_reports` is never read\nwarning: fields `neuron_a`, `neuron_b`, and `radius_cv` are never read\nwarning: associated function `default_for_modulus` is never used\nwarning: fields `neuron_index`, `diagonal_score`, `axis_score`, and `ratio` are never read\nwarning: associated function `default_for_modulus` is never used\nwarning: field `plateau_min_step` is never read\nwarning: fields `relative_decrease` and `grok_step_norm` are never read\nwarning: fields `train_converged_by_step`, `val_plateau_max_acc`, `val_grok_step`, and `curve_shape_valid` are never read\nwarning: `grokking` (bin \"grokking\") generated 14 warnings","stderr":"","interrupted":false,"isImage":false}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01VdgwLeWfZEDnQJuEfd8bNs","type":"message","role":"assistant","content":[{"type":"text","text":"These are struct fields that are never read. This is actually fine - these fields are part of verification report structs that are constructed and returned but the fields aren't necessarily accessed. I'll mark the entire structs with `#[allow(dead_code)]` since they're part of the verification API:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":491,"cache_read_input_tokens":70850,"cache_creation":{"ephemeral_5m_input_tokens":491,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"87e9631f-1379-499d-bf6c-eb2d6f1902ac"}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01VdgwLeWfZEDnQJuEfd8bNs","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01Fq4QpykJZwgkbahyxLaVnG","name":"Read","input":{"file_path":"/Users/admin/MechInt/src/verify.rs","offset":40,"limit":30}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":491,"cache_read_input_tokens":70850,"cache_creation":{"ephemeral_5m_input_tokens":491,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"1909b878-a390-4691-bf95-2d31d1383032"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01Fq4QpykJZwgkbahyxLaVnG","type":"tool_result","content":"    40â†’}\n    41â†’\n    42â†’#[derive(Debug, Clone)]\n    43â†’pub struct GrokkingPhaseReport {\n    44â†’    pub early_train_step: usize,\n    45â†’    pub transition_step: usize,\n    46â†’    pub plateau_max_val_acc: f32,\n    47â†’    pub loss_drop_ratio: f64,\n    48â†’}\n    49â†’\n    50â†’#[derive(Debug, Clone)]\n    51â†’pub struct RestrictedLossVerificationConfig {\n    52â†’    pub plateau_min_step: usize,\n    53â†’    pub drop_window: usize,\n    54â†’    pub drop_fraction: f64,\n    55â†’    pub min_step_lead: usize,\n    56â†’}\n    57â†’\n    58â†’#[derive(Debug, Clone)]\n    59â†’pub struct RestrictedLossReport {\n    60â†’    pub restricted_drop_step: usize,\n    61â†’    pub full_drop_step: usize,\n    62â†’}\n    63â†’\n    64â†’#[derive(Debug, Clone)]\n    65â†’pub struct RestrictedLossDropConfig {\n    66â†’    pub min_relative_decrease: f64,\n    67â†’}\n    68â†’\n    69â†’#[derive(Debug, Clone)]\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"99043a6e-9510-4ec0-88a7-11b8c6b1b2ab","tool_use_result":{"type":"text","file":{"filePath":"/Users/admin/MechInt/src/verify.rs","content":"}\n\n#[derive(Debug, Clone)]\npub struct GrokkingPhaseReport {\n    pub early_train_step: usize,\n    pub transition_step: usize,\n    pub plateau_max_val_acc: f32,\n    pub loss_drop_ratio: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossVerificationConfig {\n    pub plateau_min_step: usize,\n    pub drop_window: usize,\n    pub drop_fraction: f64,\n    pub min_step_lead: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossReport {\n    pub restricted_drop_step: usize,\n    pub full_drop_step: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossDropConfig {\n    pub min_relative_decrease: f64,\n}\n\n#[derive(Debug, Clone)]","numLines":30,"startLine":40,"totalLines":2211}}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01EPKmqNFviri6c91devjYtK","type":"message","role":"assistant","content":[{"type":"text","text":"Let me add `#[allow(dead_code)]` to the report structs that have unread fields:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":678,"cache_read_input_tokens":71341,"cache_creation":{"ephemeral_5m_input_tokens":678,"ephemeral_1h_input_tokens":0},"output_tokens":3,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"4ede7b40-d830-44eb-aed7-2598581bb0b6"}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01EPKmqNFviri6c91devjYtK","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01EoQnwjfnV7Lm6sddSKV6JM","name":"Edit","input":{"replace_all":false,"file_path":"/Users/admin/MechInt/src/verify.rs","old_string":"#[derive(Debug, Clone)]\npub struct GrokkingPhaseReport {\n    pub early_train_step: usize,\n    pub transition_step: usize,\n    pub plateau_max_val_acc: f32,\n    pub loss_drop_ratio: f64,\n}","new_string":"#[derive(Debug, Clone)]\n#[allow(dead_code)]\npub struct GrokkingPhaseReport {\n    pub early_train_step: usize,\n    pub transition_step: usize,\n    pub plateau_max_val_acc: f32,\n    pub loss_drop_ratio: f64,\n}"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":678,"cache_read_input_tokens":71341,"cache_creation":{"ephemeral_5m_input_tokens":678,"ephemeral_1h_input_tokens":0},"output_tokens":291,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"9d376dd2-bc28-4aff-b08a-776e3e9befc2"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01EoQnwjfnV7Lm6sddSKV6JM","type":"tool_result","content":"The file /Users/admin/MechInt/src/verify.rs has been updated. Here's the result of running `cat -n` on a snippet of the edited file:\n    38â†’        }\n    39â†’    }\n    40â†’}\n    41â†’\n    42â†’#[derive(Debug, Clone)]\n    43â†’#[allow(dead_code)]\n    44â†’pub struct GrokkingPhaseReport {\n    45â†’    pub early_train_step: usize,\n    46â†’    pub transition_step: usize,\n    47â†’    pub plateau_max_val_acc: f32,\n    48â†’    pub loss_drop_ratio: f64,\n    49â†’}\n    50â†’\n    51â†’#[derive(Debug, Clone)]\n    52â†’pub struct RestrictedLossVerificationConfig {\n    53â†’    pub plateau_min_step: usize,"}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"8f59e6b1-e4c1-4b06-9940-5fe98495e8d6","tool_use_result":{"filePath":"/Users/admin/MechInt/src/verify.rs","oldString":"#[derive(Debug, Clone)]\npub struct GrokkingPhaseReport {\n    pub early_train_step: usize,\n    pub transition_step: usize,\n    pub plateau_max_val_acc: f32,\n    pub loss_drop_ratio: f64,\n}","newString":"#[derive(Debug, Clone)]\n#[allow(dead_code)]\npub struct GrokkingPhaseReport {\n    pub early_train_step: usize,\n    pub transition_step: usize,\n    pub plateau_max_val_acc: f32,\n    pub loss_drop_ratio: f64,\n}","originalFile":"use burn::{\n    data::dataset::Dataset,\n    tensor::{backend::Backend, Int, Tensor},\n};\nuse rustfft::{num_complex::Complex, FftPlanner};\n\nuse crate::analysis::{AccuracyHistory, LossHistory};\nuse crate::data::ModularAdditionDataset;\nuse crate::model::Transformer;\n\n#[derive(Debug, Clone)]\npub struct GrokkingVerificationConfig {\n    pub early_train_acc_threshold: f32,\n    pub early_step_max: usize,\n    pub plateau_min_step: usize,\n    pub generalization_window: usize,\n    pub chance_accuracy: f32,\n    pub chance_tolerance: f32,\n    pub target_val_acc_threshold: f32,\n    pub accuracy_window: usize,\n    pub loss_window: usize,\n    pub loss_drop_fraction: f64,\n}\n\nimpl GrokkingVerificationConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        Self {\n            early_train_acc_threshold: 0.99,\n            early_step_max: 1000,\n            plateau_min_step: 1000,\n            generalization_window: 10_000,\n            chance_accuracy: 1.0 / modulus as f32,\n            chance_tolerance: 0.02,\n            target_val_acc_threshold: 0.99,\n            accuracy_window: 5,\n            loss_window: 5,\n            loss_drop_fraction: 0.5,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct GrokkingPhaseReport {\n    pub early_train_step: usize,\n    pub transition_step: usize,\n    pub plateau_max_val_acc: f32,\n    pub loss_drop_ratio: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossVerificationConfig {\n    pub plateau_min_step: usize,\n    pub drop_window: usize,\n    pub drop_fraction: f64,\n    pub min_step_lead: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossReport {\n    pub restricted_drop_step: usize,\n    pub full_drop_step: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossDropConfig {\n    pub min_relative_decrease: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossDropReport {\n    pub drop_step: usize,\n    pub drop_loss: f64,\n    pub baseline_loss: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct ExcludedLossSpikeConfig {\n    pub min_relative_increase: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct ExcludedLossSpikeReport {\n    pub spike_step: usize,\n    pub spike_loss: f64,\n    pub baseline_loss: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct MlpWaveVerificationConfig {\n    pub max_negative_fraction: f64,\n    pub negative_tolerance: f64,\n    pub min_dominant_ratio: f64,\n    pub min_correlation: f64,\n    pub phase_steps: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct MlpWaveNeuronReport {\n    pub neuron_index: usize,\n    pub dominant_frequency: usize,\n    pub dominant_ratio: f64,\n    pub negative_fraction: f64,\n    pub best_correlation: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct MlpWaveReport {\n    pub neuron_reports: Vec<MlpWaveNeuronReport>,\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldConfig {\n    pub fixed_y: usize,\n    pub neuron_indices: Vec<usize>,\n    pub min_post_score: f64,\n    pub max_pre_score: f64,\n    pub max_axis_ratio: f64,\n    pub min_pairs_passing: usize,\n}\n\nimpl PairwiseManifoldConfig {\n    pub fn default_for_modulus(_modulus: usize) -> Self {\n        Self {\n            fixed_y: 0,\n            neuron_indices: (0..7).collect(),\n            min_post_score: 0.75,\n            max_pre_score: 0.55,\n            max_axis_ratio: 8.0,\n            min_pairs_passing: 10,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldPairReport {\n    pub neuron_a: usize,\n    pub neuron_b: usize,\n    pub circularity_score: f64,\n    pub radius_cv: f64,\n    pub axis_ratio: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldReport {\n    pub pair_reports: Vec<PairwiseManifoldPairReport>,\n    pub average_score: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldTransitionReport {\n    pub pre_average_score: f64,\n    pub post_average_score: f64,\n    pub passing_pairs: usize,\n    pub total_pairs: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct DiagonalRidgeConfig {\n    pub neuron_index: usize,\n    pub max_lag: usize,\n    pub min_diagonal_ratio: f64,\n    pub min_diagonal_score: f64,\n}\n\nimpl DiagonalRidgeConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        let max_lag = (modulus / 10).max(4).min(20);\n        Self {\n            neuron_index: 10,\n            max_lag,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct DiagonalRidgeReport {\n    pub neuron_index: usize,\n    pub diagonal_score: f64,\n    pub axis_score: f64,\n    pub ratio: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct SnakeCurveConfig {\n    pub early_step_max: usize,\n    pub train_convergence_threshold: f32,\n    pub val_plateau_min_step: usize,\n    pub val_plateau_max_acc: f32,\n    pub val_final_threshold: f32,\n}\n\nimpl SnakeCurveConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        Self {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 1000,\n            val_plateau_max_acc: 1.0 / modulus as f32 + 0.05,\n            val_final_threshold: 0.90,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct WeightNormDecayConfig {\n    pub min_relative_decrease: f64,\n    pub plateau_min_step: usize,\n    pub grok_step_window: usize,\n}\n\nimpl WeightNormDecayConfig {\n    pub fn default_for_grokking() -> Self {\n        Self {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct WeightNormDecayReport {\n    pub initial_norm: f64,\n    pub final_norm: f64,\n    pub relative_decrease: f64,\n    pub grok_step_norm: Option<f64>,\n}\n\n#[derive(Debug, Clone)]\npub struct SnakeCurveReport {\n    pub train_converged_by_step: usize,\n    pub val_plateau_max_acc: f32,\n    pub val_grok_step: usize,\n    pub curve_shape_valid: bool,\n}\n\npub fn verify_grokking_phase_transition(\n    loss_history: &LossHistory,\n    accuracy_history: &AccuracyHistory,\n    config: &GrokkingVerificationConfig,\n) -> Result<GrokkingPhaseReport, String> {\n    let (early_train_step, _) = accuracy_history\n        .train_snapshots\n        .iter()\n        .find(|(_, acc)| *acc >= config.early_train_acc_threshold)\n        .ok_or_else(|| \"train accuracy never reaches threshold\".to_string())?;\n\n    if *early_train_step > config.early_step_max {\n        return Err(format!(\n            \"train accuracy reaches {:.2} after step {}, expected before {}\",\n            config.early_train_acc_threshold, early_train_step, config.early_step_max\n        ));\n    }\n\n    let plateau_max_val_acc = accuracy_history\n        .val_snapshots\n        .iter()\n        .filter(|(step, _)| *step <= config.plateau_min_step)\n        .map(|(_, acc)| *acc)\n        .fold(None::<f32>, |accum, value| {\n            Some(accum.map_or(value, |max_val| max_val.max(value)))\n        })\n        .ok_or_else(|| \"no validation accuracy samples before plateau_min_step\".to_string())?;\n\n    if plateau_max_val_acc > config.chance_accuracy + config.chance_tolerance {\n        return Err(format!(\n            \"validation accuracy before step {} exceeds chance (max {:.4})\",\n            config.plateau_min_step, plateau_max_val_acc\n        ));\n    }\n\n    let (transition_step, _) = accuracy_history\n        .val_snapshots\n        .iter()\n        .find(|(step, acc)| {\n            *step >= config.plateau_min_step && *acc >= config.target_val_acc_threshold\n        })\n        .ok_or_else(|| \"validation accuracy never reaches target threshold\".to_string())?;\n\n    let latest_allowed = config.plateau_min_step + config.generalization_window;\n    if *transition_step > latest_allowed {\n        return Err(format!(\n            \"validation accuracy reaches {:.2} at step {}, expected before {}\",\n            config.target_val_acc_threshold, transition_step, latest_allowed\n        ));\n    }\n\n    verify_accuracy_jump(\n        &accuracy_history.val_snapshots,\n        *transition_step,\n        config.accuracy_window,\n        config.chance_accuracy,\n        config.chance_tolerance,\n        config.target_val_acc_threshold,\n    )?;\n\n    let loss_drop_ratio = verify_loss_drop(\n        &loss_history.val_snapshots,\n        *transition_step,\n        config.loss_window,\n        config.loss_drop_fraction,\n    )?;\n\n    Ok(GrokkingPhaseReport {\n        early_train_step: *early_train_step,\n        transition_step: *transition_step,\n        plateau_max_val_acc,\n        loss_drop_ratio,\n    })\n}\n\nfn verify_accuracy_jump(\n    val_accuracy: &[(usize, f32)],\n    transition_step: usize,\n    window: usize,\n    chance_accuracy: f32,\n    chance_tolerance: f32,\n    target_val_acc_threshold: f32,\n) -> Result<(), String> {\n    let before_values: Vec<f32> = val_accuracy\n        .iter()\n        .filter(|(step, _)| *step < transition_step)\n        .map(|(_, acc)| *acc)\n        .collect();\n\n    let after_values: Vec<f32> = val_accuracy\n        .iter()\n        .filter(|(step, _)| *step >= transition_step)\n        .map(|(_, acc)| *acc)\n        .collect();\n\n    if before_values.len() < window || after_values.len() < window {\n        return Err(\"insufficient accuracy samples around transition\".to_string());\n    }\n\n    let before_avg: f32 =\n        before_values[before_values.len() - window..].iter().sum::<f32>() / window as f32;\n    let after_avg: f32 = after_values[..window].iter().sum::<f32>() / window as f32;\n\n    if before_avg > chance_accuracy + chance_tolerance {\n        return Err(format!(\n            \"validation accuracy plateau too high before transition (avg {:.4})\",\n            before_avg\n        ));\n    }\n\n    if after_avg < target_val_acc_threshold {\n        return Err(format!(\n            \"validation accuracy after transition too low (avg {:.4})\",\n            after_avg\n        ));\n    }\n\n    Ok(())\n}\n\nfn verify_loss_drop(\n    val_losses: &[(usize, f64)],\n    transition_step: usize,\n    loss_window: usize,\n    loss_drop_fraction: f64,\n) -> Result<f64, String> {\n    let before_losses: Vec<f64> = val_losses\n        .iter()\n        .filter(|(step, _)| *step < transition_step)\n        .map(|(_, loss)| *loss)\n        .collect();\n\n    let after_losses: Vec<f64> = val_losses\n        .iter()\n        .filter(|(step, _)| *step >= transition_step)\n        .map(|(_, loss)| *loss)\n        .collect();\n\n    if before_losses.len() < loss_window || after_losses.len() < loss_window {\n        return Err(\"insufficient loss samples around transition\".to_string());\n    }\n\n    let before_avg: f64 =\n        before_losses[before_losses.len() - loss_window..].iter().sum::<f64>()\n            / loss_window as f64;\n    let after_avg: f64 = after_losses[..loss_window].iter().sum::<f64>() / loss_window as f64;\n\n    if after_avg > before_avg * (1.0 - loss_drop_fraction) {\n        return Err(format!(\n            \"validation loss drop too small (before {:.4}, after {:.4})\",\n            before_avg, after_avg\n        ));\n    }\n\n    Ok(after_avg / before_avg)\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_restricted_loss_early_drop(\n    full_losses: &[(usize, f64)],\n    restricted_losses: &[(usize, f64)],\n    config: &RestrictedLossVerificationConfig,\n) -> Result<RestrictedLossReport, String> {\n    let restricted_baseline =\n        baseline_loss(restricted_losses, config.plateau_min_step, config.drop_window)?;\n    let full_baseline = baseline_loss(full_losses, config.plateau_min_step, config.drop_window)?;\n\n    let restricted_drop_step = find_drop_step(\n        restricted_losses,\n        config.plateau_min_step,\n        restricted_baseline,\n        config.drop_fraction,\n    )?;\n    let full_drop_step = find_drop_step(\n        full_losses,\n        config.plateau_min_step,\n        full_baseline,\n        config.drop_fraction,\n    )?;\n\n    if restricted_drop_step + config.min_step_lead > full_drop_step {\n        return Err(format!(\n            \"restricted loss drops at step {}, expected at least {} steps before full loss (step {})\",\n            restricted_drop_step, config.min_step_lead, full_drop_step\n        ));\n    }\n\n    Ok(RestrictedLossReport {\n        restricted_drop_step,\n        full_drop_step,\n    })\n}\n\npub fn verify_restricted_loss_drop(\n    restricted_losses: &[(usize, f64)],\n    config: &RestrictedLossDropConfig,\n) -> Result<RestrictedLossDropReport, String> {\n    if restricted_losses.len() < 3 {\n        return Err(\"restricted loss history requires at least 3 points\".to_string());\n    }\n\n    let first = restricted_losses.first().ok_or_else(|| {\n        \"restricted loss history is empty\".to_string()\n    })?;\n    let baseline_loss = first.1;\n\n    let (min_idx, (min_step, min_loss)) = restricted_losses\n        .iter()\n        .enumerate()\n        .min_by(|(_, a), (_, b)| {\n            a.1.partial_cmp(&b.1).unwrap_or(std::cmp::Ordering::Equal)\n        })\n        .ok_or_else(|| \"restricted loss history is empty\".to_string())?;\n\n    if min_idx == 0 {\n        return Err(\"restricted loss minimum occurs at start (no drop detected)\".to_string());\n    }\n\n    let relative_decrease = (baseline_loss - min_loss) / baseline_loss;\n    if relative_decrease < config.min_relative_decrease {\n        return Err(format!(\n            \"restricted loss decrease too small (baseline {:.4}, min {:.4}, decrease {:.1}%)\",\n            baseline_loss, min_loss, relative_decrease * 100.0\n        ));\n    }\n\n    Ok(RestrictedLossDropReport {\n        drop_step: *min_step,\n        drop_loss: *min_loss,\n        baseline_loss,\n    })\n}\n\npub fn verify_excluded_loss_spike(\n    excluded_losses: &[(usize, f64)],\n    config: &ExcludedLossSpikeConfig,\n) -> Result<ExcludedLossSpikeReport, String> {\n    if excluded_losses.len() < 3 {\n        return Err(\"excluded loss history requires at least 3 points\".to_string());\n    }\n\n    let first = excluded_losses.first().ok_or_else(|| {\n        \"excluded loss history is empty\".to_string()\n    })?;\n    let last = excluded_losses.last().ok_or_else(|| {\n        \"excluded loss history is empty\".to_string()\n    })?;\n    let baseline_loss = (first.1 + last.1) / 2.0;\n\n    let (peak_idx, (peak_step, peak_loss)) = excluded_losses\n        .iter()\n        .enumerate()\n        .max_by(|(_, a), (_, b)| {\n            a.1.partial_cmp(&b.1).unwrap_or(std::cmp::Ordering::Equal)\n        })\n        .ok_or_else(|| \"excluded loss history is empty\".to_string())?;\n\n    if peak_idx == 0 || peak_idx + 1 == excluded_losses.len() {\n        return Err(\"excluded loss spike occurs at boundary\".to_string());\n    }\n\n    if peak_loss < &(baseline_loss * (1.0 + config.min_relative_increase)) {\n        return Err(format!(\n            \"excluded loss spike too small (baseline {:.4}, peak {:.4})\",\n            baseline_loss, peak_loss\n        ));\n    }\n\n    Ok(ExcludedLossSpikeReport {\n        spike_step: *peak_step,\n        spike_loss: *peak_loss,\n        baseline_loss,\n    })\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_mlp_activation_waves<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n    fixed_y: usize,\n    neuron_indices: &[usize],\n    config: &MlpWaveVerificationConfig,\n) -> Result<MlpWaveReport, String> {\n    let activations = collect_mlp_post_relu_activations(model, device, fixed_y)?;\n    if neuron_indices.is_empty() {\n        return Err(\"neuron indices list is empty\".to_string());\n    }\n\n    let d_ff = activations.len();\n    for &idx in neuron_indices {\n        if idx >= d_ff {\n            return Err(format!(\n                \"neuron index {} out of range (d_ff={})\",\n                idx, d_ff\n            ));\n        }\n    }\n\n    let signals: Vec<(usize, Vec<f64>)> = neuron_indices\n        .iter()\n        .map(|&idx| (idx, activations[idx].clone()))\n        .collect();\n    verify_mlp_activation_waves_from_signals(&signals, config)\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_mlp_activation_waves_from_signals(\n    signals: &[(usize, Vec<f64>)],\n    config: &MlpWaveVerificationConfig,\n) -> Result<MlpWaveReport, String> {\n    if signals.is_empty() {\n        return Err(\"no activation signals provided\".to_string());\n    }\n\n    let mut reports = Vec::with_capacity(signals.len());\n    for (neuron_index, signal) in signals {\n        let report = analyze_mlp_activation_wave(*neuron_index, signal, config)?;\n        reports.push(report);\n    }\n\n    Ok(MlpWaveReport {\n        neuron_reports: reports,\n    })\n}\n\npub fn verify_mlp_pairwise_manifold_transition<B: Backend>(\n    pre_model: &Transformer<B>,\n    post_model: &Transformer<B>,\n    device: &B::Device,\n    config: &PairwiseManifoldConfig,\n) -> Result<PairwiseManifoldTransitionReport, String> {\n    let modulus = ModularAdditionDataset::modulus();\n    if config.fixed_y >= modulus {\n        return Err(format!(\n            \"fixed_y {} out of range (modulus {})\",\n            config.fixed_y, modulus\n        ));\n    }\n\n    let pre_activations = collect_mlp_post_relu_activations(pre_model, device, config.fixed_y)?;\n    let post_activations = collect_mlp_post_relu_activations(post_model, device, config.fixed_y)?;\n\n    let d_ff = pre_activations.len();\n    if post_activations.len() != d_ff {\n        return Err(\"post activations shape mismatch\".to_string());\n    }\n\n    if config.neuron_indices.len() < 2 {\n        return Err(\"need at least two neuron indices\".to_string());\n    }\n\n    for &idx in &config.neuron_indices {\n        if idx >= d_ff {\n            return Err(format!(\n                \"neuron index {} out of range (d_ff={})\",\n                idx, d_ff\n            ));\n        }\n    }\n\n    let pre_signals: Vec<(usize, Vec<f64>)> = config\n        .neuron_indices\n        .iter()\n        .map(|&idx| (idx, pre_activations[idx].clone()))\n        .collect();\n    let post_signals: Vec<(usize, Vec<f64>)> = config\n        .neuron_indices\n        .iter()\n        .map(|&idx| (idx, post_activations[idx].clone()))\n        .collect();\n\n    verify_pairwise_manifold_transition_from_signals(&pre_signals, &post_signals, config)\n}\n\npub fn collect_mlp_activation_surface<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n    neuron_index: usize,\n) -> Result<Vec<Vec<f64>>, String> {\n    let modulus = ModularAdditionDataset::modulus();\n    let equals_token = ModularAdditionDataset::equals_token();\n\n    let mut inputs_vec = Vec::with_capacity(modulus * modulus * 3);\n    for x in 0..modulus {\n        for y in 0..modulus {\n            inputs_vec.push(x as i32);\n            inputs_vec.push(y as i32);\n            inputs_vec.push(equals_token as i32);\n        }\n    }\n\n    let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n        .reshape([modulus * modulus, 3]);\n    let (_, mlp_acts) = model.forward_with_mlp_activations(inputs);\n    let [batch_size, d_ff] = mlp_acts.dims();\n    if batch_size != modulus * modulus {\n        return Err(format!(\n            \"unexpected activation batch size {} (expected {})\",\n            batch_size,\n            modulus * modulus\n        ));\n    }\n    if neuron_index >= d_ff {\n        return Err(format!(\n            \"neuron index {} out of range (d_ff={})\",\n            neuron_index, d_ff\n        ));\n    }\n\n    let data: Vec<f32> = mlp_acts.into_data().to_vec().unwrap();\n    let mut surface = vec![vec![0.0f64; modulus]; modulus];\n    for x in 0..modulus {\n        for y in 0..modulus {\n            let batch = x * modulus + y;\n            let idx = batch * d_ff + neuron_index;\n            surface[x][y] = data[idx] as f64;\n        }\n    }\n\n    Ok(surface)\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_constructive_interference_surface_from_grid(\n    surface: &[Vec<f64>],\n    config: &DiagonalRidgeConfig,\n) -> Result<DiagonalRidgeReport, String> {\n    let (diag_main, diag_anti, axis_x, axis_y) =\n        surface_autocorrelation_scores(surface, config.max_lag)?;\n\n    let diagonal_score = diag_main.max(diag_anti);\n    let axis_score = axis_x.max(axis_y);\n    let ratio = if axis_score > 0.0 {\n        diagonal_score / axis_score\n    } else {\n        f64::INFINITY\n    };\n\n    if diagonal_score < config.min_diagonal_score {\n        return Err(format!(\n            \"diagonal autocorrelation {:.3} below threshold {:.3}\",\n            diagonal_score, config.min_diagonal_score\n        ));\n    }\n\n    if ratio < config.min_diagonal_ratio {\n        return Err(format!(\n            \"diagonal/axis ratio {:.3} below threshold {:.3}\",\n            ratio, config.min_diagonal_ratio\n        ));\n    }\n\n    Ok(DiagonalRidgeReport {\n        neuron_index: config.neuron_index,\n        diagonal_score,\n        axis_score,\n        ratio,\n    })\n}\n\npub fn verify_pairwise_manifold_transition_from_signals(\n    pre_signals: &[(usize, Vec<f64>)],\n    post_signals: &[(usize, Vec<f64>)],\n    config: &PairwiseManifoldConfig,\n) -> Result<PairwiseManifoldTransitionReport, String> {\n    let pre_report = evaluate_pairwise_manifolds_from_signals(pre_signals)?;\n    let post_report = evaluate_pairwise_manifolds_from_signals(post_signals)?;\n\n    if pre_report.average_score > config.max_pre_score {\n        return Err(format!(\n            \"pre-grok average circularity {:.3} exceeds threshold {:.3}\",\n            pre_report.average_score, config.max_pre_score\n        ));\n    }\n\n    let passing_pairs = post_report\n        .pair_reports\n        .iter()\n        .filter(|pair| {\n            pair.circularity_score >= config.min_post_score\n                && pair.axis_ratio <= config.max_axis_ratio\n        })\n        .count();\n    if passing_pairs < config.min_pairs_passing {\n        return Err(format!(\n            \"post-grok only {} pairs meet circularity threshold (need {})\",\n            passing_pairs, config.min_pairs_passing\n        ));\n    }\n\n    Ok(PairwiseManifoldTransitionReport {\n        pre_average_score: pre_report.average_score,\n        post_average_score: post_report.average_score,\n        passing_pairs,\n        total_pairs: post_report.pair_reports.len(),\n    })\n}\n\nfn evaluate_pairwise_manifolds_from_signals(\n    signals: &[(usize, Vec<f64>)],\n) -> Result<PairwiseManifoldReport, String> {\n    if signals.len() < 2 {\n        return Err(\"need at least two activation signals\".to_string());\n    }\n\n    let signal_len = signals[0].1.len();\n    if signal_len < 3 {\n        return Err(\"activation signal too short\".to_string());\n    }\n\n    for (idx, signal) in signals {\n        if signal.len() != signal_len {\n            return Err(format!(\n                \"signal length mismatch for neuron {}\",\n                idx\n            ));\n        }\n    }\n\n    let mut reports = Vec::new();\n    for i in 0..signals.len() {\n        for j in (i + 1)..signals.len() {\n            let (neuron_a, signal_a) = &signals[i];\n            let (neuron_b, signal_b) = &signals[j];\n            let points: Vec<(f64, f64)> = signal_a\n                .iter()\n                .zip(signal_b.iter())\n                .map(|(x, y)| (*x, *y))\n                .collect();\n            let (score, radius_cv, axis_ratio) =\n                match pairwise_circularity_metrics(&points) {\n                    Ok(metrics) => metrics,\n                    Err(_) => (0.0, 1.0, f64::INFINITY),\n                };\n            reports.push(PairwiseManifoldPairReport {\n                neuron_a: *neuron_a,\n                neuron_b: *neuron_b,\n                circularity_score: score,\n                radius_cv,\n                axis_ratio,\n            });\n        }\n    }\n\n    let average_score = reports\n        .iter()\n        .map(|pair| pair.circularity_score)\n        .sum::<f64>()\n        / reports.len() as f64;\n\n    Ok(PairwiseManifoldReport {\n        pair_reports: reports,\n        average_score,\n    })\n}\n\nfn pairwise_circularity_metrics(points: &[(f64, f64)]) -> Result<(f64, f64, f64), String> {\n    if points.len() < 3 {\n        return Err(\"not enough points for circularity\".to_string());\n    }\n\n    let n = points.len() as f64;\n    let mean_x = points.iter().map(|(x, _)| *x).sum::<f64>() / n;\n    let mean_y = points.iter().map(|(_, y)| *y).sum::<f64>() / n;\n\n    let mut var_x = 0.0;\n    let mut var_y = 0.0;\n    let mut cov_xy = 0.0;\n    for (x, y) in points {\n        let dx = x - mean_x;\n        let dy = y - mean_y;\n        var_x += dx * dx;\n        var_y += dy * dy;\n        cov_xy += dx * dy;\n    }\n    var_x /= n;\n    var_y /= n;\n    cov_xy /= n;\n\n    let trace = var_x + var_y;\n    let det = var_x * var_y - cov_xy * cov_xy;\n    if det <= 1.0e-12 || trace <= 0.0 {\n        return Err(\"degenerate covariance\".to_string());\n    }\n\n    let half_trace = trace * 0.5;\n    let disc = (half_trace * half_trace - det).max(0.0).sqrt();\n    let lambda1 = half_trace + disc;\n    let lambda2 = half_trace - disc;\n    if lambda2 <= 0.0 {\n        return Err(\"degenerate eigenvalues\".to_string());\n    }\n\n    let axis_ratio = (lambda1 / lambda2).sqrt();\n\n    let mut v1_x = cov_xy;\n    let mut v1_y = lambda1 - var_x;\n    if v1_x.abs() < 1.0e-12 && v1_y.abs() < 1.0e-12 {\n        v1_x = 1.0;\n        v1_y = 0.0;\n    }\n    let norm = (v1_x * v1_x + v1_y * v1_y).sqrt();\n    v1_x /= norm;\n    v1_y /= norm;\n    let v2_x = -v1_y;\n    let v2_y = v1_x;\n\n    let mut radii = Vec::with_capacity(points.len());\n    for (x, y) in points {\n        let dx = x - mean_x;\n        let dy = y - mean_y;\n        let p1 = dx * v1_x + dy * v1_y;\n        let p2 = dx * v2_x + dy * v2_y;\n        let w1 = p1 / lambda1.sqrt();\n        let w2 = p2 / lambda2.sqrt();\n        radii.push((w1 * w1 + w2 * w2).sqrt());\n    }\n\n    let mean_radius = radii.iter().sum::<f64>() / radii.len() as f64;\n    if mean_radius == 0.0 {\n        return Err(\"zero mean radius\".to_string());\n    }\n\n    let mut variance = 0.0;\n    for r in &radii {\n        let diff = r - mean_radius;\n        variance += diff * diff;\n    }\n    let std = (variance / radii.len() as f64).sqrt();\n    let radius_cv = std / mean_radius;\n    let score = 1.0 / (1.0 + radius_cv);\n\n    Ok((score, radius_cv, axis_ratio))\n}\n\n// Helper functions for verification - reserved for future integration\n#[allow(dead_code)]\nfn baseline_loss(\n    losses: &[(usize, f64)],\n    plateau_min_step: usize,\n    drop_window: usize,\n) -> Result<f64, String> {\n    let before: Vec<f64> = losses\n        .iter()\n        .filter(|(step, _)| *step <= plateau_min_step)\n        .map(|(_, loss)| *loss)\n        .collect();\n\n    if before.len() < drop_window || drop_window == 0 {\n        return Err(\"insufficient loss samples for baseline\".to_string());\n    }\n\n    let window = &before[before.len() - drop_window..];\n    Ok(window.iter().sum::<f64>() / drop_window as f64)\n}\n\n#[allow(dead_code)]\nfn find_drop_step(\n    losses: &[(usize, f64)],\n    plateau_min_step: usize,\n    baseline: f64,\n    drop_fraction: f64,\n) -> Result<usize, String> {\n    losses\n        .iter()\n        .filter(|(step, _)| *step >= plateau_min_step)\n        .find(|(_, loss)| *loss <= baseline * (1.0 - drop_fraction))\n        .map(|(step, _)| *step)\n        .ok_or_else(|| \"loss never drops below threshold\".to_string())\n}\n\nfn collect_mlp_post_relu_activations<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n    fixed_y: usize,\n) -> Result<Vec<Vec<f64>>, String> {\n    let modulus = ModularAdditionDataset::modulus();\n    let equals_token = ModularAdditionDataset::equals_token();\n\n    let mut inputs_vec = Vec::with_capacity(modulus * 3);\n    for x in 0..modulus {\n        inputs_vec.push(x as i32);\n        inputs_vec.push(fixed_y as i32);\n        inputs_vec.push(equals_token as i32);\n    }\n\n    let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n        .reshape([modulus, 3]);\n    let (_, mlp_acts) = model.forward_with_mlp_activations(inputs);\n    let [batch_size, d_ff] = mlp_acts.dims();\n\n    if batch_size != modulus {\n        return Err(format!(\n            \"unexpected activation batch size {} (expected {})\",\n            batch_size, modulus\n        ));\n    }\n\n    let data: Vec<f32> = mlp_acts.into_data().to_vec().unwrap();\n    let mut per_neuron = vec![Vec::with_capacity(batch_size); d_ff];\n    for batch in 0..batch_size {\n        for neuron in 0..d_ff {\n            let idx = batch * d_ff + neuron;\n            per_neuron[neuron].push(data[idx] as f64);\n        }\n    }\n\n    Ok(per_neuron)\n}\n\n#[allow(dead_code)]\nfn surface_autocorrelation_scores(\n    surface: &[Vec<f64>],\n    max_lag: usize,\n) -> Result<(f64, f64, f64, f64), String> {\n    let (height, width) = surface_dimensions(surface)?;\n    if height < 2 || width < 2 {\n        return Err(\"surface grid too small\".to_string());\n    }\n\n    let (mean, var) = surface_mean_variance(surface)?;\n    if var == 0.0 {\n        return Err(\"surface variance is zero\".to_string());\n    }\n\n    let max_lag = max_lag.min(height.saturating_sub(1)).min(width.saturating_sub(1));\n    if max_lag == 0 {\n        return Err(\"max_lag too small for surface\".to_string());\n    }\n\n    let diag_main = average_autocorrelation(surface, mean, var, max_lag, 1, 1)?;\n    let diag_anti = average_autocorrelation(surface, mean, var, max_lag, 1, -1)?;\n    let axis_x = average_autocorrelation(surface, mean, var, max_lag, 1, 0)?;\n    let axis_y = average_autocorrelation(surface, mean, var, max_lag, 0, 1)?;\n\n    Ok((diag_main, diag_anti, axis_x, axis_y))\n}\n\n#[allow(dead_code)]\nfn surface_dimensions(surface: &[Vec<f64>]) -> Result<(usize, usize), String> {\n    let height = surface.len();\n    if height == 0 {\n        return Err(\"surface is empty\".to_string());\n    }\n    let width = surface[0].len();\n    if width == 0 {\n        return Err(\"surface has empty rows\".to_string());\n    }\n    for (idx, row) in surface.iter().enumerate() {\n        if row.len() != width {\n            return Err(format!(\n                \"surface row {} has length {}, expected {}\",\n                idx,\n                row.len(),\n                width\n            ));\n        }\n    }\n    Ok((height, width))\n}\n\n#[allow(dead_code)]\nfn surface_mean_variance(surface: &[Vec<f64>]) -> Result<(f64, f64), String> {\n    let (height, width) = surface_dimensions(surface)?;\n    let mut sum = 0.0;\n    for row in surface {\n        for value in row {\n            sum += *value;\n        }\n    }\n    let count = (height * width) as f64;\n    let mean = sum / count;\n    let mut variance = 0.0;\n    for row in surface {\n        for value in row {\n            let diff = *value - mean;\n            variance += diff * diff;\n        }\n    }\n    variance /= count;\n    Ok((mean, variance))\n}\n\n#[allow(dead_code)]\nfn average_autocorrelation(\n    surface: &[Vec<f64>],\n    mean: f64,\n    variance: f64,\n    max_lag: usize,\n    dx: i32,\n    dy: i32,\n) -> Result<f64, String> {\n    let (height, width) = surface_dimensions(surface)?;\n    let mut total = 0.0;\n    let mut lag_count = 0usize;\n\n    for lag in 1..=max_lag {\n        let mut sum = 0.0;\n        let mut count = 0usize;\n        let lag = lag as i32;\n        for x in 0..height {\n            for y in 0..width {\n                let x2 = x as i32 + dx * lag;\n                let y2 = y as i32 + dy * lag;\n                if x2 < 0 || y2 < 0 {\n                    continue;\n                }\n                let x2 = x2 as usize;\n                let y2 = y2 as usize;\n                if x2 >= height || y2 >= width {\n                    continue;\n                }\n                let a = surface[x][y] - mean;\n                let b = surface[x2][y2] - mean;\n                sum += a * b;\n                count += 1;\n            }\n        }\n        if count == 0 {\n            continue;\n        }\n        let corr = sum / (count as f64 * variance);\n        total += corr.abs();\n        lag_count += 1;\n    }\n\n    if lag_count == 0 {\n        return Err(\"no valid lags for autocorrelation\".to_string());\n    }\n\n    Ok(total / lag_count as f64)\n}\n\n#[allow(dead_code)]\nfn analyze_mlp_activation_wave(\n    neuron_index: usize,\n    signal: &[f64],\n    config: &MlpWaveVerificationConfig,\n) -> Result<MlpWaveNeuronReport, String> {\n    if signal.len() < 3 {\n        return Err(format!(\n            \"activation signal for neuron {} is too short\",\n            neuron_index\n        ));\n    }\n\n    let negative_count = signal\n        .iter()\n        .filter(|value| **value < -config.negative_tolerance)\n        .count();\n    let negative_fraction = negative_count as f64 / signal.len() as f64;\n    if negative_fraction > config.max_negative_fraction {\n        return Err(format!(\n            \"neuron {} has {:.3} negative activations\",\n            neuron_index, negative_fraction\n        ));\n    }\n\n    let (dominant_frequency, dominant_ratio) = dominant_frequency_ratio(signal)?;\n    if dominant_ratio < config.min_dominant_ratio {\n        return Err(format!(\n            \"neuron {} dominant frequency ratio {:.3} below threshold {:.3}\",\n            neuron_index, dominant_ratio, config.min_dominant_ratio\n        ));\n    }\n\n    let best_correlation =\n        best_rectified_sine_correlation(signal, dominant_frequency, config.phase_steps);\n    if best_correlation < config.min_correlation {\n        return Err(format!(\n            \"neuron {} rectified sine correlation {:.3} below threshold {:.3}\",\n            neuron_index, best_correlation, config.min_correlation\n        ));\n    }\n\n    Ok(MlpWaveNeuronReport {\n        neuron_index,\n        dominant_frequency,\n        dominant_ratio,\n        negative_fraction,\n        best_correlation,\n    })\n}\n\n#[allow(dead_code)]\nfn dominant_frequency_ratio(signal: &[f64]) -> Result<(usize, f64), String> {\n    if signal.len() < 2 {\n        return Err(\"signal too short for FFT\".to_string());\n    }\n\n    let mut planner = FftPlanner::new();\n    let fft = planner.plan_fft_forward(signal.len());\n    let mut buffer: Vec<Complex<f64>> = signal\n        .iter()\n        .map(|&x| Complex::new(x, 0.0))\n        .collect();\n    fft.process(&mut buffer);\n    let magnitudes: Vec<f64> = buffer.iter().map(|c| c.norm()).collect();\n\n    if magnitudes.len() < 2 {\n        return Err(\"insufficient FFT bins\".to_string());\n    }\n\n    let mut dominant_idx = 1usize;\n    let mut dominant_mag = magnitudes[1];\n    for (idx, &mag) in magnitudes.iter().enumerate().skip(2) {\n        if mag > dominant_mag {\n            dominant_mag = mag;\n            dominant_idx = idx;\n        }\n    }\n\n    if dominant_idx == 0 {\n        return Err(\"dominant frequency is zero\".to_string());\n    }\n\n    let mut nonzero = magnitudes[1..].to_vec();\n    nonzero.sort_by(|a, b| a.partial_cmp(b).unwrap_or(std::cmp::Ordering::Equal));\n    let median_mag = nonzero[nonzero.len() / 2];\n    let ratio = if median_mag == 0.0 {\n        dominant_mag\n    } else {\n        dominant_mag / median_mag\n    };\n\n    Ok((dominant_idx, ratio))\n}\n\n#[allow(dead_code)]\nfn best_rectified_sine_correlation(signal: &[f64], frequency: usize, phase_steps: usize) -> f64 {\n    if signal.is_empty() || frequency == 0 || phase_steps == 0 {\n        return 0.0;\n    }\n\n    let tau = std::f64::consts::PI * 2.0;\n    let mut best = -1.0;\n    for step in 0..phase_steps {\n        let phase = tau * step as f64 / phase_steps as f64;\n        let wave: Vec<f64> = (0..signal.len())\n            .map(|idx| {\n                let angle = tau * frequency as f64 * idx as f64 / signal.len() as f64 + phase;\n                angle.sin().max(0.0)\n            })\n            .collect();\n        let corr = pearson_correlation(signal, &wave);\n        if corr > best {\n            best = corr;\n        }\n    }\n\n    best\n}\n\n#[allow(dead_code)]\nfn pearson_correlation(a: &[f64], b: &[f64]) -> f64 {\n    if a.len() != b.len() || a.is_empty() {\n        return 0.0;\n    }\n\n    let mean_a = a.iter().sum::<f64>() / a.len() as f64;\n    let mean_b = b.iter().sum::<f64>() / b.len() as f64;\n\n    let mut num = 0.0;\n    let mut denom_a = 0.0;\n    let mut denom_b = 0.0;\n    for (x, y) in a.iter().zip(b.iter()) {\n        let da = x - mean_a;\n        let db = y - mean_b;\n        num += da * db;\n        denom_a += da * da;\n        denom_b += db * db;\n    }\n\n    if denom_a == 0.0 || denom_b == 0.0 {\n        return 0.0;\n    }\n\n    num / (denom_a.sqrt() * denom_b.sqrt())\n}\n\n/// Verify that the model actually learned modular addition\n/// by testing it on ALL possible examples systematically\npub fn verify_full_accuracy<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n) -> (f32, f32) {\n    println!(\"ðŸ” Verifying model on COMPLETE dataset (no sampling)...\");\n\n    let train_dataset = ModularAdditionDataset::new(true, 42);\n    let val_dataset = ModularAdditionDataset::new(false, 42);\n\n    let train_acc = test_all_examples(model, &train_dataset, device);\n    let val_acc = test_all_examples(model, &val_dataset, device);\n\n    println!(\"ðŸ“Š FULL Dataset Results:\");\n    println!(\"   Training: {}/{} = {:.2}%\",\n        (train_acc * train_dataset.len() as f32) as usize,\n        train_dataset.len(),\n        train_acc * 100.0);\n    println!(\"   Validation: {}/{} = {:.2}%\",\n        (val_acc * val_dataset.len() as f32) as usize,\n        val_dataset.len(),\n        val_acc * 100.0);\n\n    (train_acc, val_acc)\n}\n\n/// Test model on ALL examples in dataset (no random sampling)\nfn test_all_examples<B: Backend>(\n    model: &Transformer<B>,\n    dataset: &ModularAdditionDataset,\n    device: &B::Device,\n) -> f32 {\n    let mut total_correct = 0;\n    let total = dataset.len();\n\n    // Test in batches to avoid memory issues\n    let batch_size = 100;\n    for batch_start in (0..total).step_by(batch_size) {\n        let batch_end = (batch_start + batch_size).min(total);\n        let batch_len = batch_end - batch_start;\n\n        let mut inputs_vec = Vec::new();\n        let mut targets_vec = Vec::new();\n\n        for idx in batch_start..batch_end {\n            let (input, target) = dataset.get(idx).unwrap();\n            inputs_vec.extend(input);\n            targets_vec.push(target as i32);\n        }\n\n        let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n            .reshape([batch_len, 3]);\n\n        let logits = model.forward(inputs);\n        let predictions = logits.argmax(1).squeeze::<1>();\n        let predictions_vec: Vec<i32> = predictions.into_data().to_vec().unwrap();\n\n        let correct = predictions_vec\n            .iter()\n            .zip(targets_vec.iter())\n            .filter(|(p, t)| p == t)\n            .count();\n\n        total_correct += correct;\n    }\n\n    total_correct as f32 / total as f32\n}\n\n/// Verify that weight norms decrease over time with weight decay\n/// and show correlation with the grokking transition\npub fn verify_weight_norm_decay(\n    weight_norm_history: &[(usize, f64)],\n    grok_step: Option<usize>,\n    config: &WeightNormDecayConfig,\n) -> Result<WeightNormDecayReport, String> {\n    if weight_norm_history.len() < 2 {\n        return Err(\"weight norm history requires at least 2 snapshots\".to_string());\n    }\n\n    let initial_norm = weight_norm_history\n        .first()\n        .ok_or_else(|| \"weight norm history is empty\".to_string())?\n        .1;\n    let final_norm = weight_norm_history\n        .last()\n        .ok_or_else(|| \"weight norm history is empty\".to_string())?\n        .1;\n\n    if initial_norm <= 0.0 {\n        return Err(\"initial weight norm must be positive\".to_string());\n    }\n\n    // Check that norms are generally decreasing (with weight decay)\n    let relative_decrease = (initial_norm - final_norm) / initial_norm;\n    if relative_decrease < config.min_relative_decrease {\n        return Err(format!(\n            \"weight norm decrease {:.4} below threshold {:.4} (initial {:.4}, final {:.4})\",\n            relative_decrease, config.min_relative_decrease, initial_norm, final_norm\n        ));\n    }\n\n    // Check that norms are not increasing (weight decay should monotonically decrease norms)\n    for window in weight_norm_history.windows(2) {\n        let prev_norm = window[0].1;\n        let next_norm = window[1].1;\n        if next_norm > prev_norm * 1.01 {\n            // Allow 1% tolerance for numerical noise\n            return Err(format!(\n                \"weight norm increased from {:.4} to {:.4} at step {}\",\n                prev_norm, next_norm, window[1].0\n            ));\n        }\n    }\n\n    // If grok step is provided, check that norms show a shift around grokking\n    let grok_step_norm = if let Some(step) = grok_step {\n        weight_norm_history\n            .iter()\n            .filter(|(s, _)| {\n                *s >= step.saturating_sub(config.grok_step_window)\n                    && *s <= step + config.grok_step_window\n            })\n            .map(|(_, norm)| *norm)\n            .next()\n    } else {\n        None\n    };\n\n    Ok(WeightNormDecayReport {\n        initial_norm,\n        final_norm,\n        relative_decrease,\n        grok_step_norm,\n    })\n}\n\n/// Verify the characteristic \"snake curve\" shape of grokking phenomenon\n/// on log-scale plots: train accuracy rises early, validation stays flat then jumps\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_snake_curve_shape(\n    train_acc: &[(usize, f32)],\n    val_acc: &[(usize, f32)],\n    config: &SnakeCurveConfig,\n) -> Result<SnakeCurveReport, String> {\n    if train_acc.is_empty() || val_acc.is_empty() {\n        return Err(\"empty accuracy history\".to_string());\n    }\n\n    // Phase 1: Verify early train convergence (immediate drop on log scale)\n    let train_converged_step = train_acc\n        .iter()\n        .find(|(_, acc)| *acc >= config.train_convergence_threshold)\n        .map(|(step, _)| *step)\n        .ok_or_else(|| {\n            format!(\n                \"train accuracy never reaches {} threshold\",\n                config.train_convergence_threshold\n            )\n        })?;\n\n    if train_converged_step > config.early_step_max {\n        return Err(format!(\n            \"train accuracy converges at step {}, expected before {}\",\n            train_converged_step, config.early_step_max\n        ));\n    }\n\n    // Phase 2: Verify validation plateau (flat on log scale)\n    let plateau_samples: Vec<f32> = val_acc\n        .iter()\n        .filter(|(step, _)| *step >= 1 && *step <= config.val_plateau_min_step)\n        .map(|(_, acc)| *acc)\n        .collect();\n\n    if plateau_samples.is_empty() {\n        return Err(\"no validation samples in plateau region\".to_string());\n    }\n\n    let val_plateau_max = plateau_samples\n        .iter()\n        .fold(0.0f32, |max_val, &acc| max_val.max(acc));\n\n    if val_plateau_max > config.val_plateau_max_acc {\n        return Err(format!(\n            \"validation accuracy in plateau region too high: max {:.4} exceeds {:.4}\",\n            val_plateau_max, config.val_plateau_max_acc\n        ));\n    }\n\n    // Phase 3: Verify delayed validation jump (crash up on log scale)\n    let val_grok_step = val_acc\n        .iter()\n        .filter(|(step, _)| *step > config.val_plateau_min_step)\n        .find(|(_, acc)| *acc >= config.val_final_threshold)\n        .map(|(step, _)| *step)\n        .ok_or_else(|| {\n            format!(\n                \"validation accuracy never reaches {} after plateau\",\n                config.val_final_threshold\n            )\n        })?;\n\n    // Phase 4: Verify phase ordering (snake shape: early train, late val)\n    let curve_shape_valid = train_converged_step < config.val_plateau_min_step\n        && val_grok_step > config.val_plateau_min_step\n        && train_converged_step < val_grok_step;\n\n    if !curve_shape_valid {\n        return Err(format!(\n            \"invalid curve shape: train converged at {}, val plateau until {}, val grokked at {}\",\n            train_converged_step, config.val_plateau_min_step, val_grok_step\n        ));\n    }\n\n    Ok(SnakeCurveReport {\n        train_converged_by_step: train_converged_step,\n        val_plateau_max_acc: val_plateau_max,\n        val_grok_step,\n        curve_shape_valid,\n    })\n}\n\n/// Test specific examples to see if model actually computes mod p\npub fn test_specific_examples<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n) {\n    println!();\n    println!(\"ðŸ§ª Testing specific examples:\");\n\n    let modulus = ModularAdditionDataset::modulus();\n    let equals_token = ModularAdditionDataset::equals_token();\n    let test_cases = vec![\n        (0, 0, 0),\n        (1, 1, 2),\n        (50, 50, (50 + 50) % modulus),\n        (112, 1, (112 + 1) % modulus),\n        (56, 57, (56 + 57) % modulus),\n        (112, 112, (112 + 112) % modulus),\n        (10, 20, 30),\n        (60, 60, (60 + 60) % modulus),\n    ];\n\n    for (a, b, expected) in test_cases {\n        let input_vec = vec![a as i32, b as i32, equals_token as i32];\n        let input = Tensor::<B, 1, Int>::from_ints(input_vec.as_slice(), device)\n            .reshape([1, 3]);\n\n        let logits = model.forward(input);\n        let prediction = logits.argmax(1).squeeze::<1>();\n        let pred_value: i32 = prediction.into_data().to_vec().unwrap()[0];\n\n        let correct = if pred_value == expected as i32 { \"âœ“\" } else { \"âœ—\" };\n        println!(\n            \"   {} + {} mod {} = {} | Model: {} {}\",\n            a, b, modulus, expected, pred_value, correct\n        );\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    fn make_histories(\n        train_acc: &[(usize, f32)],\n        val_acc: &[(usize, f32)],\n        val_loss: &[(usize, f64)],\n    ) -> (LossHistory, AccuracyHistory) {\n        let mut loss_history = LossHistory::new();\n        loss_history.val_snapshots = val_loss.to_vec();\n        let mut accuracy_history = AccuracyHistory::new();\n        accuracy_history.train_snapshots = train_acc.to_vec();\n        accuracy_history.val_snapshots = val_acc.to_vec();\n        (loss_history, accuracy_history)\n    }\n\n    #[test]\n    fn grokking_phase_verification_passes_on_synthetic_signal() {\n        let train_acc = vec![(0, 0.2), (500, 0.995), (1000, 0.999)];\n        let val_acc = vec![\n            (0, 0.01),\n            (500, 0.01),\n            (1500, 0.01),\n            (2200, 0.995),\n            (2500, 0.999),\n        ];\n        let val_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 2.0),\n            (2200, 0.2),\n            (2500, 0.15),\n        ];\n\n        let (loss_history, accuracy_history) = make_histories(&train_acc, &val_acc, &val_loss);\n        let config = GrokkingVerificationConfig {\n            early_step_max: 1000,\n            plateau_min_step: 1500,\n            generalization_window: 1500,\n            accuracy_window: 2,\n            loss_window: 2,\n            ..GrokkingVerificationConfig::default_for_modulus(ModularAdditionDataset::modulus())\n        };\n\n        let report = verify_grokking_phase_transition(&loss_history, &accuracy_history, &config)\n            .expect(\"expected grokking verification to pass\");\n        assert!(report.transition_step >= config.plateau_min_step);\n        assert!(report.early_train_step <= config.early_step_max);\n    }\n\n    #[test]\n    fn grokking_phase_verification_fails_without_transition() {\n        let train_acc = vec![(0, 0.2), (500, 0.995)];\n        let val_acc = vec![(0, 0.01), (500, 0.01), (1500, 0.02)];\n        let val_loss = vec![(0, 2.0), (500, 2.0), (1500, 1.9)];\n\n        let (loss_history, accuracy_history) = make_histories(&train_acc, &val_acc, &val_loss);\n        let config = GrokkingVerificationConfig {\n            early_step_max: 1000,\n            plateau_min_step: 1000,\n            generalization_window: 1000,\n            accuracy_window: 2,\n            loss_window: 2,\n            ..GrokkingVerificationConfig::default_for_modulus(ModularAdditionDataset::modulus())\n        };\n\n        let err = verify_grokking_phase_transition(&loss_history, &accuracy_history, &config)\n            .expect_err(\"expected grokking verification to fail\");\n        assert!(err.contains(\"validation accuracy never reaches target\"));\n    }\n\n    #[test]\n    fn grokking_phase_verification_fails_with_slow_accuracy_rise() {\n        let train_acc = vec![(0, 0.2), (500, 0.995), (1000, 0.999)];\n        let val_acc = vec![\n            (0, 0.01),\n            (500, 0.01),\n            (1500, 0.5),\n            (2000, 0.6),\n            (2200, 0.995),\n            (2500, 0.999),\n        ];\n        let val_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 2.0),\n            (2200, 0.2),\n            (2500, 0.15),\n        ];\n\n        let (loss_history, accuracy_history) = make_histories(&train_acc, &val_acc, &val_loss);\n        let config = GrokkingVerificationConfig {\n            early_step_max: 1000,\n            plateau_min_step: 1000,\n            generalization_window: 2000,\n            accuracy_window: 2,\n            loss_window: 2,\n            ..GrokkingVerificationConfig::default_for_modulus(ModularAdditionDataset::modulus())\n        };\n\n        let err = verify_grokking_phase_transition(&loss_history, &accuracy_history, &config)\n            .expect_err(\"expected grokking verification to fail\");\n        assert!(err.contains(\"plateau too high\"));\n    }\n\n    #[test]\n    fn restricted_loss_detection_passes_with_early_drop() {\n        let full_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 2.0),\n            (2500, 2.0),\n            (3500, 0.3),\n        ];\n        let restricted_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 0.8),\n            (2500, 0.4),\n            (3500, 0.3),\n        ];\n\n        let config = RestrictedLossVerificationConfig {\n            plateau_min_step: 1000,\n            drop_window: 2,\n            drop_fraction: 0.5,\n            min_step_lead: 500,\n        };\n\n        let report = verify_restricted_loss_early_drop(&full_loss, &restricted_loss, &config)\n            .expect(\"expected restricted loss verification to pass\");\n        assert!(report.restricted_drop_step < report.full_drop_step);\n    }\n\n    #[test]\n    fn restricted_loss_detection_fails_when_full_drops_first() {\n        let full_loss = vec![(0, 2.0), (1000, 1.0), (1500, 0.8)];\n        let restricted_loss = vec![(0, 2.0), (1000, 2.0), (1500, 0.8)];\n\n        let config = RestrictedLossVerificationConfig {\n            plateau_min_step: 500,\n            drop_window: 1,\n            drop_fraction: 0.3,\n            min_step_lead: 200,\n        };\n\n        let err = verify_restricted_loss_early_drop(&full_loss, &restricted_loss, &config)\n            .expect_err(\"expected restricted loss verification to fail\");\n        assert!(err.contains(\"restricted loss drops\"));\n    }\n\n    #[test]\n    fn excluded_loss_spike_verification_passes() {\n        let excluded_loss = vec![(0, 1.0), (1500, 2.2), (3000, 1.1)];\n        let config = ExcludedLossSpikeConfig {\n            min_relative_increase: 0.5,\n        };\n\n        let report = verify_excluded_loss_spike(&excluded_loss, &config)\n            .expect(\"expected excluded loss spike verification to pass\");\n        assert_eq!(report.spike_step, 1500);\n    }\n\n    #[test]\n    fn excluded_loss_spike_verification_fails_without_spike() {\n        let excluded_loss = vec![(0, 1.0), (1500, 1.1), (3000, 1.0)];\n        let config = ExcludedLossSpikeConfig {\n            min_relative_increase: 0.5,\n        };\n\n        let err = verify_excluded_loss_spike(&excluded_loss, &config)\n            .expect_err(\"expected excluded loss spike verification to fail\");\n        assert!(err.contains(\"excluded loss spike too small\"));\n    }\n\n    #[test]\n    fn restricted_loss_drop_verification_passes() {\n        let restricted_loss = vec![\n            (0, 2.0),\n            (1000, 1.8),\n            (5000, 1.2),\n            (7000, 0.8),\n            (10000, 0.9),\n        ];\n        let config = RestrictedLossDropConfig {\n            min_relative_decrease: 0.3,\n        };\n\n        let report = verify_restricted_loss_drop(&restricted_loss, &config)\n            .expect(\"expected restricted loss drop verification to pass\");\n        assert_eq!(report.drop_step, 7000);\n        assert!(report.baseline_loss > report.drop_loss);\n        assert!((report.baseline_loss - report.drop_loss) / report.baseline_loss >= 0.3);\n    }\n\n    #[test]\n    fn restricted_loss_drop_verification_fails_without_drop() {\n        let restricted_loss = vec![\n            (0, 2.0),\n            (1000, 2.1),\n            (5000, 1.9),\n            (7000, 2.0),\n            (10000, 2.05),\n        ];\n        let config = RestrictedLossDropConfig {\n            min_relative_decrease: 0.3,\n        };\n\n        let err = verify_restricted_loss_drop(&restricted_loss, &config)\n            .expect_err(\"expected restricted loss drop verification to fail\");\n        assert!(err.contains(\"restricted loss decrease too small\"));\n    }\n\n    #[test]\n    fn mlp_activation_wave_verification_passes_on_rectified_sine() {\n        let n = ModularAdditionDataset::modulus();\n        let freq = 5.0;\n        let mut signal = Vec::with_capacity(n);\n        for idx in 0..n {\n            let angle = 2.0 * std::f64::consts::PI * freq * idx as f64 / n as f64;\n            let rectified = angle.sin().max(0.0);\n            let wobble = (2.0 * std::f64::consts::PI * 2.0 * idx as f64 / n as f64)\n                .sin()\n                .abs()\n                * 0.02;\n            signal.push(rectified + wobble);\n        }\n\n        let config = MlpWaveVerificationConfig {\n            max_negative_fraction: 0.0,\n            negative_tolerance: 1e-6,\n            min_dominant_ratio: 3.0,\n            min_correlation: 0.7,\n            phase_steps: 32,\n        };\n\n        let report =\n            verify_mlp_activation_waves_from_signals(&[(0, signal)], &config)\n                .expect(\"expected rectified sine to pass\");\n        assert_eq!(report.neuron_reports.len(), 1);\n    }\n\n    #[test]\n    fn mlp_activation_wave_verification_fails_on_noise() {\n        let n = ModularAdditionDataset::modulus();\n        let signal: Vec<f64> = (0..n).map(|idx| idx as f64 / n as f64).collect();\n\n        let config = MlpWaveVerificationConfig {\n            max_negative_fraction: 0.0,\n            negative_tolerance: 1e-6,\n            min_dominant_ratio: 4.0,\n            min_correlation: 0.8,\n            phase_steps: 16,\n        };\n\n        let err =\n            verify_mlp_activation_waves_from_signals(&[(0, signal)], &config)\n                .expect_err(\"expected linear signal to fail\");\n        assert!(err.contains(\"dominant frequency ratio\") || err.contains(\"correlation\"));\n    }\n\n    #[test]\n    fn pairwise_manifold_transition_passes_on_elliptic_signals() {\n        let n = ModularAdditionDataset::modulus();\n        let freq = 5.0;\n        let mut post_signals = Vec::new();\n        for neuron_idx in 0..7usize {\n            let phase = 2.0 * std::f64::consts::PI * neuron_idx as f64 / 7.0;\n            let signal: Vec<f64> = (0..n)\n                .map(|idx| {\n                    let angle = 2.0 * std::f64::consts::PI * freq * idx as f64 / n as f64 + phase;\n                    angle.sin()\n                })\n                .collect();\n            post_signals.push((neuron_idx, signal));\n        }\n\n        let mut pre_signals = Vec::new();\n        for neuron_idx in 0..7usize {\n            let signal: Vec<f64> = (0..n)\n                .map(|idx| idx as f64 / n as f64 + neuron_idx as f64 * 0.01)\n                .collect();\n            pre_signals.push((neuron_idx, signal));\n        }\n\n        let config = PairwiseManifoldConfig {\n            fixed_y: 0,\n            neuron_indices: (0..7).collect(),\n            min_post_score: 0.75,\n            max_pre_score: 0.55,\n            max_axis_ratio: 8.0,\n            min_pairs_passing: 8,\n        };\n\n        let report = verify_pairwise_manifold_transition_from_signals(\n            &pre_signals,\n            &post_signals,\n            &config,\n        )\n        .expect(\"expected pairwise manifold transition to pass\");\n        assert!(report.passing_pairs >= config.min_pairs_passing);\n    }\n\n    #[test]\n    fn pairwise_manifold_transition_fails_without_rings() {\n        let n = ModularAdditionDataset::modulus();\n        let mut signals = Vec::new();\n        for neuron_idx in 0..7usize {\n            let signal: Vec<f64> = (0..n)\n                .map(|idx| idx as f64 / n as f64 + neuron_idx as f64 * 0.02)\n                .collect();\n            signals.push((neuron_idx, signal));\n        }\n\n        let config = PairwiseManifoldConfig {\n            fixed_y: 0,\n            neuron_indices: (0..7).collect(),\n            min_post_score: 0.8,\n            max_pre_score: 0.55,\n            max_axis_ratio: 4.0,\n            min_pairs_passing: 5,\n        };\n\n        let err = verify_pairwise_manifold_transition_from_signals(\n            &signals,\n            &signals,\n            &config,\n        )\n        .expect_err(\"expected pairwise manifold transition to fail\");\n        assert!(err.contains(\"post-grok\") || err.contains(\"pre-grok\"));\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_passes_on_diagonal_corrugation() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create diagonal ridges: f(x,y) = sin(2Ï€ * freq * (x+y) / n)\n        let freq = 3.0;\n        for x in 0..n {\n            for y in 0..n {\n                let angle = 2.0 * std::f64::consts::PI * freq * (x + y) as f64 / n as f64;\n                surface[x][y] = angle.sin();\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let report = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect(\"expected diagonal ridge verification to pass\");\n        assert!(report.diagonal_score > report.axis_score);\n        assert!(report.ratio >= config.min_diagonal_ratio);\n        assert!(report.diagonal_score >= config.min_diagonal_score);\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_passes_on_anti_diagonal_pattern() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create anti-diagonal ridges: f(x,y) = sin(2Ï€ * freq * (x-y) / n)\n        let freq = 3.0;\n        for x in 0..n {\n            for y in 0..n {\n                let angle = 2.0 * std::f64::consts::PI * freq * (x as i32 - y as i32) as f64 / n as f64;\n                surface[x][y] = angle.sin();\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let report = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect(\"expected anti-diagonal ridge verification to pass\");\n        assert!(report.diagonal_score > report.axis_score);\n        assert!(report.ratio >= config.min_diagonal_ratio);\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_fails_on_axis_aligned_pattern() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create axis-aligned ridges: f(x,y) = sin(2Ï€ * freq * x / n)\n        let freq = 3.0;\n        for x in 0..n {\n            for y in 0..n {\n                let angle = 2.0 * std::f64::consts::PI * freq * x as f64 / n as f64;\n                surface[x][y] = angle.sin();\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let err = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect_err(\"expected axis-aligned pattern to fail diagonal verification\");\n        assert!(err.contains(\"diagonal/axis ratio\") || err.contains(\"diagonal autocorrelation\"));\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_fails_on_flat_surface() {\n        let n = 50;\n        let surface = vec![vec![1.5; n]; n];\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let err = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect_err(\"expected flat surface to fail\");\n        assert!(err.contains(\"surface variance is zero\"));\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_fails_on_random_noise() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create random noise (low autocorrelation everywhere)\n        for x in 0..n {\n            for y in 0..n {\n                surface[x][y] = ((x * 17 + y * 31) % 100) as f64 / 100.0;\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let err = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect_err(\"expected random noise to fail diagonal verification\");\n        assert!(err.contains(\"diagonal/axis ratio\") || err.contains(\"diagonal autocorrelation\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_passes_on_valid_grokking_curve() {\n        // Simulate typical grokking: train converges early, val plateaus then jumps\n        let train_acc = vec![\n            (1, 0.10),\n            (10, 0.50),\n            (100, 0.80),\n            (500, 0.98),\n            (1000, 0.99),\n            (2000, 0.995),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (10, 0.009),\n            (100, 0.01),\n            (500, 0.011),\n            (1000, 0.009),\n            (1500, 0.01),\n            (2000, 0.92),\n            (3000, 0.98),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 1500,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let report = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect(\"expected snake curve verification to pass\");\n\n        assert_eq!(report.train_converged_by_step, 500);\n        assert!(report.val_plateau_max_acc < config.val_plateau_max_acc);\n        assert_eq!(report.val_grok_step, 2000);\n        assert!(report.curve_shape_valid);\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_on_simultaneous_convergence() {\n        // Both train and val converge at the same time (no grokking)\n        let train_acc = vec![\n            (1, 0.10),\n            (100, 0.98),\n            (500, 0.99),\n        ];\n        let val_acc = vec![\n            (1, 0.10),\n            (100, 0.97),\n            (500, 0.98),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 200,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let err = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect_err(\"expected snake curve verification to fail\");\n        assert!(err.contains(\"plateau region too high\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_on_slow_train_convergence() {\n        // Train takes too long to converge (not the typical snake curve)\n        let train_acc = vec![\n            (1, 0.10),\n            (500, 0.50),\n            (1000, 0.70),\n            (2000, 0.98),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (500, 0.01),\n            (1000, 0.01),\n            (2000, 0.92),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 1500,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let err = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect_err(\"expected snake curve verification to fail\");\n        assert!(err.contains(\"converges at step 2000\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_without_val_jump() {\n        // Validation never jumps up (no generalization)\n        let train_acc = vec![\n            (1, 0.10),\n            (100, 0.98),\n            (500, 0.99),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (100, 0.01),\n            (500, 0.01),\n            (2000, 0.02),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 200,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let err = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect_err(\"expected snake curve verification to fail\");\n        assert!(err.contains(\"validation accuracy never reaches\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_on_wrong_phase_ordering() {\n        // Val jumps before train converges (backwards snake)\n        let train_acc = vec![\n            (1, 0.10),\n            (500, 0.50),\n            (2000, 0.98),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (100, 0.95),\n            (500, 0.98),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 200,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let result = verify_snake_curve_shape(&train_acc, &val_acc, &config);\n        // Should fail because val jumps before train converges (not a proper snake curve)\n        // This can fail for multiple reasons, but should definitely fail\n        assert!(result.is_err(), \"Expected snake curve verification to fail but it passed\");\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_passes_on_decreasing_norms() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 85.0),\n            (1000, 72.0),\n            (1500, 63.0),\n            (2000, 58.0),\n            (2500, 55.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let report = verify_weight_norm_decay(&weight_norms, Some(2000), &config)\n            .expect(\"expected weight norm decay verification to pass\");\n\n        assert_eq!(report.initial_norm, 100.0);\n        assert_eq!(report.final_norm, 55.0);\n        assert!((report.relative_decrease - 0.45).abs() < 0.01);\n        assert!(report.grok_step_norm.is_some());\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_fails_on_flat_norms() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 99.0),\n            (1000, 98.5),\n            (1500, 98.0),\n            (2000, 97.5),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let err = verify_weight_norm_decay(&weight_norms, None, &config)\n            .expect_err(\"expected weight norm decay verification to fail\");\n        assert!(err.contains(\"weight norm decrease\"));\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_fails_on_increasing_norms() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 95.0),\n            (1000, 105.0), // Increase here\n            (1500, 90.0),\n            (2000, 85.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let err = verify_weight_norm_decay(&weight_norms, None, &config)\n            .expect_err(\"expected weight norm decay verification to fail\");\n        assert!(err.contains(\"weight norm increased\"));\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_allows_small_noise() {\n        // Weight decay with small numerical noise (within 1% tolerance)\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 95.0),\n            (1000, 90.5),  // Small increase within tolerance\n            (1500, 90.0),\n            (2000, 85.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let report = verify_weight_norm_decay(&weight_norms, None, &config)\n            .expect(\"expected weight norm decay with small noise to pass\");\n\n        assert_eq!(report.initial_norm, 100.0);\n        assert_eq!(report.final_norm, 85.0);\n        assert!((report.relative_decrease - 0.15).abs() < 0.01);\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_finds_grok_step_norm() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (1000, 80.0),\n            (1800, 68.0),  // Within window of grok step (2200 - 500 = 1700)\n            (2000, 65.0),\n            (2200, 62.0),  // Around grok step\n            (3000, 55.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let report = verify_weight_norm_decay(&weight_norms, Some(2200), &config)\n            .expect(\"expected weight norm decay verification to pass\");\n\n        assert!(report.grok_step_norm.is_some());\n        let grok_norm = report.grok_step_norm.unwrap();\n        // Should find the first norm in the window [1700, 2700], which is 68.0 at step 1800\n        assert!((grok_norm - 68.0).abs() < 0.01);\n    }\n}\n","structuredPatch":[{"oldStart":40,"oldLines":6,"newStart":40,"newLines":7,"lines":[" }"," "," #[derive(Debug, Clone)]","+#[allow(dead_code)]"," pub struct GrokkingPhaseReport {","     pub early_train_step: usize,","     pub transition_step: usize,"]}],"userModified":false,"replaceAll":false}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01MPn2QQj53FRY6met5athKj","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_016RL7Pvdgfs6ZZwVYUtnWHS","name":"Edit","input":{"replace_all":false,"file_path":"/Users/admin/MechInt/src/verify.rs","old_string":"#[derive(Debug, Clone)]\npub struct RestrictedLossReport {\n    pub restricted_drop_step: usize,\n    pub full_drop_step: usize,\n}","new_string":"#[derive(Debug, Clone)]\n#[allow(dead_code)]\npub struct RestrictedLossReport {\n    pub restricted_drop_step: usize,\n    pub full_drop_step: usize,\n}"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":512,"cache_read_input_tokens":72019,"cache_creation":{"ephemeral_5m_input_tokens":512,"ephemeral_1h_input_tokens":0},"output_tokens":216,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"dd80c3c1-066d-4f2e-bd30-647711a27ce3"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_016RL7Pvdgfs6ZZwVYUtnWHS","type":"tool_result","content":"The file /Users/admin/MechInt/src/verify.rs has been updated. Here's the result of running `cat -n` on a snippet of the edited file:\n    55â†’    pub drop_fraction: f64,\n    56â†’    pub min_step_lead: usize,\n    57â†’}\n    58â†’\n    59â†’#[derive(Debug, Clone)]\n    60â†’#[allow(dead_code)]\n    61â†’pub struct RestrictedLossReport {\n    62â†’    pub restricted_drop_step: usize,\n    63â†’    pub full_drop_step: usize,\n    64â†’}\n    65â†’\n    66â†’#[derive(Debug, Clone)]\n    67â†’pub struct RestrictedLossDropConfig {\n    68â†’    pub min_relative_decrease: f64,"}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"691a504c-edd2-49f5-85e2-00c1d78710a3","tool_use_result":{"filePath":"/Users/admin/MechInt/src/verify.rs","oldString":"#[derive(Debug, Clone)]\npub struct RestrictedLossReport {\n    pub restricted_drop_step: usize,\n    pub full_drop_step: usize,\n}","newString":"#[derive(Debug, Clone)]\n#[allow(dead_code)]\npub struct RestrictedLossReport {\n    pub restricted_drop_step: usize,\n    pub full_drop_step: usize,\n}","originalFile":"use burn::{\n    data::dataset::Dataset,\n    tensor::{backend::Backend, Int, Tensor},\n};\nuse rustfft::{num_complex::Complex, FftPlanner};\n\nuse crate::analysis::{AccuracyHistory, LossHistory};\nuse crate::data::ModularAdditionDataset;\nuse crate::model::Transformer;\n\n#[derive(Debug, Clone)]\npub struct GrokkingVerificationConfig {\n    pub early_train_acc_threshold: f32,\n    pub early_step_max: usize,\n    pub plateau_min_step: usize,\n    pub generalization_window: usize,\n    pub chance_accuracy: f32,\n    pub chance_tolerance: f32,\n    pub target_val_acc_threshold: f32,\n    pub accuracy_window: usize,\n    pub loss_window: usize,\n    pub loss_drop_fraction: f64,\n}\n\nimpl GrokkingVerificationConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        Self {\n            early_train_acc_threshold: 0.99,\n            early_step_max: 1000,\n            plateau_min_step: 1000,\n            generalization_window: 10_000,\n            chance_accuracy: 1.0 / modulus as f32,\n            chance_tolerance: 0.02,\n            target_val_acc_threshold: 0.99,\n            accuracy_window: 5,\n            loss_window: 5,\n            loss_drop_fraction: 0.5,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\n#[allow(dead_code)]\npub struct GrokkingPhaseReport {\n    pub early_train_step: usize,\n    pub transition_step: usize,\n    pub plateau_max_val_acc: f32,\n    pub loss_drop_ratio: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossVerificationConfig {\n    pub plateau_min_step: usize,\n    pub drop_window: usize,\n    pub drop_fraction: f64,\n    pub min_step_lead: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossReport {\n    pub restricted_drop_step: usize,\n    pub full_drop_step: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossDropConfig {\n    pub min_relative_decrease: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossDropReport {\n    pub drop_step: usize,\n    pub drop_loss: f64,\n    pub baseline_loss: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct ExcludedLossSpikeConfig {\n    pub min_relative_increase: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct ExcludedLossSpikeReport {\n    pub spike_step: usize,\n    pub spike_loss: f64,\n    pub baseline_loss: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct MlpWaveVerificationConfig {\n    pub max_negative_fraction: f64,\n    pub negative_tolerance: f64,\n    pub min_dominant_ratio: f64,\n    pub min_correlation: f64,\n    pub phase_steps: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct MlpWaveNeuronReport {\n    pub neuron_index: usize,\n    pub dominant_frequency: usize,\n    pub dominant_ratio: f64,\n    pub negative_fraction: f64,\n    pub best_correlation: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct MlpWaveReport {\n    pub neuron_reports: Vec<MlpWaveNeuronReport>,\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldConfig {\n    pub fixed_y: usize,\n    pub neuron_indices: Vec<usize>,\n    pub min_post_score: f64,\n    pub max_pre_score: f64,\n    pub max_axis_ratio: f64,\n    pub min_pairs_passing: usize,\n}\n\nimpl PairwiseManifoldConfig {\n    pub fn default_for_modulus(_modulus: usize) -> Self {\n        Self {\n            fixed_y: 0,\n            neuron_indices: (0..7).collect(),\n            min_post_score: 0.75,\n            max_pre_score: 0.55,\n            max_axis_ratio: 8.0,\n            min_pairs_passing: 10,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldPairReport {\n    pub neuron_a: usize,\n    pub neuron_b: usize,\n    pub circularity_score: f64,\n    pub radius_cv: f64,\n    pub axis_ratio: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldReport {\n    pub pair_reports: Vec<PairwiseManifoldPairReport>,\n    pub average_score: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldTransitionReport {\n    pub pre_average_score: f64,\n    pub post_average_score: f64,\n    pub passing_pairs: usize,\n    pub total_pairs: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct DiagonalRidgeConfig {\n    pub neuron_index: usize,\n    pub max_lag: usize,\n    pub min_diagonal_ratio: f64,\n    pub min_diagonal_score: f64,\n}\n\nimpl DiagonalRidgeConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        let max_lag = (modulus / 10).max(4).min(20);\n        Self {\n            neuron_index: 10,\n            max_lag,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct DiagonalRidgeReport {\n    pub neuron_index: usize,\n    pub diagonal_score: f64,\n    pub axis_score: f64,\n    pub ratio: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct SnakeCurveConfig {\n    pub early_step_max: usize,\n    pub train_convergence_threshold: f32,\n    pub val_plateau_min_step: usize,\n    pub val_plateau_max_acc: f32,\n    pub val_final_threshold: f32,\n}\n\nimpl SnakeCurveConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        Self {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 1000,\n            val_plateau_max_acc: 1.0 / modulus as f32 + 0.05,\n            val_final_threshold: 0.90,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct WeightNormDecayConfig {\n    pub min_relative_decrease: f64,\n    pub plateau_min_step: usize,\n    pub grok_step_window: usize,\n}\n\nimpl WeightNormDecayConfig {\n    pub fn default_for_grokking() -> Self {\n        Self {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct WeightNormDecayReport {\n    pub initial_norm: f64,\n    pub final_norm: f64,\n    pub relative_decrease: f64,\n    pub grok_step_norm: Option<f64>,\n}\n\n#[derive(Debug, Clone)]\npub struct SnakeCurveReport {\n    pub train_converged_by_step: usize,\n    pub val_plateau_max_acc: f32,\n    pub val_grok_step: usize,\n    pub curve_shape_valid: bool,\n}\n\npub fn verify_grokking_phase_transition(\n    loss_history: &LossHistory,\n    accuracy_history: &AccuracyHistory,\n    config: &GrokkingVerificationConfig,\n) -> Result<GrokkingPhaseReport, String> {\n    let (early_train_step, _) = accuracy_history\n        .train_snapshots\n        .iter()\n        .find(|(_, acc)| *acc >= config.early_train_acc_threshold)\n        .ok_or_else(|| \"train accuracy never reaches threshold\".to_string())?;\n\n    if *early_train_step > config.early_step_max {\n        return Err(format!(\n            \"train accuracy reaches {:.2} after step {}, expected before {}\",\n            config.early_train_acc_threshold, early_train_step, config.early_step_max\n        ));\n    }\n\n    let plateau_max_val_acc = accuracy_history\n        .val_snapshots\n        .iter()\n        .filter(|(step, _)| *step <= config.plateau_min_step)\n        .map(|(_, acc)| *acc)\n        .fold(None::<f32>, |accum, value| {\n            Some(accum.map_or(value, |max_val| max_val.max(value)))\n        })\n        .ok_or_else(|| \"no validation accuracy samples before plateau_min_step\".to_string())?;\n\n    if plateau_max_val_acc > config.chance_accuracy + config.chance_tolerance {\n        return Err(format!(\n            \"validation accuracy before step {} exceeds chance (max {:.4})\",\n            config.plateau_min_step, plateau_max_val_acc\n        ));\n    }\n\n    let (transition_step, _) = accuracy_history\n        .val_snapshots\n        .iter()\n        .find(|(step, acc)| {\n            *step >= config.plateau_min_step && *acc >= config.target_val_acc_threshold\n        })\n        .ok_or_else(|| \"validation accuracy never reaches target threshold\".to_string())?;\n\n    let latest_allowed = config.plateau_min_step + config.generalization_window;\n    if *transition_step > latest_allowed {\n        return Err(format!(\n            \"validation accuracy reaches {:.2} at step {}, expected before {}\",\n            config.target_val_acc_threshold, transition_step, latest_allowed\n        ));\n    }\n\n    verify_accuracy_jump(\n        &accuracy_history.val_snapshots,\n        *transition_step,\n        config.accuracy_window,\n        config.chance_accuracy,\n        config.chance_tolerance,\n        config.target_val_acc_threshold,\n    )?;\n\n    let loss_drop_ratio = verify_loss_drop(\n        &loss_history.val_snapshots,\n        *transition_step,\n        config.loss_window,\n        config.loss_drop_fraction,\n    )?;\n\n    Ok(GrokkingPhaseReport {\n        early_train_step: *early_train_step,\n        transition_step: *transition_step,\n        plateau_max_val_acc,\n        loss_drop_ratio,\n    })\n}\n\nfn verify_accuracy_jump(\n    val_accuracy: &[(usize, f32)],\n    transition_step: usize,\n    window: usize,\n    chance_accuracy: f32,\n    chance_tolerance: f32,\n    target_val_acc_threshold: f32,\n) -> Result<(), String> {\n    let before_values: Vec<f32> = val_accuracy\n        .iter()\n        .filter(|(step, _)| *step < transition_step)\n        .map(|(_, acc)| *acc)\n        .collect();\n\n    let after_values: Vec<f32> = val_accuracy\n        .iter()\n        .filter(|(step, _)| *step >= transition_step)\n        .map(|(_, acc)| *acc)\n        .collect();\n\n    if before_values.len() < window || after_values.len() < window {\n        return Err(\"insufficient accuracy samples around transition\".to_string());\n    }\n\n    let before_avg: f32 =\n        before_values[before_values.len() - window..].iter().sum::<f32>() / window as f32;\n    let after_avg: f32 = after_values[..window].iter().sum::<f32>() / window as f32;\n\n    if before_avg > chance_accuracy + chance_tolerance {\n        return Err(format!(\n            \"validation accuracy plateau too high before transition (avg {:.4})\",\n            before_avg\n        ));\n    }\n\n    if after_avg < target_val_acc_threshold {\n        return Err(format!(\n            \"validation accuracy after transition too low (avg {:.4})\",\n            after_avg\n        ));\n    }\n\n    Ok(())\n}\n\nfn verify_loss_drop(\n    val_losses: &[(usize, f64)],\n    transition_step: usize,\n    loss_window: usize,\n    loss_drop_fraction: f64,\n) -> Result<f64, String> {\n    let before_losses: Vec<f64> = val_losses\n        .iter()\n        .filter(|(step, _)| *step < transition_step)\n        .map(|(_, loss)| *loss)\n        .collect();\n\n    let after_losses: Vec<f64> = val_losses\n        .iter()\n        .filter(|(step, _)| *step >= transition_step)\n        .map(|(_, loss)| *loss)\n        .collect();\n\n    if before_losses.len() < loss_window || after_losses.len() < loss_window {\n        return Err(\"insufficient loss samples around transition\".to_string());\n    }\n\n    let before_avg: f64 =\n        before_losses[before_losses.len() - loss_window..].iter().sum::<f64>()\n            / loss_window as f64;\n    let after_avg: f64 = after_losses[..loss_window].iter().sum::<f64>() / loss_window as f64;\n\n    if after_avg > before_avg * (1.0 - loss_drop_fraction) {\n        return Err(format!(\n            \"validation loss drop too small (before {:.4}, after {:.4})\",\n            before_avg, after_avg\n        ));\n    }\n\n    Ok(after_avg / before_avg)\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_restricted_loss_early_drop(\n    full_losses: &[(usize, f64)],\n    restricted_losses: &[(usize, f64)],\n    config: &RestrictedLossVerificationConfig,\n) -> Result<RestrictedLossReport, String> {\n    let restricted_baseline =\n        baseline_loss(restricted_losses, config.plateau_min_step, config.drop_window)?;\n    let full_baseline = baseline_loss(full_losses, config.plateau_min_step, config.drop_window)?;\n\n    let restricted_drop_step = find_drop_step(\n        restricted_losses,\n        config.plateau_min_step,\n        restricted_baseline,\n        config.drop_fraction,\n    )?;\n    let full_drop_step = find_drop_step(\n        full_losses,\n        config.plateau_min_step,\n        full_baseline,\n        config.drop_fraction,\n    )?;\n\n    if restricted_drop_step + config.min_step_lead > full_drop_step {\n        return Err(format!(\n            \"restricted loss drops at step {}, expected at least {} steps before full loss (step {})\",\n            restricted_drop_step, config.min_step_lead, full_drop_step\n        ));\n    }\n\n    Ok(RestrictedLossReport {\n        restricted_drop_step,\n        full_drop_step,\n    })\n}\n\npub fn verify_restricted_loss_drop(\n    restricted_losses: &[(usize, f64)],\n    config: &RestrictedLossDropConfig,\n) -> Result<RestrictedLossDropReport, String> {\n    if restricted_losses.len() < 3 {\n        return Err(\"restricted loss history requires at least 3 points\".to_string());\n    }\n\n    let first = restricted_losses.first().ok_or_else(|| {\n        \"restricted loss history is empty\".to_string()\n    })?;\n    let baseline_loss = first.1;\n\n    let (min_idx, (min_step, min_loss)) = restricted_losses\n        .iter()\n        .enumerate()\n        .min_by(|(_, a), (_, b)| {\n            a.1.partial_cmp(&b.1).unwrap_or(std::cmp::Ordering::Equal)\n        })\n        .ok_or_else(|| \"restricted loss history is empty\".to_string())?;\n\n    if min_idx == 0 {\n        return Err(\"restricted loss minimum occurs at start (no drop detected)\".to_string());\n    }\n\n    let relative_decrease = (baseline_loss - min_loss) / baseline_loss;\n    if relative_decrease < config.min_relative_decrease {\n        return Err(format!(\n            \"restricted loss decrease too small (baseline {:.4}, min {:.4}, decrease {:.1}%)\",\n            baseline_loss, min_loss, relative_decrease * 100.0\n        ));\n    }\n\n    Ok(RestrictedLossDropReport {\n        drop_step: *min_step,\n        drop_loss: *min_loss,\n        baseline_loss,\n    })\n}\n\npub fn verify_excluded_loss_spike(\n    excluded_losses: &[(usize, f64)],\n    config: &ExcludedLossSpikeConfig,\n) -> Result<ExcludedLossSpikeReport, String> {\n    if excluded_losses.len() < 3 {\n        return Err(\"excluded loss history requires at least 3 points\".to_string());\n    }\n\n    let first = excluded_losses.first().ok_or_else(|| {\n        \"excluded loss history is empty\".to_string()\n    })?;\n    let last = excluded_losses.last().ok_or_else(|| {\n        \"excluded loss history is empty\".to_string()\n    })?;\n    let baseline_loss = (first.1 + last.1) / 2.0;\n\n    let (peak_idx, (peak_step, peak_loss)) = excluded_losses\n        .iter()\n        .enumerate()\n        .max_by(|(_, a), (_, b)| {\n            a.1.partial_cmp(&b.1).unwrap_or(std::cmp::Ordering::Equal)\n        })\n        .ok_or_else(|| \"excluded loss history is empty\".to_string())?;\n\n    if peak_idx == 0 || peak_idx + 1 == excluded_losses.len() {\n        return Err(\"excluded loss spike occurs at boundary\".to_string());\n    }\n\n    if peak_loss < &(baseline_loss * (1.0 + config.min_relative_increase)) {\n        return Err(format!(\n            \"excluded loss spike too small (baseline {:.4}, peak {:.4})\",\n            baseline_loss, peak_loss\n        ));\n    }\n\n    Ok(ExcludedLossSpikeReport {\n        spike_step: *peak_step,\n        spike_loss: *peak_loss,\n        baseline_loss,\n    })\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_mlp_activation_waves<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n    fixed_y: usize,\n    neuron_indices: &[usize],\n    config: &MlpWaveVerificationConfig,\n) -> Result<MlpWaveReport, String> {\n    let activations = collect_mlp_post_relu_activations(model, device, fixed_y)?;\n    if neuron_indices.is_empty() {\n        return Err(\"neuron indices list is empty\".to_string());\n    }\n\n    let d_ff = activations.len();\n    for &idx in neuron_indices {\n        if idx >= d_ff {\n            return Err(format!(\n                \"neuron index {} out of range (d_ff={})\",\n                idx, d_ff\n            ));\n        }\n    }\n\n    let signals: Vec<(usize, Vec<f64>)> = neuron_indices\n        .iter()\n        .map(|&idx| (idx, activations[idx].clone()))\n        .collect();\n    verify_mlp_activation_waves_from_signals(&signals, config)\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_mlp_activation_waves_from_signals(\n    signals: &[(usize, Vec<f64>)],\n    config: &MlpWaveVerificationConfig,\n) -> Result<MlpWaveReport, String> {\n    if signals.is_empty() {\n        return Err(\"no activation signals provided\".to_string());\n    }\n\n    let mut reports = Vec::with_capacity(signals.len());\n    for (neuron_index, signal) in signals {\n        let report = analyze_mlp_activation_wave(*neuron_index, signal, config)?;\n        reports.push(report);\n    }\n\n    Ok(MlpWaveReport {\n        neuron_reports: reports,\n    })\n}\n\npub fn verify_mlp_pairwise_manifold_transition<B: Backend>(\n    pre_model: &Transformer<B>,\n    post_model: &Transformer<B>,\n    device: &B::Device,\n    config: &PairwiseManifoldConfig,\n) -> Result<PairwiseManifoldTransitionReport, String> {\n    let modulus = ModularAdditionDataset::modulus();\n    if config.fixed_y >= modulus {\n        return Err(format!(\n            \"fixed_y {} out of range (modulus {})\",\n            config.fixed_y, modulus\n        ));\n    }\n\n    let pre_activations = collect_mlp_post_relu_activations(pre_model, device, config.fixed_y)?;\n    let post_activations = collect_mlp_post_relu_activations(post_model, device, config.fixed_y)?;\n\n    let d_ff = pre_activations.len();\n    if post_activations.len() != d_ff {\n        return Err(\"post activations shape mismatch\".to_string());\n    }\n\n    if config.neuron_indices.len() < 2 {\n        return Err(\"need at least two neuron indices\".to_string());\n    }\n\n    for &idx in &config.neuron_indices {\n        if idx >= d_ff {\n            return Err(format!(\n                \"neuron index {} out of range (d_ff={})\",\n                idx, d_ff\n            ));\n        }\n    }\n\n    let pre_signals: Vec<(usize, Vec<f64>)> = config\n        .neuron_indices\n        .iter()\n        .map(|&idx| (idx, pre_activations[idx].clone()))\n        .collect();\n    let post_signals: Vec<(usize, Vec<f64>)> = config\n        .neuron_indices\n        .iter()\n        .map(|&idx| (idx, post_activations[idx].clone()))\n        .collect();\n\n    verify_pairwise_manifold_transition_from_signals(&pre_signals, &post_signals, config)\n}\n\npub fn collect_mlp_activation_surface<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n    neuron_index: usize,\n) -> Result<Vec<Vec<f64>>, String> {\n    let modulus = ModularAdditionDataset::modulus();\n    let equals_token = ModularAdditionDataset::equals_token();\n\n    let mut inputs_vec = Vec::with_capacity(modulus * modulus * 3);\n    for x in 0..modulus {\n        for y in 0..modulus {\n            inputs_vec.push(x as i32);\n            inputs_vec.push(y as i32);\n            inputs_vec.push(equals_token as i32);\n        }\n    }\n\n    let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n        .reshape([modulus * modulus, 3]);\n    let (_, mlp_acts) = model.forward_with_mlp_activations(inputs);\n    let [batch_size, d_ff] = mlp_acts.dims();\n    if batch_size != modulus * modulus {\n        return Err(format!(\n            \"unexpected activation batch size {} (expected {})\",\n            batch_size,\n            modulus * modulus\n        ));\n    }\n    if neuron_index >= d_ff {\n        return Err(format!(\n            \"neuron index {} out of range (d_ff={})\",\n            neuron_index, d_ff\n        ));\n    }\n\n    let data: Vec<f32> = mlp_acts.into_data().to_vec().unwrap();\n    let mut surface = vec![vec![0.0f64; modulus]; modulus];\n    for x in 0..modulus {\n        for y in 0..modulus {\n            let batch = x * modulus + y;\n            let idx = batch * d_ff + neuron_index;\n            surface[x][y] = data[idx] as f64;\n        }\n    }\n\n    Ok(surface)\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_constructive_interference_surface_from_grid(\n    surface: &[Vec<f64>],\n    config: &DiagonalRidgeConfig,\n) -> Result<DiagonalRidgeReport, String> {\n    let (diag_main, diag_anti, axis_x, axis_y) =\n        surface_autocorrelation_scores(surface, config.max_lag)?;\n\n    let diagonal_score = diag_main.max(diag_anti);\n    let axis_score = axis_x.max(axis_y);\n    let ratio = if axis_score > 0.0 {\n        diagonal_score / axis_score\n    } else {\n        f64::INFINITY\n    };\n\n    if diagonal_score < config.min_diagonal_score {\n        return Err(format!(\n            \"diagonal autocorrelation {:.3} below threshold {:.3}\",\n            diagonal_score, config.min_diagonal_score\n        ));\n    }\n\n    if ratio < config.min_diagonal_ratio {\n        return Err(format!(\n            \"diagonal/axis ratio {:.3} below threshold {:.3}\",\n            ratio, config.min_diagonal_ratio\n        ));\n    }\n\n    Ok(DiagonalRidgeReport {\n        neuron_index: config.neuron_index,\n        diagonal_score,\n        axis_score,\n        ratio,\n    })\n}\n\npub fn verify_pairwise_manifold_transition_from_signals(\n    pre_signals: &[(usize, Vec<f64>)],\n    post_signals: &[(usize, Vec<f64>)],\n    config: &PairwiseManifoldConfig,\n) -> Result<PairwiseManifoldTransitionReport, String> {\n    let pre_report = evaluate_pairwise_manifolds_from_signals(pre_signals)?;\n    let post_report = evaluate_pairwise_manifolds_from_signals(post_signals)?;\n\n    if pre_report.average_score > config.max_pre_score {\n        return Err(format!(\n            \"pre-grok average circularity {:.3} exceeds threshold {:.3}\",\n            pre_report.average_score, config.max_pre_score\n        ));\n    }\n\n    let passing_pairs = post_report\n        .pair_reports\n        .iter()\n        .filter(|pair| {\n            pair.circularity_score >= config.min_post_score\n                && pair.axis_ratio <= config.max_axis_ratio\n        })\n        .count();\n    if passing_pairs < config.min_pairs_passing {\n        return Err(format!(\n            \"post-grok only {} pairs meet circularity threshold (need {})\",\n            passing_pairs, config.min_pairs_passing\n        ));\n    }\n\n    Ok(PairwiseManifoldTransitionReport {\n        pre_average_score: pre_report.average_score,\n        post_average_score: post_report.average_score,\n        passing_pairs,\n        total_pairs: post_report.pair_reports.len(),\n    })\n}\n\nfn evaluate_pairwise_manifolds_from_signals(\n    signals: &[(usize, Vec<f64>)],\n) -> Result<PairwiseManifoldReport, String> {\n    if signals.len() < 2 {\n        return Err(\"need at least two activation signals\".to_string());\n    }\n\n    let signal_len = signals[0].1.len();\n    if signal_len < 3 {\n        return Err(\"activation signal too short\".to_string());\n    }\n\n    for (idx, signal) in signals {\n        if signal.len() != signal_len {\n            return Err(format!(\n                \"signal length mismatch for neuron {}\",\n                idx\n            ));\n        }\n    }\n\n    let mut reports = Vec::new();\n    for i in 0..signals.len() {\n        for j in (i + 1)..signals.len() {\n            let (neuron_a, signal_a) = &signals[i];\n            let (neuron_b, signal_b) = &signals[j];\n            let points: Vec<(f64, f64)> = signal_a\n                .iter()\n                .zip(signal_b.iter())\n                .map(|(x, y)| (*x, *y))\n                .collect();\n            let (score, radius_cv, axis_ratio) =\n                match pairwise_circularity_metrics(&points) {\n                    Ok(metrics) => metrics,\n                    Err(_) => (0.0, 1.0, f64::INFINITY),\n                };\n            reports.push(PairwiseManifoldPairReport {\n                neuron_a: *neuron_a,\n                neuron_b: *neuron_b,\n                circularity_score: score,\n                radius_cv,\n                axis_ratio,\n            });\n        }\n    }\n\n    let average_score = reports\n        .iter()\n        .map(|pair| pair.circularity_score)\n        .sum::<f64>()\n        / reports.len() as f64;\n\n    Ok(PairwiseManifoldReport {\n        pair_reports: reports,\n        average_score,\n    })\n}\n\nfn pairwise_circularity_metrics(points: &[(f64, f64)]) -> Result<(f64, f64, f64), String> {\n    if points.len() < 3 {\n        return Err(\"not enough points for circularity\".to_string());\n    }\n\n    let n = points.len() as f64;\n    let mean_x = points.iter().map(|(x, _)| *x).sum::<f64>() / n;\n    let mean_y = points.iter().map(|(_, y)| *y).sum::<f64>() / n;\n\n    let mut var_x = 0.0;\n    let mut var_y = 0.0;\n    let mut cov_xy = 0.0;\n    for (x, y) in points {\n        let dx = x - mean_x;\n        let dy = y - mean_y;\n        var_x += dx * dx;\n        var_y += dy * dy;\n        cov_xy += dx * dy;\n    }\n    var_x /= n;\n    var_y /= n;\n    cov_xy /= n;\n\n    let trace = var_x + var_y;\n    let det = var_x * var_y - cov_xy * cov_xy;\n    if det <= 1.0e-12 || trace <= 0.0 {\n        return Err(\"degenerate covariance\".to_string());\n    }\n\n    let half_trace = trace * 0.5;\n    let disc = (half_trace * half_trace - det).max(0.0).sqrt();\n    let lambda1 = half_trace + disc;\n    let lambda2 = half_trace - disc;\n    if lambda2 <= 0.0 {\n        return Err(\"degenerate eigenvalues\".to_string());\n    }\n\n    let axis_ratio = (lambda1 / lambda2).sqrt();\n\n    let mut v1_x = cov_xy;\n    let mut v1_y = lambda1 - var_x;\n    if v1_x.abs() < 1.0e-12 && v1_y.abs() < 1.0e-12 {\n        v1_x = 1.0;\n        v1_y = 0.0;\n    }\n    let norm = (v1_x * v1_x + v1_y * v1_y).sqrt();\n    v1_x /= norm;\n    v1_y /= norm;\n    let v2_x = -v1_y;\n    let v2_y = v1_x;\n\n    let mut radii = Vec::with_capacity(points.len());\n    for (x, y) in points {\n        let dx = x - mean_x;\n        let dy = y - mean_y;\n        let p1 = dx * v1_x + dy * v1_y;\n        let p2 = dx * v2_x + dy * v2_y;\n        let w1 = p1 / lambda1.sqrt();\n        let w2 = p2 / lambda2.sqrt();\n        radii.push((w1 * w1 + w2 * w2).sqrt());\n    }\n\n    let mean_radius = radii.iter().sum::<f64>() / radii.len() as f64;\n    if mean_radius == 0.0 {\n        return Err(\"zero mean radius\".to_string());\n    }\n\n    let mut variance = 0.0;\n    for r in &radii {\n        let diff = r - mean_radius;\n        variance += diff * diff;\n    }\n    let std = (variance / radii.len() as f64).sqrt();\n    let radius_cv = std / mean_radius;\n    let score = 1.0 / (1.0 + radius_cv);\n\n    Ok((score, radius_cv, axis_ratio))\n}\n\n// Helper functions for verification - reserved for future integration\n#[allow(dead_code)]\nfn baseline_loss(\n    losses: &[(usize, f64)],\n    plateau_min_step: usize,\n    drop_window: usize,\n) -> Result<f64, String> {\n    let before: Vec<f64> = losses\n        .iter()\n        .filter(|(step, _)| *step <= plateau_min_step)\n        .map(|(_, loss)| *loss)\n        .collect();\n\n    if before.len() < drop_window || drop_window == 0 {\n        return Err(\"insufficient loss samples for baseline\".to_string());\n    }\n\n    let window = &before[before.len() - drop_window..];\n    Ok(window.iter().sum::<f64>() / drop_window as f64)\n}\n\n#[allow(dead_code)]\nfn find_drop_step(\n    losses: &[(usize, f64)],\n    plateau_min_step: usize,\n    baseline: f64,\n    drop_fraction: f64,\n) -> Result<usize, String> {\n    losses\n        .iter()\n        .filter(|(step, _)| *step >= plateau_min_step)\n        .find(|(_, loss)| *loss <= baseline * (1.0 - drop_fraction))\n        .map(|(step, _)| *step)\n        .ok_or_else(|| \"loss never drops below threshold\".to_string())\n}\n\nfn collect_mlp_post_relu_activations<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n    fixed_y: usize,\n) -> Result<Vec<Vec<f64>>, String> {\n    let modulus = ModularAdditionDataset::modulus();\n    let equals_token = ModularAdditionDataset::equals_token();\n\n    let mut inputs_vec = Vec::with_capacity(modulus * 3);\n    for x in 0..modulus {\n        inputs_vec.push(x as i32);\n        inputs_vec.push(fixed_y as i32);\n        inputs_vec.push(equals_token as i32);\n    }\n\n    let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n        .reshape([modulus, 3]);\n    let (_, mlp_acts) = model.forward_with_mlp_activations(inputs);\n    let [batch_size, d_ff] = mlp_acts.dims();\n\n    if batch_size != modulus {\n        return Err(format!(\n            \"unexpected activation batch size {} (expected {})\",\n            batch_size, modulus\n        ));\n    }\n\n    let data: Vec<f32> = mlp_acts.into_data().to_vec().unwrap();\n    let mut per_neuron = vec![Vec::with_capacity(batch_size); d_ff];\n    for batch in 0..batch_size {\n        for neuron in 0..d_ff {\n            let idx = batch * d_ff + neuron;\n            per_neuron[neuron].push(data[idx] as f64);\n        }\n    }\n\n    Ok(per_neuron)\n}\n\n#[allow(dead_code)]\nfn surface_autocorrelation_scores(\n    surface: &[Vec<f64>],\n    max_lag: usize,\n) -> Result<(f64, f64, f64, f64), String> {\n    let (height, width) = surface_dimensions(surface)?;\n    if height < 2 || width < 2 {\n        return Err(\"surface grid too small\".to_string());\n    }\n\n    let (mean, var) = surface_mean_variance(surface)?;\n    if var == 0.0 {\n        return Err(\"surface variance is zero\".to_string());\n    }\n\n    let max_lag = max_lag.min(height.saturating_sub(1)).min(width.saturating_sub(1));\n    if max_lag == 0 {\n        return Err(\"max_lag too small for surface\".to_string());\n    }\n\n    let diag_main = average_autocorrelation(surface, mean, var, max_lag, 1, 1)?;\n    let diag_anti = average_autocorrelation(surface, mean, var, max_lag, 1, -1)?;\n    let axis_x = average_autocorrelation(surface, mean, var, max_lag, 1, 0)?;\n    let axis_y = average_autocorrelation(surface, mean, var, max_lag, 0, 1)?;\n\n    Ok((diag_main, diag_anti, axis_x, axis_y))\n}\n\n#[allow(dead_code)]\nfn surface_dimensions(surface: &[Vec<f64>]) -> Result<(usize, usize), String> {\n    let height = surface.len();\n    if height == 0 {\n        return Err(\"surface is empty\".to_string());\n    }\n    let width = surface[0].len();\n    if width == 0 {\n        return Err(\"surface has empty rows\".to_string());\n    }\n    for (idx, row) in surface.iter().enumerate() {\n        if row.len() != width {\n            return Err(format!(\n                \"surface row {} has length {}, expected {}\",\n                idx,\n                row.len(),\n                width\n            ));\n        }\n    }\n    Ok((height, width))\n}\n\n#[allow(dead_code)]\nfn surface_mean_variance(surface: &[Vec<f64>]) -> Result<(f64, f64), String> {\n    let (height, width) = surface_dimensions(surface)?;\n    let mut sum = 0.0;\n    for row in surface {\n        for value in row {\n            sum += *value;\n        }\n    }\n    let count = (height * width) as f64;\n    let mean = sum / count;\n    let mut variance = 0.0;\n    for row in surface {\n        for value in row {\n            let diff = *value - mean;\n            variance += diff * diff;\n        }\n    }\n    variance /= count;\n    Ok((mean, variance))\n}\n\n#[allow(dead_code)]\nfn average_autocorrelation(\n    surface: &[Vec<f64>],\n    mean: f64,\n    variance: f64,\n    max_lag: usize,\n    dx: i32,\n    dy: i32,\n) -> Result<f64, String> {\n    let (height, width) = surface_dimensions(surface)?;\n    let mut total = 0.0;\n    let mut lag_count = 0usize;\n\n    for lag in 1..=max_lag {\n        let mut sum = 0.0;\n        let mut count = 0usize;\n        let lag = lag as i32;\n        for x in 0..height {\n            for y in 0..width {\n                let x2 = x as i32 + dx * lag;\n                let y2 = y as i32 + dy * lag;\n                if x2 < 0 || y2 < 0 {\n                    continue;\n                }\n                let x2 = x2 as usize;\n                let y2 = y2 as usize;\n                if x2 >= height || y2 >= width {\n                    continue;\n                }\n                let a = surface[x][y] - mean;\n                let b = surface[x2][y2] - mean;\n                sum += a * b;\n                count += 1;\n            }\n        }\n        if count == 0 {\n            continue;\n        }\n        let corr = sum / (count as f64 * variance);\n        total += corr.abs();\n        lag_count += 1;\n    }\n\n    if lag_count == 0 {\n        return Err(\"no valid lags for autocorrelation\".to_string());\n    }\n\n    Ok(total / lag_count as f64)\n}\n\n#[allow(dead_code)]\nfn analyze_mlp_activation_wave(\n    neuron_index: usize,\n    signal: &[f64],\n    config: &MlpWaveVerificationConfig,\n) -> Result<MlpWaveNeuronReport, String> {\n    if signal.len() < 3 {\n        return Err(format!(\n            \"activation signal for neuron {} is too short\",\n            neuron_index\n        ));\n    }\n\n    let negative_count = signal\n        .iter()\n        .filter(|value| **value < -config.negative_tolerance)\n        .count();\n    let negative_fraction = negative_count as f64 / signal.len() as f64;\n    if negative_fraction > config.max_negative_fraction {\n        return Err(format!(\n            \"neuron {} has {:.3} negative activations\",\n            neuron_index, negative_fraction\n        ));\n    }\n\n    let (dominant_frequency, dominant_ratio) = dominant_frequency_ratio(signal)?;\n    if dominant_ratio < config.min_dominant_ratio {\n        return Err(format!(\n            \"neuron {} dominant frequency ratio {:.3} below threshold {:.3}\",\n            neuron_index, dominant_ratio, config.min_dominant_ratio\n        ));\n    }\n\n    let best_correlation =\n        best_rectified_sine_correlation(signal, dominant_frequency, config.phase_steps);\n    if best_correlation < config.min_correlation {\n        return Err(format!(\n            \"neuron {} rectified sine correlation {:.3} below threshold {:.3}\",\n            neuron_index, best_correlation, config.min_correlation\n        ));\n    }\n\n    Ok(MlpWaveNeuronReport {\n        neuron_index,\n        dominant_frequency,\n        dominant_ratio,\n        negative_fraction,\n        best_correlation,\n    })\n}\n\n#[allow(dead_code)]\nfn dominant_frequency_ratio(signal: &[f64]) -> Result<(usize, f64), String> {\n    if signal.len() < 2 {\n        return Err(\"signal too short for FFT\".to_string());\n    }\n\n    let mut planner = FftPlanner::new();\n    let fft = planner.plan_fft_forward(signal.len());\n    let mut buffer: Vec<Complex<f64>> = signal\n        .iter()\n        .map(|&x| Complex::new(x, 0.0))\n        .collect();\n    fft.process(&mut buffer);\n    let magnitudes: Vec<f64> = buffer.iter().map(|c| c.norm()).collect();\n\n    if magnitudes.len() < 2 {\n        return Err(\"insufficient FFT bins\".to_string());\n    }\n\n    let mut dominant_idx = 1usize;\n    let mut dominant_mag = magnitudes[1];\n    for (idx, &mag) in magnitudes.iter().enumerate().skip(2) {\n        if mag > dominant_mag {\n            dominant_mag = mag;\n            dominant_idx = idx;\n        }\n    }\n\n    if dominant_idx == 0 {\n        return Err(\"dominant frequency is zero\".to_string());\n    }\n\n    let mut nonzero = magnitudes[1..].to_vec();\n    nonzero.sort_by(|a, b| a.partial_cmp(b).unwrap_or(std::cmp::Ordering::Equal));\n    let median_mag = nonzero[nonzero.len() / 2];\n    let ratio = if median_mag == 0.0 {\n        dominant_mag\n    } else {\n        dominant_mag / median_mag\n    };\n\n    Ok((dominant_idx, ratio))\n}\n\n#[allow(dead_code)]\nfn best_rectified_sine_correlation(signal: &[f64], frequency: usize, phase_steps: usize) -> f64 {\n    if signal.is_empty() || frequency == 0 || phase_steps == 0 {\n        return 0.0;\n    }\n\n    let tau = std::f64::consts::PI * 2.0;\n    let mut best = -1.0;\n    for step in 0..phase_steps {\n        let phase = tau * step as f64 / phase_steps as f64;\n        let wave: Vec<f64> = (0..signal.len())\n            .map(|idx| {\n                let angle = tau * frequency as f64 * idx as f64 / signal.len() as f64 + phase;\n                angle.sin().max(0.0)\n            })\n            .collect();\n        let corr = pearson_correlation(signal, &wave);\n        if corr > best {\n            best = corr;\n        }\n    }\n\n    best\n}\n\n#[allow(dead_code)]\nfn pearson_correlation(a: &[f64], b: &[f64]) -> f64 {\n    if a.len() != b.len() || a.is_empty() {\n        return 0.0;\n    }\n\n    let mean_a = a.iter().sum::<f64>() / a.len() as f64;\n    let mean_b = b.iter().sum::<f64>() / b.len() as f64;\n\n    let mut num = 0.0;\n    let mut denom_a = 0.0;\n    let mut denom_b = 0.0;\n    for (x, y) in a.iter().zip(b.iter()) {\n        let da = x - mean_a;\n        let db = y - mean_b;\n        num += da * db;\n        denom_a += da * da;\n        denom_b += db * db;\n    }\n\n    if denom_a == 0.0 || denom_b == 0.0 {\n        return 0.0;\n    }\n\n    num / (denom_a.sqrt() * denom_b.sqrt())\n}\n\n/// Verify that the model actually learned modular addition\n/// by testing it on ALL possible examples systematically\npub fn verify_full_accuracy<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n) -> (f32, f32) {\n    println!(\"ðŸ” Verifying model on COMPLETE dataset (no sampling)...\");\n\n    let train_dataset = ModularAdditionDataset::new(true, 42);\n    let val_dataset = ModularAdditionDataset::new(false, 42);\n\n    let train_acc = test_all_examples(model, &train_dataset, device);\n    let val_acc = test_all_examples(model, &val_dataset, device);\n\n    println!(\"ðŸ“Š FULL Dataset Results:\");\n    println!(\"   Training: {}/{} = {:.2}%\",\n        (train_acc * train_dataset.len() as f32) as usize,\n        train_dataset.len(),\n        train_acc * 100.0);\n    println!(\"   Validation: {}/{} = {:.2}%\",\n        (val_acc * val_dataset.len() as f32) as usize,\n        val_dataset.len(),\n        val_acc * 100.0);\n\n    (train_acc, val_acc)\n}\n\n/// Test model on ALL examples in dataset (no random sampling)\nfn test_all_examples<B: Backend>(\n    model: &Transformer<B>,\n    dataset: &ModularAdditionDataset,\n    device: &B::Device,\n) -> f32 {\n    let mut total_correct = 0;\n    let total = dataset.len();\n\n    // Test in batches to avoid memory issues\n    let batch_size = 100;\n    for batch_start in (0..total).step_by(batch_size) {\n        let batch_end = (batch_start + batch_size).min(total);\n        let batch_len = batch_end - batch_start;\n\n        let mut inputs_vec = Vec::new();\n        let mut targets_vec = Vec::new();\n\n        for idx in batch_start..batch_end {\n            let (input, target) = dataset.get(idx).unwrap();\n            inputs_vec.extend(input);\n            targets_vec.push(target as i32);\n        }\n\n        let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n            .reshape([batch_len, 3]);\n\n        let logits = model.forward(inputs);\n        let predictions = logits.argmax(1).squeeze::<1>();\n        let predictions_vec: Vec<i32> = predictions.into_data().to_vec().unwrap();\n\n        let correct = predictions_vec\n            .iter()\n            .zip(targets_vec.iter())\n            .filter(|(p, t)| p == t)\n            .count();\n\n        total_correct += correct;\n    }\n\n    total_correct as f32 / total as f32\n}\n\n/// Verify that weight norms decrease over time with weight decay\n/// and show correlation with the grokking transition\npub fn verify_weight_norm_decay(\n    weight_norm_history: &[(usize, f64)],\n    grok_step: Option<usize>,\n    config: &WeightNormDecayConfig,\n) -> Result<WeightNormDecayReport, String> {\n    if weight_norm_history.len() < 2 {\n        return Err(\"weight norm history requires at least 2 snapshots\".to_string());\n    }\n\n    let initial_norm = weight_norm_history\n        .first()\n        .ok_or_else(|| \"weight norm history is empty\".to_string())?\n        .1;\n    let final_norm = weight_norm_history\n        .last()\n        .ok_or_else(|| \"weight norm history is empty\".to_string())?\n        .1;\n\n    if initial_norm <= 0.0 {\n        return Err(\"initial weight norm must be positive\".to_string());\n    }\n\n    // Check that norms are generally decreasing (with weight decay)\n    let relative_decrease = (initial_norm - final_norm) / initial_norm;\n    if relative_decrease < config.min_relative_decrease {\n        return Err(format!(\n            \"weight norm decrease {:.4} below threshold {:.4} (initial {:.4}, final {:.4})\",\n            relative_decrease, config.min_relative_decrease, initial_norm, final_norm\n        ));\n    }\n\n    // Check that norms are not increasing (weight decay should monotonically decrease norms)\n    for window in weight_norm_history.windows(2) {\n        let prev_norm = window[0].1;\n        let next_norm = window[1].1;\n        if next_norm > prev_norm * 1.01 {\n            // Allow 1% tolerance for numerical noise\n            return Err(format!(\n                \"weight norm increased from {:.4} to {:.4} at step {}\",\n                prev_norm, next_norm, window[1].0\n            ));\n        }\n    }\n\n    // If grok step is provided, check that norms show a shift around grokking\n    let grok_step_norm = if let Some(step) = grok_step {\n        weight_norm_history\n            .iter()\n            .filter(|(s, _)| {\n                *s >= step.saturating_sub(config.grok_step_window)\n                    && *s <= step + config.grok_step_window\n            })\n            .map(|(_, norm)| *norm)\n            .next()\n    } else {\n        None\n    };\n\n    Ok(WeightNormDecayReport {\n        initial_norm,\n        final_norm,\n        relative_decrease,\n        grok_step_norm,\n    })\n}\n\n/// Verify the characteristic \"snake curve\" shape of grokking phenomenon\n/// on log-scale plots: train accuracy rises early, validation stays flat then jumps\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_snake_curve_shape(\n    train_acc: &[(usize, f32)],\n    val_acc: &[(usize, f32)],\n    config: &SnakeCurveConfig,\n) -> Result<SnakeCurveReport, String> {\n    if train_acc.is_empty() || val_acc.is_empty() {\n        return Err(\"empty accuracy history\".to_string());\n    }\n\n    // Phase 1: Verify early train convergence (immediate drop on log scale)\n    let train_converged_step = train_acc\n        .iter()\n        .find(|(_, acc)| *acc >= config.train_convergence_threshold)\n        .map(|(step, _)| *step)\n        .ok_or_else(|| {\n            format!(\n                \"train accuracy never reaches {} threshold\",\n                config.train_convergence_threshold\n            )\n        })?;\n\n    if train_converged_step > config.early_step_max {\n        return Err(format!(\n            \"train accuracy converges at step {}, expected before {}\",\n            train_converged_step, config.early_step_max\n        ));\n    }\n\n    // Phase 2: Verify validation plateau (flat on log scale)\n    let plateau_samples: Vec<f32> = val_acc\n        .iter()\n        .filter(|(step, _)| *step >= 1 && *step <= config.val_plateau_min_step)\n        .map(|(_, acc)| *acc)\n        .collect();\n\n    if plateau_samples.is_empty() {\n        return Err(\"no validation samples in plateau region\".to_string());\n    }\n\n    let val_plateau_max = plateau_samples\n        .iter()\n        .fold(0.0f32, |max_val, &acc| max_val.max(acc));\n\n    if val_plateau_max > config.val_plateau_max_acc {\n        return Err(format!(\n            \"validation accuracy in plateau region too high: max {:.4} exceeds {:.4}\",\n            val_plateau_max, config.val_plateau_max_acc\n        ));\n    }\n\n    // Phase 3: Verify delayed validation jump (crash up on log scale)\n    let val_grok_step = val_acc\n        .iter()\n        .filter(|(step, _)| *step > config.val_plateau_min_step)\n        .find(|(_, acc)| *acc >= config.val_final_threshold)\n        .map(|(step, _)| *step)\n        .ok_or_else(|| {\n            format!(\n                \"validation accuracy never reaches {} after plateau\",\n                config.val_final_threshold\n            )\n        })?;\n\n    // Phase 4: Verify phase ordering (snake shape: early train, late val)\n    let curve_shape_valid = train_converged_step < config.val_plateau_min_step\n        && val_grok_step > config.val_plateau_min_step\n        && train_converged_step < val_grok_step;\n\n    if !curve_shape_valid {\n        return Err(format!(\n            \"invalid curve shape: train converged at {}, val plateau until {}, val grokked at {}\",\n            train_converged_step, config.val_plateau_min_step, val_grok_step\n        ));\n    }\n\n    Ok(SnakeCurveReport {\n        train_converged_by_step: train_converged_step,\n        val_plateau_max_acc: val_plateau_max,\n        val_grok_step,\n        curve_shape_valid,\n    })\n}\n\n/// Test specific examples to see if model actually computes mod p\npub fn test_specific_examples<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n) {\n    println!();\n    println!(\"ðŸ§ª Testing specific examples:\");\n\n    let modulus = ModularAdditionDataset::modulus();\n    let equals_token = ModularAdditionDataset::equals_token();\n    let test_cases = vec![\n        (0, 0, 0),\n        (1, 1, 2),\n        (50, 50, (50 + 50) % modulus),\n        (112, 1, (112 + 1) % modulus),\n        (56, 57, (56 + 57) % modulus),\n        (112, 112, (112 + 112) % modulus),\n        (10, 20, 30),\n        (60, 60, (60 + 60) % modulus),\n    ];\n\n    for (a, b, expected) in test_cases {\n        let input_vec = vec![a as i32, b as i32, equals_token as i32];\n        let input = Tensor::<B, 1, Int>::from_ints(input_vec.as_slice(), device)\n            .reshape([1, 3]);\n\n        let logits = model.forward(input);\n        let prediction = logits.argmax(1).squeeze::<1>();\n        let pred_value: i32 = prediction.into_data().to_vec().unwrap()[0];\n\n        let correct = if pred_value == expected as i32 { \"âœ“\" } else { \"âœ—\" };\n        println!(\n            \"   {} + {} mod {} = {} | Model: {} {}\",\n            a, b, modulus, expected, pred_value, correct\n        );\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    fn make_histories(\n        train_acc: &[(usize, f32)],\n        val_acc: &[(usize, f32)],\n        val_loss: &[(usize, f64)],\n    ) -> (LossHistory, AccuracyHistory) {\n        let mut loss_history = LossHistory::new();\n        loss_history.val_snapshots = val_loss.to_vec();\n        let mut accuracy_history = AccuracyHistory::new();\n        accuracy_history.train_snapshots = train_acc.to_vec();\n        accuracy_history.val_snapshots = val_acc.to_vec();\n        (loss_history, accuracy_history)\n    }\n\n    #[test]\n    fn grokking_phase_verification_passes_on_synthetic_signal() {\n        let train_acc = vec![(0, 0.2), (500, 0.995), (1000, 0.999)];\n        let val_acc = vec![\n            (0, 0.01),\n            (500, 0.01),\n            (1500, 0.01),\n            (2200, 0.995),\n            (2500, 0.999),\n        ];\n        let val_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 2.0),\n            (2200, 0.2),\n            (2500, 0.15),\n        ];\n\n        let (loss_history, accuracy_history) = make_histories(&train_acc, &val_acc, &val_loss);\n        let config = GrokkingVerificationConfig {\n            early_step_max: 1000,\n            plateau_min_step: 1500,\n            generalization_window: 1500,\n            accuracy_window: 2,\n            loss_window: 2,\n            ..GrokkingVerificationConfig::default_for_modulus(ModularAdditionDataset::modulus())\n        };\n\n        let report = verify_grokking_phase_transition(&loss_history, &accuracy_history, &config)\n            .expect(\"expected grokking verification to pass\");\n        assert!(report.transition_step >= config.plateau_min_step);\n        assert!(report.early_train_step <= config.early_step_max);\n    }\n\n    #[test]\n    fn grokking_phase_verification_fails_without_transition() {\n        let train_acc = vec![(0, 0.2), (500, 0.995)];\n        let val_acc = vec![(0, 0.01), (500, 0.01), (1500, 0.02)];\n        let val_loss = vec![(0, 2.0), (500, 2.0), (1500, 1.9)];\n\n        let (loss_history, accuracy_history) = make_histories(&train_acc, &val_acc, &val_loss);\n        let config = GrokkingVerificationConfig {\n            early_step_max: 1000,\n            plateau_min_step: 1000,\n            generalization_window: 1000,\n            accuracy_window: 2,\n            loss_window: 2,\n            ..GrokkingVerificationConfig::default_for_modulus(ModularAdditionDataset::modulus())\n        };\n\n        let err = verify_grokking_phase_transition(&loss_history, &accuracy_history, &config)\n            .expect_err(\"expected grokking verification to fail\");\n        assert!(err.contains(\"validation accuracy never reaches target\"));\n    }\n\n    #[test]\n    fn grokking_phase_verification_fails_with_slow_accuracy_rise() {\n        let train_acc = vec![(0, 0.2), (500, 0.995), (1000, 0.999)];\n        let val_acc = vec![\n            (0, 0.01),\n            (500, 0.01),\n            (1500, 0.5),\n            (2000, 0.6),\n            (2200, 0.995),\n            (2500, 0.999),\n        ];\n        let val_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 2.0),\n            (2200, 0.2),\n            (2500, 0.15),\n        ];\n\n        let (loss_history, accuracy_history) = make_histories(&train_acc, &val_acc, &val_loss);\n        let config = GrokkingVerificationConfig {\n            early_step_max: 1000,\n            plateau_min_step: 1000,\n            generalization_window: 2000,\n            accuracy_window: 2,\n            loss_window: 2,\n            ..GrokkingVerificationConfig::default_for_modulus(ModularAdditionDataset::modulus())\n        };\n\n        let err = verify_grokking_phase_transition(&loss_history, &accuracy_history, &config)\n            .expect_err(\"expected grokking verification to fail\");\n        assert!(err.contains(\"plateau too high\"));\n    }\n\n    #[test]\n    fn restricted_loss_detection_passes_with_early_drop() {\n        let full_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 2.0),\n            (2500, 2.0),\n            (3500, 0.3),\n        ];\n        let restricted_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 0.8),\n            (2500, 0.4),\n            (3500, 0.3),\n        ];\n\n        let config = RestrictedLossVerificationConfig {\n            plateau_min_step: 1000,\n            drop_window: 2,\n            drop_fraction: 0.5,\n            min_step_lead: 500,\n        };\n\n        let report = verify_restricted_loss_early_drop(&full_loss, &restricted_loss, &config)\n            .expect(\"expected restricted loss verification to pass\");\n        assert!(report.restricted_drop_step < report.full_drop_step);\n    }\n\n    #[test]\n    fn restricted_loss_detection_fails_when_full_drops_first() {\n        let full_loss = vec![(0, 2.0), (1000, 1.0), (1500, 0.8)];\n        let restricted_loss = vec![(0, 2.0), (1000, 2.0), (1500, 0.8)];\n\n        let config = RestrictedLossVerificationConfig {\n            plateau_min_step: 500,\n            drop_window: 1,\n            drop_fraction: 0.3,\n            min_step_lead: 200,\n        };\n\n        let err = verify_restricted_loss_early_drop(&full_loss, &restricted_loss, &config)\n            .expect_err(\"expected restricted loss verification to fail\");\n        assert!(err.contains(\"restricted loss drops\"));\n    }\n\n    #[test]\n    fn excluded_loss_spike_verification_passes() {\n        let excluded_loss = vec![(0, 1.0), (1500, 2.2), (3000, 1.1)];\n        let config = ExcludedLossSpikeConfig {\n            min_relative_increase: 0.5,\n        };\n\n        let report = verify_excluded_loss_spike(&excluded_loss, &config)\n            .expect(\"expected excluded loss spike verification to pass\");\n        assert_eq!(report.spike_step, 1500);\n    }\n\n    #[test]\n    fn excluded_loss_spike_verification_fails_without_spike() {\n        let excluded_loss = vec![(0, 1.0), (1500, 1.1), (3000, 1.0)];\n        let config = ExcludedLossSpikeConfig {\n            min_relative_increase: 0.5,\n        };\n\n        let err = verify_excluded_loss_spike(&excluded_loss, &config)\n            .expect_err(\"expected excluded loss spike verification to fail\");\n        assert!(err.contains(\"excluded loss spike too small\"));\n    }\n\n    #[test]\n    fn restricted_loss_drop_verification_passes() {\n        let restricted_loss = vec![\n            (0, 2.0),\n            (1000, 1.8),\n            (5000, 1.2),\n            (7000, 0.8),\n            (10000, 0.9),\n        ];\n        let config = RestrictedLossDropConfig {\n            min_relative_decrease: 0.3,\n        };\n\n        let report = verify_restricted_loss_drop(&restricted_loss, &config)\n            .expect(\"expected restricted loss drop verification to pass\");\n        assert_eq!(report.drop_step, 7000);\n        assert!(report.baseline_loss > report.drop_loss);\n        assert!((report.baseline_loss - report.drop_loss) / report.baseline_loss >= 0.3);\n    }\n\n    #[test]\n    fn restricted_loss_drop_verification_fails_without_drop() {\n        let restricted_loss = vec![\n            (0, 2.0),\n            (1000, 2.1),\n            (5000, 1.9),\n            (7000, 2.0),\n            (10000, 2.05),\n        ];\n        let config = RestrictedLossDropConfig {\n            min_relative_decrease: 0.3,\n        };\n\n        let err = verify_restricted_loss_drop(&restricted_loss, &config)\n            .expect_err(\"expected restricted loss drop verification to fail\");\n        assert!(err.contains(\"restricted loss decrease too small\"));\n    }\n\n    #[test]\n    fn mlp_activation_wave_verification_passes_on_rectified_sine() {\n        let n = ModularAdditionDataset::modulus();\n        let freq = 5.0;\n        let mut signal = Vec::with_capacity(n);\n        for idx in 0..n {\n            let angle = 2.0 * std::f64::consts::PI * freq * idx as f64 / n as f64;\n            let rectified = angle.sin().max(0.0);\n            let wobble = (2.0 * std::f64::consts::PI * 2.0 * idx as f64 / n as f64)\n                .sin()\n                .abs()\n                * 0.02;\n            signal.push(rectified + wobble);\n        }\n\n        let config = MlpWaveVerificationConfig {\n            max_negative_fraction: 0.0,\n            negative_tolerance: 1e-6,\n            min_dominant_ratio: 3.0,\n            min_correlation: 0.7,\n            phase_steps: 32,\n        };\n\n        let report =\n            verify_mlp_activation_waves_from_signals(&[(0, signal)], &config)\n                .expect(\"expected rectified sine to pass\");\n        assert_eq!(report.neuron_reports.len(), 1);\n    }\n\n    #[test]\n    fn mlp_activation_wave_verification_fails_on_noise() {\n        let n = ModularAdditionDataset::modulus();\n        let signal: Vec<f64> = (0..n).map(|idx| idx as f64 / n as f64).collect();\n\n        let config = MlpWaveVerificationConfig {\n            max_negative_fraction: 0.0,\n            negative_tolerance: 1e-6,\n            min_dominant_ratio: 4.0,\n            min_correlation: 0.8,\n            phase_steps: 16,\n        };\n\n        let err =\n            verify_mlp_activation_waves_from_signals(&[(0, signal)], &config)\n                .expect_err(\"expected linear signal to fail\");\n        assert!(err.contains(\"dominant frequency ratio\") || err.contains(\"correlation\"));\n    }\n\n    #[test]\n    fn pairwise_manifold_transition_passes_on_elliptic_signals() {\n        let n = ModularAdditionDataset::modulus();\n        let freq = 5.0;\n        let mut post_signals = Vec::new();\n        for neuron_idx in 0..7usize {\n            let phase = 2.0 * std::f64::consts::PI * neuron_idx as f64 / 7.0;\n            let signal: Vec<f64> = (0..n)\n                .map(|idx| {\n                    let angle = 2.0 * std::f64::consts::PI * freq * idx as f64 / n as f64 + phase;\n                    angle.sin()\n                })\n                .collect();\n            post_signals.push((neuron_idx, signal));\n        }\n\n        let mut pre_signals = Vec::new();\n        for neuron_idx in 0..7usize {\n            let signal: Vec<f64> = (0..n)\n                .map(|idx| idx as f64 / n as f64 + neuron_idx as f64 * 0.01)\n                .collect();\n            pre_signals.push((neuron_idx, signal));\n        }\n\n        let config = PairwiseManifoldConfig {\n            fixed_y: 0,\n            neuron_indices: (0..7).collect(),\n            min_post_score: 0.75,\n            max_pre_score: 0.55,\n            max_axis_ratio: 8.0,\n            min_pairs_passing: 8,\n        };\n\n        let report = verify_pairwise_manifold_transition_from_signals(\n            &pre_signals,\n            &post_signals,\n            &config,\n        )\n        .expect(\"expected pairwise manifold transition to pass\");\n        assert!(report.passing_pairs >= config.min_pairs_passing);\n    }\n\n    #[test]\n    fn pairwise_manifold_transition_fails_without_rings() {\n        let n = ModularAdditionDataset::modulus();\n        let mut signals = Vec::new();\n        for neuron_idx in 0..7usize {\n            let signal: Vec<f64> = (0..n)\n                .map(|idx| idx as f64 / n as f64 + neuron_idx as f64 * 0.02)\n                .collect();\n            signals.push((neuron_idx, signal));\n        }\n\n        let config = PairwiseManifoldConfig {\n            fixed_y: 0,\n            neuron_indices: (0..7).collect(),\n            min_post_score: 0.8,\n            max_pre_score: 0.55,\n            max_axis_ratio: 4.0,\n            min_pairs_passing: 5,\n        };\n\n        let err = verify_pairwise_manifold_transition_from_signals(\n            &signals,\n            &signals,\n            &config,\n        )\n        .expect_err(\"expected pairwise manifold transition to fail\");\n        assert!(err.contains(\"post-grok\") || err.contains(\"pre-grok\"));\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_passes_on_diagonal_corrugation() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create diagonal ridges: f(x,y) = sin(2Ï€ * freq * (x+y) / n)\n        let freq = 3.0;\n        for x in 0..n {\n            for y in 0..n {\n                let angle = 2.0 * std::f64::consts::PI * freq * (x + y) as f64 / n as f64;\n                surface[x][y] = angle.sin();\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let report = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect(\"expected diagonal ridge verification to pass\");\n        assert!(report.diagonal_score > report.axis_score);\n        assert!(report.ratio >= config.min_diagonal_ratio);\n        assert!(report.diagonal_score >= config.min_diagonal_score);\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_passes_on_anti_diagonal_pattern() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create anti-diagonal ridges: f(x,y) = sin(2Ï€ * freq * (x-y) / n)\n        let freq = 3.0;\n        for x in 0..n {\n            for y in 0..n {\n                let angle = 2.0 * std::f64::consts::PI * freq * (x as i32 - y as i32) as f64 / n as f64;\n                surface[x][y] = angle.sin();\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let report = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect(\"expected anti-diagonal ridge verification to pass\");\n        assert!(report.diagonal_score > report.axis_score);\n        assert!(report.ratio >= config.min_diagonal_ratio);\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_fails_on_axis_aligned_pattern() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create axis-aligned ridges: f(x,y) = sin(2Ï€ * freq * x / n)\n        let freq = 3.0;\n        for x in 0..n {\n            for y in 0..n {\n                let angle = 2.0 * std::f64::consts::PI * freq * x as f64 / n as f64;\n                surface[x][y] = angle.sin();\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let err = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect_err(\"expected axis-aligned pattern to fail diagonal verification\");\n        assert!(err.contains(\"diagonal/axis ratio\") || err.contains(\"diagonal autocorrelation\"));\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_fails_on_flat_surface() {\n        let n = 50;\n        let surface = vec![vec![1.5; n]; n];\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let err = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect_err(\"expected flat surface to fail\");\n        assert!(err.contains(\"surface variance is zero\"));\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_fails_on_random_noise() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create random noise (low autocorrelation everywhere)\n        for x in 0..n {\n            for y in 0..n {\n                surface[x][y] = ((x * 17 + y * 31) % 100) as f64 / 100.0;\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let err = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect_err(\"expected random noise to fail diagonal verification\");\n        assert!(err.contains(\"diagonal/axis ratio\") || err.contains(\"diagonal autocorrelation\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_passes_on_valid_grokking_curve() {\n        // Simulate typical grokking: train converges early, val plateaus then jumps\n        let train_acc = vec![\n            (1, 0.10),\n            (10, 0.50),\n            (100, 0.80),\n            (500, 0.98),\n            (1000, 0.99),\n            (2000, 0.995),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (10, 0.009),\n            (100, 0.01),\n            (500, 0.011),\n            (1000, 0.009),\n            (1500, 0.01),\n            (2000, 0.92),\n            (3000, 0.98),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 1500,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let report = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect(\"expected snake curve verification to pass\");\n\n        assert_eq!(report.train_converged_by_step, 500);\n        assert!(report.val_plateau_max_acc < config.val_plateau_max_acc);\n        assert_eq!(report.val_grok_step, 2000);\n        assert!(report.curve_shape_valid);\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_on_simultaneous_convergence() {\n        // Both train and val converge at the same time (no grokking)\n        let train_acc = vec![\n            (1, 0.10),\n            (100, 0.98),\n            (500, 0.99),\n        ];\n        let val_acc = vec![\n            (1, 0.10),\n            (100, 0.97),\n            (500, 0.98),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 200,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let err = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect_err(\"expected snake curve verification to fail\");\n        assert!(err.contains(\"plateau region too high\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_on_slow_train_convergence() {\n        // Train takes too long to converge (not the typical snake curve)\n        let train_acc = vec![\n            (1, 0.10),\n            (500, 0.50),\n            (1000, 0.70),\n            (2000, 0.98),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (500, 0.01),\n            (1000, 0.01),\n            (2000, 0.92),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 1500,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let err = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect_err(\"expected snake curve verification to fail\");\n        assert!(err.contains(\"converges at step 2000\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_without_val_jump() {\n        // Validation never jumps up (no generalization)\n        let train_acc = vec![\n            (1, 0.10),\n            (100, 0.98),\n            (500, 0.99),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (100, 0.01),\n            (500, 0.01),\n            (2000, 0.02),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 200,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let err = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect_err(\"expected snake curve verification to fail\");\n        assert!(err.contains(\"validation accuracy never reaches\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_on_wrong_phase_ordering() {\n        // Val jumps before train converges (backwards snake)\n        let train_acc = vec![\n            (1, 0.10),\n            (500, 0.50),\n            (2000, 0.98),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (100, 0.95),\n            (500, 0.98),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 200,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let result = verify_snake_curve_shape(&train_acc, &val_acc, &config);\n        // Should fail because val jumps before train converges (not a proper snake curve)\n        // This can fail for multiple reasons, but should definitely fail\n        assert!(result.is_err(), \"Expected snake curve verification to fail but it passed\");\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_passes_on_decreasing_norms() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 85.0),\n            (1000, 72.0),\n            (1500, 63.0),\n            (2000, 58.0),\n            (2500, 55.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let report = verify_weight_norm_decay(&weight_norms, Some(2000), &config)\n            .expect(\"expected weight norm decay verification to pass\");\n\n        assert_eq!(report.initial_norm, 100.0);\n        assert_eq!(report.final_norm, 55.0);\n        assert!((report.relative_decrease - 0.45).abs() < 0.01);\n        assert!(report.grok_step_norm.is_some());\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_fails_on_flat_norms() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 99.0),\n            (1000, 98.5),\n            (1500, 98.0),\n            (2000, 97.5),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let err = verify_weight_norm_decay(&weight_norms, None, &config)\n            .expect_err(\"expected weight norm decay verification to fail\");\n        assert!(err.contains(\"weight norm decrease\"));\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_fails_on_increasing_norms() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 95.0),\n            (1000, 105.0), // Increase here\n            (1500, 90.0),\n            (2000, 85.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let err = verify_weight_norm_decay(&weight_norms, None, &config)\n            .expect_err(\"expected weight norm decay verification to fail\");\n        assert!(err.contains(\"weight norm increased\"));\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_allows_small_noise() {\n        // Weight decay with small numerical noise (within 1% tolerance)\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 95.0),\n            (1000, 90.5),  // Small increase within tolerance\n            (1500, 90.0),\n            (2000, 85.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let report = verify_weight_norm_decay(&weight_norms, None, &config)\n            .expect(\"expected weight norm decay with small noise to pass\");\n\n        assert_eq!(report.initial_norm, 100.0);\n        assert_eq!(report.final_norm, 85.0);\n        assert!((report.relative_decrease - 0.15).abs() < 0.01);\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_finds_grok_step_norm() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (1000, 80.0),\n            (1800, 68.0),  // Within window of grok step (2200 - 500 = 1700)\n            (2000, 65.0),\n            (2200, 62.0),  // Around grok step\n            (3000, 55.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let report = verify_weight_norm_decay(&weight_norms, Some(2200), &config)\n            .expect(\"expected weight norm decay verification to pass\");\n\n        assert!(report.grok_step_norm.is_some());\n        let grok_norm = report.grok_step_norm.unwrap();\n        // Should find the first norm in the window [1700, 2700], which is 68.0 at step 1800\n        assert!((grok_norm - 68.0).abs() < 0.01);\n    }\n}\n","structuredPatch":[{"oldStart":57,"oldLines":6,"newStart":57,"newLines":7,"lines":[" }"," "," #[derive(Debug, Clone)]","+#[allow(dead_code)]"," pub struct RestrictedLossReport {","     pub restricted_drop_step: usize,","     pub full_drop_step: usize,"]}],"userModified":false,"replaceAll":false}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01WazLYpmxpZyYLha5jZQsz8","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01Wn2VaMDSuqJYPnEjoSWYji","name":"Read","input":{"file_path":"/Users/admin/MechInt/src/verify.rs","offset":95,"limit":60}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":1287,"cache_read_input_tokens":72531,"cache_creation":{"ephemeral_5m_input_tokens":1287,"ephemeral_1h_input_tokens":0},"output_tokens":18,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"7d45daa4-b380-4277-b0b2-eef867b3a1cb"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01Wn2VaMDSuqJYPnEjoSWYji","type":"tool_result","content":"    95â†’    pub min_correlation: f64,\n    96â†’    pub phase_steps: usize,\n    97â†’}\n    98â†’\n    99â†’#[derive(Debug, Clone)]\n   100â†’pub struct MlpWaveNeuronReport {\n   101â†’    pub neuron_index: usize,\n   102â†’    pub dominant_frequency: usize,\n   103â†’    pub dominant_ratio: f64,\n   104â†’    pub negative_fraction: f64,\n   105â†’    pub best_correlation: f64,\n   106â†’}\n   107â†’\n   108â†’#[derive(Debug, Clone)]\n   109â†’pub struct MlpWaveReport {\n   110â†’    pub neuron_reports: Vec<MlpWaveNeuronReport>,\n   111â†’}\n   112â†’\n   113â†’#[derive(Debug, Clone)]\n   114â†’pub struct PairwiseManifoldConfig {\n   115â†’    pub fixed_y: usize,\n   116â†’    pub neuron_indices: Vec<usize>,\n   117â†’    pub min_post_score: f64,\n   118â†’    pub max_pre_score: f64,\n   119â†’    pub max_axis_ratio: f64,\n   120â†’    pub min_pairs_passing: usize,\n   121â†’}\n   122â†’\n   123â†’impl PairwiseManifoldConfig {\n   124â†’    pub fn default_for_modulus(_modulus: usize) -> Self {\n   125â†’        Self {\n   126â†’            fixed_y: 0,\n   127â†’            neuron_indices: (0..7).collect(),\n   128â†’            min_post_score: 0.75,\n   129â†’            max_pre_score: 0.55,\n   130â†’            max_axis_ratio: 8.0,\n   131â†’            min_pairs_passing: 10,\n   132â†’        }\n   133â†’    }\n   134â†’}\n   135â†’\n   136â†’#[derive(Debug, Clone)]\n   137â†’pub struct PairwiseManifoldPairReport {\n   138â†’    pub neuron_a: usize,\n   139â†’    pub neuron_b: usize,\n   140â†’    pub circularity_score: f64,\n   141â†’    pub radius_cv: f64,\n   142â†’    pub axis_ratio: f64,\n   143â†’}\n   144â†’\n   145â†’#[derive(Debug, Clone)]\n   146â†’pub struct PairwiseManifoldReport {\n   147â†’    pub pair_reports: Vec<PairwiseManifoldPairReport>,\n   148â†’    pub average_score: f64,\n   149â†’}\n   150â†’\n   151â†’#[derive(Debug, Clone)]\n   152â†’pub struct PairwiseManifoldTransitionReport {\n   153â†’    pub pre_average_score: f64,\n   154â†’    pub post_average_score: f64,\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"bd1b5d20-f01e-4896-b9c5-ff5a25cb5f35","tool_use_result":{"type":"text","file":{"filePath":"/Users/admin/MechInt/src/verify.rs","content":"    pub min_correlation: f64,\n    pub phase_steps: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct MlpWaveNeuronReport {\n    pub neuron_index: usize,\n    pub dominant_frequency: usize,\n    pub dominant_ratio: f64,\n    pub negative_fraction: f64,\n    pub best_correlation: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct MlpWaveReport {\n    pub neuron_reports: Vec<MlpWaveNeuronReport>,\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldConfig {\n    pub fixed_y: usize,\n    pub neuron_indices: Vec<usize>,\n    pub min_post_score: f64,\n    pub max_pre_score: f64,\n    pub max_axis_ratio: f64,\n    pub min_pairs_passing: usize,\n}\n\nimpl PairwiseManifoldConfig {\n    pub fn default_for_modulus(_modulus: usize) -> Self {\n        Self {\n            fixed_y: 0,\n            neuron_indices: (0..7).collect(),\n            min_post_score: 0.75,\n            max_pre_score: 0.55,\n            max_axis_ratio: 8.0,\n            min_pairs_passing: 10,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldPairReport {\n    pub neuron_a: usize,\n    pub neuron_b: usize,\n    pub circularity_score: f64,\n    pub radius_cv: f64,\n    pub axis_ratio: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldReport {\n    pub pair_reports: Vec<PairwiseManifoldPairReport>,\n    pub average_score: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldTransitionReport {\n    pub pre_average_score: f64,\n    pub post_average_score: f64,","numLines":60,"startLine":95,"totalLines":2213}}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01FHgRiw8BqzbNCsiMKoo2bY","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01TcdcJMJ4iBvazQbX9qD8nA","name":"Edit","input":{"replace_all":false,"file_path":"/Users/admin/MechInt/src/verify.rs","old_string":"#[derive(Debug, Clone)]\npub struct MlpWaveNeuronReport {\n    pub neuron_index: usize,\n    pub dominant_frequency: usize,\n    pub dominant_ratio: f64,\n    pub negative_fraction: f64,\n    pub best_correlation: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct MlpWaveReport {\n    pub neuron_reports: Vec<MlpWaveNeuronReport>,\n}","new_string":"#[derive(Debug, Clone)]\n#[allow(dead_code)]\npub struct MlpWaveNeuronReport {\n    pub neuron_index: usize,\n    pub dominant_frequency: usize,\n    pub dominant_ratio: f64,\n    pub negative_fraction: f64,\n    pub best_correlation: f64,\n}\n\n#[derive(Debug, Clone)]\n#[allow(dead_code)]\npub struct MlpWaveReport {\n    pub neuron_reports: Vec<MlpWaveNeuronReport>,\n}"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":1724,"cache_read_input_tokens":73818,"cache_creation":{"ephemeral_5m_input_tokens":1724,"ephemeral_1h_input_tokens":0},"output_tokens":367,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"6bdc8ab9-2918-4647-b6cf-1661541d9eda"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01TcdcJMJ4iBvazQbX9qD8nA","type":"tool_result","content":"The file /Users/admin/MechInt/src/verify.rs has been updated. Here's the result of running `cat -n` on a snippet of the edited file:\n    95â†’    pub min_correlation: f64,\n    96â†’    pub phase_steps: usize,\n    97â†’}\n    98â†’\n    99â†’#[derive(Debug, Clone)]\n   100â†’#[allow(dead_code)]\n   101â†’pub struct MlpWaveNeuronReport {\n   102â†’    pub neuron_index: usize,\n   103â†’    pub dominant_frequency: usize,\n   104â†’    pub dominant_ratio: f64,\n   105â†’    pub negative_fraction: f64,\n   106â†’    pub best_correlation: f64,\n   107â†’}\n   108â†’\n   109â†’#[derive(Debug, Clone)]\n   110â†’#[allow(dead_code)]\n   111â†’pub struct MlpWaveReport {\n   112â†’    pub neuron_reports: Vec<MlpWaveNeuronReport>,\n   113â†’}\n   114â†’\n   115â†’#[derive(Debug, Clone)]\n   116â†’pub struct PairwiseManifoldConfig {\n   117â†’    pub fixed_y: usize,"}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"f41d98ad-4088-4e7f-9377-53f46cdcebb0","tool_use_result":{"filePath":"/Users/admin/MechInt/src/verify.rs","oldString":"#[derive(Debug, Clone)]\npub struct MlpWaveNeuronReport {\n    pub neuron_index: usize,\n    pub dominant_frequency: usize,\n    pub dominant_ratio: f64,\n    pub negative_fraction: f64,\n    pub best_correlation: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct MlpWaveReport {\n    pub neuron_reports: Vec<MlpWaveNeuronReport>,\n}","newString":"#[derive(Debug, Clone)]\n#[allow(dead_code)]\npub struct MlpWaveNeuronReport {\n    pub neuron_index: usize,\n    pub dominant_frequency: usize,\n    pub dominant_ratio: f64,\n    pub negative_fraction: f64,\n    pub best_correlation: f64,\n}\n\n#[derive(Debug, Clone)]\n#[allow(dead_code)]\npub struct MlpWaveReport {\n    pub neuron_reports: Vec<MlpWaveNeuronReport>,\n}","originalFile":"use burn::{\n    data::dataset::Dataset,\n    tensor::{backend::Backend, Int, Tensor},\n};\nuse rustfft::{num_complex::Complex, FftPlanner};\n\nuse crate::analysis::{AccuracyHistory, LossHistory};\nuse crate::data::ModularAdditionDataset;\nuse crate::model::Transformer;\n\n#[derive(Debug, Clone)]\npub struct GrokkingVerificationConfig {\n    pub early_train_acc_threshold: f32,\n    pub early_step_max: usize,\n    pub plateau_min_step: usize,\n    pub generalization_window: usize,\n    pub chance_accuracy: f32,\n    pub chance_tolerance: f32,\n    pub target_val_acc_threshold: f32,\n    pub accuracy_window: usize,\n    pub loss_window: usize,\n    pub loss_drop_fraction: f64,\n}\n\nimpl GrokkingVerificationConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        Self {\n            early_train_acc_threshold: 0.99,\n            early_step_max: 1000,\n            plateau_min_step: 1000,\n            generalization_window: 10_000,\n            chance_accuracy: 1.0 / modulus as f32,\n            chance_tolerance: 0.02,\n            target_val_acc_threshold: 0.99,\n            accuracy_window: 5,\n            loss_window: 5,\n            loss_drop_fraction: 0.5,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\n#[allow(dead_code)]\npub struct GrokkingPhaseReport {\n    pub early_train_step: usize,\n    pub transition_step: usize,\n    pub plateau_max_val_acc: f32,\n    pub loss_drop_ratio: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossVerificationConfig {\n    pub plateau_min_step: usize,\n    pub drop_window: usize,\n    pub drop_fraction: f64,\n    pub min_step_lead: usize,\n}\n\n#[derive(Debug, Clone)]\n#[allow(dead_code)]\npub struct RestrictedLossReport {\n    pub restricted_drop_step: usize,\n    pub full_drop_step: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossDropConfig {\n    pub min_relative_decrease: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossDropReport {\n    pub drop_step: usize,\n    pub drop_loss: f64,\n    pub baseline_loss: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct ExcludedLossSpikeConfig {\n    pub min_relative_increase: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct ExcludedLossSpikeReport {\n    pub spike_step: usize,\n    pub spike_loss: f64,\n    pub baseline_loss: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct MlpWaveVerificationConfig {\n    pub max_negative_fraction: f64,\n    pub negative_tolerance: f64,\n    pub min_dominant_ratio: f64,\n    pub min_correlation: f64,\n    pub phase_steps: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct MlpWaveNeuronReport {\n    pub neuron_index: usize,\n    pub dominant_frequency: usize,\n    pub dominant_ratio: f64,\n    pub negative_fraction: f64,\n    pub best_correlation: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct MlpWaveReport {\n    pub neuron_reports: Vec<MlpWaveNeuronReport>,\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldConfig {\n    pub fixed_y: usize,\n    pub neuron_indices: Vec<usize>,\n    pub min_post_score: f64,\n    pub max_pre_score: f64,\n    pub max_axis_ratio: f64,\n    pub min_pairs_passing: usize,\n}\n\nimpl PairwiseManifoldConfig {\n    pub fn default_for_modulus(_modulus: usize) -> Self {\n        Self {\n            fixed_y: 0,\n            neuron_indices: (0..7).collect(),\n            min_post_score: 0.75,\n            max_pre_score: 0.55,\n            max_axis_ratio: 8.0,\n            min_pairs_passing: 10,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldPairReport {\n    pub neuron_a: usize,\n    pub neuron_b: usize,\n    pub circularity_score: f64,\n    pub radius_cv: f64,\n    pub axis_ratio: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldReport {\n    pub pair_reports: Vec<PairwiseManifoldPairReport>,\n    pub average_score: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldTransitionReport {\n    pub pre_average_score: f64,\n    pub post_average_score: f64,\n    pub passing_pairs: usize,\n    pub total_pairs: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct DiagonalRidgeConfig {\n    pub neuron_index: usize,\n    pub max_lag: usize,\n    pub min_diagonal_ratio: f64,\n    pub min_diagonal_score: f64,\n}\n\nimpl DiagonalRidgeConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        let max_lag = (modulus / 10).max(4).min(20);\n        Self {\n            neuron_index: 10,\n            max_lag,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct DiagonalRidgeReport {\n    pub neuron_index: usize,\n    pub diagonal_score: f64,\n    pub axis_score: f64,\n    pub ratio: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct SnakeCurveConfig {\n    pub early_step_max: usize,\n    pub train_convergence_threshold: f32,\n    pub val_plateau_min_step: usize,\n    pub val_plateau_max_acc: f32,\n    pub val_final_threshold: f32,\n}\n\nimpl SnakeCurveConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        Self {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 1000,\n            val_plateau_max_acc: 1.0 / modulus as f32 + 0.05,\n            val_final_threshold: 0.90,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct WeightNormDecayConfig {\n    pub min_relative_decrease: f64,\n    pub plateau_min_step: usize,\n    pub grok_step_window: usize,\n}\n\nimpl WeightNormDecayConfig {\n    pub fn default_for_grokking() -> Self {\n        Self {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct WeightNormDecayReport {\n    pub initial_norm: f64,\n    pub final_norm: f64,\n    pub relative_decrease: f64,\n    pub grok_step_norm: Option<f64>,\n}\n\n#[derive(Debug, Clone)]\npub struct SnakeCurveReport {\n    pub train_converged_by_step: usize,\n    pub val_plateau_max_acc: f32,\n    pub val_grok_step: usize,\n    pub curve_shape_valid: bool,\n}\n\npub fn verify_grokking_phase_transition(\n    loss_history: &LossHistory,\n    accuracy_history: &AccuracyHistory,\n    config: &GrokkingVerificationConfig,\n) -> Result<GrokkingPhaseReport, String> {\n    let (early_train_step, _) = accuracy_history\n        .train_snapshots\n        .iter()\n        .find(|(_, acc)| *acc >= config.early_train_acc_threshold)\n        .ok_or_else(|| \"train accuracy never reaches threshold\".to_string())?;\n\n    if *early_train_step > config.early_step_max {\n        return Err(format!(\n            \"train accuracy reaches {:.2} after step {}, expected before {}\",\n            config.early_train_acc_threshold, early_train_step, config.early_step_max\n        ));\n    }\n\n    let plateau_max_val_acc = accuracy_history\n        .val_snapshots\n        .iter()\n        .filter(|(step, _)| *step <= config.plateau_min_step)\n        .map(|(_, acc)| *acc)\n        .fold(None::<f32>, |accum, value| {\n            Some(accum.map_or(value, |max_val| max_val.max(value)))\n        })\n        .ok_or_else(|| \"no validation accuracy samples before plateau_min_step\".to_string())?;\n\n    if plateau_max_val_acc > config.chance_accuracy + config.chance_tolerance {\n        return Err(format!(\n            \"validation accuracy before step {} exceeds chance (max {:.4})\",\n            config.plateau_min_step, plateau_max_val_acc\n        ));\n    }\n\n    let (transition_step, _) = accuracy_history\n        .val_snapshots\n        .iter()\n        .find(|(step, acc)| {\n            *step >= config.plateau_min_step && *acc >= config.target_val_acc_threshold\n        })\n        .ok_or_else(|| \"validation accuracy never reaches target threshold\".to_string())?;\n\n    let latest_allowed = config.plateau_min_step + config.generalization_window;\n    if *transition_step > latest_allowed {\n        return Err(format!(\n            \"validation accuracy reaches {:.2} at step {}, expected before {}\",\n            config.target_val_acc_threshold, transition_step, latest_allowed\n        ));\n    }\n\n    verify_accuracy_jump(\n        &accuracy_history.val_snapshots,\n        *transition_step,\n        config.accuracy_window,\n        config.chance_accuracy,\n        config.chance_tolerance,\n        config.target_val_acc_threshold,\n    )?;\n\n    let loss_drop_ratio = verify_loss_drop(\n        &loss_history.val_snapshots,\n        *transition_step,\n        config.loss_window,\n        config.loss_drop_fraction,\n    )?;\n\n    Ok(GrokkingPhaseReport {\n        early_train_step: *early_train_step,\n        transition_step: *transition_step,\n        plateau_max_val_acc,\n        loss_drop_ratio,\n    })\n}\n\nfn verify_accuracy_jump(\n    val_accuracy: &[(usize, f32)],\n    transition_step: usize,\n    window: usize,\n    chance_accuracy: f32,\n    chance_tolerance: f32,\n    target_val_acc_threshold: f32,\n) -> Result<(), String> {\n    let before_values: Vec<f32> = val_accuracy\n        .iter()\n        .filter(|(step, _)| *step < transition_step)\n        .map(|(_, acc)| *acc)\n        .collect();\n\n    let after_values: Vec<f32> = val_accuracy\n        .iter()\n        .filter(|(step, _)| *step >= transition_step)\n        .map(|(_, acc)| *acc)\n        .collect();\n\n    if before_values.len() < window || after_values.len() < window {\n        return Err(\"insufficient accuracy samples around transition\".to_string());\n    }\n\n    let before_avg: f32 =\n        before_values[before_values.len() - window..].iter().sum::<f32>() / window as f32;\n    let after_avg: f32 = after_values[..window].iter().sum::<f32>() / window as f32;\n\n    if before_avg > chance_accuracy + chance_tolerance {\n        return Err(format!(\n            \"validation accuracy plateau too high before transition (avg {:.4})\",\n            before_avg\n        ));\n    }\n\n    if after_avg < target_val_acc_threshold {\n        return Err(format!(\n            \"validation accuracy after transition too low (avg {:.4})\",\n            after_avg\n        ));\n    }\n\n    Ok(())\n}\n\nfn verify_loss_drop(\n    val_losses: &[(usize, f64)],\n    transition_step: usize,\n    loss_window: usize,\n    loss_drop_fraction: f64,\n) -> Result<f64, String> {\n    let before_losses: Vec<f64> = val_losses\n        .iter()\n        .filter(|(step, _)| *step < transition_step)\n        .map(|(_, loss)| *loss)\n        .collect();\n\n    let after_losses: Vec<f64> = val_losses\n        .iter()\n        .filter(|(step, _)| *step >= transition_step)\n        .map(|(_, loss)| *loss)\n        .collect();\n\n    if before_losses.len() < loss_window || after_losses.len() < loss_window {\n        return Err(\"insufficient loss samples around transition\".to_string());\n    }\n\n    let before_avg: f64 =\n        before_losses[before_losses.len() - loss_window..].iter().sum::<f64>()\n            / loss_window as f64;\n    let after_avg: f64 = after_losses[..loss_window].iter().sum::<f64>() / loss_window as f64;\n\n    if after_avg > before_avg * (1.0 - loss_drop_fraction) {\n        return Err(format!(\n            \"validation loss drop too small (before {:.4}, after {:.4})\",\n            before_avg, after_avg\n        ));\n    }\n\n    Ok(after_avg / before_avg)\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_restricted_loss_early_drop(\n    full_losses: &[(usize, f64)],\n    restricted_losses: &[(usize, f64)],\n    config: &RestrictedLossVerificationConfig,\n) -> Result<RestrictedLossReport, String> {\n    let restricted_baseline =\n        baseline_loss(restricted_losses, config.plateau_min_step, config.drop_window)?;\n    let full_baseline = baseline_loss(full_losses, config.plateau_min_step, config.drop_window)?;\n\n    let restricted_drop_step = find_drop_step(\n        restricted_losses,\n        config.plateau_min_step,\n        restricted_baseline,\n        config.drop_fraction,\n    )?;\n    let full_drop_step = find_drop_step(\n        full_losses,\n        config.plateau_min_step,\n        full_baseline,\n        config.drop_fraction,\n    )?;\n\n    if restricted_drop_step + config.min_step_lead > full_drop_step {\n        return Err(format!(\n            \"restricted loss drops at step {}, expected at least {} steps before full loss (step {})\",\n            restricted_drop_step, config.min_step_lead, full_drop_step\n        ));\n    }\n\n    Ok(RestrictedLossReport {\n        restricted_drop_step,\n        full_drop_step,\n    })\n}\n\npub fn verify_restricted_loss_drop(\n    restricted_losses: &[(usize, f64)],\n    config: &RestrictedLossDropConfig,\n) -> Result<RestrictedLossDropReport, String> {\n    if restricted_losses.len() < 3 {\n        return Err(\"restricted loss history requires at least 3 points\".to_string());\n    }\n\n    let first = restricted_losses.first().ok_or_else(|| {\n        \"restricted loss history is empty\".to_string()\n    })?;\n    let baseline_loss = first.1;\n\n    let (min_idx, (min_step, min_loss)) = restricted_losses\n        .iter()\n        .enumerate()\n        .min_by(|(_, a), (_, b)| {\n            a.1.partial_cmp(&b.1).unwrap_or(std::cmp::Ordering::Equal)\n        })\n        .ok_or_else(|| \"restricted loss history is empty\".to_string())?;\n\n    if min_idx == 0 {\n        return Err(\"restricted loss minimum occurs at start (no drop detected)\".to_string());\n    }\n\n    let relative_decrease = (baseline_loss - min_loss) / baseline_loss;\n    if relative_decrease < config.min_relative_decrease {\n        return Err(format!(\n            \"restricted loss decrease too small (baseline {:.4}, min {:.4}, decrease {:.1}%)\",\n            baseline_loss, min_loss, relative_decrease * 100.0\n        ));\n    }\n\n    Ok(RestrictedLossDropReport {\n        drop_step: *min_step,\n        drop_loss: *min_loss,\n        baseline_loss,\n    })\n}\n\npub fn verify_excluded_loss_spike(\n    excluded_losses: &[(usize, f64)],\n    config: &ExcludedLossSpikeConfig,\n) -> Result<ExcludedLossSpikeReport, String> {\n    if excluded_losses.len() < 3 {\n        return Err(\"excluded loss history requires at least 3 points\".to_string());\n    }\n\n    let first = excluded_losses.first().ok_or_else(|| {\n        \"excluded loss history is empty\".to_string()\n    })?;\n    let last = excluded_losses.last().ok_or_else(|| {\n        \"excluded loss history is empty\".to_string()\n    })?;\n    let baseline_loss = (first.1 + last.1) / 2.0;\n\n    let (peak_idx, (peak_step, peak_loss)) = excluded_losses\n        .iter()\n        .enumerate()\n        .max_by(|(_, a), (_, b)| {\n            a.1.partial_cmp(&b.1).unwrap_or(std::cmp::Ordering::Equal)\n        })\n        .ok_or_else(|| \"excluded loss history is empty\".to_string())?;\n\n    if peak_idx == 0 || peak_idx + 1 == excluded_losses.len() {\n        return Err(\"excluded loss spike occurs at boundary\".to_string());\n    }\n\n    if peak_loss < &(baseline_loss * (1.0 + config.min_relative_increase)) {\n        return Err(format!(\n            \"excluded loss spike too small (baseline {:.4}, peak {:.4})\",\n            baseline_loss, peak_loss\n        ));\n    }\n\n    Ok(ExcludedLossSpikeReport {\n        spike_step: *peak_step,\n        spike_loss: *peak_loss,\n        baseline_loss,\n    })\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_mlp_activation_waves<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n    fixed_y: usize,\n    neuron_indices: &[usize],\n    config: &MlpWaveVerificationConfig,\n) -> Result<MlpWaveReport, String> {\n    let activations = collect_mlp_post_relu_activations(model, device, fixed_y)?;\n    if neuron_indices.is_empty() {\n        return Err(\"neuron indices list is empty\".to_string());\n    }\n\n    let d_ff = activations.len();\n    for &idx in neuron_indices {\n        if idx >= d_ff {\n            return Err(format!(\n                \"neuron index {} out of range (d_ff={})\",\n                idx, d_ff\n            ));\n        }\n    }\n\n    let signals: Vec<(usize, Vec<f64>)> = neuron_indices\n        .iter()\n        .map(|&idx| (idx, activations[idx].clone()))\n        .collect();\n    verify_mlp_activation_waves_from_signals(&signals, config)\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_mlp_activation_waves_from_signals(\n    signals: &[(usize, Vec<f64>)],\n    config: &MlpWaveVerificationConfig,\n) -> Result<MlpWaveReport, String> {\n    if signals.is_empty() {\n        return Err(\"no activation signals provided\".to_string());\n    }\n\n    let mut reports = Vec::with_capacity(signals.len());\n    for (neuron_index, signal) in signals {\n        let report = analyze_mlp_activation_wave(*neuron_index, signal, config)?;\n        reports.push(report);\n    }\n\n    Ok(MlpWaveReport {\n        neuron_reports: reports,\n    })\n}\n\npub fn verify_mlp_pairwise_manifold_transition<B: Backend>(\n    pre_model: &Transformer<B>,\n    post_model: &Transformer<B>,\n    device: &B::Device,\n    config: &PairwiseManifoldConfig,\n) -> Result<PairwiseManifoldTransitionReport, String> {\n    let modulus = ModularAdditionDataset::modulus();\n    if config.fixed_y >= modulus {\n        return Err(format!(\n            \"fixed_y {} out of range (modulus {})\",\n            config.fixed_y, modulus\n        ));\n    }\n\n    let pre_activations = collect_mlp_post_relu_activations(pre_model, device, config.fixed_y)?;\n    let post_activations = collect_mlp_post_relu_activations(post_model, device, config.fixed_y)?;\n\n    let d_ff = pre_activations.len();\n    if post_activations.len() != d_ff {\n        return Err(\"post activations shape mismatch\".to_string());\n    }\n\n    if config.neuron_indices.len() < 2 {\n        return Err(\"need at least two neuron indices\".to_string());\n    }\n\n    for &idx in &config.neuron_indices {\n        if idx >= d_ff {\n            return Err(format!(\n                \"neuron index {} out of range (d_ff={})\",\n                idx, d_ff\n            ));\n        }\n    }\n\n    let pre_signals: Vec<(usize, Vec<f64>)> = config\n        .neuron_indices\n        .iter()\n        .map(|&idx| (idx, pre_activations[idx].clone()))\n        .collect();\n    let post_signals: Vec<(usize, Vec<f64>)> = config\n        .neuron_indices\n        .iter()\n        .map(|&idx| (idx, post_activations[idx].clone()))\n        .collect();\n\n    verify_pairwise_manifold_transition_from_signals(&pre_signals, &post_signals, config)\n}\n\npub fn collect_mlp_activation_surface<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n    neuron_index: usize,\n) -> Result<Vec<Vec<f64>>, String> {\n    let modulus = ModularAdditionDataset::modulus();\n    let equals_token = ModularAdditionDataset::equals_token();\n\n    let mut inputs_vec = Vec::with_capacity(modulus * modulus * 3);\n    for x in 0..modulus {\n        for y in 0..modulus {\n            inputs_vec.push(x as i32);\n            inputs_vec.push(y as i32);\n            inputs_vec.push(equals_token as i32);\n        }\n    }\n\n    let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n        .reshape([modulus * modulus, 3]);\n    let (_, mlp_acts) = model.forward_with_mlp_activations(inputs);\n    let [batch_size, d_ff] = mlp_acts.dims();\n    if batch_size != modulus * modulus {\n        return Err(format!(\n            \"unexpected activation batch size {} (expected {})\",\n            batch_size,\n            modulus * modulus\n        ));\n    }\n    if neuron_index >= d_ff {\n        return Err(format!(\n            \"neuron index {} out of range (d_ff={})\",\n            neuron_index, d_ff\n        ));\n    }\n\n    let data: Vec<f32> = mlp_acts.into_data().to_vec().unwrap();\n    let mut surface = vec![vec![0.0f64; modulus]; modulus];\n    for x in 0..modulus {\n        for y in 0..modulus {\n            let batch = x * modulus + y;\n            let idx = batch * d_ff + neuron_index;\n            surface[x][y] = data[idx] as f64;\n        }\n    }\n\n    Ok(surface)\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_constructive_interference_surface_from_grid(\n    surface: &[Vec<f64>],\n    config: &DiagonalRidgeConfig,\n) -> Result<DiagonalRidgeReport, String> {\n    let (diag_main, diag_anti, axis_x, axis_y) =\n        surface_autocorrelation_scores(surface, config.max_lag)?;\n\n    let diagonal_score = diag_main.max(diag_anti);\n    let axis_score = axis_x.max(axis_y);\n    let ratio = if axis_score > 0.0 {\n        diagonal_score / axis_score\n    } else {\n        f64::INFINITY\n    };\n\n    if diagonal_score < config.min_diagonal_score {\n        return Err(format!(\n            \"diagonal autocorrelation {:.3} below threshold {:.3}\",\n            diagonal_score, config.min_diagonal_score\n        ));\n    }\n\n    if ratio < config.min_diagonal_ratio {\n        return Err(format!(\n            \"diagonal/axis ratio {:.3} below threshold {:.3}\",\n            ratio, config.min_diagonal_ratio\n        ));\n    }\n\n    Ok(DiagonalRidgeReport {\n        neuron_index: config.neuron_index,\n        diagonal_score,\n        axis_score,\n        ratio,\n    })\n}\n\npub fn verify_pairwise_manifold_transition_from_signals(\n    pre_signals: &[(usize, Vec<f64>)],\n    post_signals: &[(usize, Vec<f64>)],\n    config: &PairwiseManifoldConfig,\n) -> Result<PairwiseManifoldTransitionReport, String> {\n    let pre_report = evaluate_pairwise_manifolds_from_signals(pre_signals)?;\n    let post_report = evaluate_pairwise_manifolds_from_signals(post_signals)?;\n\n    if pre_report.average_score > config.max_pre_score {\n        return Err(format!(\n            \"pre-grok average circularity {:.3} exceeds threshold {:.3}\",\n            pre_report.average_score, config.max_pre_score\n        ));\n    }\n\n    let passing_pairs = post_report\n        .pair_reports\n        .iter()\n        .filter(|pair| {\n            pair.circularity_score >= config.min_post_score\n                && pair.axis_ratio <= config.max_axis_ratio\n        })\n        .count();\n    if passing_pairs < config.min_pairs_passing {\n        return Err(format!(\n            \"post-grok only {} pairs meet circularity threshold (need {})\",\n            passing_pairs, config.min_pairs_passing\n        ));\n    }\n\n    Ok(PairwiseManifoldTransitionReport {\n        pre_average_score: pre_report.average_score,\n        post_average_score: post_report.average_score,\n        passing_pairs,\n        total_pairs: post_report.pair_reports.len(),\n    })\n}\n\nfn evaluate_pairwise_manifolds_from_signals(\n    signals: &[(usize, Vec<f64>)],\n) -> Result<PairwiseManifoldReport, String> {\n    if signals.len() < 2 {\n        return Err(\"need at least two activation signals\".to_string());\n    }\n\n    let signal_len = signals[0].1.len();\n    if signal_len < 3 {\n        return Err(\"activation signal too short\".to_string());\n    }\n\n    for (idx, signal) in signals {\n        if signal.len() != signal_len {\n            return Err(format!(\n                \"signal length mismatch for neuron {}\",\n                idx\n            ));\n        }\n    }\n\n    let mut reports = Vec::new();\n    for i in 0..signals.len() {\n        for j in (i + 1)..signals.len() {\n            let (neuron_a, signal_a) = &signals[i];\n            let (neuron_b, signal_b) = &signals[j];\n            let points: Vec<(f64, f64)> = signal_a\n                .iter()\n                .zip(signal_b.iter())\n                .map(|(x, y)| (*x, *y))\n                .collect();\n            let (score, radius_cv, axis_ratio) =\n                match pairwise_circularity_metrics(&points) {\n                    Ok(metrics) => metrics,\n                    Err(_) => (0.0, 1.0, f64::INFINITY),\n                };\n            reports.push(PairwiseManifoldPairReport {\n                neuron_a: *neuron_a,\n                neuron_b: *neuron_b,\n                circularity_score: score,\n                radius_cv,\n                axis_ratio,\n            });\n        }\n    }\n\n    let average_score = reports\n        .iter()\n        .map(|pair| pair.circularity_score)\n        .sum::<f64>()\n        / reports.len() as f64;\n\n    Ok(PairwiseManifoldReport {\n        pair_reports: reports,\n        average_score,\n    })\n}\n\nfn pairwise_circularity_metrics(points: &[(f64, f64)]) -> Result<(f64, f64, f64), String> {\n    if points.len() < 3 {\n        return Err(\"not enough points for circularity\".to_string());\n    }\n\n    let n = points.len() as f64;\n    let mean_x = points.iter().map(|(x, _)| *x).sum::<f64>() / n;\n    let mean_y = points.iter().map(|(_, y)| *y).sum::<f64>() / n;\n\n    let mut var_x = 0.0;\n    let mut var_y = 0.0;\n    let mut cov_xy = 0.0;\n    for (x, y) in points {\n        let dx = x - mean_x;\n        let dy = y - mean_y;\n        var_x += dx * dx;\n        var_y += dy * dy;\n        cov_xy += dx * dy;\n    }\n    var_x /= n;\n    var_y /= n;\n    cov_xy /= n;\n\n    let trace = var_x + var_y;\n    let det = var_x * var_y - cov_xy * cov_xy;\n    if det <= 1.0e-12 || trace <= 0.0 {\n        return Err(\"degenerate covariance\".to_string());\n    }\n\n    let half_trace = trace * 0.5;\n    let disc = (half_trace * half_trace - det).max(0.0).sqrt();\n    let lambda1 = half_trace + disc;\n    let lambda2 = half_trace - disc;\n    if lambda2 <= 0.0 {\n        return Err(\"degenerate eigenvalues\".to_string());\n    }\n\n    let axis_ratio = (lambda1 / lambda2).sqrt();\n\n    let mut v1_x = cov_xy;\n    let mut v1_y = lambda1 - var_x;\n    if v1_x.abs() < 1.0e-12 && v1_y.abs() < 1.0e-12 {\n        v1_x = 1.0;\n        v1_y = 0.0;\n    }\n    let norm = (v1_x * v1_x + v1_y * v1_y).sqrt();\n    v1_x /= norm;\n    v1_y /= norm;\n    let v2_x = -v1_y;\n    let v2_y = v1_x;\n\n    let mut radii = Vec::with_capacity(points.len());\n    for (x, y) in points {\n        let dx = x - mean_x;\n        let dy = y - mean_y;\n        let p1 = dx * v1_x + dy * v1_y;\n        let p2 = dx * v2_x + dy * v2_y;\n        let w1 = p1 / lambda1.sqrt();\n        let w2 = p2 / lambda2.sqrt();\n        radii.push((w1 * w1 + w2 * w2).sqrt());\n    }\n\n    let mean_radius = radii.iter().sum::<f64>() / radii.len() as f64;\n    if mean_radius == 0.0 {\n        return Err(\"zero mean radius\".to_string());\n    }\n\n    let mut variance = 0.0;\n    for r in &radii {\n        let diff = r - mean_radius;\n        variance += diff * diff;\n    }\n    let std = (variance / radii.len() as f64).sqrt();\n    let radius_cv = std / mean_radius;\n    let score = 1.0 / (1.0 + radius_cv);\n\n    Ok((score, radius_cv, axis_ratio))\n}\n\n// Helper functions for verification - reserved for future integration\n#[allow(dead_code)]\nfn baseline_loss(\n    losses: &[(usize, f64)],\n    plateau_min_step: usize,\n    drop_window: usize,\n) -> Result<f64, String> {\n    let before: Vec<f64> = losses\n        .iter()\n        .filter(|(step, _)| *step <= plateau_min_step)\n        .map(|(_, loss)| *loss)\n        .collect();\n\n    if before.len() < drop_window || drop_window == 0 {\n        return Err(\"insufficient loss samples for baseline\".to_string());\n    }\n\n    let window = &before[before.len() - drop_window..];\n    Ok(window.iter().sum::<f64>() / drop_window as f64)\n}\n\n#[allow(dead_code)]\nfn find_drop_step(\n    losses: &[(usize, f64)],\n    plateau_min_step: usize,\n    baseline: f64,\n    drop_fraction: f64,\n) -> Result<usize, String> {\n    losses\n        .iter()\n        .filter(|(step, _)| *step >= plateau_min_step)\n        .find(|(_, loss)| *loss <= baseline * (1.0 - drop_fraction))\n        .map(|(step, _)| *step)\n        .ok_or_else(|| \"loss never drops below threshold\".to_string())\n}\n\nfn collect_mlp_post_relu_activations<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n    fixed_y: usize,\n) -> Result<Vec<Vec<f64>>, String> {\n    let modulus = ModularAdditionDataset::modulus();\n    let equals_token = ModularAdditionDataset::equals_token();\n\n    let mut inputs_vec = Vec::with_capacity(modulus * 3);\n    for x in 0..modulus {\n        inputs_vec.push(x as i32);\n        inputs_vec.push(fixed_y as i32);\n        inputs_vec.push(equals_token as i32);\n    }\n\n    let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n        .reshape([modulus, 3]);\n    let (_, mlp_acts) = model.forward_with_mlp_activations(inputs);\n    let [batch_size, d_ff] = mlp_acts.dims();\n\n    if batch_size != modulus {\n        return Err(format!(\n            \"unexpected activation batch size {} (expected {})\",\n            batch_size, modulus\n        ));\n    }\n\n    let data: Vec<f32> = mlp_acts.into_data().to_vec().unwrap();\n    let mut per_neuron = vec![Vec::with_capacity(batch_size); d_ff];\n    for batch in 0..batch_size {\n        for neuron in 0..d_ff {\n            let idx = batch * d_ff + neuron;\n            per_neuron[neuron].push(data[idx] as f64);\n        }\n    }\n\n    Ok(per_neuron)\n}\n\n#[allow(dead_code)]\nfn surface_autocorrelation_scores(\n    surface: &[Vec<f64>],\n    max_lag: usize,\n) -> Result<(f64, f64, f64, f64), String> {\n    let (height, width) = surface_dimensions(surface)?;\n    if height < 2 || width < 2 {\n        return Err(\"surface grid too small\".to_string());\n    }\n\n    let (mean, var) = surface_mean_variance(surface)?;\n    if var == 0.0 {\n        return Err(\"surface variance is zero\".to_string());\n    }\n\n    let max_lag = max_lag.min(height.saturating_sub(1)).min(width.saturating_sub(1));\n    if max_lag == 0 {\n        return Err(\"max_lag too small for surface\".to_string());\n    }\n\n    let diag_main = average_autocorrelation(surface, mean, var, max_lag, 1, 1)?;\n    let diag_anti = average_autocorrelation(surface, mean, var, max_lag, 1, -1)?;\n    let axis_x = average_autocorrelation(surface, mean, var, max_lag, 1, 0)?;\n    let axis_y = average_autocorrelation(surface, mean, var, max_lag, 0, 1)?;\n\n    Ok((diag_main, diag_anti, axis_x, axis_y))\n}\n\n#[allow(dead_code)]\nfn surface_dimensions(surface: &[Vec<f64>]) -> Result<(usize, usize), String> {\n    let height = surface.len();\n    if height == 0 {\n        return Err(\"surface is empty\".to_string());\n    }\n    let width = surface[0].len();\n    if width == 0 {\n        return Err(\"surface has empty rows\".to_string());\n    }\n    for (idx, row) in surface.iter().enumerate() {\n        if row.len() != width {\n            return Err(format!(\n                \"surface row {} has length {}, expected {}\",\n                idx,\n                row.len(),\n                width\n            ));\n        }\n    }\n    Ok((height, width))\n}\n\n#[allow(dead_code)]\nfn surface_mean_variance(surface: &[Vec<f64>]) -> Result<(f64, f64), String> {\n    let (height, width) = surface_dimensions(surface)?;\n    let mut sum = 0.0;\n    for row in surface {\n        for value in row {\n            sum += *value;\n        }\n    }\n    let count = (height * width) as f64;\n    let mean = sum / count;\n    let mut variance = 0.0;\n    for row in surface {\n        for value in row {\n            let diff = *value - mean;\n            variance += diff * diff;\n        }\n    }\n    variance /= count;\n    Ok((mean, variance))\n}\n\n#[allow(dead_code)]\nfn average_autocorrelation(\n    surface: &[Vec<f64>],\n    mean: f64,\n    variance: f64,\n    max_lag: usize,\n    dx: i32,\n    dy: i32,\n) -> Result<f64, String> {\n    let (height, width) = surface_dimensions(surface)?;\n    let mut total = 0.0;\n    let mut lag_count = 0usize;\n\n    for lag in 1..=max_lag {\n        let mut sum = 0.0;\n        let mut count = 0usize;\n        let lag = lag as i32;\n        for x in 0..height {\n            for y in 0..width {\n                let x2 = x as i32 + dx * lag;\n                let y2 = y as i32 + dy * lag;\n                if x2 < 0 || y2 < 0 {\n                    continue;\n                }\n                let x2 = x2 as usize;\n                let y2 = y2 as usize;\n                if x2 >= height || y2 >= width {\n                    continue;\n                }\n                let a = surface[x][y] - mean;\n                let b = surface[x2][y2] - mean;\n                sum += a * b;\n                count += 1;\n            }\n        }\n        if count == 0 {\n            continue;\n        }\n        let corr = sum / (count as f64 * variance);\n        total += corr.abs();\n        lag_count += 1;\n    }\n\n    if lag_count == 0 {\n        return Err(\"no valid lags for autocorrelation\".to_string());\n    }\n\n    Ok(total / lag_count as f64)\n}\n\n#[allow(dead_code)]\nfn analyze_mlp_activation_wave(\n    neuron_index: usize,\n    signal: &[f64],\n    config: &MlpWaveVerificationConfig,\n) -> Result<MlpWaveNeuronReport, String> {\n    if signal.len() < 3 {\n        return Err(format!(\n            \"activation signal for neuron {} is too short\",\n            neuron_index\n        ));\n    }\n\n    let negative_count = signal\n        .iter()\n        .filter(|value| **value < -config.negative_tolerance)\n        .count();\n    let negative_fraction = negative_count as f64 / signal.len() as f64;\n    if negative_fraction > config.max_negative_fraction {\n        return Err(format!(\n            \"neuron {} has {:.3} negative activations\",\n            neuron_index, negative_fraction\n        ));\n    }\n\n    let (dominant_frequency, dominant_ratio) = dominant_frequency_ratio(signal)?;\n    if dominant_ratio < config.min_dominant_ratio {\n        return Err(format!(\n            \"neuron {} dominant frequency ratio {:.3} below threshold {:.3}\",\n            neuron_index, dominant_ratio, config.min_dominant_ratio\n        ));\n    }\n\n    let best_correlation =\n        best_rectified_sine_correlation(signal, dominant_frequency, config.phase_steps);\n    if best_correlation < config.min_correlation {\n        return Err(format!(\n            \"neuron {} rectified sine correlation {:.3} below threshold {:.3}\",\n            neuron_index, best_correlation, config.min_correlation\n        ));\n    }\n\n    Ok(MlpWaveNeuronReport {\n        neuron_index,\n        dominant_frequency,\n        dominant_ratio,\n        negative_fraction,\n        best_correlation,\n    })\n}\n\n#[allow(dead_code)]\nfn dominant_frequency_ratio(signal: &[f64]) -> Result<(usize, f64), String> {\n    if signal.len() < 2 {\n        return Err(\"signal too short for FFT\".to_string());\n    }\n\n    let mut planner = FftPlanner::new();\n    let fft = planner.plan_fft_forward(signal.len());\n    let mut buffer: Vec<Complex<f64>> = signal\n        .iter()\n        .map(|&x| Complex::new(x, 0.0))\n        .collect();\n    fft.process(&mut buffer);\n    let magnitudes: Vec<f64> = buffer.iter().map(|c| c.norm()).collect();\n\n    if magnitudes.len() < 2 {\n        return Err(\"insufficient FFT bins\".to_string());\n    }\n\n    let mut dominant_idx = 1usize;\n    let mut dominant_mag = magnitudes[1];\n    for (idx, &mag) in magnitudes.iter().enumerate().skip(2) {\n        if mag > dominant_mag {\n            dominant_mag = mag;\n            dominant_idx = idx;\n        }\n    }\n\n    if dominant_idx == 0 {\n        return Err(\"dominant frequency is zero\".to_string());\n    }\n\n    let mut nonzero = magnitudes[1..].to_vec();\n    nonzero.sort_by(|a, b| a.partial_cmp(b).unwrap_or(std::cmp::Ordering::Equal));\n    let median_mag = nonzero[nonzero.len() / 2];\n    let ratio = if median_mag == 0.0 {\n        dominant_mag\n    } else {\n        dominant_mag / median_mag\n    };\n\n    Ok((dominant_idx, ratio))\n}\n\n#[allow(dead_code)]\nfn best_rectified_sine_correlation(signal: &[f64], frequency: usize, phase_steps: usize) -> f64 {\n    if signal.is_empty() || frequency == 0 || phase_steps == 0 {\n        return 0.0;\n    }\n\n    let tau = std::f64::consts::PI * 2.0;\n    let mut best = -1.0;\n    for step in 0..phase_steps {\n        let phase = tau * step as f64 / phase_steps as f64;\n        let wave: Vec<f64> = (0..signal.len())\n            .map(|idx| {\n                let angle = tau * frequency as f64 * idx as f64 / signal.len() as f64 + phase;\n                angle.sin().max(0.0)\n            })\n            .collect();\n        let corr = pearson_correlation(signal, &wave);\n        if corr > best {\n            best = corr;\n        }\n    }\n\n    best\n}\n\n#[allow(dead_code)]\nfn pearson_correlation(a: &[f64], b: &[f64]) -> f64 {\n    if a.len() != b.len() || a.is_empty() {\n        return 0.0;\n    }\n\n    let mean_a = a.iter().sum::<f64>() / a.len() as f64;\n    let mean_b = b.iter().sum::<f64>() / b.len() as f64;\n\n    let mut num = 0.0;\n    let mut denom_a = 0.0;\n    let mut denom_b = 0.0;\n    for (x, y) in a.iter().zip(b.iter()) {\n        let da = x - mean_a;\n        let db = y - mean_b;\n        num += da * db;\n        denom_a += da * da;\n        denom_b += db * db;\n    }\n\n    if denom_a == 0.0 || denom_b == 0.0 {\n        return 0.0;\n    }\n\n    num / (denom_a.sqrt() * denom_b.sqrt())\n}\n\n/// Verify that the model actually learned modular addition\n/// by testing it on ALL possible examples systematically\npub fn verify_full_accuracy<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n) -> (f32, f32) {\n    println!(\"ðŸ” Verifying model on COMPLETE dataset (no sampling)...\");\n\n    let train_dataset = ModularAdditionDataset::new(true, 42);\n    let val_dataset = ModularAdditionDataset::new(false, 42);\n\n    let train_acc = test_all_examples(model, &train_dataset, device);\n    let val_acc = test_all_examples(model, &val_dataset, device);\n\n    println!(\"ðŸ“Š FULL Dataset Results:\");\n    println!(\"   Training: {}/{} = {:.2}%\",\n        (train_acc * train_dataset.len() as f32) as usize,\n        train_dataset.len(),\n        train_acc * 100.0);\n    println!(\"   Validation: {}/{} = {:.2}%\",\n        (val_acc * val_dataset.len() as f32) as usize,\n        val_dataset.len(),\n        val_acc * 100.0);\n\n    (train_acc, val_acc)\n}\n\n/// Test model on ALL examples in dataset (no random sampling)\nfn test_all_examples<B: Backend>(\n    model: &Transformer<B>,\n    dataset: &ModularAdditionDataset,\n    device: &B::Device,\n) -> f32 {\n    let mut total_correct = 0;\n    let total = dataset.len();\n\n    // Test in batches to avoid memory issues\n    let batch_size = 100;\n    for batch_start in (0..total).step_by(batch_size) {\n        let batch_end = (batch_start + batch_size).min(total);\n        let batch_len = batch_end - batch_start;\n\n        let mut inputs_vec = Vec::new();\n        let mut targets_vec = Vec::new();\n\n        for idx in batch_start..batch_end {\n            let (input, target) = dataset.get(idx).unwrap();\n            inputs_vec.extend(input);\n            targets_vec.push(target as i32);\n        }\n\n        let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n            .reshape([batch_len, 3]);\n\n        let logits = model.forward(inputs);\n        let predictions = logits.argmax(1).squeeze::<1>();\n        let predictions_vec: Vec<i32> = predictions.into_data().to_vec().unwrap();\n\n        let correct = predictions_vec\n            .iter()\n            .zip(targets_vec.iter())\n            .filter(|(p, t)| p == t)\n            .count();\n\n        total_correct += correct;\n    }\n\n    total_correct as f32 / total as f32\n}\n\n/// Verify that weight norms decrease over time with weight decay\n/// and show correlation with the grokking transition\npub fn verify_weight_norm_decay(\n    weight_norm_history: &[(usize, f64)],\n    grok_step: Option<usize>,\n    config: &WeightNormDecayConfig,\n) -> Result<WeightNormDecayReport, String> {\n    if weight_norm_history.len() < 2 {\n        return Err(\"weight norm history requires at least 2 snapshots\".to_string());\n    }\n\n    let initial_norm = weight_norm_history\n        .first()\n        .ok_or_else(|| \"weight norm history is empty\".to_string())?\n        .1;\n    let final_norm = weight_norm_history\n        .last()\n        .ok_or_else(|| \"weight norm history is empty\".to_string())?\n        .1;\n\n    if initial_norm <= 0.0 {\n        return Err(\"initial weight norm must be positive\".to_string());\n    }\n\n    // Check that norms are generally decreasing (with weight decay)\n    let relative_decrease = (initial_norm - final_norm) / initial_norm;\n    if relative_decrease < config.min_relative_decrease {\n        return Err(format!(\n            \"weight norm decrease {:.4} below threshold {:.4} (initial {:.4}, final {:.4})\",\n            relative_decrease, config.min_relative_decrease, initial_norm, final_norm\n        ));\n    }\n\n    // Check that norms are not increasing (weight decay should monotonically decrease norms)\n    for window in weight_norm_history.windows(2) {\n        let prev_norm = window[0].1;\n        let next_norm = window[1].1;\n        if next_norm > prev_norm * 1.01 {\n            // Allow 1% tolerance for numerical noise\n            return Err(format!(\n                \"weight norm increased from {:.4} to {:.4} at step {}\",\n                prev_norm, next_norm, window[1].0\n            ));\n        }\n    }\n\n    // If grok step is provided, check that norms show a shift around grokking\n    let grok_step_norm = if let Some(step) = grok_step {\n        weight_norm_history\n            .iter()\n            .filter(|(s, _)| {\n                *s >= step.saturating_sub(config.grok_step_window)\n                    && *s <= step + config.grok_step_window\n            })\n            .map(|(_, norm)| *norm)\n            .next()\n    } else {\n        None\n    };\n\n    Ok(WeightNormDecayReport {\n        initial_norm,\n        final_norm,\n        relative_decrease,\n        grok_step_norm,\n    })\n}\n\n/// Verify the characteristic \"snake curve\" shape of grokking phenomenon\n/// on log-scale plots: train accuracy rises early, validation stays flat then jumps\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_snake_curve_shape(\n    train_acc: &[(usize, f32)],\n    val_acc: &[(usize, f32)],\n    config: &SnakeCurveConfig,\n) -> Result<SnakeCurveReport, String> {\n    if train_acc.is_empty() || val_acc.is_empty() {\n        return Err(\"empty accuracy history\".to_string());\n    }\n\n    // Phase 1: Verify early train convergence (immediate drop on log scale)\n    let train_converged_step = train_acc\n        .iter()\n        .find(|(_, acc)| *acc >= config.train_convergence_threshold)\n        .map(|(step, _)| *step)\n        .ok_or_else(|| {\n            format!(\n                \"train accuracy never reaches {} threshold\",\n                config.train_convergence_threshold\n            )\n        })?;\n\n    if train_converged_step > config.early_step_max {\n        return Err(format!(\n            \"train accuracy converges at step {}, expected before {}\",\n            train_converged_step, config.early_step_max\n        ));\n    }\n\n    // Phase 2: Verify validation plateau (flat on log scale)\n    let plateau_samples: Vec<f32> = val_acc\n        .iter()\n        .filter(|(step, _)| *step >= 1 && *step <= config.val_plateau_min_step)\n        .map(|(_, acc)| *acc)\n        .collect();\n\n    if plateau_samples.is_empty() {\n        return Err(\"no validation samples in plateau region\".to_string());\n    }\n\n    let val_plateau_max = plateau_samples\n        .iter()\n        .fold(0.0f32, |max_val, &acc| max_val.max(acc));\n\n    if val_plateau_max > config.val_plateau_max_acc {\n        return Err(format!(\n            \"validation accuracy in plateau region too high: max {:.4} exceeds {:.4}\",\n            val_plateau_max, config.val_plateau_max_acc\n        ));\n    }\n\n    // Phase 3: Verify delayed validation jump (crash up on log scale)\n    let val_grok_step = val_acc\n        .iter()\n        .filter(|(step, _)| *step > config.val_plateau_min_step)\n        .find(|(_, acc)| *acc >= config.val_final_threshold)\n        .map(|(step, _)| *step)\n        .ok_or_else(|| {\n            format!(\n                \"validation accuracy never reaches {} after plateau\",\n                config.val_final_threshold\n            )\n        })?;\n\n    // Phase 4: Verify phase ordering (snake shape: early train, late val)\n    let curve_shape_valid = train_converged_step < config.val_plateau_min_step\n        && val_grok_step > config.val_plateau_min_step\n        && train_converged_step < val_grok_step;\n\n    if !curve_shape_valid {\n        return Err(format!(\n            \"invalid curve shape: train converged at {}, val plateau until {}, val grokked at {}\",\n            train_converged_step, config.val_plateau_min_step, val_grok_step\n        ));\n    }\n\n    Ok(SnakeCurveReport {\n        train_converged_by_step: train_converged_step,\n        val_plateau_max_acc: val_plateau_max,\n        val_grok_step,\n        curve_shape_valid,\n    })\n}\n\n/// Test specific examples to see if model actually computes mod p\npub fn test_specific_examples<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n) {\n    println!();\n    println!(\"ðŸ§ª Testing specific examples:\");\n\n    let modulus = ModularAdditionDataset::modulus();\n    let equals_token = ModularAdditionDataset::equals_token();\n    let test_cases = vec![\n        (0, 0, 0),\n        (1, 1, 2),\n        (50, 50, (50 + 50) % modulus),\n        (112, 1, (112 + 1) % modulus),\n        (56, 57, (56 + 57) % modulus),\n        (112, 112, (112 + 112) % modulus),\n        (10, 20, 30),\n        (60, 60, (60 + 60) % modulus),\n    ];\n\n    for (a, b, expected) in test_cases {\n        let input_vec = vec![a as i32, b as i32, equals_token as i32];\n        let input = Tensor::<B, 1, Int>::from_ints(input_vec.as_slice(), device)\n            .reshape([1, 3]);\n\n        let logits = model.forward(input);\n        let prediction = logits.argmax(1).squeeze::<1>();\n        let pred_value: i32 = prediction.into_data().to_vec().unwrap()[0];\n\n        let correct = if pred_value == expected as i32 { \"âœ“\" } else { \"âœ—\" };\n        println!(\n            \"   {} + {} mod {} = {} | Model: {} {}\",\n            a, b, modulus, expected, pred_value, correct\n        );\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    fn make_histories(\n        train_acc: &[(usize, f32)],\n        val_acc: &[(usize, f32)],\n        val_loss: &[(usize, f64)],\n    ) -> (LossHistory, AccuracyHistory) {\n        let mut loss_history = LossHistory::new();\n        loss_history.val_snapshots = val_loss.to_vec();\n        let mut accuracy_history = AccuracyHistory::new();\n        accuracy_history.train_snapshots = train_acc.to_vec();\n        accuracy_history.val_snapshots = val_acc.to_vec();\n        (loss_history, accuracy_history)\n    }\n\n    #[test]\n    fn grokking_phase_verification_passes_on_synthetic_signal() {\n        let train_acc = vec![(0, 0.2), (500, 0.995), (1000, 0.999)];\n        let val_acc = vec![\n            (0, 0.01),\n            (500, 0.01),\n            (1500, 0.01),\n            (2200, 0.995),\n            (2500, 0.999),\n        ];\n        let val_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 2.0),\n            (2200, 0.2),\n            (2500, 0.15),\n        ];\n\n        let (loss_history, accuracy_history) = make_histories(&train_acc, &val_acc, &val_loss);\n        let config = GrokkingVerificationConfig {\n            early_step_max: 1000,\n            plateau_min_step: 1500,\n            generalization_window: 1500,\n            accuracy_window: 2,\n            loss_window: 2,\n            ..GrokkingVerificationConfig::default_for_modulus(ModularAdditionDataset::modulus())\n        };\n\n        let report = verify_grokking_phase_transition(&loss_history, &accuracy_history, &config)\n            .expect(\"expected grokking verification to pass\");\n        assert!(report.transition_step >= config.plateau_min_step);\n        assert!(report.early_train_step <= config.early_step_max);\n    }\n\n    #[test]\n    fn grokking_phase_verification_fails_without_transition() {\n        let train_acc = vec![(0, 0.2), (500, 0.995)];\n        let val_acc = vec![(0, 0.01), (500, 0.01), (1500, 0.02)];\n        let val_loss = vec![(0, 2.0), (500, 2.0), (1500, 1.9)];\n\n        let (loss_history, accuracy_history) = make_histories(&train_acc, &val_acc, &val_loss);\n        let config = GrokkingVerificationConfig {\n            early_step_max: 1000,\n            plateau_min_step: 1000,\n            generalization_window: 1000,\n            accuracy_window: 2,\n            loss_window: 2,\n            ..GrokkingVerificationConfig::default_for_modulus(ModularAdditionDataset::modulus())\n        };\n\n        let err = verify_grokking_phase_transition(&loss_history, &accuracy_history, &config)\n            .expect_err(\"expected grokking verification to fail\");\n        assert!(err.contains(\"validation accuracy never reaches target\"));\n    }\n\n    #[test]\n    fn grokking_phase_verification_fails_with_slow_accuracy_rise() {\n        let train_acc = vec![(0, 0.2), (500, 0.995), (1000, 0.999)];\n        let val_acc = vec![\n            (0, 0.01),\n            (500, 0.01),\n            (1500, 0.5),\n            (2000, 0.6),\n            (2200, 0.995),\n            (2500, 0.999),\n        ];\n        let val_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 2.0),\n            (2200, 0.2),\n            (2500, 0.15),\n        ];\n\n        let (loss_history, accuracy_history) = make_histories(&train_acc, &val_acc, &val_loss);\n        let config = GrokkingVerificationConfig {\n            early_step_max: 1000,\n            plateau_min_step: 1000,\n            generalization_window: 2000,\n            accuracy_window: 2,\n            loss_window: 2,\n            ..GrokkingVerificationConfig::default_for_modulus(ModularAdditionDataset::modulus())\n        };\n\n        let err = verify_grokking_phase_transition(&loss_history, &accuracy_history, &config)\n            .expect_err(\"expected grokking verification to fail\");\n        assert!(err.contains(\"plateau too high\"));\n    }\n\n    #[test]\n    fn restricted_loss_detection_passes_with_early_drop() {\n        let full_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 2.0),\n            (2500, 2.0),\n            (3500, 0.3),\n        ];\n        let restricted_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 0.8),\n            (2500, 0.4),\n            (3500, 0.3),\n        ];\n\n        let config = RestrictedLossVerificationConfig {\n            plateau_min_step: 1000,\n            drop_window: 2,\n            drop_fraction: 0.5,\n            min_step_lead: 500,\n        };\n\n        let report = verify_restricted_loss_early_drop(&full_loss, &restricted_loss, &config)\n            .expect(\"expected restricted loss verification to pass\");\n        assert!(report.restricted_drop_step < report.full_drop_step);\n    }\n\n    #[test]\n    fn restricted_loss_detection_fails_when_full_drops_first() {\n        let full_loss = vec![(0, 2.0), (1000, 1.0), (1500, 0.8)];\n        let restricted_loss = vec![(0, 2.0), (1000, 2.0), (1500, 0.8)];\n\n        let config = RestrictedLossVerificationConfig {\n            plateau_min_step: 500,\n            drop_window: 1,\n            drop_fraction: 0.3,\n            min_step_lead: 200,\n        };\n\n        let err = verify_restricted_loss_early_drop(&full_loss, &restricted_loss, &config)\n            .expect_err(\"expected restricted loss verification to fail\");\n        assert!(err.contains(\"restricted loss drops\"));\n    }\n\n    #[test]\n    fn excluded_loss_spike_verification_passes() {\n        let excluded_loss = vec![(0, 1.0), (1500, 2.2), (3000, 1.1)];\n        let config = ExcludedLossSpikeConfig {\n            min_relative_increase: 0.5,\n        };\n\n        let report = verify_excluded_loss_spike(&excluded_loss, &config)\n            .expect(\"expected excluded loss spike verification to pass\");\n        assert_eq!(report.spike_step, 1500);\n    }\n\n    #[test]\n    fn excluded_loss_spike_verification_fails_without_spike() {\n        let excluded_loss = vec![(0, 1.0), (1500, 1.1), (3000, 1.0)];\n        let config = ExcludedLossSpikeConfig {\n            min_relative_increase: 0.5,\n        };\n\n        let err = verify_excluded_loss_spike(&excluded_loss, &config)\n            .expect_err(\"expected excluded loss spike verification to fail\");\n        assert!(err.contains(\"excluded loss spike too small\"));\n    }\n\n    #[test]\n    fn restricted_loss_drop_verification_passes() {\n        let restricted_loss = vec![\n            (0, 2.0),\n            (1000, 1.8),\n            (5000, 1.2),\n            (7000, 0.8),\n            (10000, 0.9),\n        ];\n        let config = RestrictedLossDropConfig {\n            min_relative_decrease: 0.3,\n        };\n\n        let report = verify_restricted_loss_drop(&restricted_loss, &config)\n            .expect(\"expected restricted loss drop verification to pass\");\n        assert_eq!(report.drop_step, 7000);\n        assert!(report.baseline_loss > report.drop_loss);\n        assert!((report.baseline_loss - report.drop_loss) / report.baseline_loss >= 0.3);\n    }\n\n    #[test]\n    fn restricted_loss_drop_verification_fails_without_drop() {\n        let restricted_loss = vec![\n            (0, 2.0),\n            (1000, 2.1),\n            (5000, 1.9),\n            (7000, 2.0),\n            (10000, 2.05),\n        ];\n        let config = RestrictedLossDropConfig {\n            min_relative_decrease: 0.3,\n        };\n\n        let err = verify_restricted_loss_drop(&restricted_loss, &config)\n            .expect_err(\"expected restricted loss drop verification to fail\");\n        assert!(err.contains(\"restricted loss decrease too small\"));\n    }\n\n    #[test]\n    fn mlp_activation_wave_verification_passes_on_rectified_sine() {\n        let n = ModularAdditionDataset::modulus();\n        let freq = 5.0;\n        let mut signal = Vec::with_capacity(n);\n        for idx in 0..n {\n            let angle = 2.0 * std::f64::consts::PI * freq * idx as f64 / n as f64;\n            let rectified = angle.sin().max(0.0);\n            let wobble = (2.0 * std::f64::consts::PI * 2.0 * idx as f64 / n as f64)\n                .sin()\n                .abs()\n                * 0.02;\n            signal.push(rectified + wobble);\n        }\n\n        let config = MlpWaveVerificationConfig {\n            max_negative_fraction: 0.0,\n            negative_tolerance: 1e-6,\n            min_dominant_ratio: 3.0,\n            min_correlation: 0.7,\n            phase_steps: 32,\n        };\n\n        let report =\n            verify_mlp_activation_waves_from_signals(&[(0, signal)], &config)\n                .expect(\"expected rectified sine to pass\");\n        assert_eq!(report.neuron_reports.len(), 1);\n    }\n\n    #[test]\n    fn mlp_activation_wave_verification_fails_on_noise() {\n        let n = ModularAdditionDataset::modulus();\n        let signal: Vec<f64> = (0..n).map(|idx| idx as f64 / n as f64).collect();\n\n        let config = MlpWaveVerificationConfig {\n            max_negative_fraction: 0.0,\n            negative_tolerance: 1e-6,\n            min_dominant_ratio: 4.0,\n            min_correlation: 0.8,\n            phase_steps: 16,\n        };\n\n        let err =\n            verify_mlp_activation_waves_from_signals(&[(0, signal)], &config)\n                .expect_err(\"expected linear signal to fail\");\n        assert!(err.contains(\"dominant frequency ratio\") || err.contains(\"correlation\"));\n    }\n\n    #[test]\n    fn pairwise_manifold_transition_passes_on_elliptic_signals() {\n        let n = ModularAdditionDataset::modulus();\n        let freq = 5.0;\n        let mut post_signals = Vec::new();\n        for neuron_idx in 0..7usize {\n            let phase = 2.0 * std::f64::consts::PI * neuron_idx as f64 / 7.0;\n            let signal: Vec<f64> = (0..n)\n                .map(|idx| {\n                    let angle = 2.0 * std::f64::consts::PI * freq * idx as f64 / n as f64 + phase;\n                    angle.sin()\n                })\n                .collect();\n            post_signals.push((neuron_idx, signal));\n        }\n\n        let mut pre_signals = Vec::new();\n        for neuron_idx in 0..7usize {\n            let signal: Vec<f64> = (0..n)\n                .map(|idx| idx as f64 / n as f64 + neuron_idx as f64 * 0.01)\n                .collect();\n            pre_signals.push((neuron_idx, signal));\n        }\n\n        let config = PairwiseManifoldConfig {\n            fixed_y: 0,\n            neuron_indices: (0..7).collect(),\n            min_post_score: 0.75,\n            max_pre_score: 0.55,\n            max_axis_ratio: 8.0,\n            min_pairs_passing: 8,\n        };\n\n        let report = verify_pairwise_manifold_transition_from_signals(\n            &pre_signals,\n            &post_signals,\n            &config,\n        )\n        .expect(\"expected pairwise manifold transition to pass\");\n        assert!(report.passing_pairs >= config.min_pairs_passing);\n    }\n\n    #[test]\n    fn pairwise_manifold_transition_fails_without_rings() {\n        let n = ModularAdditionDataset::modulus();\n        let mut signals = Vec::new();\n        for neuron_idx in 0..7usize {\n            let signal: Vec<f64> = (0..n)\n                .map(|idx| idx as f64 / n as f64 + neuron_idx as f64 * 0.02)\n                .collect();\n            signals.push((neuron_idx, signal));\n        }\n\n        let config = PairwiseManifoldConfig {\n            fixed_y: 0,\n            neuron_indices: (0..7).collect(),\n            min_post_score: 0.8,\n            max_pre_score: 0.55,\n            max_axis_ratio: 4.0,\n            min_pairs_passing: 5,\n        };\n\n        let err = verify_pairwise_manifold_transition_from_signals(\n            &signals,\n            &signals,\n            &config,\n        )\n        .expect_err(\"expected pairwise manifold transition to fail\");\n        assert!(err.contains(\"post-grok\") || err.contains(\"pre-grok\"));\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_passes_on_diagonal_corrugation() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create diagonal ridges: f(x,y) = sin(2Ï€ * freq * (x+y) / n)\n        let freq = 3.0;\n        for x in 0..n {\n            for y in 0..n {\n                let angle = 2.0 * std::f64::consts::PI * freq * (x + y) as f64 / n as f64;\n                surface[x][y] = angle.sin();\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let report = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect(\"expected diagonal ridge verification to pass\");\n        assert!(report.diagonal_score > report.axis_score);\n        assert!(report.ratio >= config.min_diagonal_ratio);\n        assert!(report.diagonal_score >= config.min_diagonal_score);\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_passes_on_anti_diagonal_pattern() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create anti-diagonal ridges: f(x,y) = sin(2Ï€ * freq * (x-y) / n)\n        let freq = 3.0;\n        for x in 0..n {\n            for y in 0..n {\n                let angle = 2.0 * std::f64::consts::PI * freq * (x as i32 - y as i32) as f64 / n as f64;\n                surface[x][y] = angle.sin();\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let report = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect(\"expected anti-diagonal ridge verification to pass\");\n        assert!(report.diagonal_score > report.axis_score);\n        assert!(report.ratio >= config.min_diagonal_ratio);\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_fails_on_axis_aligned_pattern() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create axis-aligned ridges: f(x,y) = sin(2Ï€ * freq * x / n)\n        let freq = 3.0;\n        for x in 0..n {\n            for y in 0..n {\n                let angle = 2.0 * std::f64::consts::PI * freq * x as f64 / n as f64;\n                surface[x][y] = angle.sin();\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let err = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect_err(\"expected axis-aligned pattern to fail diagonal verification\");\n        assert!(err.contains(\"diagonal/axis ratio\") || err.contains(\"diagonal autocorrelation\"));\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_fails_on_flat_surface() {\n        let n = 50;\n        let surface = vec![vec![1.5; n]; n];\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let err = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect_err(\"expected flat surface to fail\");\n        assert!(err.contains(\"surface variance is zero\"));\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_fails_on_random_noise() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create random noise (low autocorrelation everywhere)\n        for x in 0..n {\n            for y in 0..n {\n                surface[x][y] = ((x * 17 + y * 31) % 100) as f64 / 100.0;\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let err = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect_err(\"expected random noise to fail diagonal verification\");\n        assert!(err.contains(\"diagonal/axis ratio\") || err.contains(\"diagonal autocorrelation\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_passes_on_valid_grokking_curve() {\n        // Simulate typical grokking: train converges early, val plateaus then jumps\n        let train_acc = vec![\n            (1, 0.10),\n            (10, 0.50),\n            (100, 0.80),\n            (500, 0.98),\n            (1000, 0.99),\n            (2000, 0.995),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (10, 0.009),\n            (100, 0.01),\n            (500, 0.011),\n            (1000, 0.009),\n            (1500, 0.01),\n            (2000, 0.92),\n            (3000, 0.98),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 1500,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let report = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect(\"expected snake curve verification to pass\");\n\n        assert_eq!(report.train_converged_by_step, 500);\n        assert!(report.val_plateau_max_acc < config.val_plateau_max_acc);\n        assert_eq!(report.val_grok_step, 2000);\n        assert!(report.curve_shape_valid);\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_on_simultaneous_convergence() {\n        // Both train and val converge at the same time (no grokking)\n        let train_acc = vec![\n            (1, 0.10),\n            (100, 0.98),\n            (500, 0.99),\n        ];\n        let val_acc = vec![\n            (1, 0.10),\n            (100, 0.97),\n            (500, 0.98),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 200,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let err = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect_err(\"expected snake curve verification to fail\");\n        assert!(err.contains(\"plateau region too high\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_on_slow_train_convergence() {\n        // Train takes too long to converge (not the typical snake curve)\n        let train_acc = vec![\n            (1, 0.10),\n            (500, 0.50),\n            (1000, 0.70),\n            (2000, 0.98),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (500, 0.01),\n            (1000, 0.01),\n            (2000, 0.92),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 1500,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let err = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect_err(\"expected snake curve verification to fail\");\n        assert!(err.contains(\"converges at step 2000\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_without_val_jump() {\n        // Validation never jumps up (no generalization)\n        let train_acc = vec![\n            (1, 0.10),\n            (100, 0.98),\n            (500, 0.99),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (100, 0.01),\n            (500, 0.01),\n            (2000, 0.02),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 200,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let err = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect_err(\"expected snake curve verification to fail\");\n        assert!(err.contains(\"validation accuracy never reaches\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_on_wrong_phase_ordering() {\n        // Val jumps before train converges (backwards snake)\n        let train_acc = vec![\n            (1, 0.10),\n            (500, 0.50),\n            (2000, 0.98),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (100, 0.95),\n            (500, 0.98),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 200,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let result = verify_snake_curve_shape(&train_acc, &val_acc, &config);\n        // Should fail because val jumps before train converges (not a proper snake curve)\n        // This can fail for multiple reasons, but should definitely fail\n        assert!(result.is_err(), \"Expected snake curve verification to fail but it passed\");\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_passes_on_decreasing_norms() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 85.0),\n            (1000, 72.0),\n            (1500, 63.0),\n            (2000, 58.0),\n            (2500, 55.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let report = verify_weight_norm_decay(&weight_norms, Some(2000), &config)\n            .expect(\"expected weight norm decay verification to pass\");\n\n        assert_eq!(report.initial_norm, 100.0);\n        assert_eq!(report.final_norm, 55.0);\n        assert!((report.relative_decrease - 0.45).abs() < 0.01);\n        assert!(report.grok_step_norm.is_some());\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_fails_on_flat_norms() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 99.0),\n            (1000, 98.5),\n            (1500, 98.0),\n            (2000, 97.5),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let err = verify_weight_norm_decay(&weight_norms, None, &config)\n            .expect_err(\"expected weight norm decay verification to fail\");\n        assert!(err.contains(\"weight norm decrease\"));\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_fails_on_increasing_norms() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 95.0),\n            (1000, 105.0), // Increase here\n            (1500, 90.0),\n            (2000, 85.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let err = verify_weight_norm_decay(&weight_norms, None, &config)\n            .expect_err(\"expected weight norm decay verification to fail\");\n        assert!(err.contains(\"weight norm increased\"));\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_allows_small_noise() {\n        // Weight decay with small numerical noise (within 1% tolerance)\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 95.0),\n            (1000, 90.5),  // Small increase within tolerance\n            (1500, 90.0),\n            (2000, 85.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let report = verify_weight_norm_decay(&weight_norms, None, &config)\n            .expect(\"expected weight norm decay with small noise to pass\");\n\n        assert_eq!(report.initial_norm, 100.0);\n        assert_eq!(report.final_norm, 85.0);\n        assert!((report.relative_decrease - 0.15).abs() < 0.01);\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_finds_grok_step_norm() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (1000, 80.0),\n            (1800, 68.0),  // Within window of grok step (2200 - 500 = 1700)\n            (2000, 65.0),\n            (2200, 62.0),  // Around grok step\n            (3000, 55.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let report = verify_weight_norm_decay(&weight_norms, Some(2200), &config)\n            .expect(\"expected weight norm decay verification to pass\");\n\n        assert!(report.grok_step_norm.is_some());\n        let grok_norm = report.grok_step_norm.unwrap();\n        // Should find the first norm in the window [1700, 2700], which is 68.0 at step 1800\n        assert!((grok_norm - 68.0).abs() < 0.01);\n    }\n}\n","structuredPatch":[{"oldStart":97,"oldLines":6,"newStart":97,"newLines":7,"lines":[" }"," "," #[derive(Debug, Clone)]","+#[allow(dead_code)]"," pub struct MlpWaveNeuronReport {","     pub neuron_index: usize,","     pub dominant_frequency: usize,"]},{"oldStart":106,"oldLines":6,"newStart":107,"newLines":7,"lines":[" }"," "," #[derive(Debug, Clone)]","+#[allow(dead_code)]"," pub struct MlpWaveReport {","     pub neuron_reports: Vec<MlpWaveNeuronReport>,"," }"]}],"userModified":false,"replaceAll":false}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01Xj3EpqxAGYnmXU7tdN4gpY","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01VyxXcgK5yMoKHEAPUVAfYj","name":"Edit","input":{"replace_all":false,"file_path":"/Users/admin/MechInt/src/verify.rs","old_string":"#[derive(Debug, Clone)]\npub struct PairwiseManifoldPairReport {\n    pub neuron_a: usize,\n    pub neuron_b: usize,\n    pub circularity_score: f64,\n    pub radius_cv: f64,\n    pub axis_ratio: f64,\n}","new_string":"#[derive(Debug, Clone)]\n#[allow(dead_code)]\npub struct PairwiseManifoldPairReport {\n    pub neuron_a: usize,\n    pub neuron_b: usize,\n    pub circularity_score: f64,\n    pub radius_cv: f64,\n    pub axis_ratio: f64,\n}"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":677,"cache_read_input_tokens":75542,"cache_creation":{"ephemeral_5m_input_tokens":677,"ephemeral_1h_input_tokens":0},"output_tokens":284,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"75b54108-d82a-469f-bf57-fbebd973c6e3"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01VyxXcgK5yMoKHEAPUVAfYj","type":"tool_result","content":"The file /Users/admin/MechInt/src/verify.rs has been updated. Here's the result of running `cat -n` on a snippet of the edited file:\n   134â†’        }\n   135â†’    }\n   136â†’}\n   137â†’\n   138â†’#[derive(Debug, Clone)]\n   139â†’#[allow(dead_code)]\n   140â†’pub struct PairwiseManifoldPairReport {\n   141â†’    pub neuron_a: usize,\n   142â†’    pub neuron_b: usize,\n   143â†’    pub circularity_score: f64,\n   144â†’    pub radius_cv: f64,\n   145â†’    pub axis_ratio: f64,\n   146â†’}\n   147â†’\n   148â†’#[derive(Debug, Clone)]\n   149â†’pub struct PairwiseManifoldReport {\n   150â†’    pub pair_reports: Vec<PairwiseManifoldPairReport>,"}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"01848930-326e-4b0a-899c-c524c6c93929","tool_use_result":{"filePath":"/Users/admin/MechInt/src/verify.rs","oldString":"#[derive(Debug, Clone)]\npub struct PairwiseManifoldPairReport {\n    pub neuron_a: usize,\n    pub neuron_b: usize,\n    pub circularity_score: f64,\n    pub radius_cv: f64,\n    pub axis_ratio: f64,\n}","newString":"#[derive(Debug, Clone)]\n#[allow(dead_code)]\npub struct PairwiseManifoldPairReport {\n    pub neuron_a: usize,\n    pub neuron_b: usize,\n    pub circularity_score: f64,\n    pub radius_cv: f64,\n    pub axis_ratio: f64,\n}","originalFile":"use burn::{\n    data::dataset::Dataset,\n    tensor::{backend::Backend, Int, Tensor},\n};\nuse rustfft::{num_complex::Complex, FftPlanner};\n\nuse crate::analysis::{AccuracyHistory, LossHistory};\nuse crate::data::ModularAdditionDataset;\nuse crate::model::Transformer;\n\n#[derive(Debug, Clone)]\npub struct GrokkingVerificationConfig {\n    pub early_train_acc_threshold: f32,\n    pub early_step_max: usize,\n    pub plateau_min_step: usize,\n    pub generalization_window: usize,\n    pub chance_accuracy: f32,\n    pub chance_tolerance: f32,\n    pub target_val_acc_threshold: f32,\n    pub accuracy_window: usize,\n    pub loss_window: usize,\n    pub loss_drop_fraction: f64,\n}\n\nimpl GrokkingVerificationConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        Self {\n            early_train_acc_threshold: 0.99,\n            early_step_max: 1000,\n            plateau_min_step: 1000,\n            generalization_window: 10_000,\n            chance_accuracy: 1.0 / modulus as f32,\n            chance_tolerance: 0.02,\n            target_val_acc_threshold: 0.99,\n            accuracy_window: 5,\n            loss_window: 5,\n            loss_drop_fraction: 0.5,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\n#[allow(dead_code)]\npub struct GrokkingPhaseReport {\n    pub early_train_step: usize,\n    pub transition_step: usize,\n    pub plateau_max_val_acc: f32,\n    pub loss_drop_ratio: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossVerificationConfig {\n    pub plateau_min_step: usize,\n    pub drop_window: usize,\n    pub drop_fraction: f64,\n    pub min_step_lead: usize,\n}\n\n#[derive(Debug, Clone)]\n#[allow(dead_code)]\npub struct RestrictedLossReport {\n    pub restricted_drop_step: usize,\n    pub full_drop_step: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossDropConfig {\n    pub min_relative_decrease: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossDropReport {\n    pub drop_step: usize,\n    pub drop_loss: f64,\n    pub baseline_loss: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct ExcludedLossSpikeConfig {\n    pub min_relative_increase: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct ExcludedLossSpikeReport {\n    pub spike_step: usize,\n    pub spike_loss: f64,\n    pub baseline_loss: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct MlpWaveVerificationConfig {\n    pub max_negative_fraction: f64,\n    pub negative_tolerance: f64,\n    pub min_dominant_ratio: f64,\n    pub min_correlation: f64,\n    pub phase_steps: usize,\n}\n\n#[derive(Debug, Clone)]\n#[allow(dead_code)]\npub struct MlpWaveNeuronReport {\n    pub neuron_index: usize,\n    pub dominant_frequency: usize,\n    pub dominant_ratio: f64,\n    pub negative_fraction: f64,\n    pub best_correlation: f64,\n}\n\n#[derive(Debug, Clone)]\n#[allow(dead_code)]\npub struct MlpWaveReport {\n    pub neuron_reports: Vec<MlpWaveNeuronReport>,\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldConfig {\n    pub fixed_y: usize,\n    pub neuron_indices: Vec<usize>,\n    pub min_post_score: f64,\n    pub max_pre_score: f64,\n    pub max_axis_ratio: f64,\n    pub min_pairs_passing: usize,\n}\n\nimpl PairwiseManifoldConfig {\n    pub fn default_for_modulus(_modulus: usize) -> Self {\n        Self {\n            fixed_y: 0,\n            neuron_indices: (0..7).collect(),\n            min_post_score: 0.75,\n            max_pre_score: 0.55,\n            max_axis_ratio: 8.0,\n            min_pairs_passing: 10,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldPairReport {\n    pub neuron_a: usize,\n    pub neuron_b: usize,\n    pub circularity_score: f64,\n    pub radius_cv: f64,\n    pub axis_ratio: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldReport {\n    pub pair_reports: Vec<PairwiseManifoldPairReport>,\n    pub average_score: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldTransitionReport {\n    pub pre_average_score: f64,\n    pub post_average_score: f64,\n    pub passing_pairs: usize,\n    pub total_pairs: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct DiagonalRidgeConfig {\n    pub neuron_index: usize,\n    pub max_lag: usize,\n    pub min_diagonal_ratio: f64,\n    pub min_diagonal_score: f64,\n}\n\nimpl DiagonalRidgeConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        let max_lag = (modulus / 10).max(4).min(20);\n        Self {\n            neuron_index: 10,\n            max_lag,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct DiagonalRidgeReport {\n    pub neuron_index: usize,\n    pub diagonal_score: f64,\n    pub axis_score: f64,\n    pub ratio: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct SnakeCurveConfig {\n    pub early_step_max: usize,\n    pub train_convergence_threshold: f32,\n    pub val_plateau_min_step: usize,\n    pub val_plateau_max_acc: f32,\n    pub val_final_threshold: f32,\n}\n\nimpl SnakeCurveConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        Self {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 1000,\n            val_plateau_max_acc: 1.0 / modulus as f32 + 0.05,\n            val_final_threshold: 0.90,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct WeightNormDecayConfig {\n    pub min_relative_decrease: f64,\n    pub plateau_min_step: usize,\n    pub grok_step_window: usize,\n}\n\nimpl WeightNormDecayConfig {\n    pub fn default_for_grokking() -> Self {\n        Self {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct WeightNormDecayReport {\n    pub initial_norm: f64,\n    pub final_norm: f64,\n    pub relative_decrease: f64,\n    pub grok_step_norm: Option<f64>,\n}\n\n#[derive(Debug, Clone)]\npub struct SnakeCurveReport {\n    pub train_converged_by_step: usize,\n    pub val_plateau_max_acc: f32,\n    pub val_grok_step: usize,\n    pub curve_shape_valid: bool,\n}\n\npub fn verify_grokking_phase_transition(\n    loss_history: &LossHistory,\n    accuracy_history: &AccuracyHistory,\n    config: &GrokkingVerificationConfig,\n) -> Result<GrokkingPhaseReport, String> {\n    let (early_train_step, _) = accuracy_history\n        .train_snapshots\n        .iter()\n        .find(|(_, acc)| *acc >= config.early_train_acc_threshold)\n        .ok_or_else(|| \"train accuracy never reaches threshold\".to_string())?;\n\n    if *early_train_step > config.early_step_max {\n        return Err(format!(\n            \"train accuracy reaches {:.2} after step {}, expected before {}\",\n            config.early_train_acc_threshold, early_train_step, config.early_step_max\n        ));\n    }\n\n    let plateau_max_val_acc = accuracy_history\n        .val_snapshots\n        .iter()\n        .filter(|(step, _)| *step <= config.plateau_min_step)\n        .map(|(_, acc)| *acc)\n        .fold(None::<f32>, |accum, value| {\n            Some(accum.map_or(value, |max_val| max_val.max(value)))\n        })\n        .ok_or_else(|| \"no validation accuracy samples before plateau_min_step\".to_string())?;\n\n    if plateau_max_val_acc > config.chance_accuracy + config.chance_tolerance {\n        return Err(format!(\n            \"validation accuracy before step {} exceeds chance (max {:.4})\",\n            config.plateau_min_step, plateau_max_val_acc\n        ));\n    }\n\n    let (transition_step, _) = accuracy_history\n        .val_snapshots\n        .iter()\n        .find(|(step, acc)| {\n            *step >= config.plateau_min_step && *acc >= config.target_val_acc_threshold\n        })\n        .ok_or_else(|| \"validation accuracy never reaches target threshold\".to_string())?;\n\n    let latest_allowed = config.plateau_min_step + config.generalization_window;\n    if *transition_step > latest_allowed {\n        return Err(format!(\n            \"validation accuracy reaches {:.2} at step {}, expected before {}\",\n            config.target_val_acc_threshold, transition_step, latest_allowed\n        ));\n    }\n\n    verify_accuracy_jump(\n        &accuracy_history.val_snapshots,\n        *transition_step,\n        config.accuracy_window,\n        config.chance_accuracy,\n        config.chance_tolerance,\n        config.target_val_acc_threshold,\n    )?;\n\n    let loss_drop_ratio = verify_loss_drop(\n        &loss_history.val_snapshots,\n        *transition_step,\n        config.loss_window,\n        config.loss_drop_fraction,\n    )?;\n\n    Ok(GrokkingPhaseReport {\n        early_train_step: *early_train_step,\n        transition_step: *transition_step,\n        plateau_max_val_acc,\n        loss_drop_ratio,\n    })\n}\n\nfn verify_accuracy_jump(\n    val_accuracy: &[(usize, f32)],\n    transition_step: usize,\n    window: usize,\n    chance_accuracy: f32,\n    chance_tolerance: f32,\n    target_val_acc_threshold: f32,\n) -> Result<(), String> {\n    let before_values: Vec<f32> = val_accuracy\n        .iter()\n        .filter(|(step, _)| *step < transition_step)\n        .map(|(_, acc)| *acc)\n        .collect();\n\n    let after_values: Vec<f32> = val_accuracy\n        .iter()\n        .filter(|(step, _)| *step >= transition_step)\n        .map(|(_, acc)| *acc)\n        .collect();\n\n    if before_values.len() < window || after_values.len() < window {\n        return Err(\"insufficient accuracy samples around transition\".to_string());\n    }\n\n    let before_avg: f32 =\n        before_values[before_values.len() - window..].iter().sum::<f32>() / window as f32;\n    let after_avg: f32 = after_values[..window].iter().sum::<f32>() / window as f32;\n\n    if before_avg > chance_accuracy + chance_tolerance {\n        return Err(format!(\n            \"validation accuracy plateau too high before transition (avg {:.4})\",\n            before_avg\n        ));\n    }\n\n    if after_avg < target_val_acc_threshold {\n        return Err(format!(\n            \"validation accuracy after transition too low (avg {:.4})\",\n            after_avg\n        ));\n    }\n\n    Ok(())\n}\n\nfn verify_loss_drop(\n    val_losses: &[(usize, f64)],\n    transition_step: usize,\n    loss_window: usize,\n    loss_drop_fraction: f64,\n) -> Result<f64, String> {\n    let before_losses: Vec<f64> = val_losses\n        .iter()\n        .filter(|(step, _)| *step < transition_step)\n        .map(|(_, loss)| *loss)\n        .collect();\n\n    let after_losses: Vec<f64> = val_losses\n        .iter()\n        .filter(|(step, _)| *step >= transition_step)\n        .map(|(_, loss)| *loss)\n        .collect();\n\n    if before_losses.len() < loss_window || after_losses.len() < loss_window {\n        return Err(\"insufficient loss samples around transition\".to_string());\n    }\n\n    let before_avg: f64 =\n        before_losses[before_losses.len() - loss_window..].iter().sum::<f64>()\n            / loss_window as f64;\n    let after_avg: f64 = after_losses[..loss_window].iter().sum::<f64>() / loss_window as f64;\n\n    if after_avg > before_avg * (1.0 - loss_drop_fraction) {\n        return Err(format!(\n            \"validation loss drop too small (before {:.4}, after {:.4})\",\n            before_avg, after_avg\n        ));\n    }\n\n    Ok(after_avg / before_avg)\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_restricted_loss_early_drop(\n    full_losses: &[(usize, f64)],\n    restricted_losses: &[(usize, f64)],\n    config: &RestrictedLossVerificationConfig,\n) -> Result<RestrictedLossReport, String> {\n    let restricted_baseline =\n        baseline_loss(restricted_losses, config.plateau_min_step, config.drop_window)?;\n    let full_baseline = baseline_loss(full_losses, config.plateau_min_step, config.drop_window)?;\n\n    let restricted_drop_step = find_drop_step(\n        restricted_losses,\n        config.plateau_min_step,\n        restricted_baseline,\n        config.drop_fraction,\n    )?;\n    let full_drop_step = find_drop_step(\n        full_losses,\n        config.plateau_min_step,\n        full_baseline,\n        config.drop_fraction,\n    )?;\n\n    if restricted_drop_step + config.min_step_lead > full_drop_step {\n        return Err(format!(\n            \"restricted loss drops at step {}, expected at least {} steps before full loss (step {})\",\n            restricted_drop_step, config.min_step_lead, full_drop_step\n        ));\n    }\n\n    Ok(RestrictedLossReport {\n        restricted_drop_step,\n        full_drop_step,\n    })\n}\n\npub fn verify_restricted_loss_drop(\n    restricted_losses: &[(usize, f64)],\n    config: &RestrictedLossDropConfig,\n) -> Result<RestrictedLossDropReport, String> {\n    if restricted_losses.len() < 3 {\n        return Err(\"restricted loss history requires at least 3 points\".to_string());\n    }\n\n    let first = restricted_losses.first().ok_or_else(|| {\n        \"restricted loss history is empty\".to_string()\n    })?;\n    let baseline_loss = first.1;\n\n    let (min_idx, (min_step, min_loss)) = restricted_losses\n        .iter()\n        .enumerate()\n        .min_by(|(_, a), (_, b)| {\n            a.1.partial_cmp(&b.1).unwrap_or(std::cmp::Ordering::Equal)\n        })\n        .ok_or_else(|| \"restricted loss history is empty\".to_string())?;\n\n    if min_idx == 0 {\n        return Err(\"restricted loss minimum occurs at start (no drop detected)\".to_string());\n    }\n\n    let relative_decrease = (baseline_loss - min_loss) / baseline_loss;\n    if relative_decrease < config.min_relative_decrease {\n        return Err(format!(\n            \"restricted loss decrease too small (baseline {:.4}, min {:.4}, decrease {:.1}%)\",\n            baseline_loss, min_loss, relative_decrease * 100.0\n        ));\n    }\n\n    Ok(RestrictedLossDropReport {\n        drop_step: *min_step,\n        drop_loss: *min_loss,\n        baseline_loss,\n    })\n}\n\npub fn verify_excluded_loss_spike(\n    excluded_losses: &[(usize, f64)],\n    config: &ExcludedLossSpikeConfig,\n) -> Result<ExcludedLossSpikeReport, String> {\n    if excluded_losses.len() < 3 {\n        return Err(\"excluded loss history requires at least 3 points\".to_string());\n    }\n\n    let first = excluded_losses.first().ok_or_else(|| {\n        \"excluded loss history is empty\".to_string()\n    })?;\n    let last = excluded_losses.last().ok_or_else(|| {\n        \"excluded loss history is empty\".to_string()\n    })?;\n    let baseline_loss = (first.1 + last.1) / 2.0;\n\n    let (peak_idx, (peak_step, peak_loss)) = excluded_losses\n        .iter()\n        .enumerate()\n        .max_by(|(_, a), (_, b)| {\n            a.1.partial_cmp(&b.1).unwrap_or(std::cmp::Ordering::Equal)\n        })\n        .ok_or_else(|| \"excluded loss history is empty\".to_string())?;\n\n    if peak_idx == 0 || peak_idx + 1 == excluded_losses.len() {\n        return Err(\"excluded loss spike occurs at boundary\".to_string());\n    }\n\n    if peak_loss < &(baseline_loss * (1.0 + config.min_relative_increase)) {\n        return Err(format!(\n            \"excluded loss spike too small (baseline {:.4}, peak {:.4})\",\n            baseline_loss, peak_loss\n        ));\n    }\n\n    Ok(ExcludedLossSpikeReport {\n        spike_step: *peak_step,\n        spike_loss: *peak_loss,\n        baseline_loss,\n    })\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_mlp_activation_waves<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n    fixed_y: usize,\n    neuron_indices: &[usize],\n    config: &MlpWaveVerificationConfig,\n) -> Result<MlpWaveReport, String> {\n    let activations = collect_mlp_post_relu_activations(model, device, fixed_y)?;\n    if neuron_indices.is_empty() {\n        return Err(\"neuron indices list is empty\".to_string());\n    }\n\n    let d_ff = activations.len();\n    for &idx in neuron_indices {\n        if idx >= d_ff {\n            return Err(format!(\n                \"neuron index {} out of range (d_ff={})\",\n                idx, d_ff\n            ));\n        }\n    }\n\n    let signals: Vec<(usize, Vec<f64>)> = neuron_indices\n        .iter()\n        .map(|&idx| (idx, activations[idx].clone()))\n        .collect();\n    verify_mlp_activation_waves_from_signals(&signals, config)\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_mlp_activation_waves_from_signals(\n    signals: &[(usize, Vec<f64>)],\n    config: &MlpWaveVerificationConfig,\n) -> Result<MlpWaveReport, String> {\n    if signals.is_empty() {\n        return Err(\"no activation signals provided\".to_string());\n    }\n\n    let mut reports = Vec::with_capacity(signals.len());\n    for (neuron_index, signal) in signals {\n        let report = analyze_mlp_activation_wave(*neuron_index, signal, config)?;\n        reports.push(report);\n    }\n\n    Ok(MlpWaveReport {\n        neuron_reports: reports,\n    })\n}\n\npub fn verify_mlp_pairwise_manifold_transition<B: Backend>(\n    pre_model: &Transformer<B>,\n    post_model: &Transformer<B>,\n    device: &B::Device,\n    config: &PairwiseManifoldConfig,\n) -> Result<PairwiseManifoldTransitionReport, String> {\n    let modulus = ModularAdditionDataset::modulus();\n    if config.fixed_y >= modulus {\n        return Err(format!(\n            \"fixed_y {} out of range (modulus {})\",\n            config.fixed_y, modulus\n        ));\n    }\n\n    let pre_activations = collect_mlp_post_relu_activations(pre_model, device, config.fixed_y)?;\n    let post_activations = collect_mlp_post_relu_activations(post_model, device, config.fixed_y)?;\n\n    let d_ff = pre_activations.len();\n    if post_activations.len() != d_ff {\n        return Err(\"post activations shape mismatch\".to_string());\n    }\n\n    if config.neuron_indices.len() < 2 {\n        return Err(\"need at least two neuron indices\".to_string());\n    }\n\n    for &idx in &config.neuron_indices {\n        if idx >= d_ff {\n            return Err(format!(\n                \"neuron index {} out of range (d_ff={})\",\n                idx, d_ff\n            ));\n        }\n    }\n\n    let pre_signals: Vec<(usize, Vec<f64>)> = config\n        .neuron_indices\n        .iter()\n        .map(|&idx| (idx, pre_activations[idx].clone()))\n        .collect();\n    let post_signals: Vec<(usize, Vec<f64>)> = config\n        .neuron_indices\n        .iter()\n        .map(|&idx| (idx, post_activations[idx].clone()))\n        .collect();\n\n    verify_pairwise_manifold_transition_from_signals(&pre_signals, &post_signals, config)\n}\n\npub fn collect_mlp_activation_surface<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n    neuron_index: usize,\n) -> Result<Vec<Vec<f64>>, String> {\n    let modulus = ModularAdditionDataset::modulus();\n    let equals_token = ModularAdditionDataset::equals_token();\n\n    let mut inputs_vec = Vec::with_capacity(modulus * modulus * 3);\n    for x in 0..modulus {\n        for y in 0..modulus {\n            inputs_vec.push(x as i32);\n            inputs_vec.push(y as i32);\n            inputs_vec.push(equals_token as i32);\n        }\n    }\n\n    let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n        .reshape([modulus * modulus, 3]);\n    let (_, mlp_acts) = model.forward_with_mlp_activations(inputs);\n    let [batch_size, d_ff] = mlp_acts.dims();\n    if batch_size != modulus * modulus {\n        return Err(format!(\n            \"unexpected activation batch size {} (expected {})\",\n            batch_size,\n            modulus * modulus\n        ));\n    }\n    if neuron_index >= d_ff {\n        return Err(format!(\n            \"neuron index {} out of range (d_ff={})\",\n            neuron_index, d_ff\n        ));\n    }\n\n    let data: Vec<f32> = mlp_acts.into_data().to_vec().unwrap();\n    let mut surface = vec![vec![0.0f64; modulus]; modulus];\n    for x in 0..modulus {\n        for y in 0..modulus {\n            let batch = x * modulus + y;\n            let idx = batch * d_ff + neuron_index;\n            surface[x][y] = data[idx] as f64;\n        }\n    }\n\n    Ok(surface)\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_constructive_interference_surface_from_grid(\n    surface: &[Vec<f64>],\n    config: &DiagonalRidgeConfig,\n) -> Result<DiagonalRidgeReport, String> {\n    let (diag_main, diag_anti, axis_x, axis_y) =\n        surface_autocorrelation_scores(surface, config.max_lag)?;\n\n    let diagonal_score = diag_main.max(diag_anti);\n    let axis_score = axis_x.max(axis_y);\n    let ratio = if axis_score > 0.0 {\n        diagonal_score / axis_score\n    } else {\n        f64::INFINITY\n    };\n\n    if diagonal_score < config.min_diagonal_score {\n        return Err(format!(\n            \"diagonal autocorrelation {:.3} below threshold {:.3}\",\n            diagonal_score, config.min_diagonal_score\n        ));\n    }\n\n    if ratio < config.min_diagonal_ratio {\n        return Err(format!(\n            \"diagonal/axis ratio {:.3} below threshold {:.3}\",\n            ratio, config.min_diagonal_ratio\n        ));\n    }\n\n    Ok(DiagonalRidgeReport {\n        neuron_index: config.neuron_index,\n        diagonal_score,\n        axis_score,\n        ratio,\n    })\n}\n\npub fn verify_pairwise_manifold_transition_from_signals(\n    pre_signals: &[(usize, Vec<f64>)],\n    post_signals: &[(usize, Vec<f64>)],\n    config: &PairwiseManifoldConfig,\n) -> Result<PairwiseManifoldTransitionReport, String> {\n    let pre_report = evaluate_pairwise_manifolds_from_signals(pre_signals)?;\n    let post_report = evaluate_pairwise_manifolds_from_signals(post_signals)?;\n\n    if pre_report.average_score > config.max_pre_score {\n        return Err(format!(\n            \"pre-grok average circularity {:.3} exceeds threshold {:.3}\",\n            pre_report.average_score, config.max_pre_score\n        ));\n    }\n\n    let passing_pairs = post_report\n        .pair_reports\n        .iter()\n        .filter(|pair| {\n            pair.circularity_score >= config.min_post_score\n                && pair.axis_ratio <= config.max_axis_ratio\n        })\n        .count();\n    if passing_pairs < config.min_pairs_passing {\n        return Err(format!(\n            \"post-grok only {} pairs meet circularity threshold (need {})\",\n            passing_pairs, config.min_pairs_passing\n        ));\n    }\n\n    Ok(PairwiseManifoldTransitionReport {\n        pre_average_score: pre_report.average_score,\n        post_average_score: post_report.average_score,\n        passing_pairs,\n        total_pairs: post_report.pair_reports.len(),\n    })\n}\n\nfn evaluate_pairwise_manifolds_from_signals(\n    signals: &[(usize, Vec<f64>)],\n) -> Result<PairwiseManifoldReport, String> {\n    if signals.len() < 2 {\n        return Err(\"need at least two activation signals\".to_string());\n    }\n\n    let signal_len = signals[0].1.len();\n    if signal_len < 3 {\n        return Err(\"activation signal too short\".to_string());\n    }\n\n    for (idx, signal) in signals {\n        if signal.len() != signal_len {\n            return Err(format!(\n                \"signal length mismatch for neuron {}\",\n                idx\n            ));\n        }\n    }\n\n    let mut reports = Vec::new();\n    for i in 0..signals.len() {\n        for j in (i + 1)..signals.len() {\n            let (neuron_a, signal_a) = &signals[i];\n            let (neuron_b, signal_b) = &signals[j];\n            let points: Vec<(f64, f64)> = signal_a\n                .iter()\n                .zip(signal_b.iter())\n                .map(|(x, y)| (*x, *y))\n                .collect();\n            let (score, radius_cv, axis_ratio) =\n                match pairwise_circularity_metrics(&points) {\n                    Ok(metrics) => metrics,\n                    Err(_) => (0.0, 1.0, f64::INFINITY),\n                };\n            reports.push(PairwiseManifoldPairReport {\n                neuron_a: *neuron_a,\n                neuron_b: *neuron_b,\n                circularity_score: score,\n                radius_cv,\n                axis_ratio,\n            });\n        }\n    }\n\n    let average_score = reports\n        .iter()\n        .map(|pair| pair.circularity_score)\n        .sum::<f64>()\n        / reports.len() as f64;\n\n    Ok(PairwiseManifoldReport {\n        pair_reports: reports,\n        average_score,\n    })\n}\n\nfn pairwise_circularity_metrics(points: &[(f64, f64)]) -> Result<(f64, f64, f64), String> {\n    if points.len() < 3 {\n        return Err(\"not enough points for circularity\".to_string());\n    }\n\n    let n = points.len() as f64;\n    let mean_x = points.iter().map(|(x, _)| *x).sum::<f64>() / n;\n    let mean_y = points.iter().map(|(_, y)| *y).sum::<f64>() / n;\n\n    let mut var_x = 0.0;\n    let mut var_y = 0.0;\n    let mut cov_xy = 0.0;\n    for (x, y) in points {\n        let dx = x - mean_x;\n        let dy = y - mean_y;\n        var_x += dx * dx;\n        var_y += dy * dy;\n        cov_xy += dx * dy;\n    }\n    var_x /= n;\n    var_y /= n;\n    cov_xy /= n;\n\n    let trace = var_x + var_y;\n    let det = var_x * var_y - cov_xy * cov_xy;\n    if det <= 1.0e-12 || trace <= 0.0 {\n        return Err(\"degenerate covariance\".to_string());\n    }\n\n    let half_trace = trace * 0.5;\n    let disc = (half_trace * half_trace - det).max(0.0).sqrt();\n    let lambda1 = half_trace + disc;\n    let lambda2 = half_trace - disc;\n    if lambda2 <= 0.0 {\n        return Err(\"degenerate eigenvalues\".to_string());\n    }\n\n    let axis_ratio = (lambda1 / lambda2).sqrt();\n\n    let mut v1_x = cov_xy;\n    let mut v1_y = lambda1 - var_x;\n    if v1_x.abs() < 1.0e-12 && v1_y.abs() < 1.0e-12 {\n        v1_x = 1.0;\n        v1_y = 0.0;\n    }\n    let norm = (v1_x * v1_x + v1_y * v1_y).sqrt();\n    v1_x /= norm;\n    v1_y /= norm;\n    let v2_x = -v1_y;\n    let v2_y = v1_x;\n\n    let mut radii = Vec::with_capacity(points.len());\n    for (x, y) in points {\n        let dx = x - mean_x;\n        let dy = y - mean_y;\n        let p1 = dx * v1_x + dy * v1_y;\n        let p2 = dx * v2_x + dy * v2_y;\n        let w1 = p1 / lambda1.sqrt();\n        let w2 = p2 / lambda2.sqrt();\n        radii.push((w1 * w1 + w2 * w2).sqrt());\n    }\n\n    let mean_radius = radii.iter().sum::<f64>() / radii.len() as f64;\n    if mean_radius == 0.0 {\n        return Err(\"zero mean radius\".to_string());\n    }\n\n    let mut variance = 0.0;\n    for r in &radii {\n        let diff = r - mean_radius;\n        variance += diff * diff;\n    }\n    let std = (variance / radii.len() as f64).sqrt();\n    let radius_cv = std / mean_radius;\n    let score = 1.0 / (1.0 + radius_cv);\n\n    Ok((score, radius_cv, axis_ratio))\n}\n\n// Helper functions for verification - reserved for future integration\n#[allow(dead_code)]\nfn baseline_loss(\n    losses: &[(usize, f64)],\n    plateau_min_step: usize,\n    drop_window: usize,\n) -> Result<f64, String> {\n    let before: Vec<f64> = losses\n        .iter()\n        .filter(|(step, _)| *step <= plateau_min_step)\n        .map(|(_, loss)| *loss)\n        .collect();\n\n    if before.len() < drop_window || drop_window == 0 {\n        return Err(\"insufficient loss samples for baseline\".to_string());\n    }\n\n    let window = &before[before.len() - drop_window..];\n    Ok(window.iter().sum::<f64>() / drop_window as f64)\n}\n\n#[allow(dead_code)]\nfn find_drop_step(\n    losses: &[(usize, f64)],\n    plateau_min_step: usize,\n    baseline: f64,\n    drop_fraction: f64,\n) -> Result<usize, String> {\n    losses\n        .iter()\n        .filter(|(step, _)| *step >= plateau_min_step)\n        .find(|(_, loss)| *loss <= baseline * (1.0 - drop_fraction))\n        .map(|(step, _)| *step)\n        .ok_or_else(|| \"loss never drops below threshold\".to_string())\n}\n\nfn collect_mlp_post_relu_activations<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n    fixed_y: usize,\n) -> Result<Vec<Vec<f64>>, String> {\n    let modulus = ModularAdditionDataset::modulus();\n    let equals_token = ModularAdditionDataset::equals_token();\n\n    let mut inputs_vec = Vec::with_capacity(modulus * 3);\n    for x in 0..modulus {\n        inputs_vec.push(x as i32);\n        inputs_vec.push(fixed_y as i32);\n        inputs_vec.push(equals_token as i32);\n    }\n\n    let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n        .reshape([modulus, 3]);\n    let (_, mlp_acts) = model.forward_with_mlp_activations(inputs);\n    let [batch_size, d_ff] = mlp_acts.dims();\n\n    if batch_size != modulus {\n        return Err(format!(\n            \"unexpected activation batch size {} (expected {})\",\n            batch_size, modulus\n        ));\n    }\n\n    let data: Vec<f32> = mlp_acts.into_data().to_vec().unwrap();\n    let mut per_neuron = vec![Vec::with_capacity(batch_size); d_ff];\n    for batch in 0..batch_size {\n        for neuron in 0..d_ff {\n            let idx = batch * d_ff + neuron;\n            per_neuron[neuron].push(data[idx] as f64);\n        }\n    }\n\n    Ok(per_neuron)\n}\n\n#[allow(dead_code)]\nfn surface_autocorrelation_scores(\n    surface: &[Vec<f64>],\n    max_lag: usize,\n) -> Result<(f64, f64, f64, f64), String> {\n    let (height, width) = surface_dimensions(surface)?;\n    if height < 2 || width < 2 {\n        return Err(\"surface grid too small\".to_string());\n    }\n\n    let (mean, var) = surface_mean_variance(surface)?;\n    if var == 0.0 {\n        return Err(\"surface variance is zero\".to_string());\n    }\n\n    let max_lag = max_lag.min(height.saturating_sub(1)).min(width.saturating_sub(1));\n    if max_lag == 0 {\n        return Err(\"max_lag too small for surface\".to_string());\n    }\n\n    let diag_main = average_autocorrelation(surface, mean, var, max_lag, 1, 1)?;\n    let diag_anti = average_autocorrelation(surface, mean, var, max_lag, 1, -1)?;\n    let axis_x = average_autocorrelation(surface, mean, var, max_lag, 1, 0)?;\n    let axis_y = average_autocorrelation(surface, mean, var, max_lag, 0, 1)?;\n\n    Ok((diag_main, diag_anti, axis_x, axis_y))\n}\n\n#[allow(dead_code)]\nfn surface_dimensions(surface: &[Vec<f64>]) -> Result<(usize, usize), String> {\n    let height = surface.len();\n    if height == 0 {\n        return Err(\"surface is empty\".to_string());\n    }\n    let width = surface[0].len();\n    if width == 0 {\n        return Err(\"surface has empty rows\".to_string());\n    }\n    for (idx, row) in surface.iter().enumerate() {\n        if row.len() != width {\n            return Err(format!(\n                \"surface row {} has length {}, expected {}\",\n                idx,\n                row.len(),\n                width\n            ));\n        }\n    }\n    Ok((height, width))\n}\n\n#[allow(dead_code)]\nfn surface_mean_variance(surface: &[Vec<f64>]) -> Result<(f64, f64), String> {\n    let (height, width) = surface_dimensions(surface)?;\n    let mut sum = 0.0;\n    for row in surface {\n        for value in row {\n            sum += *value;\n        }\n    }\n    let count = (height * width) as f64;\n    let mean = sum / count;\n    let mut variance = 0.0;\n    for row in surface {\n        for value in row {\n            let diff = *value - mean;\n            variance += diff * diff;\n        }\n    }\n    variance /= count;\n    Ok((mean, variance))\n}\n\n#[allow(dead_code)]\nfn average_autocorrelation(\n    surface: &[Vec<f64>],\n    mean: f64,\n    variance: f64,\n    max_lag: usize,\n    dx: i32,\n    dy: i32,\n) -> Result<f64, String> {\n    let (height, width) = surface_dimensions(surface)?;\n    let mut total = 0.0;\n    let mut lag_count = 0usize;\n\n    for lag in 1..=max_lag {\n        let mut sum = 0.0;\n        let mut count = 0usize;\n        let lag = lag as i32;\n        for x in 0..height {\n            for y in 0..width {\n                let x2 = x as i32 + dx * lag;\n                let y2 = y as i32 + dy * lag;\n                if x2 < 0 || y2 < 0 {\n                    continue;\n                }\n                let x2 = x2 as usize;\n                let y2 = y2 as usize;\n                if x2 >= height || y2 >= width {\n                    continue;\n                }\n                let a = surface[x][y] - mean;\n                let b = surface[x2][y2] - mean;\n                sum += a * b;\n                count += 1;\n            }\n        }\n        if count == 0 {\n            continue;\n        }\n        let corr = sum / (count as f64 * variance);\n        total += corr.abs();\n        lag_count += 1;\n    }\n\n    if lag_count == 0 {\n        return Err(\"no valid lags for autocorrelation\".to_string());\n    }\n\n    Ok(total / lag_count as f64)\n}\n\n#[allow(dead_code)]\nfn analyze_mlp_activation_wave(\n    neuron_index: usize,\n    signal: &[f64],\n    config: &MlpWaveVerificationConfig,\n) -> Result<MlpWaveNeuronReport, String> {\n    if signal.len() < 3 {\n        return Err(format!(\n            \"activation signal for neuron {} is too short\",\n            neuron_index\n        ));\n    }\n\n    let negative_count = signal\n        .iter()\n        .filter(|value| **value < -config.negative_tolerance)\n        .count();\n    let negative_fraction = negative_count as f64 / signal.len() as f64;\n    if negative_fraction > config.max_negative_fraction {\n        return Err(format!(\n            \"neuron {} has {:.3} negative activations\",\n            neuron_index, negative_fraction\n        ));\n    }\n\n    let (dominant_frequency, dominant_ratio) = dominant_frequency_ratio(signal)?;\n    if dominant_ratio < config.min_dominant_ratio {\n        return Err(format!(\n            \"neuron {} dominant frequency ratio {:.3} below threshold {:.3}\",\n            neuron_index, dominant_ratio, config.min_dominant_ratio\n        ));\n    }\n\n    let best_correlation =\n        best_rectified_sine_correlation(signal, dominant_frequency, config.phase_steps);\n    if best_correlation < config.min_correlation {\n        return Err(format!(\n            \"neuron {} rectified sine correlation {:.3} below threshold {:.3}\",\n            neuron_index, best_correlation, config.min_correlation\n        ));\n    }\n\n    Ok(MlpWaveNeuronReport {\n        neuron_index,\n        dominant_frequency,\n        dominant_ratio,\n        negative_fraction,\n        best_correlation,\n    })\n}\n\n#[allow(dead_code)]\nfn dominant_frequency_ratio(signal: &[f64]) -> Result<(usize, f64), String> {\n    if signal.len() < 2 {\n        return Err(\"signal too short for FFT\".to_string());\n    }\n\n    let mut planner = FftPlanner::new();\n    let fft = planner.plan_fft_forward(signal.len());\n    let mut buffer: Vec<Complex<f64>> = signal\n        .iter()\n        .map(|&x| Complex::new(x, 0.0))\n        .collect();\n    fft.process(&mut buffer);\n    let magnitudes: Vec<f64> = buffer.iter().map(|c| c.norm()).collect();\n\n    if magnitudes.len() < 2 {\n        return Err(\"insufficient FFT bins\".to_string());\n    }\n\n    let mut dominant_idx = 1usize;\n    let mut dominant_mag = magnitudes[1];\n    for (idx, &mag) in magnitudes.iter().enumerate().skip(2) {\n        if mag > dominant_mag {\n            dominant_mag = mag;\n            dominant_idx = idx;\n        }\n    }\n\n    if dominant_idx == 0 {\n        return Err(\"dominant frequency is zero\".to_string());\n    }\n\n    let mut nonzero = magnitudes[1..].to_vec();\n    nonzero.sort_by(|a, b| a.partial_cmp(b).unwrap_or(std::cmp::Ordering::Equal));\n    let median_mag = nonzero[nonzero.len() / 2];\n    let ratio = if median_mag == 0.0 {\n        dominant_mag\n    } else {\n        dominant_mag / median_mag\n    };\n\n    Ok((dominant_idx, ratio))\n}\n\n#[allow(dead_code)]\nfn best_rectified_sine_correlation(signal: &[f64], frequency: usize, phase_steps: usize) -> f64 {\n    if signal.is_empty() || frequency == 0 || phase_steps == 0 {\n        return 0.0;\n    }\n\n    let tau = std::f64::consts::PI * 2.0;\n    let mut best = -1.0;\n    for step in 0..phase_steps {\n        let phase = tau * step as f64 / phase_steps as f64;\n        let wave: Vec<f64> = (0..signal.len())\n            .map(|idx| {\n                let angle = tau * frequency as f64 * idx as f64 / signal.len() as f64 + phase;\n                angle.sin().max(0.0)\n            })\n            .collect();\n        let corr = pearson_correlation(signal, &wave);\n        if corr > best {\n            best = corr;\n        }\n    }\n\n    best\n}\n\n#[allow(dead_code)]\nfn pearson_correlation(a: &[f64], b: &[f64]) -> f64 {\n    if a.len() != b.len() || a.is_empty() {\n        return 0.0;\n    }\n\n    let mean_a = a.iter().sum::<f64>() / a.len() as f64;\n    let mean_b = b.iter().sum::<f64>() / b.len() as f64;\n\n    let mut num = 0.0;\n    let mut denom_a = 0.0;\n    let mut denom_b = 0.0;\n    for (x, y) in a.iter().zip(b.iter()) {\n        let da = x - mean_a;\n        let db = y - mean_b;\n        num += da * db;\n        denom_a += da * da;\n        denom_b += db * db;\n    }\n\n    if denom_a == 0.0 || denom_b == 0.0 {\n        return 0.0;\n    }\n\n    num / (denom_a.sqrt() * denom_b.sqrt())\n}\n\n/// Verify that the model actually learned modular addition\n/// by testing it on ALL possible examples systematically\npub fn verify_full_accuracy<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n) -> (f32, f32) {\n    println!(\"ðŸ” Verifying model on COMPLETE dataset (no sampling)...\");\n\n    let train_dataset = ModularAdditionDataset::new(true, 42);\n    let val_dataset = ModularAdditionDataset::new(false, 42);\n\n    let train_acc = test_all_examples(model, &train_dataset, device);\n    let val_acc = test_all_examples(model, &val_dataset, device);\n\n    println!(\"ðŸ“Š FULL Dataset Results:\");\n    println!(\"   Training: {}/{} = {:.2}%\",\n        (train_acc * train_dataset.len() as f32) as usize,\n        train_dataset.len(),\n        train_acc * 100.0);\n    println!(\"   Validation: {}/{} = {:.2}%\",\n        (val_acc * val_dataset.len() as f32) as usize,\n        val_dataset.len(),\n        val_acc * 100.0);\n\n    (train_acc, val_acc)\n}\n\n/// Test model on ALL examples in dataset (no random sampling)\nfn test_all_examples<B: Backend>(\n    model: &Transformer<B>,\n    dataset: &ModularAdditionDataset,\n    device: &B::Device,\n) -> f32 {\n    let mut total_correct = 0;\n    let total = dataset.len();\n\n    // Test in batches to avoid memory issues\n    let batch_size = 100;\n    for batch_start in (0..total).step_by(batch_size) {\n        let batch_end = (batch_start + batch_size).min(total);\n        let batch_len = batch_end - batch_start;\n\n        let mut inputs_vec = Vec::new();\n        let mut targets_vec = Vec::new();\n\n        for idx in batch_start..batch_end {\n            let (input, target) = dataset.get(idx).unwrap();\n            inputs_vec.extend(input);\n            targets_vec.push(target as i32);\n        }\n\n        let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n            .reshape([batch_len, 3]);\n\n        let logits = model.forward(inputs);\n        let predictions = logits.argmax(1).squeeze::<1>();\n        let predictions_vec: Vec<i32> = predictions.into_data().to_vec().unwrap();\n\n        let correct = predictions_vec\n            .iter()\n            .zip(targets_vec.iter())\n            .filter(|(p, t)| p == t)\n            .count();\n\n        total_correct += correct;\n    }\n\n    total_correct as f32 / total as f32\n}\n\n/// Verify that weight norms decrease over time with weight decay\n/// and show correlation with the grokking transition\npub fn verify_weight_norm_decay(\n    weight_norm_history: &[(usize, f64)],\n    grok_step: Option<usize>,\n    config: &WeightNormDecayConfig,\n) -> Result<WeightNormDecayReport, String> {\n    if weight_norm_history.len() < 2 {\n        return Err(\"weight norm history requires at least 2 snapshots\".to_string());\n    }\n\n    let initial_norm = weight_norm_history\n        .first()\n        .ok_or_else(|| \"weight norm history is empty\".to_string())?\n        .1;\n    let final_norm = weight_norm_history\n        .last()\n        .ok_or_else(|| \"weight norm history is empty\".to_string())?\n        .1;\n\n    if initial_norm <= 0.0 {\n        return Err(\"initial weight norm must be positive\".to_string());\n    }\n\n    // Check that norms are generally decreasing (with weight decay)\n    let relative_decrease = (initial_norm - final_norm) / initial_norm;\n    if relative_decrease < config.min_relative_decrease {\n        return Err(format!(\n            \"weight norm decrease {:.4} below threshold {:.4} (initial {:.4}, final {:.4})\",\n            relative_decrease, config.min_relative_decrease, initial_norm, final_norm\n        ));\n    }\n\n    // Check that norms are not increasing (weight decay should monotonically decrease norms)\n    for window in weight_norm_history.windows(2) {\n        let prev_norm = window[0].1;\n        let next_norm = window[1].1;\n        if next_norm > prev_norm * 1.01 {\n            // Allow 1% tolerance for numerical noise\n            return Err(format!(\n                \"weight norm increased from {:.4} to {:.4} at step {}\",\n                prev_norm, next_norm, window[1].0\n            ));\n        }\n    }\n\n    // If grok step is provided, check that norms show a shift around grokking\n    let grok_step_norm = if let Some(step) = grok_step {\n        weight_norm_history\n            .iter()\n            .filter(|(s, _)| {\n                *s >= step.saturating_sub(config.grok_step_window)\n                    && *s <= step + config.grok_step_window\n            })\n            .map(|(_, norm)| *norm)\n            .next()\n    } else {\n        None\n    };\n\n    Ok(WeightNormDecayReport {\n        initial_norm,\n        final_norm,\n        relative_decrease,\n        grok_step_norm,\n    })\n}\n\n/// Verify the characteristic \"snake curve\" shape of grokking phenomenon\n/// on log-scale plots: train accuracy rises early, validation stays flat then jumps\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_snake_curve_shape(\n    train_acc: &[(usize, f32)],\n    val_acc: &[(usize, f32)],\n    config: &SnakeCurveConfig,\n) -> Result<SnakeCurveReport, String> {\n    if train_acc.is_empty() || val_acc.is_empty() {\n        return Err(\"empty accuracy history\".to_string());\n    }\n\n    // Phase 1: Verify early train convergence (immediate drop on log scale)\n    let train_converged_step = train_acc\n        .iter()\n        .find(|(_, acc)| *acc >= config.train_convergence_threshold)\n        .map(|(step, _)| *step)\n        .ok_or_else(|| {\n            format!(\n                \"train accuracy never reaches {} threshold\",\n                config.train_convergence_threshold\n            )\n        })?;\n\n    if train_converged_step > config.early_step_max {\n        return Err(format!(\n            \"train accuracy converges at step {}, expected before {}\",\n            train_converged_step, config.early_step_max\n        ));\n    }\n\n    // Phase 2: Verify validation plateau (flat on log scale)\n    let plateau_samples: Vec<f32> = val_acc\n        .iter()\n        .filter(|(step, _)| *step >= 1 && *step <= config.val_plateau_min_step)\n        .map(|(_, acc)| *acc)\n        .collect();\n\n    if plateau_samples.is_empty() {\n        return Err(\"no validation samples in plateau region\".to_string());\n    }\n\n    let val_plateau_max = plateau_samples\n        .iter()\n        .fold(0.0f32, |max_val, &acc| max_val.max(acc));\n\n    if val_plateau_max > config.val_plateau_max_acc {\n        return Err(format!(\n            \"validation accuracy in plateau region too high: max {:.4} exceeds {:.4}\",\n            val_plateau_max, config.val_plateau_max_acc\n        ));\n    }\n\n    // Phase 3: Verify delayed validation jump (crash up on log scale)\n    let val_grok_step = val_acc\n        .iter()\n        .filter(|(step, _)| *step > config.val_plateau_min_step)\n        .find(|(_, acc)| *acc >= config.val_final_threshold)\n        .map(|(step, _)| *step)\n        .ok_or_else(|| {\n            format!(\n                \"validation accuracy never reaches {} after plateau\",\n                config.val_final_threshold\n            )\n        })?;\n\n    // Phase 4: Verify phase ordering (snake shape: early train, late val)\n    let curve_shape_valid = train_converged_step < config.val_plateau_min_step\n        && val_grok_step > config.val_plateau_min_step\n        && train_converged_step < val_grok_step;\n\n    if !curve_shape_valid {\n        return Err(format!(\n            \"invalid curve shape: train converged at {}, val plateau until {}, val grokked at {}\",\n            train_converged_step, config.val_plateau_min_step, val_grok_step\n        ));\n    }\n\n    Ok(SnakeCurveReport {\n        train_converged_by_step: train_converged_step,\n        val_plateau_max_acc: val_plateau_max,\n        val_grok_step,\n        curve_shape_valid,\n    })\n}\n\n/// Test specific examples to see if model actually computes mod p\npub fn test_specific_examples<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n) {\n    println!();\n    println!(\"ðŸ§ª Testing specific examples:\");\n\n    let modulus = ModularAdditionDataset::modulus();\n    let equals_token = ModularAdditionDataset::equals_token();\n    let test_cases = vec![\n        (0, 0, 0),\n        (1, 1, 2),\n        (50, 50, (50 + 50) % modulus),\n        (112, 1, (112 + 1) % modulus),\n        (56, 57, (56 + 57) % modulus),\n        (112, 112, (112 + 112) % modulus),\n        (10, 20, 30),\n        (60, 60, (60 + 60) % modulus),\n    ];\n\n    for (a, b, expected) in test_cases {\n        let input_vec = vec![a as i32, b as i32, equals_token as i32];\n        let input = Tensor::<B, 1, Int>::from_ints(input_vec.as_slice(), device)\n            .reshape([1, 3]);\n\n        let logits = model.forward(input);\n        let prediction = logits.argmax(1).squeeze::<1>();\n        let pred_value: i32 = prediction.into_data().to_vec().unwrap()[0];\n\n        let correct = if pred_value == expected as i32 { \"âœ“\" } else { \"âœ—\" };\n        println!(\n            \"   {} + {} mod {} = {} | Model: {} {}\",\n            a, b, modulus, expected, pred_value, correct\n        );\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    fn make_histories(\n        train_acc: &[(usize, f32)],\n        val_acc: &[(usize, f32)],\n        val_loss: &[(usize, f64)],\n    ) -> (LossHistory, AccuracyHistory) {\n        let mut loss_history = LossHistory::new();\n        loss_history.val_snapshots = val_loss.to_vec();\n        let mut accuracy_history = AccuracyHistory::new();\n        accuracy_history.train_snapshots = train_acc.to_vec();\n        accuracy_history.val_snapshots = val_acc.to_vec();\n        (loss_history, accuracy_history)\n    }\n\n    #[test]\n    fn grokking_phase_verification_passes_on_synthetic_signal() {\n        let train_acc = vec![(0, 0.2), (500, 0.995), (1000, 0.999)];\n        let val_acc = vec![\n            (0, 0.01),\n            (500, 0.01),\n            (1500, 0.01),\n            (2200, 0.995),\n            (2500, 0.999),\n        ];\n        let val_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 2.0),\n            (2200, 0.2),\n            (2500, 0.15),\n        ];\n\n        let (loss_history, accuracy_history) = make_histories(&train_acc, &val_acc, &val_loss);\n        let config = GrokkingVerificationConfig {\n            early_step_max: 1000,\n            plateau_min_step: 1500,\n            generalization_window: 1500,\n            accuracy_window: 2,\n            loss_window: 2,\n            ..GrokkingVerificationConfig::default_for_modulus(ModularAdditionDataset::modulus())\n        };\n\n        let report = verify_grokking_phase_transition(&loss_history, &accuracy_history, &config)\n            .expect(\"expected grokking verification to pass\");\n        assert!(report.transition_step >= config.plateau_min_step);\n        assert!(report.early_train_step <= config.early_step_max);\n    }\n\n    #[test]\n    fn grokking_phase_verification_fails_without_transition() {\n        let train_acc = vec![(0, 0.2), (500, 0.995)];\n        let val_acc = vec![(0, 0.01), (500, 0.01), (1500, 0.02)];\n        let val_loss = vec![(0, 2.0), (500, 2.0), (1500, 1.9)];\n\n        let (loss_history, accuracy_history) = make_histories(&train_acc, &val_acc, &val_loss);\n        let config = GrokkingVerificationConfig {\n            early_step_max: 1000,\n            plateau_min_step: 1000,\n            generalization_window: 1000,\n            accuracy_window: 2,\n            loss_window: 2,\n            ..GrokkingVerificationConfig::default_for_modulus(ModularAdditionDataset::modulus())\n        };\n\n        let err = verify_grokking_phase_transition(&loss_history, &accuracy_history, &config)\n            .expect_err(\"expected grokking verification to fail\");\n        assert!(err.contains(\"validation accuracy never reaches target\"));\n    }\n\n    #[test]\n    fn grokking_phase_verification_fails_with_slow_accuracy_rise() {\n        let train_acc = vec![(0, 0.2), (500, 0.995), (1000, 0.999)];\n        let val_acc = vec![\n            (0, 0.01),\n            (500, 0.01),\n            (1500, 0.5),\n            (2000, 0.6),\n            (2200, 0.995),\n            (2500, 0.999),\n        ];\n        let val_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 2.0),\n            (2200, 0.2),\n            (2500, 0.15),\n        ];\n\n        let (loss_history, accuracy_history) = make_histories(&train_acc, &val_acc, &val_loss);\n        let config = GrokkingVerificationConfig {\n            early_step_max: 1000,\n            plateau_min_step: 1000,\n            generalization_window: 2000,\n            accuracy_window: 2,\n            loss_window: 2,\n            ..GrokkingVerificationConfig::default_for_modulus(ModularAdditionDataset::modulus())\n        };\n\n        let err = verify_grokking_phase_transition(&loss_history, &accuracy_history, &config)\n            .expect_err(\"expected grokking verification to fail\");\n        assert!(err.contains(\"plateau too high\"));\n    }\n\n    #[test]\n    fn restricted_loss_detection_passes_with_early_drop() {\n        let full_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 2.0),\n            (2500, 2.0),\n            (3500, 0.3),\n        ];\n        let restricted_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 0.8),\n            (2500, 0.4),\n            (3500, 0.3),\n        ];\n\n        let config = RestrictedLossVerificationConfig {\n            plateau_min_step: 1000,\n            drop_window: 2,\n            drop_fraction: 0.5,\n            min_step_lead: 500,\n        };\n\n        let report = verify_restricted_loss_early_drop(&full_loss, &restricted_loss, &config)\n            .expect(\"expected restricted loss verification to pass\");\n        assert!(report.restricted_drop_step < report.full_drop_step);\n    }\n\n    #[test]\n    fn restricted_loss_detection_fails_when_full_drops_first() {\n        let full_loss = vec![(0, 2.0), (1000, 1.0), (1500, 0.8)];\n        let restricted_loss = vec![(0, 2.0), (1000, 2.0), (1500, 0.8)];\n\n        let config = RestrictedLossVerificationConfig {\n            plateau_min_step: 500,\n            drop_window: 1,\n            drop_fraction: 0.3,\n            min_step_lead: 200,\n        };\n\n        let err = verify_restricted_loss_early_drop(&full_loss, &restricted_loss, &config)\n            .expect_err(\"expected restricted loss verification to fail\");\n        assert!(err.contains(\"restricted loss drops\"));\n    }\n\n    #[test]\n    fn excluded_loss_spike_verification_passes() {\n        let excluded_loss = vec![(0, 1.0), (1500, 2.2), (3000, 1.1)];\n        let config = ExcludedLossSpikeConfig {\n            min_relative_increase: 0.5,\n        };\n\n        let report = verify_excluded_loss_spike(&excluded_loss, &config)\n            .expect(\"expected excluded loss spike verification to pass\");\n        assert_eq!(report.spike_step, 1500);\n    }\n\n    #[test]\n    fn excluded_loss_spike_verification_fails_without_spike() {\n        let excluded_loss = vec![(0, 1.0), (1500, 1.1), (3000, 1.0)];\n        let config = ExcludedLossSpikeConfig {\n            min_relative_increase: 0.5,\n        };\n\n        let err = verify_excluded_loss_spike(&excluded_loss, &config)\n            .expect_err(\"expected excluded loss spike verification to fail\");\n        assert!(err.contains(\"excluded loss spike too small\"));\n    }\n\n    #[test]\n    fn restricted_loss_drop_verification_passes() {\n        let restricted_loss = vec![\n            (0, 2.0),\n            (1000, 1.8),\n            (5000, 1.2),\n            (7000, 0.8),\n            (10000, 0.9),\n        ];\n        let config = RestrictedLossDropConfig {\n            min_relative_decrease: 0.3,\n        };\n\n        let report = verify_restricted_loss_drop(&restricted_loss, &config)\n            .expect(\"expected restricted loss drop verification to pass\");\n        assert_eq!(report.drop_step, 7000);\n        assert!(report.baseline_loss > report.drop_loss);\n        assert!((report.baseline_loss - report.drop_loss) / report.baseline_loss >= 0.3);\n    }\n\n    #[test]\n    fn restricted_loss_drop_verification_fails_without_drop() {\n        let restricted_loss = vec![\n            (0, 2.0),\n            (1000, 2.1),\n            (5000, 1.9),\n            (7000, 2.0),\n            (10000, 2.05),\n        ];\n        let config = RestrictedLossDropConfig {\n            min_relative_decrease: 0.3,\n        };\n\n        let err = verify_restricted_loss_drop(&restricted_loss, &config)\n            .expect_err(\"expected restricted loss drop verification to fail\");\n        assert!(err.contains(\"restricted loss decrease too small\"));\n    }\n\n    #[test]\n    fn mlp_activation_wave_verification_passes_on_rectified_sine() {\n        let n = ModularAdditionDataset::modulus();\n        let freq = 5.0;\n        let mut signal = Vec::with_capacity(n);\n        for idx in 0..n {\n            let angle = 2.0 * std::f64::consts::PI * freq * idx as f64 / n as f64;\n            let rectified = angle.sin().max(0.0);\n            let wobble = (2.0 * std::f64::consts::PI * 2.0 * idx as f64 / n as f64)\n                .sin()\n                .abs()\n                * 0.02;\n            signal.push(rectified + wobble);\n        }\n\n        let config = MlpWaveVerificationConfig {\n            max_negative_fraction: 0.0,\n            negative_tolerance: 1e-6,\n            min_dominant_ratio: 3.0,\n            min_correlation: 0.7,\n            phase_steps: 32,\n        };\n\n        let report =\n            verify_mlp_activation_waves_from_signals(&[(0, signal)], &config)\n                .expect(\"expected rectified sine to pass\");\n        assert_eq!(report.neuron_reports.len(), 1);\n    }\n\n    #[test]\n    fn mlp_activation_wave_verification_fails_on_noise() {\n        let n = ModularAdditionDataset::modulus();\n        let signal: Vec<f64> = (0..n).map(|idx| idx as f64 / n as f64).collect();\n\n        let config = MlpWaveVerificationConfig {\n            max_negative_fraction: 0.0,\n            negative_tolerance: 1e-6,\n            min_dominant_ratio: 4.0,\n            min_correlation: 0.8,\n            phase_steps: 16,\n        };\n\n        let err =\n            verify_mlp_activation_waves_from_signals(&[(0, signal)], &config)\n                .expect_err(\"expected linear signal to fail\");\n        assert!(err.contains(\"dominant frequency ratio\") || err.contains(\"correlation\"));\n    }\n\n    #[test]\n    fn pairwise_manifold_transition_passes_on_elliptic_signals() {\n        let n = ModularAdditionDataset::modulus();\n        let freq = 5.0;\n        let mut post_signals = Vec::new();\n        for neuron_idx in 0..7usize {\n            let phase = 2.0 * std::f64::consts::PI * neuron_idx as f64 / 7.0;\n            let signal: Vec<f64> = (0..n)\n                .map(|idx| {\n                    let angle = 2.0 * std::f64::consts::PI * freq * idx as f64 / n as f64 + phase;\n                    angle.sin()\n                })\n                .collect();\n            post_signals.push((neuron_idx, signal));\n        }\n\n        let mut pre_signals = Vec::new();\n        for neuron_idx in 0..7usize {\n            let signal: Vec<f64> = (0..n)\n                .map(|idx| idx as f64 / n as f64 + neuron_idx as f64 * 0.01)\n                .collect();\n            pre_signals.push((neuron_idx, signal));\n        }\n\n        let config = PairwiseManifoldConfig {\n            fixed_y: 0,\n            neuron_indices: (0..7).collect(),\n            min_post_score: 0.75,\n            max_pre_score: 0.55,\n            max_axis_ratio: 8.0,\n            min_pairs_passing: 8,\n        };\n\n        let report = verify_pairwise_manifold_transition_from_signals(\n            &pre_signals,\n            &post_signals,\n            &config,\n        )\n        .expect(\"expected pairwise manifold transition to pass\");\n        assert!(report.passing_pairs >= config.min_pairs_passing);\n    }\n\n    #[test]\n    fn pairwise_manifold_transition_fails_without_rings() {\n        let n = ModularAdditionDataset::modulus();\n        let mut signals = Vec::new();\n        for neuron_idx in 0..7usize {\n            let signal: Vec<f64> = (0..n)\n                .map(|idx| idx as f64 / n as f64 + neuron_idx as f64 * 0.02)\n                .collect();\n            signals.push((neuron_idx, signal));\n        }\n\n        let config = PairwiseManifoldConfig {\n            fixed_y: 0,\n            neuron_indices: (0..7).collect(),\n            min_post_score: 0.8,\n            max_pre_score: 0.55,\n            max_axis_ratio: 4.0,\n            min_pairs_passing: 5,\n        };\n\n        let err = verify_pairwise_manifold_transition_from_signals(\n            &signals,\n            &signals,\n            &config,\n        )\n        .expect_err(\"expected pairwise manifold transition to fail\");\n        assert!(err.contains(\"post-grok\") || err.contains(\"pre-grok\"));\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_passes_on_diagonal_corrugation() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create diagonal ridges: f(x,y) = sin(2Ï€ * freq * (x+y) / n)\n        let freq = 3.0;\n        for x in 0..n {\n            for y in 0..n {\n                let angle = 2.0 * std::f64::consts::PI * freq * (x + y) as f64 / n as f64;\n                surface[x][y] = angle.sin();\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let report = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect(\"expected diagonal ridge verification to pass\");\n        assert!(report.diagonal_score > report.axis_score);\n        assert!(report.ratio >= config.min_diagonal_ratio);\n        assert!(report.diagonal_score >= config.min_diagonal_score);\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_passes_on_anti_diagonal_pattern() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create anti-diagonal ridges: f(x,y) = sin(2Ï€ * freq * (x-y) / n)\n        let freq = 3.0;\n        for x in 0..n {\n            for y in 0..n {\n                let angle = 2.0 * std::f64::consts::PI * freq * (x as i32 - y as i32) as f64 / n as f64;\n                surface[x][y] = angle.sin();\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let report = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect(\"expected anti-diagonal ridge verification to pass\");\n        assert!(report.diagonal_score > report.axis_score);\n        assert!(report.ratio >= config.min_diagonal_ratio);\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_fails_on_axis_aligned_pattern() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create axis-aligned ridges: f(x,y) = sin(2Ï€ * freq * x / n)\n        let freq = 3.0;\n        for x in 0..n {\n            for y in 0..n {\n                let angle = 2.0 * std::f64::consts::PI * freq * x as f64 / n as f64;\n                surface[x][y] = angle.sin();\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let err = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect_err(\"expected axis-aligned pattern to fail diagonal verification\");\n        assert!(err.contains(\"diagonal/axis ratio\") || err.contains(\"diagonal autocorrelation\"));\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_fails_on_flat_surface() {\n        let n = 50;\n        let surface = vec![vec![1.5; n]; n];\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let err = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect_err(\"expected flat surface to fail\");\n        assert!(err.contains(\"surface variance is zero\"));\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_fails_on_random_noise() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create random noise (low autocorrelation everywhere)\n        for x in 0..n {\n            for y in 0..n {\n                surface[x][y] = ((x * 17 + y * 31) % 100) as f64 / 100.0;\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let err = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect_err(\"expected random noise to fail diagonal verification\");\n        assert!(err.contains(\"diagonal/axis ratio\") || err.contains(\"diagonal autocorrelation\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_passes_on_valid_grokking_curve() {\n        // Simulate typical grokking: train converges early, val plateaus then jumps\n        let train_acc = vec![\n            (1, 0.10),\n            (10, 0.50),\n            (100, 0.80),\n            (500, 0.98),\n            (1000, 0.99),\n            (2000, 0.995),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (10, 0.009),\n            (100, 0.01),\n            (500, 0.011),\n            (1000, 0.009),\n            (1500, 0.01),\n            (2000, 0.92),\n            (3000, 0.98),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 1500,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let report = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect(\"expected snake curve verification to pass\");\n\n        assert_eq!(report.train_converged_by_step, 500);\n        assert!(report.val_plateau_max_acc < config.val_plateau_max_acc);\n        assert_eq!(report.val_grok_step, 2000);\n        assert!(report.curve_shape_valid);\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_on_simultaneous_convergence() {\n        // Both train and val converge at the same time (no grokking)\n        let train_acc = vec![\n            (1, 0.10),\n            (100, 0.98),\n            (500, 0.99),\n        ];\n        let val_acc = vec![\n            (1, 0.10),\n            (100, 0.97),\n            (500, 0.98),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 200,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let err = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect_err(\"expected snake curve verification to fail\");\n        assert!(err.contains(\"plateau region too high\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_on_slow_train_convergence() {\n        // Train takes too long to converge (not the typical snake curve)\n        let train_acc = vec![\n            (1, 0.10),\n            (500, 0.50),\n            (1000, 0.70),\n            (2000, 0.98),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (500, 0.01),\n            (1000, 0.01),\n            (2000, 0.92),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 1500,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let err = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect_err(\"expected snake curve verification to fail\");\n        assert!(err.contains(\"converges at step 2000\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_without_val_jump() {\n        // Validation never jumps up (no generalization)\n        let train_acc = vec![\n            (1, 0.10),\n            (100, 0.98),\n            (500, 0.99),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (100, 0.01),\n            (500, 0.01),\n            (2000, 0.02),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 200,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let err = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect_err(\"expected snake curve verification to fail\");\n        assert!(err.contains(\"validation accuracy never reaches\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_on_wrong_phase_ordering() {\n        // Val jumps before train converges (backwards snake)\n        let train_acc = vec![\n            (1, 0.10),\n            (500, 0.50),\n            (2000, 0.98),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (100, 0.95),\n            (500, 0.98),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 200,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let result = verify_snake_curve_shape(&train_acc, &val_acc, &config);\n        // Should fail because val jumps before train converges (not a proper snake curve)\n        // This can fail for multiple reasons, but should definitely fail\n        assert!(result.is_err(), \"Expected snake curve verification to fail but it passed\");\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_passes_on_decreasing_norms() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 85.0),\n            (1000, 72.0),\n            (1500, 63.0),\n            (2000, 58.0),\n            (2500, 55.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let report = verify_weight_norm_decay(&weight_norms, Some(2000), &config)\n            .expect(\"expected weight norm decay verification to pass\");\n\n        assert_eq!(report.initial_norm, 100.0);\n        assert_eq!(report.final_norm, 55.0);\n        assert!((report.relative_decrease - 0.45).abs() < 0.01);\n        assert!(report.grok_step_norm.is_some());\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_fails_on_flat_norms() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 99.0),\n            (1000, 98.5),\n            (1500, 98.0),\n            (2000, 97.5),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let err = verify_weight_norm_decay(&weight_norms, None, &config)\n            .expect_err(\"expected weight norm decay verification to fail\");\n        assert!(err.contains(\"weight norm decrease\"));\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_fails_on_increasing_norms() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 95.0),\n            (1000, 105.0), // Increase here\n            (1500, 90.0),\n            (2000, 85.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let err = verify_weight_norm_decay(&weight_norms, None, &config)\n            .expect_err(\"expected weight norm decay verification to fail\");\n        assert!(err.contains(\"weight norm increased\"));\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_allows_small_noise() {\n        // Weight decay with small numerical noise (within 1% tolerance)\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 95.0),\n            (1000, 90.5),  // Small increase within tolerance\n            (1500, 90.0),\n            (2000, 85.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let report = verify_weight_norm_decay(&weight_norms, None, &config)\n            .expect(\"expected weight norm decay with small noise to pass\");\n\n        assert_eq!(report.initial_norm, 100.0);\n        assert_eq!(report.final_norm, 85.0);\n        assert!((report.relative_decrease - 0.15).abs() < 0.01);\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_finds_grok_step_norm() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (1000, 80.0),\n            (1800, 68.0),  // Within window of grok step (2200 - 500 = 1700)\n            (2000, 65.0),\n            (2200, 62.0),  // Around grok step\n            (3000, 55.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let report = verify_weight_norm_decay(&weight_norms, Some(2200), &config)\n            .expect(\"expected weight norm decay verification to pass\");\n\n        assert!(report.grok_step_norm.is_some());\n        let grok_norm = report.grok_step_norm.unwrap();\n        // Should find the first norm in the window [1700, 2700], which is 68.0 at step 1800\n        assert!((grok_norm - 68.0).abs() < 0.01);\n    }\n}\n","structuredPatch":[{"oldStart":136,"oldLines":6,"newStart":136,"newLines":7,"lines":[" }"," "," #[derive(Debug, Clone)]","+#[allow(dead_code)]"," pub struct PairwiseManifoldPairReport {","     pub neuron_a: usize,","     pub neuron_b: usize,"]}],"userModified":false,"replaceAll":false}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01VWJ82no6r6veT9XHU1iJ8j","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01LCP6RzwbDQCByR5f8SfheD","name":"Read","input":{"file_path":"/Users/admin/MechInt/src/verify.rs","offset":158,"limit":90}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":1271,"cache_read_input_tokens":76219,"cache_creation":{"ephemeral_5m_input_tokens":1271,"ephemeral_1h_input_tokens":0},"output_tokens":18,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"2b08a8e9-07a6-411e-bc9d-59c946173b2e"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01LCP6RzwbDQCByR5f8SfheD","type":"tool_result","content":"   158â†’    pub passing_pairs: usize,\n   159â†’    pub total_pairs: usize,\n   160â†’}\n   161â†’\n   162â†’#[derive(Debug, Clone)]\n   163â†’pub struct DiagonalRidgeConfig {\n   164â†’    pub neuron_index: usize,\n   165â†’    pub max_lag: usize,\n   166â†’    pub min_diagonal_ratio: f64,\n   167â†’    pub min_diagonal_score: f64,\n   168â†’}\n   169â†’\n   170â†’impl DiagonalRidgeConfig {\n   171â†’    pub fn default_for_modulus(modulus: usize) -> Self {\n   172â†’        let max_lag = (modulus / 10).max(4).min(20);\n   173â†’        Self {\n   174â†’            neuron_index: 10,\n   175â†’            max_lag,\n   176â†’            min_diagonal_ratio: 1.3,\n   177â†’            min_diagonal_score: 0.2,\n   178â†’        }\n   179â†’    }\n   180â†’}\n   181â†’\n   182â†’#[derive(Debug, Clone)]\n   183â†’pub struct DiagonalRidgeReport {\n   184â†’    pub neuron_index: usize,\n   185â†’    pub diagonal_score: f64,\n   186â†’    pub axis_score: f64,\n   187â†’    pub ratio: f64,\n   188â†’}\n   189â†’\n   190â†’#[derive(Debug, Clone)]\n   191â†’pub struct SnakeCurveConfig {\n   192â†’    pub early_step_max: usize,\n   193â†’    pub train_convergence_threshold: f32,\n   194â†’    pub val_plateau_min_step: usize,\n   195â†’    pub val_plateau_max_acc: f32,\n   196â†’    pub val_final_threshold: f32,\n   197â†’}\n   198â†’\n   199â†’impl SnakeCurveConfig {\n   200â†’    pub fn default_for_modulus(modulus: usize) -> Self {\n   201â†’        Self {\n   202â†’            early_step_max: 1000,\n   203â†’            train_convergence_threshold: 0.95,\n   204â†’            val_plateau_min_step: 1000,\n   205â†’            val_plateau_max_acc: 1.0 / modulus as f32 + 0.05,\n   206â†’            val_final_threshold: 0.90,\n   207â†’        }\n   208â†’    }\n   209â†’}\n   210â†’\n   211â†’#[derive(Debug, Clone)]\n   212â†’pub struct WeightNormDecayConfig {\n   213â†’    pub min_relative_decrease: f64,\n   214â†’    pub plateau_min_step: usize,\n   215â†’    pub grok_step_window: usize,\n   216â†’}\n   217â†’\n   218â†’impl WeightNormDecayConfig {\n   219â†’    pub fn default_for_grokking() -> Self {\n   220â†’        Self {\n   221â†’            min_relative_decrease: 0.1,\n   222â†’            plateau_min_step: 1000,\n   223â†’            grok_step_window: 500,\n   224â†’        }\n   225â†’    }\n   226â†’}\n   227â†’\n   228â†’#[derive(Debug, Clone)]\n   229â†’pub struct WeightNormDecayReport {\n   230â†’    pub initial_norm: f64,\n   231â†’    pub final_norm: f64,\n   232â†’    pub relative_decrease: f64,\n   233â†’    pub grok_step_norm: Option<f64>,\n   234â†’}\n   235â†’\n   236â†’#[derive(Debug, Clone)]\n   237â†’pub struct SnakeCurveReport {\n   238â†’    pub train_converged_by_step: usize,\n   239â†’    pub val_plateau_max_acc: f32,\n   240â†’    pub val_grok_step: usize,\n   241â†’    pub curve_shape_valid: bool,\n   242â†’}\n   243â†’\n   244â†’pub fn verify_grokking_phase_transition(\n   245â†’    loss_history: &LossHistory,\n   246â†’    accuracy_history: &AccuracyHistory,\n   247â†’    config: &GrokkingVerificationConfig,\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"5cd89176-c630-414f-a992-d42fe530fb13","tool_use_result":{"type":"text","file":{"filePath":"/Users/admin/MechInt/src/verify.rs","content":"    pub passing_pairs: usize,\n    pub total_pairs: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct DiagonalRidgeConfig {\n    pub neuron_index: usize,\n    pub max_lag: usize,\n    pub min_diagonal_ratio: f64,\n    pub min_diagonal_score: f64,\n}\n\nimpl DiagonalRidgeConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        let max_lag = (modulus / 10).max(4).min(20);\n        Self {\n            neuron_index: 10,\n            max_lag,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct DiagonalRidgeReport {\n    pub neuron_index: usize,\n    pub diagonal_score: f64,\n    pub axis_score: f64,\n    pub ratio: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct SnakeCurveConfig {\n    pub early_step_max: usize,\n    pub train_convergence_threshold: f32,\n    pub val_plateau_min_step: usize,\n    pub val_plateau_max_acc: f32,\n    pub val_final_threshold: f32,\n}\n\nimpl SnakeCurveConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        Self {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 1000,\n            val_plateau_max_acc: 1.0 / modulus as f32 + 0.05,\n            val_final_threshold: 0.90,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct WeightNormDecayConfig {\n    pub min_relative_decrease: f64,\n    pub plateau_min_step: usize,\n    pub grok_step_window: usize,\n}\n\nimpl WeightNormDecayConfig {\n    pub fn default_for_grokking() -> Self {\n        Self {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct WeightNormDecayReport {\n    pub initial_norm: f64,\n    pub final_norm: f64,\n    pub relative_decrease: f64,\n    pub grok_step_norm: Option<f64>,\n}\n\n#[derive(Debug, Clone)]\npub struct SnakeCurveReport {\n    pub train_converged_by_step: usize,\n    pub val_plateau_max_acc: f32,\n    pub val_grok_step: usize,\n    pub curve_shape_valid: bool,\n}\n\npub fn verify_grokking_phase_transition(\n    loss_history: &LossHistory,\n    accuracy_history: &AccuracyHistory,\n    config: &GrokkingVerificationConfig,","numLines":90,"startLine":158,"totalLines":2216}}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01M7LU41ybiDoFaDkbMSw8Zd","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01DgZaCyupDGraZkugZ27Vci","name":"Edit","input":{"replace_all":false,"file_path":"/Users/admin/MechInt/src/verify.rs","old_string":"impl DiagonalRidgeConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        let max_lag = (modulus / 10).max(4).min(20);\n        Self {\n            neuron_index: 10,\n            max_lag,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct DiagonalRidgeReport {\n    pub neuron_index: usize,\n    pub diagonal_score: f64,\n    pub axis_score: f64,\n    pub ratio: f64,\n}","new_string":"impl DiagonalRidgeConfig {\n    #[allow(dead_code)]\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        let max_lag = (modulus / 10).max(4).min(20);\n        Self {\n            neuron_index: 10,\n            max_lag,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\n#[allow(dead_code)]\npub struct DiagonalRidgeReport {\n    pub neuron_index: usize,\n    pub diagonal_score: f64,\n    pub axis_score: f64,\n    pub ratio: f64,\n}"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":2006,"cache_read_input_tokens":77490,"cache_creation":{"ephemeral_5m_input_tokens":2006,"ephemeral_1h_input_tokens":0},"output_tokens":470,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"1f653d11-6a4c-4da6-988d-1370b7e17120"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01DgZaCyupDGraZkugZ27Vci","type":"tool_result","content":"The file /Users/admin/MechInt/src/verify.rs has been updated. Here's the result of running `cat -n` on a snippet of the edited file:\n   166â†’    pub min_diagonal_ratio: f64,\n   167â†’    pub min_diagonal_score: f64,\n   168â†’}\n   169â†’\n   170â†’impl DiagonalRidgeConfig {\n   171â†’    #[allow(dead_code)]\n   172â†’    pub fn default_for_modulus(modulus: usize) -> Self {\n   173â†’        let max_lag = (modulus / 10).max(4).min(20);\n   174â†’        Self {\n   175â†’            neuron_index: 10,\n   176â†’            max_lag,\n   177â†’            min_diagonal_ratio: 1.3,\n   178â†’            min_diagonal_score: 0.2,\n   179â†’        }\n   180â†’    }\n   181â†’}\n   182â†’\n   183â†’#[derive(Debug, Clone)]\n   184â†’#[allow(dead_code)]\n   185â†’pub struct DiagonalRidgeReport {\n   186â†’    pub neuron_index: usize,\n   187â†’    pub diagonal_score: f64,\n   188â†’    pub axis_score: f64,\n   189â†’    pub ratio: f64,\n   190â†’}\n   191â†’\n   192â†’#[derive(Debug, Clone)]\n   193â†’pub struct SnakeCurveConfig {\n   194â†’    pub early_step_max: usize,"}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"f1ada9e4-d803-459d-b0d1-1fe5384ad170","tool_use_result":{"filePath":"/Users/admin/MechInt/src/verify.rs","oldString":"impl DiagonalRidgeConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        let max_lag = (modulus / 10).max(4).min(20);\n        Self {\n            neuron_index: 10,\n            max_lag,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct DiagonalRidgeReport {\n    pub neuron_index: usize,\n    pub diagonal_score: f64,\n    pub axis_score: f64,\n    pub ratio: f64,\n}","newString":"impl DiagonalRidgeConfig {\n    #[allow(dead_code)]\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        let max_lag = (modulus / 10).max(4).min(20);\n        Self {\n            neuron_index: 10,\n            max_lag,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\n#[allow(dead_code)]\npub struct DiagonalRidgeReport {\n    pub neuron_index: usize,\n    pub diagonal_score: f64,\n    pub axis_score: f64,\n    pub ratio: f64,\n}","originalFile":"use burn::{\n    data::dataset::Dataset,\n    tensor::{backend::Backend, Int, Tensor},\n};\nuse rustfft::{num_complex::Complex, FftPlanner};\n\nuse crate::analysis::{AccuracyHistory, LossHistory};\nuse crate::data::ModularAdditionDataset;\nuse crate::model::Transformer;\n\n#[derive(Debug, Clone)]\npub struct GrokkingVerificationConfig {\n    pub early_train_acc_threshold: f32,\n    pub early_step_max: usize,\n    pub plateau_min_step: usize,\n    pub generalization_window: usize,\n    pub chance_accuracy: f32,\n    pub chance_tolerance: f32,\n    pub target_val_acc_threshold: f32,\n    pub accuracy_window: usize,\n    pub loss_window: usize,\n    pub loss_drop_fraction: f64,\n}\n\nimpl GrokkingVerificationConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        Self {\n            early_train_acc_threshold: 0.99,\n            early_step_max: 1000,\n            plateau_min_step: 1000,\n            generalization_window: 10_000,\n            chance_accuracy: 1.0 / modulus as f32,\n            chance_tolerance: 0.02,\n            target_val_acc_threshold: 0.99,\n            accuracy_window: 5,\n            loss_window: 5,\n            loss_drop_fraction: 0.5,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\n#[allow(dead_code)]\npub struct GrokkingPhaseReport {\n    pub early_train_step: usize,\n    pub transition_step: usize,\n    pub plateau_max_val_acc: f32,\n    pub loss_drop_ratio: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossVerificationConfig {\n    pub plateau_min_step: usize,\n    pub drop_window: usize,\n    pub drop_fraction: f64,\n    pub min_step_lead: usize,\n}\n\n#[derive(Debug, Clone)]\n#[allow(dead_code)]\npub struct RestrictedLossReport {\n    pub restricted_drop_step: usize,\n    pub full_drop_step: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossDropConfig {\n    pub min_relative_decrease: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossDropReport {\n    pub drop_step: usize,\n    pub drop_loss: f64,\n    pub baseline_loss: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct ExcludedLossSpikeConfig {\n    pub min_relative_increase: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct ExcludedLossSpikeReport {\n    pub spike_step: usize,\n    pub spike_loss: f64,\n    pub baseline_loss: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct MlpWaveVerificationConfig {\n    pub max_negative_fraction: f64,\n    pub negative_tolerance: f64,\n    pub min_dominant_ratio: f64,\n    pub min_correlation: f64,\n    pub phase_steps: usize,\n}\n\n#[derive(Debug, Clone)]\n#[allow(dead_code)]\npub struct MlpWaveNeuronReport {\n    pub neuron_index: usize,\n    pub dominant_frequency: usize,\n    pub dominant_ratio: f64,\n    pub negative_fraction: f64,\n    pub best_correlation: f64,\n}\n\n#[derive(Debug, Clone)]\n#[allow(dead_code)]\npub struct MlpWaveReport {\n    pub neuron_reports: Vec<MlpWaveNeuronReport>,\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldConfig {\n    pub fixed_y: usize,\n    pub neuron_indices: Vec<usize>,\n    pub min_post_score: f64,\n    pub max_pre_score: f64,\n    pub max_axis_ratio: f64,\n    pub min_pairs_passing: usize,\n}\n\nimpl PairwiseManifoldConfig {\n    pub fn default_for_modulus(_modulus: usize) -> Self {\n        Self {\n            fixed_y: 0,\n            neuron_indices: (0..7).collect(),\n            min_post_score: 0.75,\n            max_pre_score: 0.55,\n            max_axis_ratio: 8.0,\n            min_pairs_passing: 10,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\n#[allow(dead_code)]\npub struct PairwiseManifoldPairReport {\n    pub neuron_a: usize,\n    pub neuron_b: usize,\n    pub circularity_score: f64,\n    pub radius_cv: f64,\n    pub axis_ratio: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldReport {\n    pub pair_reports: Vec<PairwiseManifoldPairReport>,\n    pub average_score: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldTransitionReport {\n    pub pre_average_score: f64,\n    pub post_average_score: f64,\n    pub passing_pairs: usize,\n    pub total_pairs: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct DiagonalRidgeConfig {\n    pub neuron_index: usize,\n    pub max_lag: usize,\n    pub min_diagonal_ratio: f64,\n    pub min_diagonal_score: f64,\n}\n\nimpl DiagonalRidgeConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        let max_lag = (modulus / 10).max(4).min(20);\n        Self {\n            neuron_index: 10,\n            max_lag,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct DiagonalRidgeReport {\n    pub neuron_index: usize,\n    pub diagonal_score: f64,\n    pub axis_score: f64,\n    pub ratio: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct SnakeCurveConfig {\n    pub early_step_max: usize,\n    pub train_convergence_threshold: f32,\n    pub val_plateau_min_step: usize,\n    pub val_plateau_max_acc: f32,\n    pub val_final_threshold: f32,\n}\n\nimpl SnakeCurveConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        Self {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 1000,\n            val_plateau_max_acc: 1.0 / modulus as f32 + 0.05,\n            val_final_threshold: 0.90,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct WeightNormDecayConfig {\n    pub min_relative_decrease: f64,\n    pub plateau_min_step: usize,\n    pub grok_step_window: usize,\n}\n\nimpl WeightNormDecayConfig {\n    pub fn default_for_grokking() -> Self {\n        Self {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct WeightNormDecayReport {\n    pub initial_norm: f64,\n    pub final_norm: f64,\n    pub relative_decrease: f64,\n    pub grok_step_norm: Option<f64>,\n}\n\n#[derive(Debug, Clone)]\npub struct SnakeCurveReport {\n    pub train_converged_by_step: usize,\n    pub val_plateau_max_acc: f32,\n    pub val_grok_step: usize,\n    pub curve_shape_valid: bool,\n}\n\npub fn verify_grokking_phase_transition(\n    loss_history: &LossHistory,\n    accuracy_history: &AccuracyHistory,\n    config: &GrokkingVerificationConfig,\n) -> Result<GrokkingPhaseReport, String> {\n    let (early_train_step, _) = accuracy_history\n        .train_snapshots\n        .iter()\n        .find(|(_, acc)| *acc >= config.early_train_acc_threshold)\n        .ok_or_else(|| \"train accuracy never reaches threshold\".to_string())?;\n\n    if *early_train_step > config.early_step_max {\n        return Err(format!(\n            \"train accuracy reaches {:.2} after step {}, expected before {}\",\n            config.early_train_acc_threshold, early_train_step, config.early_step_max\n        ));\n    }\n\n    let plateau_max_val_acc = accuracy_history\n        .val_snapshots\n        .iter()\n        .filter(|(step, _)| *step <= config.plateau_min_step)\n        .map(|(_, acc)| *acc)\n        .fold(None::<f32>, |accum, value| {\n            Some(accum.map_or(value, |max_val| max_val.max(value)))\n        })\n        .ok_or_else(|| \"no validation accuracy samples before plateau_min_step\".to_string())?;\n\n    if plateau_max_val_acc > config.chance_accuracy + config.chance_tolerance {\n        return Err(format!(\n            \"validation accuracy before step {} exceeds chance (max {:.4})\",\n            config.plateau_min_step, plateau_max_val_acc\n        ));\n    }\n\n    let (transition_step, _) = accuracy_history\n        .val_snapshots\n        .iter()\n        .find(|(step, acc)| {\n            *step >= config.plateau_min_step && *acc >= config.target_val_acc_threshold\n        })\n        .ok_or_else(|| \"validation accuracy never reaches target threshold\".to_string())?;\n\n    let latest_allowed = config.plateau_min_step + config.generalization_window;\n    if *transition_step > latest_allowed {\n        return Err(format!(\n            \"validation accuracy reaches {:.2} at step {}, expected before {}\",\n            config.target_val_acc_threshold, transition_step, latest_allowed\n        ));\n    }\n\n    verify_accuracy_jump(\n        &accuracy_history.val_snapshots,\n        *transition_step,\n        config.accuracy_window,\n        config.chance_accuracy,\n        config.chance_tolerance,\n        config.target_val_acc_threshold,\n    )?;\n\n    let loss_drop_ratio = verify_loss_drop(\n        &loss_history.val_snapshots,\n        *transition_step,\n        config.loss_window,\n        config.loss_drop_fraction,\n    )?;\n\n    Ok(GrokkingPhaseReport {\n        early_train_step: *early_train_step,\n        transition_step: *transition_step,\n        plateau_max_val_acc,\n        loss_drop_ratio,\n    })\n}\n\nfn verify_accuracy_jump(\n    val_accuracy: &[(usize, f32)],\n    transition_step: usize,\n    window: usize,\n    chance_accuracy: f32,\n    chance_tolerance: f32,\n    target_val_acc_threshold: f32,\n) -> Result<(), String> {\n    let before_values: Vec<f32> = val_accuracy\n        .iter()\n        .filter(|(step, _)| *step < transition_step)\n        .map(|(_, acc)| *acc)\n        .collect();\n\n    let after_values: Vec<f32> = val_accuracy\n        .iter()\n        .filter(|(step, _)| *step >= transition_step)\n        .map(|(_, acc)| *acc)\n        .collect();\n\n    if before_values.len() < window || after_values.len() < window {\n        return Err(\"insufficient accuracy samples around transition\".to_string());\n    }\n\n    let before_avg: f32 =\n        before_values[before_values.len() - window..].iter().sum::<f32>() / window as f32;\n    let after_avg: f32 = after_values[..window].iter().sum::<f32>() / window as f32;\n\n    if before_avg > chance_accuracy + chance_tolerance {\n        return Err(format!(\n            \"validation accuracy plateau too high before transition (avg {:.4})\",\n            before_avg\n        ));\n    }\n\n    if after_avg < target_val_acc_threshold {\n        return Err(format!(\n            \"validation accuracy after transition too low (avg {:.4})\",\n            after_avg\n        ));\n    }\n\n    Ok(())\n}\n\nfn verify_loss_drop(\n    val_losses: &[(usize, f64)],\n    transition_step: usize,\n    loss_window: usize,\n    loss_drop_fraction: f64,\n) -> Result<f64, String> {\n    let before_losses: Vec<f64> = val_losses\n        .iter()\n        .filter(|(step, _)| *step < transition_step)\n        .map(|(_, loss)| *loss)\n        .collect();\n\n    let after_losses: Vec<f64> = val_losses\n        .iter()\n        .filter(|(step, _)| *step >= transition_step)\n        .map(|(_, loss)| *loss)\n        .collect();\n\n    if before_losses.len() < loss_window || after_losses.len() < loss_window {\n        return Err(\"insufficient loss samples around transition\".to_string());\n    }\n\n    let before_avg: f64 =\n        before_losses[before_losses.len() - loss_window..].iter().sum::<f64>()\n            / loss_window as f64;\n    let after_avg: f64 = after_losses[..loss_window].iter().sum::<f64>() / loss_window as f64;\n\n    if after_avg > before_avg * (1.0 - loss_drop_fraction) {\n        return Err(format!(\n            \"validation loss drop too small (before {:.4}, after {:.4})\",\n            before_avg, after_avg\n        ));\n    }\n\n    Ok(after_avg / before_avg)\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_restricted_loss_early_drop(\n    full_losses: &[(usize, f64)],\n    restricted_losses: &[(usize, f64)],\n    config: &RestrictedLossVerificationConfig,\n) -> Result<RestrictedLossReport, String> {\n    let restricted_baseline =\n        baseline_loss(restricted_losses, config.plateau_min_step, config.drop_window)?;\n    let full_baseline = baseline_loss(full_losses, config.plateau_min_step, config.drop_window)?;\n\n    let restricted_drop_step = find_drop_step(\n        restricted_losses,\n        config.plateau_min_step,\n        restricted_baseline,\n        config.drop_fraction,\n    )?;\n    let full_drop_step = find_drop_step(\n        full_losses,\n        config.plateau_min_step,\n        full_baseline,\n        config.drop_fraction,\n    )?;\n\n    if restricted_drop_step + config.min_step_lead > full_drop_step {\n        return Err(format!(\n            \"restricted loss drops at step {}, expected at least {} steps before full loss (step {})\",\n            restricted_drop_step, config.min_step_lead, full_drop_step\n        ));\n    }\n\n    Ok(RestrictedLossReport {\n        restricted_drop_step,\n        full_drop_step,\n    })\n}\n\npub fn verify_restricted_loss_drop(\n    restricted_losses: &[(usize, f64)],\n    config: &RestrictedLossDropConfig,\n) -> Result<RestrictedLossDropReport, String> {\n    if restricted_losses.len() < 3 {\n        return Err(\"restricted loss history requires at least 3 points\".to_string());\n    }\n\n    let first = restricted_losses.first().ok_or_else(|| {\n        \"restricted loss history is empty\".to_string()\n    })?;\n    let baseline_loss = first.1;\n\n    let (min_idx, (min_step, min_loss)) = restricted_losses\n        .iter()\n        .enumerate()\n        .min_by(|(_, a), (_, b)| {\n            a.1.partial_cmp(&b.1).unwrap_or(std::cmp::Ordering::Equal)\n        })\n        .ok_or_else(|| \"restricted loss history is empty\".to_string())?;\n\n    if min_idx == 0 {\n        return Err(\"restricted loss minimum occurs at start (no drop detected)\".to_string());\n    }\n\n    let relative_decrease = (baseline_loss - min_loss) / baseline_loss;\n    if relative_decrease < config.min_relative_decrease {\n        return Err(format!(\n            \"restricted loss decrease too small (baseline {:.4}, min {:.4}, decrease {:.1}%)\",\n            baseline_loss, min_loss, relative_decrease * 100.0\n        ));\n    }\n\n    Ok(RestrictedLossDropReport {\n        drop_step: *min_step,\n        drop_loss: *min_loss,\n        baseline_loss,\n    })\n}\n\npub fn verify_excluded_loss_spike(\n    excluded_losses: &[(usize, f64)],\n    config: &ExcludedLossSpikeConfig,\n) -> Result<ExcludedLossSpikeReport, String> {\n    if excluded_losses.len() < 3 {\n        return Err(\"excluded loss history requires at least 3 points\".to_string());\n    }\n\n    let first = excluded_losses.first().ok_or_else(|| {\n        \"excluded loss history is empty\".to_string()\n    })?;\n    let last = excluded_losses.last().ok_or_else(|| {\n        \"excluded loss history is empty\".to_string()\n    })?;\n    let baseline_loss = (first.1 + last.1) / 2.0;\n\n    let (peak_idx, (peak_step, peak_loss)) = excluded_losses\n        .iter()\n        .enumerate()\n        .max_by(|(_, a), (_, b)| {\n            a.1.partial_cmp(&b.1).unwrap_or(std::cmp::Ordering::Equal)\n        })\n        .ok_or_else(|| \"excluded loss history is empty\".to_string())?;\n\n    if peak_idx == 0 || peak_idx + 1 == excluded_losses.len() {\n        return Err(\"excluded loss spike occurs at boundary\".to_string());\n    }\n\n    if peak_loss < &(baseline_loss * (1.0 + config.min_relative_increase)) {\n        return Err(format!(\n            \"excluded loss spike too small (baseline {:.4}, peak {:.4})\",\n            baseline_loss, peak_loss\n        ));\n    }\n\n    Ok(ExcludedLossSpikeReport {\n        spike_step: *peak_step,\n        spike_loss: *peak_loss,\n        baseline_loss,\n    })\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_mlp_activation_waves<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n    fixed_y: usize,\n    neuron_indices: &[usize],\n    config: &MlpWaveVerificationConfig,\n) -> Result<MlpWaveReport, String> {\n    let activations = collect_mlp_post_relu_activations(model, device, fixed_y)?;\n    if neuron_indices.is_empty() {\n        return Err(\"neuron indices list is empty\".to_string());\n    }\n\n    let d_ff = activations.len();\n    for &idx in neuron_indices {\n        if idx >= d_ff {\n            return Err(format!(\n                \"neuron index {} out of range (d_ff={})\",\n                idx, d_ff\n            ));\n        }\n    }\n\n    let signals: Vec<(usize, Vec<f64>)> = neuron_indices\n        .iter()\n        .map(|&idx| (idx, activations[idx].clone()))\n        .collect();\n    verify_mlp_activation_waves_from_signals(&signals, config)\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_mlp_activation_waves_from_signals(\n    signals: &[(usize, Vec<f64>)],\n    config: &MlpWaveVerificationConfig,\n) -> Result<MlpWaveReport, String> {\n    if signals.is_empty() {\n        return Err(\"no activation signals provided\".to_string());\n    }\n\n    let mut reports = Vec::with_capacity(signals.len());\n    for (neuron_index, signal) in signals {\n        let report = analyze_mlp_activation_wave(*neuron_index, signal, config)?;\n        reports.push(report);\n    }\n\n    Ok(MlpWaveReport {\n        neuron_reports: reports,\n    })\n}\n\npub fn verify_mlp_pairwise_manifold_transition<B: Backend>(\n    pre_model: &Transformer<B>,\n    post_model: &Transformer<B>,\n    device: &B::Device,\n    config: &PairwiseManifoldConfig,\n) -> Result<PairwiseManifoldTransitionReport, String> {\n    let modulus = ModularAdditionDataset::modulus();\n    if config.fixed_y >= modulus {\n        return Err(format!(\n            \"fixed_y {} out of range (modulus {})\",\n            config.fixed_y, modulus\n        ));\n    }\n\n    let pre_activations = collect_mlp_post_relu_activations(pre_model, device, config.fixed_y)?;\n    let post_activations = collect_mlp_post_relu_activations(post_model, device, config.fixed_y)?;\n\n    let d_ff = pre_activations.len();\n    if post_activations.len() != d_ff {\n        return Err(\"post activations shape mismatch\".to_string());\n    }\n\n    if config.neuron_indices.len() < 2 {\n        return Err(\"need at least two neuron indices\".to_string());\n    }\n\n    for &idx in &config.neuron_indices {\n        if idx >= d_ff {\n            return Err(format!(\n                \"neuron index {} out of range (d_ff={})\",\n                idx, d_ff\n            ));\n        }\n    }\n\n    let pre_signals: Vec<(usize, Vec<f64>)> = config\n        .neuron_indices\n        .iter()\n        .map(|&idx| (idx, pre_activations[idx].clone()))\n        .collect();\n    let post_signals: Vec<(usize, Vec<f64>)> = config\n        .neuron_indices\n        .iter()\n        .map(|&idx| (idx, post_activations[idx].clone()))\n        .collect();\n\n    verify_pairwise_manifold_transition_from_signals(&pre_signals, &post_signals, config)\n}\n\npub fn collect_mlp_activation_surface<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n    neuron_index: usize,\n) -> Result<Vec<Vec<f64>>, String> {\n    let modulus = ModularAdditionDataset::modulus();\n    let equals_token = ModularAdditionDataset::equals_token();\n\n    let mut inputs_vec = Vec::with_capacity(modulus * modulus * 3);\n    for x in 0..modulus {\n        for y in 0..modulus {\n            inputs_vec.push(x as i32);\n            inputs_vec.push(y as i32);\n            inputs_vec.push(equals_token as i32);\n        }\n    }\n\n    let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n        .reshape([modulus * modulus, 3]);\n    let (_, mlp_acts) = model.forward_with_mlp_activations(inputs);\n    let [batch_size, d_ff] = mlp_acts.dims();\n    if batch_size != modulus * modulus {\n        return Err(format!(\n            \"unexpected activation batch size {} (expected {})\",\n            batch_size,\n            modulus * modulus\n        ));\n    }\n    if neuron_index >= d_ff {\n        return Err(format!(\n            \"neuron index {} out of range (d_ff={})\",\n            neuron_index, d_ff\n        ));\n    }\n\n    let data: Vec<f32> = mlp_acts.into_data().to_vec().unwrap();\n    let mut surface = vec![vec![0.0f64; modulus]; modulus];\n    for x in 0..modulus {\n        for y in 0..modulus {\n            let batch = x * modulus + y;\n            let idx = batch * d_ff + neuron_index;\n            surface[x][y] = data[idx] as f64;\n        }\n    }\n\n    Ok(surface)\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_constructive_interference_surface_from_grid(\n    surface: &[Vec<f64>],\n    config: &DiagonalRidgeConfig,\n) -> Result<DiagonalRidgeReport, String> {\n    let (diag_main, diag_anti, axis_x, axis_y) =\n        surface_autocorrelation_scores(surface, config.max_lag)?;\n\n    let diagonal_score = diag_main.max(diag_anti);\n    let axis_score = axis_x.max(axis_y);\n    let ratio = if axis_score > 0.0 {\n        diagonal_score / axis_score\n    } else {\n        f64::INFINITY\n    };\n\n    if diagonal_score < config.min_diagonal_score {\n        return Err(format!(\n            \"diagonal autocorrelation {:.3} below threshold {:.3}\",\n            diagonal_score, config.min_diagonal_score\n        ));\n    }\n\n    if ratio < config.min_diagonal_ratio {\n        return Err(format!(\n            \"diagonal/axis ratio {:.3} below threshold {:.3}\",\n            ratio, config.min_diagonal_ratio\n        ));\n    }\n\n    Ok(DiagonalRidgeReport {\n        neuron_index: config.neuron_index,\n        diagonal_score,\n        axis_score,\n        ratio,\n    })\n}\n\npub fn verify_pairwise_manifold_transition_from_signals(\n    pre_signals: &[(usize, Vec<f64>)],\n    post_signals: &[(usize, Vec<f64>)],\n    config: &PairwiseManifoldConfig,\n) -> Result<PairwiseManifoldTransitionReport, String> {\n    let pre_report = evaluate_pairwise_manifolds_from_signals(pre_signals)?;\n    let post_report = evaluate_pairwise_manifolds_from_signals(post_signals)?;\n\n    if pre_report.average_score > config.max_pre_score {\n        return Err(format!(\n            \"pre-grok average circularity {:.3} exceeds threshold {:.3}\",\n            pre_report.average_score, config.max_pre_score\n        ));\n    }\n\n    let passing_pairs = post_report\n        .pair_reports\n        .iter()\n        .filter(|pair| {\n            pair.circularity_score >= config.min_post_score\n                && pair.axis_ratio <= config.max_axis_ratio\n        })\n        .count();\n    if passing_pairs < config.min_pairs_passing {\n        return Err(format!(\n            \"post-grok only {} pairs meet circularity threshold (need {})\",\n            passing_pairs, config.min_pairs_passing\n        ));\n    }\n\n    Ok(PairwiseManifoldTransitionReport {\n        pre_average_score: pre_report.average_score,\n        post_average_score: post_report.average_score,\n        passing_pairs,\n        total_pairs: post_report.pair_reports.len(),\n    })\n}\n\nfn evaluate_pairwise_manifolds_from_signals(\n    signals: &[(usize, Vec<f64>)],\n) -> Result<PairwiseManifoldReport, String> {\n    if signals.len() < 2 {\n        return Err(\"need at least two activation signals\".to_string());\n    }\n\n    let signal_len = signals[0].1.len();\n    if signal_len < 3 {\n        return Err(\"activation signal too short\".to_string());\n    }\n\n    for (idx, signal) in signals {\n        if signal.len() != signal_len {\n            return Err(format!(\n                \"signal length mismatch for neuron {}\",\n                idx\n            ));\n        }\n    }\n\n    let mut reports = Vec::new();\n    for i in 0..signals.len() {\n        for j in (i + 1)..signals.len() {\n            let (neuron_a, signal_a) = &signals[i];\n            let (neuron_b, signal_b) = &signals[j];\n            let points: Vec<(f64, f64)> = signal_a\n                .iter()\n                .zip(signal_b.iter())\n                .map(|(x, y)| (*x, *y))\n                .collect();\n            let (score, radius_cv, axis_ratio) =\n                match pairwise_circularity_metrics(&points) {\n                    Ok(metrics) => metrics,\n                    Err(_) => (0.0, 1.0, f64::INFINITY),\n                };\n            reports.push(PairwiseManifoldPairReport {\n                neuron_a: *neuron_a,\n                neuron_b: *neuron_b,\n                circularity_score: score,\n                radius_cv,\n                axis_ratio,\n            });\n        }\n    }\n\n    let average_score = reports\n        .iter()\n        .map(|pair| pair.circularity_score)\n        .sum::<f64>()\n        / reports.len() as f64;\n\n    Ok(PairwiseManifoldReport {\n        pair_reports: reports,\n        average_score,\n    })\n}\n\nfn pairwise_circularity_metrics(points: &[(f64, f64)]) -> Result<(f64, f64, f64), String> {\n    if points.len() < 3 {\n        return Err(\"not enough points for circularity\".to_string());\n    }\n\n    let n = points.len() as f64;\n    let mean_x = points.iter().map(|(x, _)| *x).sum::<f64>() / n;\n    let mean_y = points.iter().map(|(_, y)| *y).sum::<f64>() / n;\n\n    let mut var_x = 0.0;\n    let mut var_y = 0.0;\n    let mut cov_xy = 0.0;\n    for (x, y) in points {\n        let dx = x - mean_x;\n        let dy = y - mean_y;\n        var_x += dx * dx;\n        var_y += dy * dy;\n        cov_xy += dx * dy;\n    }\n    var_x /= n;\n    var_y /= n;\n    cov_xy /= n;\n\n    let trace = var_x + var_y;\n    let det = var_x * var_y - cov_xy * cov_xy;\n    if det <= 1.0e-12 || trace <= 0.0 {\n        return Err(\"degenerate covariance\".to_string());\n    }\n\n    let half_trace = trace * 0.5;\n    let disc = (half_trace * half_trace - det).max(0.0).sqrt();\n    let lambda1 = half_trace + disc;\n    let lambda2 = half_trace - disc;\n    if lambda2 <= 0.0 {\n        return Err(\"degenerate eigenvalues\".to_string());\n    }\n\n    let axis_ratio = (lambda1 / lambda2).sqrt();\n\n    let mut v1_x = cov_xy;\n    let mut v1_y = lambda1 - var_x;\n    if v1_x.abs() < 1.0e-12 && v1_y.abs() < 1.0e-12 {\n        v1_x = 1.0;\n        v1_y = 0.0;\n    }\n    let norm = (v1_x * v1_x + v1_y * v1_y).sqrt();\n    v1_x /= norm;\n    v1_y /= norm;\n    let v2_x = -v1_y;\n    let v2_y = v1_x;\n\n    let mut radii = Vec::with_capacity(points.len());\n    for (x, y) in points {\n        let dx = x - mean_x;\n        let dy = y - mean_y;\n        let p1 = dx * v1_x + dy * v1_y;\n        let p2 = dx * v2_x + dy * v2_y;\n        let w1 = p1 / lambda1.sqrt();\n        let w2 = p2 / lambda2.sqrt();\n        radii.push((w1 * w1 + w2 * w2).sqrt());\n    }\n\n    let mean_radius = radii.iter().sum::<f64>() / radii.len() as f64;\n    if mean_radius == 0.0 {\n        return Err(\"zero mean radius\".to_string());\n    }\n\n    let mut variance = 0.0;\n    for r in &radii {\n        let diff = r - mean_radius;\n        variance += diff * diff;\n    }\n    let std = (variance / radii.len() as f64).sqrt();\n    let radius_cv = std / mean_radius;\n    let score = 1.0 / (1.0 + radius_cv);\n\n    Ok((score, radius_cv, axis_ratio))\n}\n\n// Helper functions for verification - reserved for future integration\n#[allow(dead_code)]\nfn baseline_loss(\n    losses: &[(usize, f64)],\n    plateau_min_step: usize,\n    drop_window: usize,\n) -> Result<f64, String> {\n    let before: Vec<f64> = losses\n        .iter()\n        .filter(|(step, _)| *step <= plateau_min_step)\n        .map(|(_, loss)| *loss)\n        .collect();\n\n    if before.len() < drop_window || drop_window == 0 {\n        return Err(\"insufficient loss samples for baseline\".to_string());\n    }\n\n    let window = &before[before.len() - drop_window..];\n    Ok(window.iter().sum::<f64>() / drop_window as f64)\n}\n\n#[allow(dead_code)]\nfn find_drop_step(\n    losses: &[(usize, f64)],\n    plateau_min_step: usize,\n    baseline: f64,\n    drop_fraction: f64,\n) -> Result<usize, String> {\n    losses\n        .iter()\n        .filter(|(step, _)| *step >= plateau_min_step)\n        .find(|(_, loss)| *loss <= baseline * (1.0 - drop_fraction))\n        .map(|(step, _)| *step)\n        .ok_or_else(|| \"loss never drops below threshold\".to_string())\n}\n\nfn collect_mlp_post_relu_activations<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n    fixed_y: usize,\n) -> Result<Vec<Vec<f64>>, String> {\n    let modulus = ModularAdditionDataset::modulus();\n    let equals_token = ModularAdditionDataset::equals_token();\n\n    let mut inputs_vec = Vec::with_capacity(modulus * 3);\n    for x in 0..modulus {\n        inputs_vec.push(x as i32);\n        inputs_vec.push(fixed_y as i32);\n        inputs_vec.push(equals_token as i32);\n    }\n\n    let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n        .reshape([modulus, 3]);\n    let (_, mlp_acts) = model.forward_with_mlp_activations(inputs);\n    let [batch_size, d_ff] = mlp_acts.dims();\n\n    if batch_size != modulus {\n        return Err(format!(\n            \"unexpected activation batch size {} (expected {})\",\n            batch_size, modulus\n        ));\n    }\n\n    let data: Vec<f32> = mlp_acts.into_data().to_vec().unwrap();\n    let mut per_neuron = vec![Vec::with_capacity(batch_size); d_ff];\n    for batch in 0..batch_size {\n        for neuron in 0..d_ff {\n            let idx = batch * d_ff + neuron;\n            per_neuron[neuron].push(data[idx] as f64);\n        }\n    }\n\n    Ok(per_neuron)\n}\n\n#[allow(dead_code)]\nfn surface_autocorrelation_scores(\n    surface: &[Vec<f64>],\n    max_lag: usize,\n) -> Result<(f64, f64, f64, f64), String> {\n    let (height, width) = surface_dimensions(surface)?;\n    if height < 2 || width < 2 {\n        return Err(\"surface grid too small\".to_string());\n    }\n\n    let (mean, var) = surface_mean_variance(surface)?;\n    if var == 0.0 {\n        return Err(\"surface variance is zero\".to_string());\n    }\n\n    let max_lag = max_lag.min(height.saturating_sub(1)).min(width.saturating_sub(1));\n    if max_lag == 0 {\n        return Err(\"max_lag too small for surface\".to_string());\n    }\n\n    let diag_main = average_autocorrelation(surface, mean, var, max_lag, 1, 1)?;\n    let diag_anti = average_autocorrelation(surface, mean, var, max_lag, 1, -1)?;\n    let axis_x = average_autocorrelation(surface, mean, var, max_lag, 1, 0)?;\n    let axis_y = average_autocorrelation(surface, mean, var, max_lag, 0, 1)?;\n\n    Ok((diag_main, diag_anti, axis_x, axis_y))\n}\n\n#[allow(dead_code)]\nfn surface_dimensions(surface: &[Vec<f64>]) -> Result<(usize, usize), String> {\n    let height = surface.len();\n    if height == 0 {\n        return Err(\"surface is empty\".to_string());\n    }\n    let width = surface[0].len();\n    if width == 0 {\n        return Err(\"surface has empty rows\".to_string());\n    }\n    for (idx, row) in surface.iter().enumerate() {\n        if row.len() != width {\n            return Err(format!(\n                \"surface row {} has length {}, expected {}\",\n                idx,\n                row.len(),\n                width\n            ));\n        }\n    }\n    Ok((height, width))\n}\n\n#[allow(dead_code)]\nfn surface_mean_variance(surface: &[Vec<f64>]) -> Result<(f64, f64), String> {\n    let (height, width) = surface_dimensions(surface)?;\n    let mut sum = 0.0;\n    for row in surface {\n        for value in row {\n            sum += *value;\n        }\n    }\n    let count = (height * width) as f64;\n    let mean = sum / count;\n    let mut variance = 0.0;\n    for row in surface {\n        for value in row {\n            let diff = *value - mean;\n            variance += diff * diff;\n        }\n    }\n    variance /= count;\n    Ok((mean, variance))\n}\n\n#[allow(dead_code)]\nfn average_autocorrelation(\n    surface: &[Vec<f64>],\n    mean: f64,\n    variance: f64,\n    max_lag: usize,\n    dx: i32,\n    dy: i32,\n) -> Result<f64, String> {\n    let (height, width) = surface_dimensions(surface)?;\n    let mut total = 0.0;\n    let mut lag_count = 0usize;\n\n    for lag in 1..=max_lag {\n        let mut sum = 0.0;\n        let mut count = 0usize;\n        let lag = lag as i32;\n        for x in 0..height {\n            for y in 0..width {\n                let x2 = x as i32 + dx * lag;\n                let y2 = y as i32 + dy * lag;\n                if x2 < 0 || y2 < 0 {\n                    continue;\n                }\n                let x2 = x2 as usize;\n                let y2 = y2 as usize;\n                if x2 >= height || y2 >= width {\n                    continue;\n                }\n                let a = surface[x][y] - mean;\n                let b = surface[x2][y2] - mean;\n                sum += a * b;\n                count += 1;\n            }\n        }\n        if count == 0 {\n            continue;\n        }\n        let corr = sum / (count as f64 * variance);\n        total += corr.abs();\n        lag_count += 1;\n    }\n\n    if lag_count == 0 {\n        return Err(\"no valid lags for autocorrelation\".to_string());\n    }\n\n    Ok(total / lag_count as f64)\n}\n\n#[allow(dead_code)]\nfn analyze_mlp_activation_wave(\n    neuron_index: usize,\n    signal: &[f64],\n    config: &MlpWaveVerificationConfig,\n) -> Result<MlpWaveNeuronReport, String> {\n    if signal.len() < 3 {\n        return Err(format!(\n            \"activation signal for neuron {} is too short\",\n            neuron_index\n        ));\n    }\n\n    let negative_count = signal\n        .iter()\n        .filter(|value| **value < -config.negative_tolerance)\n        .count();\n    let negative_fraction = negative_count as f64 / signal.len() as f64;\n    if negative_fraction > config.max_negative_fraction {\n        return Err(format!(\n            \"neuron {} has {:.3} negative activations\",\n            neuron_index, negative_fraction\n        ));\n    }\n\n    let (dominant_frequency, dominant_ratio) = dominant_frequency_ratio(signal)?;\n    if dominant_ratio < config.min_dominant_ratio {\n        return Err(format!(\n            \"neuron {} dominant frequency ratio {:.3} below threshold {:.3}\",\n            neuron_index, dominant_ratio, config.min_dominant_ratio\n        ));\n    }\n\n    let best_correlation =\n        best_rectified_sine_correlation(signal, dominant_frequency, config.phase_steps);\n    if best_correlation < config.min_correlation {\n        return Err(format!(\n            \"neuron {} rectified sine correlation {:.3} below threshold {:.3}\",\n            neuron_index, best_correlation, config.min_correlation\n        ));\n    }\n\n    Ok(MlpWaveNeuronReport {\n        neuron_index,\n        dominant_frequency,\n        dominant_ratio,\n        negative_fraction,\n        best_correlation,\n    })\n}\n\n#[allow(dead_code)]\nfn dominant_frequency_ratio(signal: &[f64]) -> Result<(usize, f64), String> {\n    if signal.len() < 2 {\n        return Err(\"signal too short for FFT\".to_string());\n    }\n\n    let mut planner = FftPlanner::new();\n    let fft = planner.plan_fft_forward(signal.len());\n    let mut buffer: Vec<Complex<f64>> = signal\n        .iter()\n        .map(|&x| Complex::new(x, 0.0))\n        .collect();\n    fft.process(&mut buffer);\n    let magnitudes: Vec<f64> = buffer.iter().map(|c| c.norm()).collect();\n\n    if magnitudes.len() < 2 {\n        return Err(\"insufficient FFT bins\".to_string());\n    }\n\n    let mut dominant_idx = 1usize;\n    let mut dominant_mag = magnitudes[1];\n    for (idx, &mag) in magnitudes.iter().enumerate().skip(2) {\n        if mag > dominant_mag {\n            dominant_mag = mag;\n            dominant_idx = idx;\n        }\n    }\n\n    if dominant_idx == 0 {\n        return Err(\"dominant frequency is zero\".to_string());\n    }\n\n    let mut nonzero = magnitudes[1..].to_vec();\n    nonzero.sort_by(|a, b| a.partial_cmp(b).unwrap_or(std::cmp::Ordering::Equal));\n    let median_mag = nonzero[nonzero.len() / 2];\n    let ratio = if median_mag == 0.0 {\n        dominant_mag\n    } else {\n        dominant_mag / median_mag\n    };\n\n    Ok((dominant_idx, ratio))\n}\n\n#[allow(dead_code)]\nfn best_rectified_sine_correlation(signal: &[f64], frequency: usize, phase_steps: usize) -> f64 {\n    if signal.is_empty() || frequency == 0 || phase_steps == 0 {\n        return 0.0;\n    }\n\n    let tau = std::f64::consts::PI * 2.0;\n    let mut best = -1.0;\n    for step in 0..phase_steps {\n        let phase = tau * step as f64 / phase_steps as f64;\n        let wave: Vec<f64> = (0..signal.len())\n            .map(|idx| {\n                let angle = tau * frequency as f64 * idx as f64 / signal.len() as f64 + phase;\n                angle.sin().max(0.0)\n            })\n            .collect();\n        let corr = pearson_correlation(signal, &wave);\n        if corr > best {\n            best = corr;\n        }\n    }\n\n    best\n}\n\n#[allow(dead_code)]\nfn pearson_correlation(a: &[f64], b: &[f64]) -> f64 {\n    if a.len() != b.len() || a.is_empty() {\n        return 0.0;\n    }\n\n    let mean_a = a.iter().sum::<f64>() / a.len() as f64;\n    let mean_b = b.iter().sum::<f64>() / b.len() as f64;\n\n    let mut num = 0.0;\n    let mut denom_a = 0.0;\n    let mut denom_b = 0.0;\n    for (x, y) in a.iter().zip(b.iter()) {\n        let da = x - mean_a;\n        let db = y - mean_b;\n        num += da * db;\n        denom_a += da * da;\n        denom_b += db * db;\n    }\n\n    if denom_a == 0.0 || denom_b == 0.0 {\n        return 0.0;\n    }\n\n    num / (denom_a.sqrt() * denom_b.sqrt())\n}\n\n/// Verify that the model actually learned modular addition\n/// by testing it on ALL possible examples systematically\npub fn verify_full_accuracy<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n) -> (f32, f32) {\n    println!(\"ðŸ” Verifying model on COMPLETE dataset (no sampling)...\");\n\n    let train_dataset = ModularAdditionDataset::new(true, 42);\n    let val_dataset = ModularAdditionDataset::new(false, 42);\n\n    let train_acc = test_all_examples(model, &train_dataset, device);\n    let val_acc = test_all_examples(model, &val_dataset, device);\n\n    println!(\"ðŸ“Š FULL Dataset Results:\");\n    println!(\"   Training: {}/{} = {:.2}%\",\n        (train_acc * train_dataset.len() as f32) as usize,\n        train_dataset.len(),\n        train_acc * 100.0);\n    println!(\"   Validation: {}/{} = {:.2}%\",\n        (val_acc * val_dataset.len() as f32) as usize,\n        val_dataset.len(),\n        val_acc * 100.0);\n\n    (train_acc, val_acc)\n}\n\n/// Test model on ALL examples in dataset (no random sampling)\nfn test_all_examples<B: Backend>(\n    model: &Transformer<B>,\n    dataset: &ModularAdditionDataset,\n    device: &B::Device,\n) -> f32 {\n    let mut total_correct = 0;\n    let total = dataset.len();\n\n    // Test in batches to avoid memory issues\n    let batch_size = 100;\n    for batch_start in (0..total).step_by(batch_size) {\n        let batch_end = (batch_start + batch_size).min(total);\n        let batch_len = batch_end - batch_start;\n\n        let mut inputs_vec = Vec::new();\n        let mut targets_vec = Vec::new();\n\n        for idx in batch_start..batch_end {\n            let (input, target) = dataset.get(idx).unwrap();\n            inputs_vec.extend(input);\n            targets_vec.push(target as i32);\n        }\n\n        let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n            .reshape([batch_len, 3]);\n\n        let logits = model.forward(inputs);\n        let predictions = logits.argmax(1).squeeze::<1>();\n        let predictions_vec: Vec<i32> = predictions.into_data().to_vec().unwrap();\n\n        let correct = predictions_vec\n            .iter()\n            .zip(targets_vec.iter())\n            .filter(|(p, t)| p == t)\n            .count();\n\n        total_correct += correct;\n    }\n\n    total_correct as f32 / total as f32\n}\n\n/// Verify that weight norms decrease over time with weight decay\n/// and show correlation with the grokking transition\npub fn verify_weight_norm_decay(\n    weight_norm_history: &[(usize, f64)],\n    grok_step: Option<usize>,\n    config: &WeightNormDecayConfig,\n) -> Result<WeightNormDecayReport, String> {\n    if weight_norm_history.len() < 2 {\n        return Err(\"weight norm history requires at least 2 snapshots\".to_string());\n    }\n\n    let initial_norm = weight_norm_history\n        .first()\n        .ok_or_else(|| \"weight norm history is empty\".to_string())?\n        .1;\n    let final_norm = weight_norm_history\n        .last()\n        .ok_or_else(|| \"weight norm history is empty\".to_string())?\n        .1;\n\n    if initial_norm <= 0.0 {\n        return Err(\"initial weight norm must be positive\".to_string());\n    }\n\n    // Check that norms are generally decreasing (with weight decay)\n    let relative_decrease = (initial_norm - final_norm) / initial_norm;\n    if relative_decrease < config.min_relative_decrease {\n        return Err(format!(\n            \"weight norm decrease {:.4} below threshold {:.4} (initial {:.4}, final {:.4})\",\n            relative_decrease, config.min_relative_decrease, initial_norm, final_norm\n        ));\n    }\n\n    // Check that norms are not increasing (weight decay should monotonically decrease norms)\n    for window in weight_norm_history.windows(2) {\n        let prev_norm = window[0].1;\n        let next_norm = window[1].1;\n        if next_norm > prev_norm * 1.01 {\n            // Allow 1% tolerance for numerical noise\n            return Err(format!(\n                \"weight norm increased from {:.4} to {:.4} at step {}\",\n                prev_norm, next_norm, window[1].0\n            ));\n        }\n    }\n\n    // If grok step is provided, check that norms show a shift around grokking\n    let grok_step_norm = if let Some(step) = grok_step {\n        weight_norm_history\n            .iter()\n            .filter(|(s, _)| {\n                *s >= step.saturating_sub(config.grok_step_window)\n                    && *s <= step + config.grok_step_window\n            })\n            .map(|(_, norm)| *norm)\n            .next()\n    } else {\n        None\n    };\n\n    Ok(WeightNormDecayReport {\n        initial_norm,\n        final_norm,\n        relative_decrease,\n        grok_step_norm,\n    })\n}\n\n/// Verify the characteristic \"snake curve\" shape of grokking phenomenon\n/// on log-scale plots: train accuracy rises early, validation stays flat then jumps\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_snake_curve_shape(\n    train_acc: &[(usize, f32)],\n    val_acc: &[(usize, f32)],\n    config: &SnakeCurveConfig,\n) -> Result<SnakeCurveReport, String> {\n    if train_acc.is_empty() || val_acc.is_empty() {\n        return Err(\"empty accuracy history\".to_string());\n    }\n\n    // Phase 1: Verify early train convergence (immediate drop on log scale)\n    let train_converged_step = train_acc\n        .iter()\n        .find(|(_, acc)| *acc >= config.train_convergence_threshold)\n        .map(|(step, _)| *step)\n        .ok_or_else(|| {\n            format!(\n                \"train accuracy never reaches {} threshold\",\n                config.train_convergence_threshold\n            )\n        })?;\n\n    if train_converged_step > config.early_step_max {\n        return Err(format!(\n            \"train accuracy converges at step {}, expected before {}\",\n            train_converged_step, config.early_step_max\n        ));\n    }\n\n    // Phase 2: Verify validation plateau (flat on log scale)\n    let plateau_samples: Vec<f32> = val_acc\n        .iter()\n        .filter(|(step, _)| *step >= 1 && *step <= config.val_plateau_min_step)\n        .map(|(_, acc)| *acc)\n        .collect();\n\n    if plateau_samples.is_empty() {\n        return Err(\"no validation samples in plateau region\".to_string());\n    }\n\n    let val_plateau_max = plateau_samples\n        .iter()\n        .fold(0.0f32, |max_val, &acc| max_val.max(acc));\n\n    if val_plateau_max > config.val_plateau_max_acc {\n        return Err(format!(\n            \"validation accuracy in plateau region too high: max {:.4} exceeds {:.4}\",\n            val_plateau_max, config.val_plateau_max_acc\n        ));\n    }\n\n    // Phase 3: Verify delayed validation jump (crash up on log scale)\n    let val_grok_step = val_acc\n        .iter()\n        .filter(|(step, _)| *step > config.val_plateau_min_step)\n        .find(|(_, acc)| *acc >= config.val_final_threshold)\n        .map(|(step, _)| *step)\n        .ok_or_else(|| {\n            format!(\n                \"validation accuracy never reaches {} after plateau\",\n                config.val_final_threshold\n            )\n        })?;\n\n    // Phase 4: Verify phase ordering (snake shape: early train, late val)\n    let curve_shape_valid = train_converged_step < config.val_plateau_min_step\n        && val_grok_step > config.val_plateau_min_step\n        && train_converged_step < val_grok_step;\n\n    if !curve_shape_valid {\n        return Err(format!(\n            \"invalid curve shape: train converged at {}, val plateau until {}, val grokked at {}\",\n            train_converged_step, config.val_plateau_min_step, val_grok_step\n        ));\n    }\n\n    Ok(SnakeCurveReport {\n        train_converged_by_step: train_converged_step,\n        val_plateau_max_acc: val_plateau_max,\n        val_grok_step,\n        curve_shape_valid,\n    })\n}\n\n/// Test specific examples to see if model actually computes mod p\npub fn test_specific_examples<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n) {\n    println!();\n    println!(\"ðŸ§ª Testing specific examples:\");\n\n    let modulus = ModularAdditionDataset::modulus();\n    let equals_token = ModularAdditionDataset::equals_token();\n    let test_cases = vec![\n        (0, 0, 0),\n        (1, 1, 2),\n        (50, 50, (50 + 50) % modulus),\n        (112, 1, (112 + 1) % modulus),\n        (56, 57, (56 + 57) % modulus),\n        (112, 112, (112 + 112) % modulus),\n        (10, 20, 30),\n        (60, 60, (60 + 60) % modulus),\n    ];\n\n    for (a, b, expected) in test_cases {\n        let input_vec = vec![a as i32, b as i32, equals_token as i32];\n        let input = Tensor::<B, 1, Int>::from_ints(input_vec.as_slice(), device)\n            .reshape([1, 3]);\n\n        let logits = model.forward(input);\n        let prediction = logits.argmax(1).squeeze::<1>();\n        let pred_value: i32 = prediction.into_data().to_vec().unwrap()[0];\n\n        let correct = if pred_value == expected as i32 { \"âœ“\" } else { \"âœ—\" };\n        println!(\n            \"   {} + {} mod {} = {} | Model: {} {}\",\n            a, b, modulus, expected, pred_value, correct\n        );\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    fn make_histories(\n        train_acc: &[(usize, f32)],\n        val_acc: &[(usize, f32)],\n        val_loss: &[(usize, f64)],\n    ) -> (LossHistory, AccuracyHistory) {\n        let mut loss_history = LossHistory::new();\n        loss_history.val_snapshots = val_loss.to_vec();\n        let mut accuracy_history = AccuracyHistory::new();\n        accuracy_history.train_snapshots = train_acc.to_vec();\n        accuracy_history.val_snapshots = val_acc.to_vec();\n        (loss_history, accuracy_history)\n    }\n\n    #[test]\n    fn grokking_phase_verification_passes_on_synthetic_signal() {\n        let train_acc = vec![(0, 0.2), (500, 0.995), (1000, 0.999)];\n        let val_acc = vec![\n            (0, 0.01),\n            (500, 0.01),\n            (1500, 0.01),\n            (2200, 0.995),\n            (2500, 0.999),\n        ];\n        let val_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 2.0),\n            (2200, 0.2),\n            (2500, 0.15),\n        ];\n\n        let (loss_history, accuracy_history) = make_histories(&train_acc, &val_acc, &val_loss);\n        let config = GrokkingVerificationConfig {\n            early_step_max: 1000,\n            plateau_min_step: 1500,\n            generalization_window: 1500,\n            accuracy_window: 2,\n            loss_window: 2,\n            ..GrokkingVerificationConfig::default_for_modulus(ModularAdditionDataset::modulus())\n        };\n\n        let report = verify_grokking_phase_transition(&loss_history, &accuracy_history, &config)\n            .expect(\"expected grokking verification to pass\");\n        assert!(report.transition_step >= config.plateau_min_step);\n        assert!(report.early_train_step <= config.early_step_max);\n    }\n\n    #[test]\n    fn grokking_phase_verification_fails_without_transition() {\n        let train_acc = vec![(0, 0.2), (500, 0.995)];\n        let val_acc = vec![(0, 0.01), (500, 0.01), (1500, 0.02)];\n        let val_loss = vec![(0, 2.0), (500, 2.0), (1500, 1.9)];\n\n        let (loss_history, accuracy_history) = make_histories(&train_acc, &val_acc, &val_loss);\n        let config = GrokkingVerificationConfig {\n            early_step_max: 1000,\n            plateau_min_step: 1000,\n            generalization_window: 1000,\n            accuracy_window: 2,\n            loss_window: 2,\n            ..GrokkingVerificationConfig::default_for_modulus(ModularAdditionDataset::modulus())\n        };\n\n        let err = verify_grokking_phase_transition(&loss_history, &accuracy_history, &config)\n            .expect_err(\"expected grokking verification to fail\");\n        assert!(err.contains(\"validation accuracy never reaches target\"));\n    }\n\n    #[test]\n    fn grokking_phase_verification_fails_with_slow_accuracy_rise() {\n        let train_acc = vec![(0, 0.2), (500, 0.995), (1000, 0.999)];\n        let val_acc = vec![\n            (0, 0.01),\n            (500, 0.01),\n            (1500, 0.5),\n            (2000, 0.6),\n            (2200, 0.995),\n            (2500, 0.999),\n        ];\n        let val_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 2.0),\n            (2200, 0.2),\n            (2500, 0.15),\n        ];\n\n        let (loss_history, accuracy_history) = make_histories(&train_acc, &val_acc, &val_loss);\n        let config = GrokkingVerificationConfig {\n            early_step_max: 1000,\n            plateau_min_step: 1000,\n            generalization_window: 2000,\n            accuracy_window: 2,\n            loss_window: 2,\n            ..GrokkingVerificationConfig::default_for_modulus(ModularAdditionDataset::modulus())\n        };\n\n        let err = verify_grokking_phase_transition(&loss_history, &accuracy_history, &config)\n            .expect_err(\"expected grokking verification to fail\");\n        assert!(err.contains(\"plateau too high\"));\n    }\n\n    #[test]\n    fn restricted_loss_detection_passes_with_early_drop() {\n        let full_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 2.0),\n            (2500, 2.0),\n            (3500, 0.3),\n        ];\n        let restricted_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 0.8),\n            (2500, 0.4),\n            (3500, 0.3),\n        ];\n\n        let config = RestrictedLossVerificationConfig {\n            plateau_min_step: 1000,\n            drop_window: 2,\n            drop_fraction: 0.5,\n            min_step_lead: 500,\n        };\n\n        let report = verify_restricted_loss_early_drop(&full_loss, &restricted_loss, &config)\n            .expect(\"expected restricted loss verification to pass\");\n        assert!(report.restricted_drop_step < report.full_drop_step);\n    }\n\n    #[test]\n    fn restricted_loss_detection_fails_when_full_drops_first() {\n        let full_loss = vec![(0, 2.0), (1000, 1.0), (1500, 0.8)];\n        let restricted_loss = vec![(0, 2.0), (1000, 2.0), (1500, 0.8)];\n\n        let config = RestrictedLossVerificationConfig {\n            plateau_min_step: 500,\n            drop_window: 1,\n            drop_fraction: 0.3,\n            min_step_lead: 200,\n        };\n\n        let err = verify_restricted_loss_early_drop(&full_loss, &restricted_loss, &config)\n            .expect_err(\"expected restricted loss verification to fail\");\n        assert!(err.contains(\"restricted loss drops\"));\n    }\n\n    #[test]\n    fn excluded_loss_spike_verification_passes() {\n        let excluded_loss = vec![(0, 1.0), (1500, 2.2), (3000, 1.1)];\n        let config = ExcludedLossSpikeConfig {\n            min_relative_increase: 0.5,\n        };\n\n        let report = verify_excluded_loss_spike(&excluded_loss, &config)\n            .expect(\"expected excluded loss spike verification to pass\");\n        assert_eq!(report.spike_step, 1500);\n    }\n\n    #[test]\n    fn excluded_loss_spike_verification_fails_without_spike() {\n        let excluded_loss = vec![(0, 1.0), (1500, 1.1), (3000, 1.0)];\n        let config = ExcludedLossSpikeConfig {\n            min_relative_increase: 0.5,\n        };\n\n        let err = verify_excluded_loss_spike(&excluded_loss, &config)\n            .expect_err(\"expected excluded loss spike verification to fail\");\n        assert!(err.contains(\"excluded loss spike too small\"));\n    }\n\n    #[test]\n    fn restricted_loss_drop_verification_passes() {\n        let restricted_loss = vec![\n            (0, 2.0),\n            (1000, 1.8),\n            (5000, 1.2),\n            (7000, 0.8),\n            (10000, 0.9),\n        ];\n        let config = RestrictedLossDropConfig {\n            min_relative_decrease: 0.3,\n        };\n\n        let report = verify_restricted_loss_drop(&restricted_loss, &config)\n            .expect(\"expected restricted loss drop verification to pass\");\n        assert_eq!(report.drop_step, 7000);\n        assert!(report.baseline_loss > report.drop_loss);\n        assert!((report.baseline_loss - report.drop_loss) / report.baseline_loss >= 0.3);\n    }\n\n    #[test]\n    fn restricted_loss_drop_verification_fails_without_drop() {\n        let restricted_loss = vec![\n            (0, 2.0),\n            (1000, 2.1),\n            (5000, 1.9),\n            (7000, 2.0),\n            (10000, 2.05),\n        ];\n        let config = RestrictedLossDropConfig {\n            min_relative_decrease: 0.3,\n        };\n\n        let err = verify_restricted_loss_drop(&restricted_loss, &config)\n            .expect_err(\"expected restricted loss drop verification to fail\");\n        assert!(err.contains(\"restricted loss decrease too small\"));\n    }\n\n    #[test]\n    fn mlp_activation_wave_verification_passes_on_rectified_sine() {\n        let n = ModularAdditionDataset::modulus();\n        let freq = 5.0;\n        let mut signal = Vec::with_capacity(n);\n        for idx in 0..n {\n            let angle = 2.0 * std::f64::consts::PI * freq * idx as f64 / n as f64;\n            let rectified = angle.sin().max(0.0);\n            let wobble = (2.0 * std::f64::consts::PI * 2.0 * idx as f64 / n as f64)\n                .sin()\n                .abs()\n                * 0.02;\n            signal.push(rectified + wobble);\n        }\n\n        let config = MlpWaveVerificationConfig {\n            max_negative_fraction: 0.0,\n            negative_tolerance: 1e-6,\n            min_dominant_ratio: 3.0,\n            min_correlation: 0.7,\n            phase_steps: 32,\n        };\n\n        let report =\n            verify_mlp_activation_waves_from_signals(&[(0, signal)], &config)\n                .expect(\"expected rectified sine to pass\");\n        assert_eq!(report.neuron_reports.len(), 1);\n    }\n\n    #[test]\n    fn mlp_activation_wave_verification_fails_on_noise() {\n        let n = ModularAdditionDataset::modulus();\n        let signal: Vec<f64> = (0..n).map(|idx| idx as f64 / n as f64).collect();\n\n        let config = MlpWaveVerificationConfig {\n            max_negative_fraction: 0.0,\n            negative_tolerance: 1e-6,\n            min_dominant_ratio: 4.0,\n            min_correlation: 0.8,\n            phase_steps: 16,\n        };\n\n        let err =\n            verify_mlp_activation_waves_from_signals(&[(0, signal)], &config)\n                .expect_err(\"expected linear signal to fail\");\n        assert!(err.contains(\"dominant frequency ratio\") || err.contains(\"correlation\"));\n    }\n\n    #[test]\n    fn pairwise_manifold_transition_passes_on_elliptic_signals() {\n        let n = ModularAdditionDataset::modulus();\n        let freq = 5.0;\n        let mut post_signals = Vec::new();\n        for neuron_idx in 0..7usize {\n            let phase = 2.0 * std::f64::consts::PI * neuron_idx as f64 / 7.0;\n            let signal: Vec<f64> = (0..n)\n                .map(|idx| {\n                    let angle = 2.0 * std::f64::consts::PI * freq * idx as f64 / n as f64 + phase;\n                    angle.sin()\n                })\n                .collect();\n            post_signals.push((neuron_idx, signal));\n        }\n\n        let mut pre_signals = Vec::new();\n        for neuron_idx in 0..7usize {\n            let signal: Vec<f64> = (0..n)\n                .map(|idx| idx as f64 / n as f64 + neuron_idx as f64 * 0.01)\n                .collect();\n            pre_signals.push((neuron_idx, signal));\n        }\n\n        let config = PairwiseManifoldConfig {\n            fixed_y: 0,\n            neuron_indices: (0..7).collect(),\n            min_post_score: 0.75,\n            max_pre_score: 0.55,\n            max_axis_ratio: 8.0,\n            min_pairs_passing: 8,\n        };\n\n        let report = verify_pairwise_manifold_transition_from_signals(\n            &pre_signals,\n            &post_signals,\n            &config,\n        )\n        .expect(\"expected pairwise manifold transition to pass\");\n        assert!(report.passing_pairs >= config.min_pairs_passing);\n    }\n\n    #[test]\n    fn pairwise_manifold_transition_fails_without_rings() {\n        let n = ModularAdditionDataset::modulus();\n        let mut signals = Vec::new();\n        for neuron_idx in 0..7usize {\n            let signal: Vec<f64> = (0..n)\n                .map(|idx| idx as f64 / n as f64 + neuron_idx as f64 * 0.02)\n                .collect();\n            signals.push((neuron_idx, signal));\n        }\n\n        let config = PairwiseManifoldConfig {\n            fixed_y: 0,\n            neuron_indices: (0..7).collect(),\n            min_post_score: 0.8,\n            max_pre_score: 0.55,\n            max_axis_ratio: 4.0,\n            min_pairs_passing: 5,\n        };\n\n        let err = verify_pairwise_manifold_transition_from_signals(\n            &signals,\n            &signals,\n            &config,\n        )\n        .expect_err(\"expected pairwise manifold transition to fail\");\n        assert!(err.contains(\"post-grok\") || err.contains(\"pre-grok\"));\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_passes_on_diagonal_corrugation() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create diagonal ridges: f(x,y) = sin(2Ï€ * freq * (x+y) / n)\n        let freq = 3.0;\n        for x in 0..n {\n            for y in 0..n {\n                let angle = 2.0 * std::f64::consts::PI * freq * (x + y) as f64 / n as f64;\n                surface[x][y] = angle.sin();\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let report = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect(\"expected diagonal ridge verification to pass\");\n        assert!(report.diagonal_score > report.axis_score);\n        assert!(report.ratio >= config.min_diagonal_ratio);\n        assert!(report.diagonal_score >= config.min_diagonal_score);\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_passes_on_anti_diagonal_pattern() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create anti-diagonal ridges: f(x,y) = sin(2Ï€ * freq * (x-y) / n)\n        let freq = 3.0;\n        for x in 0..n {\n            for y in 0..n {\n                let angle = 2.0 * std::f64::consts::PI * freq * (x as i32 - y as i32) as f64 / n as f64;\n                surface[x][y] = angle.sin();\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let report = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect(\"expected anti-diagonal ridge verification to pass\");\n        assert!(report.diagonal_score > report.axis_score);\n        assert!(report.ratio >= config.min_diagonal_ratio);\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_fails_on_axis_aligned_pattern() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create axis-aligned ridges: f(x,y) = sin(2Ï€ * freq * x / n)\n        let freq = 3.0;\n        for x in 0..n {\n            for y in 0..n {\n                let angle = 2.0 * std::f64::consts::PI * freq * x as f64 / n as f64;\n                surface[x][y] = angle.sin();\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let err = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect_err(\"expected axis-aligned pattern to fail diagonal verification\");\n        assert!(err.contains(\"diagonal/axis ratio\") || err.contains(\"diagonal autocorrelation\"));\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_fails_on_flat_surface() {\n        let n = 50;\n        let surface = vec![vec![1.5; n]; n];\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let err = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect_err(\"expected flat surface to fail\");\n        assert!(err.contains(\"surface variance is zero\"));\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_fails_on_random_noise() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create random noise (low autocorrelation everywhere)\n        for x in 0..n {\n            for y in 0..n {\n                surface[x][y] = ((x * 17 + y * 31) % 100) as f64 / 100.0;\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let err = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect_err(\"expected random noise to fail diagonal verification\");\n        assert!(err.contains(\"diagonal/axis ratio\") || err.contains(\"diagonal autocorrelation\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_passes_on_valid_grokking_curve() {\n        // Simulate typical grokking: train converges early, val plateaus then jumps\n        let train_acc = vec![\n            (1, 0.10),\n            (10, 0.50),\n            (100, 0.80),\n            (500, 0.98),\n            (1000, 0.99),\n            (2000, 0.995),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (10, 0.009),\n            (100, 0.01),\n            (500, 0.011),\n            (1000, 0.009),\n            (1500, 0.01),\n            (2000, 0.92),\n            (3000, 0.98),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 1500,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let report = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect(\"expected snake curve verification to pass\");\n\n        assert_eq!(report.train_converged_by_step, 500);\n        assert!(report.val_plateau_max_acc < config.val_plateau_max_acc);\n        assert_eq!(report.val_grok_step, 2000);\n        assert!(report.curve_shape_valid);\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_on_simultaneous_convergence() {\n        // Both train and val converge at the same time (no grokking)\n        let train_acc = vec![\n            (1, 0.10),\n            (100, 0.98),\n            (500, 0.99),\n        ];\n        let val_acc = vec![\n            (1, 0.10),\n            (100, 0.97),\n            (500, 0.98),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 200,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let err = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect_err(\"expected snake curve verification to fail\");\n        assert!(err.contains(\"plateau region too high\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_on_slow_train_convergence() {\n        // Train takes too long to converge (not the typical snake curve)\n        let train_acc = vec![\n            (1, 0.10),\n            (500, 0.50),\n            (1000, 0.70),\n            (2000, 0.98),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (500, 0.01),\n            (1000, 0.01),\n            (2000, 0.92),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 1500,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let err = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect_err(\"expected snake curve verification to fail\");\n        assert!(err.contains(\"converges at step 2000\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_without_val_jump() {\n        // Validation never jumps up (no generalization)\n        let train_acc = vec![\n            (1, 0.10),\n            (100, 0.98),\n            (500, 0.99),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (100, 0.01),\n            (500, 0.01),\n            (2000, 0.02),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 200,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let err = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect_err(\"expected snake curve verification to fail\");\n        assert!(err.contains(\"validation accuracy never reaches\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_on_wrong_phase_ordering() {\n        // Val jumps before train converges (backwards snake)\n        let train_acc = vec![\n            (1, 0.10),\n            (500, 0.50),\n            (2000, 0.98),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (100, 0.95),\n            (500, 0.98),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 200,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let result = verify_snake_curve_shape(&train_acc, &val_acc, &config);\n        // Should fail because val jumps before train converges (not a proper snake curve)\n        // This can fail for multiple reasons, but should definitely fail\n        assert!(result.is_err(), \"Expected snake curve verification to fail but it passed\");\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_passes_on_decreasing_norms() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 85.0),\n            (1000, 72.0),\n            (1500, 63.0),\n            (2000, 58.0),\n            (2500, 55.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let report = verify_weight_norm_decay(&weight_norms, Some(2000), &config)\n            .expect(\"expected weight norm decay verification to pass\");\n\n        assert_eq!(report.initial_norm, 100.0);\n        assert_eq!(report.final_norm, 55.0);\n        assert!((report.relative_decrease - 0.45).abs() < 0.01);\n        assert!(report.grok_step_norm.is_some());\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_fails_on_flat_norms() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 99.0),\n            (1000, 98.5),\n            (1500, 98.0),\n            (2000, 97.5),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let err = verify_weight_norm_decay(&weight_norms, None, &config)\n            .expect_err(\"expected weight norm decay verification to fail\");\n        assert!(err.contains(\"weight norm decrease\"));\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_fails_on_increasing_norms() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 95.0),\n            (1000, 105.0), // Increase here\n            (1500, 90.0),\n            (2000, 85.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let err = verify_weight_norm_decay(&weight_norms, None, &config)\n            .expect_err(\"expected weight norm decay verification to fail\");\n        assert!(err.contains(\"weight norm increased\"));\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_allows_small_noise() {\n        // Weight decay with small numerical noise (within 1% tolerance)\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 95.0),\n            (1000, 90.5),  // Small increase within tolerance\n            (1500, 90.0),\n            (2000, 85.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let report = verify_weight_norm_decay(&weight_norms, None, &config)\n            .expect(\"expected weight norm decay with small noise to pass\");\n\n        assert_eq!(report.initial_norm, 100.0);\n        assert_eq!(report.final_norm, 85.0);\n        assert!((report.relative_decrease - 0.15).abs() < 0.01);\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_finds_grok_step_norm() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (1000, 80.0),\n            (1800, 68.0),  // Within window of grok step (2200 - 500 = 1700)\n            (2000, 65.0),\n            (2200, 62.0),  // Around grok step\n            (3000, 55.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let report = verify_weight_norm_decay(&weight_norms, Some(2200), &config)\n            .expect(\"expected weight norm decay verification to pass\");\n\n        assert!(report.grok_step_norm.is_some());\n        let grok_norm = report.grok_step_norm.unwrap();\n        // Should find the first norm in the window [1700, 2700], which is 68.0 at step 1800\n        assert!((grok_norm - 68.0).abs() < 0.01);\n    }\n}\n","structuredPatch":[{"oldStart":168,"oldLines":6,"newStart":168,"newLines":7,"lines":[" }"," "," impl DiagonalRidgeConfig {","+    #[allow(dead_code)]","     pub fn default_for_modulus(modulus: usize) -> Self {","         let max_lag = (modulus / 10).max(4).min(20);","         Self {"]},{"oldStart":180,"oldLines":6,"newStart":181,"newLines":7,"lines":[" }"," "," #[derive(Debug, Clone)]","+#[allow(dead_code)]"," pub struct DiagonalRidgeReport {","     pub neuron_index: usize,","     pub diagonal_score: f64,"]}],"userModified":false,"replaceAll":false}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_012SBcQTgPN76LV9AX2B9RQX","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01KVaxe9LeVvh2Db2SUAiKwy","name":"Edit","input":{"replace_all":false,"file_path":"/Users/admin/MechInt/src/verify.rs","old_string":"impl SnakeCurveConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        Self {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 1000,\n            val_plateau_max_acc: 1.0 / modulus as f32 + 0.05,\n            val_final_threshold: 0.90,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct WeightNormDecayConfig {\n    pub min_relative_decrease: f64,\n    pub plateau_min_step: usize,\n    pub grok_step_window: usize,\n}","new_string":"impl SnakeCurveConfig {\n    #[allow(dead_code)]\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        Self {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 1000,\n            val_plateau_max_acc: 1.0 / modulus as f32 + 0.05,\n            val_final_threshold: 0.90,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\n#[allow(dead_code)]\npub struct WeightNormDecayConfig {\n    pub min_relative_decrease: f64,\n    pub plateau_min_step: usize,\n    pub grok_step_window: usize,\n}"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":855,"cache_read_input_tokens":79496,"cache_creation":{"ephemeral_5m_input_tokens":855,"ephemeral_1h_input_tokens":0},"output_tokens":500,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"eaaba0a1-6491-4156-8b2c-0760bec9cb0f"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01KVaxe9LeVvh2Db2SUAiKwy","type":"tool_result","content":"The file /Users/admin/MechInt/src/verify.rs has been updated. Here's the result of running `cat -n` on a snippet of the edited file:\n   197â†’    pub val_plateau_max_acc: f32,\n   198â†’    pub val_final_threshold: f32,\n   199â†’}\n   200â†’\n   201â†’impl SnakeCurveConfig {\n   202â†’    #[allow(dead_code)]\n   203â†’    pub fn default_for_modulus(modulus: usize) -> Self {\n   204â†’        Self {\n   205â†’            early_step_max: 1000,\n   206â†’            train_convergence_threshold: 0.95,\n   207â†’            val_plateau_min_step: 1000,\n   208â†’            val_plateau_max_acc: 1.0 / modulus as f32 + 0.05,\n   209â†’            val_final_threshold: 0.90,\n   210â†’        }\n   211â†’    }\n   212â†’}\n   213â†’\n   214â†’#[derive(Debug, Clone)]\n   215â†’#[allow(dead_code)]\n   216â†’pub struct WeightNormDecayConfig {\n   217â†’    pub min_relative_decrease: f64,\n   218â†’    pub plateau_min_step: usize,\n   219â†’    pub grok_step_window: usize,\n   220â†’}\n   221â†’\n   222â†’impl WeightNormDecayConfig {\n   223â†’    pub fn default_for_grokking() -> Self {\n   224â†’        Self {"}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"b3a27255-a515-4c24-b7ec-70a2fa51ec2c","tool_use_result":{"filePath":"/Users/admin/MechInt/src/verify.rs","oldString":"impl SnakeCurveConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        Self {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 1000,\n            val_plateau_max_acc: 1.0 / modulus as f32 + 0.05,\n            val_final_threshold: 0.90,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct WeightNormDecayConfig {\n    pub min_relative_decrease: f64,\n    pub plateau_min_step: usize,\n    pub grok_step_window: usize,\n}","newString":"impl SnakeCurveConfig {\n    #[allow(dead_code)]\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        Self {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 1000,\n            val_plateau_max_acc: 1.0 / modulus as f32 + 0.05,\n            val_final_threshold: 0.90,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\n#[allow(dead_code)]\npub struct WeightNormDecayConfig {\n    pub min_relative_decrease: f64,\n    pub plateau_min_step: usize,\n    pub grok_step_window: usize,\n}","originalFile":"use burn::{\n    data::dataset::Dataset,\n    tensor::{backend::Backend, Int, Tensor},\n};\nuse rustfft::{num_complex::Complex, FftPlanner};\n\nuse crate::analysis::{AccuracyHistory, LossHistory};\nuse crate::data::ModularAdditionDataset;\nuse crate::model::Transformer;\n\n#[derive(Debug, Clone)]\npub struct GrokkingVerificationConfig {\n    pub early_train_acc_threshold: f32,\n    pub early_step_max: usize,\n    pub plateau_min_step: usize,\n    pub generalization_window: usize,\n    pub chance_accuracy: f32,\n    pub chance_tolerance: f32,\n    pub target_val_acc_threshold: f32,\n    pub accuracy_window: usize,\n    pub loss_window: usize,\n    pub loss_drop_fraction: f64,\n}\n\nimpl GrokkingVerificationConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        Self {\n            early_train_acc_threshold: 0.99,\n            early_step_max: 1000,\n            plateau_min_step: 1000,\n            generalization_window: 10_000,\n            chance_accuracy: 1.0 / modulus as f32,\n            chance_tolerance: 0.02,\n            target_val_acc_threshold: 0.99,\n            accuracy_window: 5,\n            loss_window: 5,\n            loss_drop_fraction: 0.5,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\n#[allow(dead_code)]\npub struct GrokkingPhaseReport {\n    pub early_train_step: usize,\n    pub transition_step: usize,\n    pub plateau_max_val_acc: f32,\n    pub loss_drop_ratio: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossVerificationConfig {\n    pub plateau_min_step: usize,\n    pub drop_window: usize,\n    pub drop_fraction: f64,\n    pub min_step_lead: usize,\n}\n\n#[derive(Debug, Clone)]\n#[allow(dead_code)]\npub struct RestrictedLossReport {\n    pub restricted_drop_step: usize,\n    pub full_drop_step: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossDropConfig {\n    pub min_relative_decrease: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossDropReport {\n    pub drop_step: usize,\n    pub drop_loss: f64,\n    pub baseline_loss: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct ExcludedLossSpikeConfig {\n    pub min_relative_increase: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct ExcludedLossSpikeReport {\n    pub spike_step: usize,\n    pub spike_loss: f64,\n    pub baseline_loss: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct MlpWaveVerificationConfig {\n    pub max_negative_fraction: f64,\n    pub negative_tolerance: f64,\n    pub min_dominant_ratio: f64,\n    pub min_correlation: f64,\n    pub phase_steps: usize,\n}\n\n#[derive(Debug, Clone)]\n#[allow(dead_code)]\npub struct MlpWaveNeuronReport {\n    pub neuron_index: usize,\n    pub dominant_frequency: usize,\n    pub dominant_ratio: f64,\n    pub negative_fraction: f64,\n    pub best_correlation: f64,\n}\n\n#[derive(Debug, Clone)]\n#[allow(dead_code)]\npub struct MlpWaveReport {\n    pub neuron_reports: Vec<MlpWaveNeuronReport>,\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldConfig {\n    pub fixed_y: usize,\n    pub neuron_indices: Vec<usize>,\n    pub min_post_score: f64,\n    pub max_pre_score: f64,\n    pub max_axis_ratio: f64,\n    pub min_pairs_passing: usize,\n}\n\nimpl PairwiseManifoldConfig {\n    pub fn default_for_modulus(_modulus: usize) -> Self {\n        Self {\n            fixed_y: 0,\n            neuron_indices: (0..7).collect(),\n            min_post_score: 0.75,\n            max_pre_score: 0.55,\n            max_axis_ratio: 8.0,\n            min_pairs_passing: 10,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\n#[allow(dead_code)]\npub struct PairwiseManifoldPairReport {\n    pub neuron_a: usize,\n    pub neuron_b: usize,\n    pub circularity_score: f64,\n    pub radius_cv: f64,\n    pub axis_ratio: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldReport {\n    pub pair_reports: Vec<PairwiseManifoldPairReport>,\n    pub average_score: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldTransitionReport {\n    pub pre_average_score: f64,\n    pub post_average_score: f64,\n    pub passing_pairs: usize,\n    pub total_pairs: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct DiagonalRidgeConfig {\n    pub neuron_index: usize,\n    pub max_lag: usize,\n    pub min_diagonal_ratio: f64,\n    pub min_diagonal_score: f64,\n}\n\nimpl DiagonalRidgeConfig {\n    #[allow(dead_code)]\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        let max_lag = (modulus / 10).max(4).min(20);\n        Self {\n            neuron_index: 10,\n            max_lag,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\n#[allow(dead_code)]\npub struct DiagonalRidgeReport {\n    pub neuron_index: usize,\n    pub diagonal_score: f64,\n    pub axis_score: f64,\n    pub ratio: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct SnakeCurveConfig {\n    pub early_step_max: usize,\n    pub train_convergence_threshold: f32,\n    pub val_plateau_min_step: usize,\n    pub val_plateau_max_acc: f32,\n    pub val_final_threshold: f32,\n}\n\nimpl SnakeCurveConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        Self {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 1000,\n            val_plateau_max_acc: 1.0 / modulus as f32 + 0.05,\n            val_final_threshold: 0.90,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct WeightNormDecayConfig {\n    pub min_relative_decrease: f64,\n    pub plateau_min_step: usize,\n    pub grok_step_window: usize,\n}\n\nimpl WeightNormDecayConfig {\n    pub fn default_for_grokking() -> Self {\n        Self {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct WeightNormDecayReport {\n    pub initial_norm: f64,\n    pub final_norm: f64,\n    pub relative_decrease: f64,\n    pub grok_step_norm: Option<f64>,\n}\n\n#[derive(Debug, Clone)]\npub struct SnakeCurveReport {\n    pub train_converged_by_step: usize,\n    pub val_plateau_max_acc: f32,\n    pub val_grok_step: usize,\n    pub curve_shape_valid: bool,\n}\n\npub fn verify_grokking_phase_transition(\n    loss_history: &LossHistory,\n    accuracy_history: &AccuracyHistory,\n    config: &GrokkingVerificationConfig,\n) -> Result<GrokkingPhaseReport, String> {\n    let (early_train_step, _) = accuracy_history\n        .train_snapshots\n        .iter()\n        .find(|(_, acc)| *acc >= config.early_train_acc_threshold)\n        .ok_or_else(|| \"train accuracy never reaches threshold\".to_string())?;\n\n    if *early_train_step > config.early_step_max {\n        return Err(format!(\n            \"train accuracy reaches {:.2} after step {}, expected before {}\",\n            config.early_train_acc_threshold, early_train_step, config.early_step_max\n        ));\n    }\n\n    let plateau_max_val_acc = accuracy_history\n        .val_snapshots\n        .iter()\n        .filter(|(step, _)| *step <= config.plateau_min_step)\n        .map(|(_, acc)| *acc)\n        .fold(None::<f32>, |accum, value| {\n            Some(accum.map_or(value, |max_val| max_val.max(value)))\n        })\n        .ok_or_else(|| \"no validation accuracy samples before plateau_min_step\".to_string())?;\n\n    if plateau_max_val_acc > config.chance_accuracy + config.chance_tolerance {\n        return Err(format!(\n            \"validation accuracy before step {} exceeds chance (max {:.4})\",\n            config.plateau_min_step, plateau_max_val_acc\n        ));\n    }\n\n    let (transition_step, _) = accuracy_history\n        .val_snapshots\n        .iter()\n        .find(|(step, acc)| {\n            *step >= config.plateau_min_step && *acc >= config.target_val_acc_threshold\n        })\n        .ok_or_else(|| \"validation accuracy never reaches target threshold\".to_string())?;\n\n    let latest_allowed = config.plateau_min_step + config.generalization_window;\n    if *transition_step > latest_allowed {\n        return Err(format!(\n            \"validation accuracy reaches {:.2} at step {}, expected before {}\",\n            config.target_val_acc_threshold, transition_step, latest_allowed\n        ));\n    }\n\n    verify_accuracy_jump(\n        &accuracy_history.val_snapshots,\n        *transition_step,\n        config.accuracy_window,\n        config.chance_accuracy,\n        config.chance_tolerance,\n        config.target_val_acc_threshold,\n    )?;\n\n    let loss_drop_ratio = verify_loss_drop(\n        &loss_history.val_snapshots,\n        *transition_step,\n        config.loss_window,\n        config.loss_drop_fraction,\n    )?;\n\n    Ok(GrokkingPhaseReport {\n        early_train_step: *early_train_step,\n        transition_step: *transition_step,\n        plateau_max_val_acc,\n        loss_drop_ratio,\n    })\n}\n\nfn verify_accuracy_jump(\n    val_accuracy: &[(usize, f32)],\n    transition_step: usize,\n    window: usize,\n    chance_accuracy: f32,\n    chance_tolerance: f32,\n    target_val_acc_threshold: f32,\n) -> Result<(), String> {\n    let before_values: Vec<f32> = val_accuracy\n        .iter()\n        .filter(|(step, _)| *step < transition_step)\n        .map(|(_, acc)| *acc)\n        .collect();\n\n    let after_values: Vec<f32> = val_accuracy\n        .iter()\n        .filter(|(step, _)| *step >= transition_step)\n        .map(|(_, acc)| *acc)\n        .collect();\n\n    if before_values.len() < window || after_values.len() < window {\n        return Err(\"insufficient accuracy samples around transition\".to_string());\n    }\n\n    let before_avg: f32 =\n        before_values[before_values.len() - window..].iter().sum::<f32>() / window as f32;\n    let after_avg: f32 = after_values[..window].iter().sum::<f32>() / window as f32;\n\n    if before_avg > chance_accuracy + chance_tolerance {\n        return Err(format!(\n            \"validation accuracy plateau too high before transition (avg {:.4})\",\n            before_avg\n        ));\n    }\n\n    if after_avg < target_val_acc_threshold {\n        return Err(format!(\n            \"validation accuracy after transition too low (avg {:.4})\",\n            after_avg\n        ));\n    }\n\n    Ok(())\n}\n\nfn verify_loss_drop(\n    val_losses: &[(usize, f64)],\n    transition_step: usize,\n    loss_window: usize,\n    loss_drop_fraction: f64,\n) -> Result<f64, String> {\n    let before_losses: Vec<f64> = val_losses\n        .iter()\n        .filter(|(step, _)| *step < transition_step)\n        .map(|(_, loss)| *loss)\n        .collect();\n\n    let after_losses: Vec<f64> = val_losses\n        .iter()\n        .filter(|(step, _)| *step >= transition_step)\n        .map(|(_, loss)| *loss)\n        .collect();\n\n    if before_losses.len() < loss_window || after_losses.len() < loss_window {\n        return Err(\"insufficient loss samples around transition\".to_string());\n    }\n\n    let before_avg: f64 =\n        before_losses[before_losses.len() - loss_window..].iter().sum::<f64>()\n            / loss_window as f64;\n    let after_avg: f64 = after_losses[..loss_window].iter().sum::<f64>() / loss_window as f64;\n\n    if after_avg > before_avg * (1.0 - loss_drop_fraction) {\n        return Err(format!(\n            \"validation loss drop too small (before {:.4}, after {:.4})\",\n            before_avg, after_avg\n        ));\n    }\n\n    Ok(after_avg / before_avg)\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_restricted_loss_early_drop(\n    full_losses: &[(usize, f64)],\n    restricted_losses: &[(usize, f64)],\n    config: &RestrictedLossVerificationConfig,\n) -> Result<RestrictedLossReport, String> {\n    let restricted_baseline =\n        baseline_loss(restricted_losses, config.plateau_min_step, config.drop_window)?;\n    let full_baseline = baseline_loss(full_losses, config.plateau_min_step, config.drop_window)?;\n\n    let restricted_drop_step = find_drop_step(\n        restricted_losses,\n        config.plateau_min_step,\n        restricted_baseline,\n        config.drop_fraction,\n    )?;\n    let full_drop_step = find_drop_step(\n        full_losses,\n        config.plateau_min_step,\n        full_baseline,\n        config.drop_fraction,\n    )?;\n\n    if restricted_drop_step + config.min_step_lead > full_drop_step {\n        return Err(format!(\n            \"restricted loss drops at step {}, expected at least {} steps before full loss (step {})\",\n            restricted_drop_step, config.min_step_lead, full_drop_step\n        ));\n    }\n\n    Ok(RestrictedLossReport {\n        restricted_drop_step,\n        full_drop_step,\n    })\n}\n\npub fn verify_restricted_loss_drop(\n    restricted_losses: &[(usize, f64)],\n    config: &RestrictedLossDropConfig,\n) -> Result<RestrictedLossDropReport, String> {\n    if restricted_losses.len() < 3 {\n        return Err(\"restricted loss history requires at least 3 points\".to_string());\n    }\n\n    let first = restricted_losses.first().ok_or_else(|| {\n        \"restricted loss history is empty\".to_string()\n    })?;\n    let baseline_loss = first.1;\n\n    let (min_idx, (min_step, min_loss)) = restricted_losses\n        .iter()\n        .enumerate()\n        .min_by(|(_, a), (_, b)| {\n            a.1.partial_cmp(&b.1).unwrap_or(std::cmp::Ordering::Equal)\n        })\n        .ok_or_else(|| \"restricted loss history is empty\".to_string())?;\n\n    if min_idx == 0 {\n        return Err(\"restricted loss minimum occurs at start (no drop detected)\".to_string());\n    }\n\n    let relative_decrease = (baseline_loss - min_loss) / baseline_loss;\n    if relative_decrease < config.min_relative_decrease {\n        return Err(format!(\n            \"restricted loss decrease too small (baseline {:.4}, min {:.4}, decrease {:.1}%)\",\n            baseline_loss, min_loss, relative_decrease * 100.0\n        ));\n    }\n\n    Ok(RestrictedLossDropReport {\n        drop_step: *min_step,\n        drop_loss: *min_loss,\n        baseline_loss,\n    })\n}\n\npub fn verify_excluded_loss_spike(\n    excluded_losses: &[(usize, f64)],\n    config: &ExcludedLossSpikeConfig,\n) -> Result<ExcludedLossSpikeReport, String> {\n    if excluded_losses.len() < 3 {\n        return Err(\"excluded loss history requires at least 3 points\".to_string());\n    }\n\n    let first = excluded_losses.first().ok_or_else(|| {\n        \"excluded loss history is empty\".to_string()\n    })?;\n    let last = excluded_losses.last().ok_or_else(|| {\n        \"excluded loss history is empty\".to_string()\n    })?;\n    let baseline_loss = (first.1 + last.1) / 2.0;\n\n    let (peak_idx, (peak_step, peak_loss)) = excluded_losses\n        .iter()\n        .enumerate()\n        .max_by(|(_, a), (_, b)| {\n            a.1.partial_cmp(&b.1).unwrap_or(std::cmp::Ordering::Equal)\n        })\n        .ok_or_else(|| \"excluded loss history is empty\".to_string())?;\n\n    if peak_idx == 0 || peak_idx + 1 == excluded_losses.len() {\n        return Err(\"excluded loss spike occurs at boundary\".to_string());\n    }\n\n    if peak_loss < &(baseline_loss * (1.0 + config.min_relative_increase)) {\n        return Err(format!(\n            \"excluded loss spike too small (baseline {:.4}, peak {:.4})\",\n            baseline_loss, peak_loss\n        ));\n    }\n\n    Ok(ExcludedLossSpikeReport {\n        spike_step: *peak_step,\n        spike_loss: *peak_loss,\n        baseline_loss,\n    })\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_mlp_activation_waves<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n    fixed_y: usize,\n    neuron_indices: &[usize],\n    config: &MlpWaveVerificationConfig,\n) -> Result<MlpWaveReport, String> {\n    let activations = collect_mlp_post_relu_activations(model, device, fixed_y)?;\n    if neuron_indices.is_empty() {\n        return Err(\"neuron indices list is empty\".to_string());\n    }\n\n    let d_ff = activations.len();\n    for &idx in neuron_indices {\n        if idx >= d_ff {\n            return Err(format!(\n                \"neuron index {} out of range (d_ff={})\",\n                idx, d_ff\n            ));\n        }\n    }\n\n    let signals: Vec<(usize, Vec<f64>)> = neuron_indices\n        .iter()\n        .map(|&idx| (idx, activations[idx].clone()))\n        .collect();\n    verify_mlp_activation_waves_from_signals(&signals, config)\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_mlp_activation_waves_from_signals(\n    signals: &[(usize, Vec<f64>)],\n    config: &MlpWaveVerificationConfig,\n) -> Result<MlpWaveReport, String> {\n    if signals.is_empty() {\n        return Err(\"no activation signals provided\".to_string());\n    }\n\n    let mut reports = Vec::with_capacity(signals.len());\n    for (neuron_index, signal) in signals {\n        let report = analyze_mlp_activation_wave(*neuron_index, signal, config)?;\n        reports.push(report);\n    }\n\n    Ok(MlpWaveReport {\n        neuron_reports: reports,\n    })\n}\n\npub fn verify_mlp_pairwise_manifold_transition<B: Backend>(\n    pre_model: &Transformer<B>,\n    post_model: &Transformer<B>,\n    device: &B::Device,\n    config: &PairwiseManifoldConfig,\n) -> Result<PairwiseManifoldTransitionReport, String> {\n    let modulus = ModularAdditionDataset::modulus();\n    if config.fixed_y >= modulus {\n        return Err(format!(\n            \"fixed_y {} out of range (modulus {})\",\n            config.fixed_y, modulus\n        ));\n    }\n\n    let pre_activations = collect_mlp_post_relu_activations(pre_model, device, config.fixed_y)?;\n    let post_activations = collect_mlp_post_relu_activations(post_model, device, config.fixed_y)?;\n\n    let d_ff = pre_activations.len();\n    if post_activations.len() != d_ff {\n        return Err(\"post activations shape mismatch\".to_string());\n    }\n\n    if config.neuron_indices.len() < 2 {\n        return Err(\"need at least two neuron indices\".to_string());\n    }\n\n    for &idx in &config.neuron_indices {\n        if idx >= d_ff {\n            return Err(format!(\n                \"neuron index {} out of range (d_ff={})\",\n                idx, d_ff\n            ));\n        }\n    }\n\n    let pre_signals: Vec<(usize, Vec<f64>)> = config\n        .neuron_indices\n        .iter()\n        .map(|&idx| (idx, pre_activations[idx].clone()))\n        .collect();\n    let post_signals: Vec<(usize, Vec<f64>)> = config\n        .neuron_indices\n        .iter()\n        .map(|&idx| (idx, post_activations[idx].clone()))\n        .collect();\n\n    verify_pairwise_manifold_transition_from_signals(&pre_signals, &post_signals, config)\n}\n\npub fn collect_mlp_activation_surface<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n    neuron_index: usize,\n) -> Result<Vec<Vec<f64>>, String> {\n    let modulus = ModularAdditionDataset::modulus();\n    let equals_token = ModularAdditionDataset::equals_token();\n\n    let mut inputs_vec = Vec::with_capacity(modulus * modulus * 3);\n    for x in 0..modulus {\n        for y in 0..modulus {\n            inputs_vec.push(x as i32);\n            inputs_vec.push(y as i32);\n            inputs_vec.push(equals_token as i32);\n        }\n    }\n\n    let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n        .reshape([modulus * modulus, 3]);\n    let (_, mlp_acts) = model.forward_with_mlp_activations(inputs);\n    let [batch_size, d_ff] = mlp_acts.dims();\n    if batch_size != modulus * modulus {\n        return Err(format!(\n            \"unexpected activation batch size {} (expected {})\",\n            batch_size,\n            modulus * modulus\n        ));\n    }\n    if neuron_index >= d_ff {\n        return Err(format!(\n            \"neuron index {} out of range (d_ff={})\",\n            neuron_index, d_ff\n        ));\n    }\n\n    let data: Vec<f32> = mlp_acts.into_data().to_vec().unwrap();\n    let mut surface = vec![vec![0.0f64; modulus]; modulus];\n    for x in 0..modulus {\n        for y in 0..modulus {\n            let batch = x * modulus + y;\n            let idx = batch * d_ff + neuron_index;\n            surface[x][y] = data[idx] as f64;\n        }\n    }\n\n    Ok(surface)\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_constructive_interference_surface_from_grid(\n    surface: &[Vec<f64>],\n    config: &DiagonalRidgeConfig,\n) -> Result<DiagonalRidgeReport, String> {\n    let (diag_main, diag_anti, axis_x, axis_y) =\n        surface_autocorrelation_scores(surface, config.max_lag)?;\n\n    let diagonal_score = diag_main.max(diag_anti);\n    let axis_score = axis_x.max(axis_y);\n    let ratio = if axis_score > 0.0 {\n        diagonal_score / axis_score\n    } else {\n        f64::INFINITY\n    };\n\n    if diagonal_score < config.min_diagonal_score {\n        return Err(format!(\n            \"diagonal autocorrelation {:.3} below threshold {:.3}\",\n            diagonal_score, config.min_diagonal_score\n        ));\n    }\n\n    if ratio < config.min_diagonal_ratio {\n        return Err(format!(\n            \"diagonal/axis ratio {:.3} below threshold {:.3}\",\n            ratio, config.min_diagonal_ratio\n        ));\n    }\n\n    Ok(DiagonalRidgeReport {\n        neuron_index: config.neuron_index,\n        diagonal_score,\n        axis_score,\n        ratio,\n    })\n}\n\npub fn verify_pairwise_manifold_transition_from_signals(\n    pre_signals: &[(usize, Vec<f64>)],\n    post_signals: &[(usize, Vec<f64>)],\n    config: &PairwiseManifoldConfig,\n) -> Result<PairwiseManifoldTransitionReport, String> {\n    let pre_report = evaluate_pairwise_manifolds_from_signals(pre_signals)?;\n    let post_report = evaluate_pairwise_manifolds_from_signals(post_signals)?;\n\n    if pre_report.average_score > config.max_pre_score {\n        return Err(format!(\n            \"pre-grok average circularity {:.3} exceeds threshold {:.3}\",\n            pre_report.average_score, config.max_pre_score\n        ));\n    }\n\n    let passing_pairs = post_report\n        .pair_reports\n        .iter()\n        .filter(|pair| {\n            pair.circularity_score >= config.min_post_score\n                && pair.axis_ratio <= config.max_axis_ratio\n        })\n        .count();\n    if passing_pairs < config.min_pairs_passing {\n        return Err(format!(\n            \"post-grok only {} pairs meet circularity threshold (need {})\",\n            passing_pairs, config.min_pairs_passing\n        ));\n    }\n\n    Ok(PairwiseManifoldTransitionReport {\n        pre_average_score: pre_report.average_score,\n        post_average_score: post_report.average_score,\n        passing_pairs,\n        total_pairs: post_report.pair_reports.len(),\n    })\n}\n\nfn evaluate_pairwise_manifolds_from_signals(\n    signals: &[(usize, Vec<f64>)],\n) -> Result<PairwiseManifoldReport, String> {\n    if signals.len() < 2 {\n        return Err(\"need at least two activation signals\".to_string());\n    }\n\n    let signal_len = signals[0].1.len();\n    if signal_len < 3 {\n        return Err(\"activation signal too short\".to_string());\n    }\n\n    for (idx, signal) in signals {\n        if signal.len() != signal_len {\n            return Err(format!(\n                \"signal length mismatch for neuron {}\",\n                idx\n            ));\n        }\n    }\n\n    let mut reports = Vec::new();\n    for i in 0..signals.len() {\n        for j in (i + 1)..signals.len() {\n            let (neuron_a, signal_a) = &signals[i];\n            let (neuron_b, signal_b) = &signals[j];\n            let points: Vec<(f64, f64)> = signal_a\n                .iter()\n                .zip(signal_b.iter())\n                .map(|(x, y)| (*x, *y))\n                .collect();\n            let (score, radius_cv, axis_ratio) =\n                match pairwise_circularity_metrics(&points) {\n                    Ok(metrics) => metrics,\n                    Err(_) => (0.0, 1.0, f64::INFINITY),\n                };\n            reports.push(PairwiseManifoldPairReport {\n                neuron_a: *neuron_a,\n                neuron_b: *neuron_b,\n                circularity_score: score,\n                radius_cv,\n                axis_ratio,\n            });\n        }\n    }\n\n    let average_score = reports\n        .iter()\n        .map(|pair| pair.circularity_score)\n        .sum::<f64>()\n        / reports.len() as f64;\n\n    Ok(PairwiseManifoldReport {\n        pair_reports: reports,\n        average_score,\n    })\n}\n\nfn pairwise_circularity_metrics(points: &[(f64, f64)]) -> Result<(f64, f64, f64), String> {\n    if points.len() < 3 {\n        return Err(\"not enough points for circularity\".to_string());\n    }\n\n    let n = points.len() as f64;\n    let mean_x = points.iter().map(|(x, _)| *x).sum::<f64>() / n;\n    let mean_y = points.iter().map(|(_, y)| *y).sum::<f64>() / n;\n\n    let mut var_x = 0.0;\n    let mut var_y = 0.0;\n    let mut cov_xy = 0.0;\n    for (x, y) in points {\n        let dx = x - mean_x;\n        let dy = y - mean_y;\n        var_x += dx * dx;\n        var_y += dy * dy;\n        cov_xy += dx * dy;\n    }\n    var_x /= n;\n    var_y /= n;\n    cov_xy /= n;\n\n    let trace = var_x + var_y;\n    let det = var_x * var_y - cov_xy * cov_xy;\n    if det <= 1.0e-12 || trace <= 0.0 {\n        return Err(\"degenerate covariance\".to_string());\n    }\n\n    let half_trace = trace * 0.5;\n    let disc = (half_trace * half_trace - det).max(0.0).sqrt();\n    let lambda1 = half_trace + disc;\n    let lambda2 = half_trace - disc;\n    if lambda2 <= 0.0 {\n        return Err(\"degenerate eigenvalues\".to_string());\n    }\n\n    let axis_ratio = (lambda1 / lambda2).sqrt();\n\n    let mut v1_x = cov_xy;\n    let mut v1_y = lambda1 - var_x;\n    if v1_x.abs() < 1.0e-12 && v1_y.abs() < 1.0e-12 {\n        v1_x = 1.0;\n        v1_y = 0.0;\n    }\n    let norm = (v1_x * v1_x + v1_y * v1_y).sqrt();\n    v1_x /= norm;\n    v1_y /= norm;\n    let v2_x = -v1_y;\n    let v2_y = v1_x;\n\n    let mut radii = Vec::with_capacity(points.len());\n    for (x, y) in points {\n        let dx = x - mean_x;\n        let dy = y - mean_y;\n        let p1 = dx * v1_x + dy * v1_y;\n        let p2 = dx * v2_x + dy * v2_y;\n        let w1 = p1 / lambda1.sqrt();\n        let w2 = p2 / lambda2.sqrt();\n        radii.push((w1 * w1 + w2 * w2).sqrt());\n    }\n\n    let mean_radius = radii.iter().sum::<f64>() / radii.len() as f64;\n    if mean_radius == 0.0 {\n        return Err(\"zero mean radius\".to_string());\n    }\n\n    let mut variance = 0.0;\n    for r in &radii {\n        let diff = r - mean_radius;\n        variance += diff * diff;\n    }\n    let std = (variance / radii.len() as f64).sqrt();\n    let radius_cv = std / mean_radius;\n    let score = 1.0 / (1.0 + radius_cv);\n\n    Ok((score, radius_cv, axis_ratio))\n}\n\n// Helper functions for verification - reserved for future integration\n#[allow(dead_code)]\nfn baseline_loss(\n    losses: &[(usize, f64)],\n    plateau_min_step: usize,\n    drop_window: usize,\n) -> Result<f64, String> {\n    let before: Vec<f64> = losses\n        .iter()\n        .filter(|(step, _)| *step <= plateau_min_step)\n        .map(|(_, loss)| *loss)\n        .collect();\n\n    if before.len() < drop_window || drop_window == 0 {\n        return Err(\"insufficient loss samples for baseline\".to_string());\n    }\n\n    let window = &before[before.len() - drop_window..];\n    Ok(window.iter().sum::<f64>() / drop_window as f64)\n}\n\n#[allow(dead_code)]\nfn find_drop_step(\n    losses: &[(usize, f64)],\n    plateau_min_step: usize,\n    baseline: f64,\n    drop_fraction: f64,\n) -> Result<usize, String> {\n    losses\n        .iter()\n        .filter(|(step, _)| *step >= plateau_min_step)\n        .find(|(_, loss)| *loss <= baseline * (1.0 - drop_fraction))\n        .map(|(step, _)| *step)\n        .ok_or_else(|| \"loss never drops below threshold\".to_string())\n}\n\nfn collect_mlp_post_relu_activations<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n    fixed_y: usize,\n) -> Result<Vec<Vec<f64>>, String> {\n    let modulus = ModularAdditionDataset::modulus();\n    let equals_token = ModularAdditionDataset::equals_token();\n\n    let mut inputs_vec = Vec::with_capacity(modulus * 3);\n    for x in 0..modulus {\n        inputs_vec.push(x as i32);\n        inputs_vec.push(fixed_y as i32);\n        inputs_vec.push(equals_token as i32);\n    }\n\n    let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n        .reshape([modulus, 3]);\n    let (_, mlp_acts) = model.forward_with_mlp_activations(inputs);\n    let [batch_size, d_ff] = mlp_acts.dims();\n\n    if batch_size != modulus {\n        return Err(format!(\n            \"unexpected activation batch size {} (expected {})\",\n            batch_size, modulus\n        ));\n    }\n\n    let data: Vec<f32> = mlp_acts.into_data().to_vec().unwrap();\n    let mut per_neuron = vec![Vec::with_capacity(batch_size); d_ff];\n    for batch in 0..batch_size {\n        for neuron in 0..d_ff {\n            let idx = batch * d_ff + neuron;\n            per_neuron[neuron].push(data[idx] as f64);\n        }\n    }\n\n    Ok(per_neuron)\n}\n\n#[allow(dead_code)]\nfn surface_autocorrelation_scores(\n    surface: &[Vec<f64>],\n    max_lag: usize,\n) -> Result<(f64, f64, f64, f64), String> {\n    let (height, width) = surface_dimensions(surface)?;\n    if height < 2 || width < 2 {\n        return Err(\"surface grid too small\".to_string());\n    }\n\n    let (mean, var) = surface_mean_variance(surface)?;\n    if var == 0.0 {\n        return Err(\"surface variance is zero\".to_string());\n    }\n\n    let max_lag = max_lag.min(height.saturating_sub(1)).min(width.saturating_sub(1));\n    if max_lag == 0 {\n        return Err(\"max_lag too small for surface\".to_string());\n    }\n\n    let diag_main = average_autocorrelation(surface, mean, var, max_lag, 1, 1)?;\n    let diag_anti = average_autocorrelation(surface, mean, var, max_lag, 1, -1)?;\n    let axis_x = average_autocorrelation(surface, mean, var, max_lag, 1, 0)?;\n    let axis_y = average_autocorrelation(surface, mean, var, max_lag, 0, 1)?;\n\n    Ok((diag_main, diag_anti, axis_x, axis_y))\n}\n\n#[allow(dead_code)]\nfn surface_dimensions(surface: &[Vec<f64>]) -> Result<(usize, usize), String> {\n    let height = surface.len();\n    if height == 0 {\n        return Err(\"surface is empty\".to_string());\n    }\n    let width = surface[0].len();\n    if width == 0 {\n        return Err(\"surface has empty rows\".to_string());\n    }\n    for (idx, row) in surface.iter().enumerate() {\n        if row.len() != width {\n            return Err(format!(\n                \"surface row {} has length {}, expected {}\",\n                idx,\n                row.len(),\n                width\n            ));\n        }\n    }\n    Ok((height, width))\n}\n\n#[allow(dead_code)]\nfn surface_mean_variance(surface: &[Vec<f64>]) -> Result<(f64, f64), String> {\n    let (height, width) = surface_dimensions(surface)?;\n    let mut sum = 0.0;\n    for row in surface {\n        for value in row {\n            sum += *value;\n        }\n    }\n    let count = (height * width) as f64;\n    let mean = sum / count;\n    let mut variance = 0.0;\n    for row in surface {\n        for value in row {\n            let diff = *value - mean;\n            variance += diff * diff;\n        }\n    }\n    variance /= count;\n    Ok((mean, variance))\n}\n\n#[allow(dead_code)]\nfn average_autocorrelation(\n    surface: &[Vec<f64>],\n    mean: f64,\n    variance: f64,\n    max_lag: usize,\n    dx: i32,\n    dy: i32,\n) -> Result<f64, String> {\n    let (height, width) = surface_dimensions(surface)?;\n    let mut total = 0.0;\n    let mut lag_count = 0usize;\n\n    for lag in 1..=max_lag {\n        let mut sum = 0.0;\n        let mut count = 0usize;\n        let lag = lag as i32;\n        for x in 0..height {\n            for y in 0..width {\n                let x2 = x as i32 + dx * lag;\n                let y2 = y as i32 + dy * lag;\n                if x2 < 0 || y2 < 0 {\n                    continue;\n                }\n                let x2 = x2 as usize;\n                let y2 = y2 as usize;\n                if x2 >= height || y2 >= width {\n                    continue;\n                }\n                let a = surface[x][y] - mean;\n                let b = surface[x2][y2] - mean;\n                sum += a * b;\n                count += 1;\n            }\n        }\n        if count == 0 {\n            continue;\n        }\n        let corr = sum / (count as f64 * variance);\n        total += corr.abs();\n        lag_count += 1;\n    }\n\n    if lag_count == 0 {\n        return Err(\"no valid lags for autocorrelation\".to_string());\n    }\n\n    Ok(total / lag_count as f64)\n}\n\n#[allow(dead_code)]\nfn analyze_mlp_activation_wave(\n    neuron_index: usize,\n    signal: &[f64],\n    config: &MlpWaveVerificationConfig,\n) -> Result<MlpWaveNeuronReport, String> {\n    if signal.len() < 3 {\n        return Err(format!(\n            \"activation signal for neuron {} is too short\",\n            neuron_index\n        ));\n    }\n\n    let negative_count = signal\n        .iter()\n        .filter(|value| **value < -config.negative_tolerance)\n        .count();\n    let negative_fraction = negative_count as f64 / signal.len() as f64;\n    if negative_fraction > config.max_negative_fraction {\n        return Err(format!(\n            \"neuron {} has {:.3} negative activations\",\n            neuron_index, negative_fraction\n        ));\n    }\n\n    let (dominant_frequency, dominant_ratio) = dominant_frequency_ratio(signal)?;\n    if dominant_ratio < config.min_dominant_ratio {\n        return Err(format!(\n            \"neuron {} dominant frequency ratio {:.3} below threshold {:.3}\",\n            neuron_index, dominant_ratio, config.min_dominant_ratio\n        ));\n    }\n\n    let best_correlation =\n        best_rectified_sine_correlation(signal, dominant_frequency, config.phase_steps);\n    if best_correlation < config.min_correlation {\n        return Err(format!(\n            \"neuron {} rectified sine correlation {:.3} below threshold {:.3}\",\n            neuron_index, best_correlation, config.min_correlation\n        ));\n    }\n\n    Ok(MlpWaveNeuronReport {\n        neuron_index,\n        dominant_frequency,\n        dominant_ratio,\n        negative_fraction,\n        best_correlation,\n    })\n}\n\n#[allow(dead_code)]\nfn dominant_frequency_ratio(signal: &[f64]) -> Result<(usize, f64), String> {\n    if signal.len() < 2 {\n        return Err(\"signal too short for FFT\".to_string());\n    }\n\n    let mut planner = FftPlanner::new();\n    let fft = planner.plan_fft_forward(signal.len());\n    let mut buffer: Vec<Complex<f64>> = signal\n        .iter()\n        .map(|&x| Complex::new(x, 0.0))\n        .collect();\n    fft.process(&mut buffer);\n    let magnitudes: Vec<f64> = buffer.iter().map(|c| c.norm()).collect();\n\n    if magnitudes.len() < 2 {\n        return Err(\"insufficient FFT bins\".to_string());\n    }\n\n    let mut dominant_idx = 1usize;\n    let mut dominant_mag = magnitudes[1];\n    for (idx, &mag) in magnitudes.iter().enumerate().skip(2) {\n        if mag > dominant_mag {\n            dominant_mag = mag;\n            dominant_idx = idx;\n        }\n    }\n\n    if dominant_idx == 0 {\n        return Err(\"dominant frequency is zero\".to_string());\n    }\n\n    let mut nonzero = magnitudes[1..].to_vec();\n    nonzero.sort_by(|a, b| a.partial_cmp(b).unwrap_or(std::cmp::Ordering::Equal));\n    let median_mag = nonzero[nonzero.len() / 2];\n    let ratio = if median_mag == 0.0 {\n        dominant_mag\n    } else {\n        dominant_mag / median_mag\n    };\n\n    Ok((dominant_idx, ratio))\n}\n\n#[allow(dead_code)]\nfn best_rectified_sine_correlation(signal: &[f64], frequency: usize, phase_steps: usize) -> f64 {\n    if signal.is_empty() || frequency == 0 || phase_steps == 0 {\n        return 0.0;\n    }\n\n    let tau = std::f64::consts::PI * 2.0;\n    let mut best = -1.0;\n    for step in 0..phase_steps {\n        let phase = tau * step as f64 / phase_steps as f64;\n        let wave: Vec<f64> = (0..signal.len())\n            .map(|idx| {\n                let angle = tau * frequency as f64 * idx as f64 / signal.len() as f64 + phase;\n                angle.sin().max(0.0)\n            })\n            .collect();\n        let corr = pearson_correlation(signal, &wave);\n        if corr > best {\n            best = corr;\n        }\n    }\n\n    best\n}\n\n#[allow(dead_code)]\nfn pearson_correlation(a: &[f64], b: &[f64]) -> f64 {\n    if a.len() != b.len() || a.is_empty() {\n        return 0.0;\n    }\n\n    let mean_a = a.iter().sum::<f64>() / a.len() as f64;\n    let mean_b = b.iter().sum::<f64>() / b.len() as f64;\n\n    let mut num = 0.0;\n    let mut denom_a = 0.0;\n    let mut denom_b = 0.0;\n    for (x, y) in a.iter().zip(b.iter()) {\n        let da = x - mean_a;\n        let db = y - mean_b;\n        num += da * db;\n        denom_a += da * da;\n        denom_b += db * db;\n    }\n\n    if denom_a == 0.0 || denom_b == 0.0 {\n        return 0.0;\n    }\n\n    num / (denom_a.sqrt() * denom_b.sqrt())\n}\n\n/// Verify that the model actually learned modular addition\n/// by testing it on ALL possible examples systematically\npub fn verify_full_accuracy<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n) -> (f32, f32) {\n    println!(\"ðŸ” Verifying model on COMPLETE dataset (no sampling)...\");\n\n    let train_dataset = ModularAdditionDataset::new(true, 42);\n    let val_dataset = ModularAdditionDataset::new(false, 42);\n\n    let train_acc = test_all_examples(model, &train_dataset, device);\n    let val_acc = test_all_examples(model, &val_dataset, device);\n\n    println!(\"ðŸ“Š FULL Dataset Results:\");\n    println!(\"   Training: {}/{} = {:.2}%\",\n        (train_acc * train_dataset.len() as f32) as usize,\n        train_dataset.len(),\n        train_acc * 100.0);\n    println!(\"   Validation: {}/{} = {:.2}%\",\n        (val_acc * val_dataset.len() as f32) as usize,\n        val_dataset.len(),\n        val_acc * 100.0);\n\n    (train_acc, val_acc)\n}\n\n/// Test model on ALL examples in dataset (no random sampling)\nfn test_all_examples<B: Backend>(\n    model: &Transformer<B>,\n    dataset: &ModularAdditionDataset,\n    device: &B::Device,\n) -> f32 {\n    let mut total_correct = 0;\n    let total = dataset.len();\n\n    // Test in batches to avoid memory issues\n    let batch_size = 100;\n    for batch_start in (0..total).step_by(batch_size) {\n        let batch_end = (batch_start + batch_size).min(total);\n        let batch_len = batch_end - batch_start;\n\n        let mut inputs_vec = Vec::new();\n        let mut targets_vec = Vec::new();\n\n        for idx in batch_start..batch_end {\n            let (input, target) = dataset.get(idx).unwrap();\n            inputs_vec.extend(input);\n            targets_vec.push(target as i32);\n        }\n\n        let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n            .reshape([batch_len, 3]);\n\n        let logits = model.forward(inputs);\n        let predictions = logits.argmax(1).squeeze::<1>();\n        let predictions_vec: Vec<i32> = predictions.into_data().to_vec().unwrap();\n\n        let correct = predictions_vec\n            .iter()\n            .zip(targets_vec.iter())\n            .filter(|(p, t)| p == t)\n            .count();\n\n        total_correct += correct;\n    }\n\n    total_correct as f32 / total as f32\n}\n\n/// Verify that weight norms decrease over time with weight decay\n/// and show correlation with the grokking transition\npub fn verify_weight_norm_decay(\n    weight_norm_history: &[(usize, f64)],\n    grok_step: Option<usize>,\n    config: &WeightNormDecayConfig,\n) -> Result<WeightNormDecayReport, String> {\n    if weight_norm_history.len() < 2 {\n        return Err(\"weight norm history requires at least 2 snapshots\".to_string());\n    }\n\n    let initial_norm = weight_norm_history\n        .first()\n        .ok_or_else(|| \"weight norm history is empty\".to_string())?\n        .1;\n    let final_norm = weight_norm_history\n        .last()\n        .ok_or_else(|| \"weight norm history is empty\".to_string())?\n        .1;\n\n    if initial_norm <= 0.0 {\n        return Err(\"initial weight norm must be positive\".to_string());\n    }\n\n    // Check that norms are generally decreasing (with weight decay)\n    let relative_decrease = (initial_norm - final_norm) / initial_norm;\n    if relative_decrease < config.min_relative_decrease {\n        return Err(format!(\n            \"weight norm decrease {:.4} below threshold {:.4} (initial {:.4}, final {:.4})\",\n            relative_decrease, config.min_relative_decrease, initial_norm, final_norm\n        ));\n    }\n\n    // Check that norms are not increasing (weight decay should monotonically decrease norms)\n    for window in weight_norm_history.windows(2) {\n        let prev_norm = window[0].1;\n        let next_norm = window[1].1;\n        if next_norm > prev_norm * 1.01 {\n            // Allow 1% tolerance for numerical noise\n            return Err(format!(\n                \"weight norm increased from {:.4} to {:.4} at step {}\",\n                prev_norm, next_norm, window[1].0\n            ));\n        }\n    }\n\n    // If grok step is provided, check that norms show a shift around grokking\n    let grok_step_norm = if let Some(step) = grok_step {\n        weight_norm_history\n            .iter()\n            .filter(|(s, _)| {\n                *s >= step.saturating_sub(config.grok_step_window)\n                    && *s <= step + config.grok_step_window\n            })\n            .map(|(_, norm)| *norm)\n            .next()\n    } else {\n        None\n    };\n\n    Ok(WeightNormDecayReport {\n        initial_norm,\n        final_norm,\n        relative_decrease,\n        grok_step_norm,\n    })\n}\n\n/// Verify the characteristic \"snake curve\" shape of grokking phenomenon\n/// on log-scale plots: train accuracy rises early, validation stays flat then jumps\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_snake_curve_shape(\n    train_acc: &[(usize, f32)],\n    val_acc: &[(usize, f32)],\n    config: &SnakeCurveConfig,\n) -> Result<SnakeCurveReport, String> {\n    if train_acc.is_empty() || val_acc.is_empty() {\n        return Err(\"empty accuracy history\".to_string());\n    }\n\n    // Phase 1: Verify early train convergence (immediate drop on log scale)\n    let train_converged_step = train_acc\n        .iter()\n        .find(|(_, acc)| *acc >= config.train_convergence_threshold)\n        .map(|(step, _)| *step)\n        .ok_or_else(|| {\n            format!(\n                \"train accuracy never reaches {} threshold\",\n                config.train_convergence_threshold\n            )\n        })?;\n\n    if train_converged_step > config.early_step_max {\n        return Err(format!(\n            \"train accuracy converges at step {}, expected before {}\",\n            train_converged_step, config.early_step_max\n        ));\n    }\n\n    // Phase 2: Verify validation plateau (flat on log scale)\n    let plateau_samples: Vec<f32> = val_acc\n        .iter()\n        .filter(|(step, _)| *step >= 1 && *step <= config.val_plateau_min_step)\n        .map(|(_, acc)| *acc)\n        .collect();\n\n    if plateau_samples.is_empty() {\n        return Err(\"no validation samples in plateau region\".to_string());\n    }\n\n    let val_plateau_max = plateau_samples\n        .iter()\n        .fold(0.0f32, |max_val, &acc| max_val.max(acc));\n\n    if val_plateau_max > config.val_plateau_max_acc {\n        return Err(format!(\n            \"validation accuracy in plateau region too high: max {:.4} exceeds {:.4}\",\n            val_plateau_max, config.val_plateau_max_acc\n        ));\n    }\n\n    // Phase 3: Verify delayed validation jump (crash up on log scale)\n    let val_grok_step = val_acc\n        .iter()\n        .filter(|(step, _)| *step > config.val_plateau_min_step)\n        .find(|(_, acc)| *acc >= config.val_final_threshold)\n        .map(|(step, _)| *step)\n        .ok_or_else(|| {\n            format!(\n                \"validation accuracy never reaches {} after plateau\",\n                config.val_final_threshold\n            )\n        })?;\n\n    // Phase 4: Verify phase ordering (snake shape: early train, late val)\n    let curve_shape_valid = train_converged_step < config.val_plateau_min_step\n        && val_grok_step > config.val_plateau_min_step\n        && train_converged_step < val_grok_step;\n\n    if !curve_shape_valid {\n        return Err(format!(\n            \"invalid curve shape: train converged at {}, val plateau until {}, val grokked at {}\",\n            train_converged_step, config.val_plateau_min_step, val_grok_step\n        ));\n    }\n\n    Ok(SnakeCurveReport {\n        train_converged_by_step: train_converged_step,\n        val_plateau_max_acc: val_plateau_max,\n        val_grok_step,\n        curve_shape_valid,\n    })\n}\n\n/// Test specific examples to see if model actually computes mod p\npub fn test_specific_examples<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n) {\n    println!();\n    println!(\"ðŸ§ª Testing specific examples:\");\n\n    let modulus = ModularAdditionDataset::modulus();\n    let equals_token = ModularAdditionDataset::equals_token();\n    let test_cases = vec![\n        (0, 0, 0),\n        (1, 1, 2),\n        (50, 50, (50 + 50) % modulus),\n        (112, 1, (112 + 1) % modulus),\n        (56, 57, (56 + 57) % modulus),\n        (112, 112, (112 + 112) % modulus),\n        (10, 20, 30),\n        (60, 60, (60 + 60) % modulus),\n    ];\n\n    for (a, b, expected) in test_cases {\n        let input_vec = vec![a as i32, b as i32, equals_token as i32];\n        let input = Tensor::<B, 1, Int>::from_ints(input_vec.as_slice(), device)\n            .reshape([1, 3]);\n\n        let logits = model.forward(input);\n        let prediction = logits.argmax(1).squeeze::<1>();\n        let pred_value: i32 = prediction.into_data().to_vec().unwrap()[0];\n\n        let correct = if pred_value == expected as i32 { \"âœ“\" } else { \"âœ—\" };\n        println!(\n            \"   {} + {} mod {} = {} | Model: {} {}\",\n            a, b, modulus, expected, pred_value, correct\n        );\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    fn make_histories(\n        train_acc: &[(usize, f32)],\n        val_acc: &[(usize, f32)],\n        val_loss: &[(usize, f64)],\n    ) -> (LossHistory, AccuracyHistory) {\n        let mut loss_history = LossHistory::new();\n        loss_history.val_snapshots = val_loss.to_vec();\n        let mut accuracy_history = AccuracyHistory::new();\n        accuracy_history.train_snapshots = train_acc.to_vec();\n        accuracy_history.val_snapshots = val_acc.to_vec();\n        (loss_history, accuracy_history)\n    }\n\n    #[test]\n    fn grokking_phase_verification_passes_on_synthetic_signal() {\n        let train_acc = vec![(0, 0.2), (500, 0.995), (1000, 0.999)];\n        let val_acc = vec![\n            (0, 0.01),\n            (500, 0.01),\n            (1500, 0.01),\n            (2200, 0.995),\n            (2500, 0.999),\n        ];\n        let val_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 2.0),\n            (2200, 0.2),\n            (2500, 0.15),\n        ];\n\n        let (loss_history, accuracy_history) = make_histories(&train_acc, &val_acc, &val_loss);\n        let config = GrokkingVerificationConfig {\n            early_step_max: 1000,\n            plateau_min_step: 1500,\n            generalization_window: 1500,\n            accuracy_window: 2,\n            loss_window: 2,\n            ..GrokkingVerificationConfig::default_for_modulus(ModularAdditionDataset::modulus())\n        };\n\n        let report = verify_grokking_phase_transition(&loss_history, &accuracy_history, &config)\n            .expect(\"expected grokking verification to pass\");\n        assert!(report.transition_step >= config.plateau_min_step);\n        assert!(report.early_train_step <= config.early_step_max);\n    }\n\n    #[test]\n    fn grokking_phase_verification_fails_without_transition() {\n        let train_acc = vec![(0, 0.2), (500, 0.995)];\n        let val_acc = vec![(0, 0.01), (500, 0.01), (1500, 0.02)];\n        let val_loss = vec![(0, 2.0), (500, 2.0), (1500, 1.9)];\n\n        let (loss_history, accuracy_history) = make_histories(&train_acc, &val_acc, &val_loss);\n        let config = GrokkingVerificationConfig {\n            early_step_max: 1000,\n            plateau_min_step: 1000,\n            generalization_window: 1000,\n            accuracy_window: 2,\n            loss_window: 2,\n            ..GrokkingVerificationConfig::default_for_modulus(ModularAdditionDataset::modulus())\n        };\n\n        let err = verify_grokking_phase_transition(&loss_history, &accuracy_history, &config)\n            .expect_err(\"expected grokking verification to fail\");\n        assert!(err.contains(\"validation accuracy never reaches target\"));\n    }\n\n    #[test]\n    fn grokking_phase_verification_fails_with_slow_accuracy_rise() {\n        let train_acc = vec![(0, 0.2), (500, 0.995), (1000, 0.999)];\n        let val_acc = vec![\n            (0, 0.01),\n            (500, 0.01),\n            (1500, 0.5),\n            (2000, 0.6),\n            (2200, 0.995),\n            (2500, 0.999),\n        ];\n        let val_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 2.0),\n            (2200, 0.2),\n            (2500, 0.15),\n        ];\n\n        let (loss_history, accuracy_history) = make_histories(&train_acc, &val_acc, &val_loss);\n        let config = GrokkingVerificationConfig {\n            early_step_max: 1000,\n            plateau_min_step: 1000,\n            generalization_window: 2000,\n            accuracy_window: 2,\n            loss_window: 2,\n            ..GrokkingVerificationConfig::default_for_modulus(ModularAdditionDataset::modulus())\n        };\n\n        let err = verify_grokking_phase_transition(&loss_history, &accuracy_history, &config)\n            .expect_err(\"expected grokking verification to fail\");\n        assert!(err.contains(\"plateau too high\"));\n    }\n\n    #[test]\n    fn restricted_loss_detection_passes_with_early_drop() {\n        let full_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 2.0),\n            (2500, 2.0),\n            (3500, 0.3),\n        ];\n        let restricted_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 0.8),\n            (2500, 0.4),\n            (3500, 0.3),\n        ];\n\n        let config = RestrictedLossVerificationConfig {\n            plateau_min_step: 1000,\n            drop_window: 2,\n            drop_fraction: 0.5,\n            min_step_lead: 500,\n        };\n\n        let report = verify_restricted_loss_early_drop(&full_loss, &restricted_loss, &config)\n            .expect(\"expected restricted loss verification to pass\");\n        assert!(report.restricted_drop_step < report.full_drop_step);\n    }\n\n    #[test]\n    fn restricted_loss_detection_fails_when_full_drops_first() {\n        let full_loss = vec![(0, 2.0), (1000, 1.0), (1500, 0.8)];\n        let restricted_loss = vec![(0, 2.0), (1000, 2.0), (1500, 0.8)];\n\n        let config = RestrictedLossVerificationConfig {\n            plateau_min_step: 500,\n            drop_window: 1,\n            drop_fraction: 0.3,\n            min_step_lead: 200,\n        };\n\n        let err = verify_restricted_loss_early_drop(&full_loss, &restricted_loss, &config)\n            .expect_err(\"expected restricted loss verification to fail\");\n        assert!(err.contains(\"restricted loss drops\"));\n    }\n\n    #[test]\n    fn excluded_loss_spike_verification_passes() {\n        let excluded_loss = vec![(0, 1.0), (1500, 2.2), (3000, 1.1)];\n        let config = ExcludedLossSpikeConfig {\n            min_relative_increase: 0.5,\n        };\n\n        let report = verify_excluded_loss_spike(&excluded_loss, &config)\n            .expect(\"expected excluded loss spike verification to pass\");\n        assert_eq!(report.spike_step, 1500);\n    }\n\n    #[test]\n    fn excluded_loss_spike_verification_fails_without_spike() {\n        let excluded_loss = vec![(0, 1.0), (1500, 1.1), (3000, 1.0)];\n        let config = ExcludedLossSpikeConfig {\n            min_relative_increase: 0.5,\n        };\n\n        let err = verify_excluded_loss_spike(&excluded_loss, &config)\n            .expect_err(\"expected excluded loss spike verification to fail\");\n        assert!(err.contains(\"excluded loss spike too small\"));\n    }\n\n    #[test]\n    fn restricted_loss_drop_verification_passes() {\n        let restricted_loss = vec![\n            (0, 2.0),\n            (1000, 1.8),\n            (5000, 1.2),\n            (7000, 0.8),\n            (10000, 0.9),\n        ];\n        let config = RestrictedLossDropConfig {\n            min_relative_decrease: 0.3,\n        };\n\n        let report = verify_restricted_loss_drop(&restricted_loss, &config)\n            .expect(\"expected restricted loss drop verification to pass\");\n        assert_eq!(report.drop_step, 7000);\n        assert!(report.baseline_loss > report.drop_loss);\n        assert!((report.baseline_loss - report.drop_loss) / report.baseline_loss >= 0.3);\n    }\n\n    #[test]\n    fn restricted_loss_drop_verification_fails_without_drop() {\n        let restricted_loss = vec![\n            (0, 2.0),\n            (1000, 2.1),\n            (5000, 1.9),\n            (7000, 2.0),\n            (10000, 2.05),\n        ];\n        let config = RestrictedLossDropConfig {\n            min_relative_decrease: 0.3,\n        };\n\n        let err = verify_restricted_loss_drop(&restricted_loss, &config)\n            .expect_err(\"expected restricted loss drop verification to fail\");\n        assert!(err.contains(\"restricted loss decrease too small\"));\n    }\n\n    #[test]\n    fn mlp_activation_wave_verification_passes_on_rectified_sine() {\n        let n = ModularAdditionDataset::modulus();\n        let freq = 5.0;\n        let mut signal = Vec::with_capacity(n);\n        for idx in 0..n {\n            let angle = 2.0 * std::f64::consts::PI * freq * idx as f64 / n as f64;\n            let rectified = angle.sin().max(0.0);\n            let wobble = (2.0 * std::f64::consts::PI * 2.0 * idx as f64 / n as f64)\n                .sin()\n                .abs()\n                * 0.02;\n            signal.push(rectified + wobble);\n        }\n\n        let config = MlpWaveVerificationConfig {\n            max_negative_fraction: 0.0,\n            negative_tolerance: 1e-6,\n            min_dominant_ratio: 3.0,\n            min_correlation: 0.7,\n            phase_steps: 32,\n        };\n\n        let report =\n            verify_mlp_activation_waves_from_signals(&[(0, signal)], &config)\n                .expect(\"expected rectified sine to pass\");\n        assert_eq!(report.neuron_reports.len(), 1);\n    }\n\n    #[test]\n    fn mlp_activation_wave_verification_fails_on_noise() {\n        let n = ModularAdditionDataset::modulus();\n        let signal: Vec<f64> = (0..n).map(|idx| idx as f64 / n as f64).collect();\n\n        let config = MlpWaveVerificationConfig {\n            max_negative_fraction: 0.0,\n            negative_tolerance: 1e-6,\n            min_dominant_ratio: 4.0,\n            min_correlation: 0.8,\n            phase_steps: 16,\n        };\n\n        let err =\n            verify_mlp_activation_waves_from_signals(&[(0, signal)], &config)\n                .expect_err(\"expected linear signal to fail\");\n        assert!(err.contains(\"dominant frequency ratio\") || err.contains(\"correlation\"));\n    }\n\n    #[test]\n    fn pairwise_manifold_transition_passes_on_elliptic_signals() {\n        let n = ModularAdditionDataset::modulus();\n        let freq = 5.0;\n        let mut post_signals = Vec::new();\n        for neuron_idx in 0..7usize {\n            let phase = 2.0 * std::f64::consts::PI * neuron_idx as f64 / 7.0;\n            let signal: Vec<f64> = (0..n)\n                .map(|idx| {\n                    let angle = 2.0 * std::f64::consts::PI * freq * idx as f64 / n as f64 + phase;\n                    angle.sin()\n                })\n                .collect();\n            post_signals.push((neuron_idx, signal));\n        }\n\n        let mut pre_signals = Vec::new();\n        for neuron_idx in 0..7usize {\n            let signal: Vec<f64> = (0..n)\n                .map(|idx| idx as f64 / n as f64 + neuron_idx as f64 * 0.01)\n                .collect();\n            pre_signals.push((neuron_idx, signal));\n        }\n\n        let config = PairwiseManifoldConfig {\n            fixed_y: 0,\n            neuron_indices: (0..7).collect(),\n            min_post_score: 0.75,\n            max_pre_score: 0.55,\n            max_axis_ratio: 8.0,\n            min_pairs_passing: 8,\n        };\n\n        let report = verify_pairwise_manifold_transition_from_signals(\n            &pre_signals,\n            &post_signals,\n            &config,\n        )\n        .expect(\"expected pairwise manifold transition to pass\");\n        assert!(report.passing_pairs >= config.min_pairs_passing);\n    }\n\n    #[test]\n    fn pairwise_manifold_transition_fails_without_rings() {\n        let n = ModularAdditionDataset::modulus();\n        let mut signals = Vec::new();\n        for neuron_idx in 0..7usize {\n            let signal: Vec<f64> = (0..n)\n                .map(|idx| idx as f64 / n as f64 + neuron_idx as f64 * 0.02)\n                .collect();\n            signals.push((neuron_idx, signal));\n        }\n\n        let config = PairwiseManifoldConfig {\n            fixed_y: 0,\n            neuron_indices: (0..7).collect(),\n            min_post_score: 0.8,\n            max_pre_score: 0.55,\n            max_axis_ratio: 4.0,\n            min_pairs_passing: 5,\n        };\n\n        let err = verify_pairwise_manifold_transition_from_signals(\n            &signals,\n            &signals,\n            &config,\n        )\n        .expect_err(\"expected pairwise manifold transition to fail\");\n        assert!(err.contains(\"post-grok\") || err.contains(\"pre-grok\"));\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_passes_on_diagonal_corrugation() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create diagonal ridges: f(x,y) = sin(2Ï€ * freq * (x+y) / n)\n        let freq = 3.0;\n        for x in 0..n {\n            for y in 0..n {\n                let angle = 2.0 * std::f64::consts::PI * freq * (x + y) as f64 / n as f64;\n                surface[x][y] = angle.sin();\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let report = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect(\"expected diagonal ridge verification to pass\");\n        assert!(report.diagonal_score > report.axis_score);\n        assert!(report.ratio >= config.min_diagonal_ratio);\n        assert!(report.diagonal_score >= config.min_diagonal_score);\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_passes_on_anti_diagonal_pattern() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create anti-diagonal ridges: f(x,y) = sin(2Ï€ * freq * (x-y) / n)\n        let freq = 3.0;\n        for x in 0..n {\n            for y in 0..n {\n                let angle = 2.0 * std::f64::consts::PI * freq * (x as i32 - y as i32) as f64 / n as f64;\n                surface[x][y] = angle.sin();\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let report = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect(\"expected anti-diagonal ridge verification to pass\");\n        assert!(report.diagonal_score > report.axis_score);\n        assert!(report.ratio >= config.min_diagonal_ratio);\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_fails_on_axis_aligned_pattern() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create axis-aligned ridges: f(x,y) = sin(2Ï€ * freq * x / n)\n        let freq = 3.0;\n        for x in 0..n {\n            for y in 0..n {\n                let angle = 2.0 * std::f64::consts::PI * freq * x as f64 / n as f64;\n                surface[x][y] = angle.sin();\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let err = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect_err(\"expected axis-aligned pattern to fail diagonal verification\");\n        assert!(err.contains(\"diagonal/axis ratio\") || err.contains(\"diagonal autocorrelation\"));\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_fails_on_flat_surface() {\n        let n = 50;\n        let surface = vec![vec![1.5; n]; n];\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let err = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect_err(\"expected flat surface to fail\");\n        assert!(err.contains(\"surface variance is zero\"));\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_fails_on_random_noise() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create random noise (low autocorrelation everywhere)\n        for x in 0..n {\n            for y in 0..n {\n                surface[x][y] = ((x * 17 + y * 31) % 100) as f64 / 100.0;\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let err = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect_err(\"expected random noise to fail diagonal verification\");\n        assert!(err.contains(\"diagonal/axis ratio\") || err.contains(\"diagonal autocorrelation\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_passes_on_valid_grokking_curve() {\n        // Simulate typical grokking: train converges early, val plateaus then jumps\n        let train_acc = vec![\n            (1, 0.10),\n            (10, 0.50),\n            (100, 0.80),\n            (500, 0.98),\n            (1000, 0.99),\n            (2000, 0.995),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (10, 0.009),\n            (100, 0.01),\n            (500, 0.011),\n            (1000, 0.009),\n            (1500, 0.01),\n            (2000, 0.92),\n            (3000, 0.98),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 1500,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let report = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect(\"expected snake curve verification to pass\");\n\n        assert_eq!(report.train_converged_by_step, 500);\n        assert!(report.val_plateau_max_acc < config.val_plateau_max_acc);\n        assert_eq!(report.val_grok_step, 2000);\n        assert!(report.curve_shape_valid);\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_on_simultaneous_convergence() {\n        // Both train and val converge at the same time (no grokking)\n        let train_acc = vec![\n            (1, 0.10),\n            (100, 0.98),\n            (500, 0.99),\n        ];\n        let val_acc = vec![\n            (1, 0.10),\n            (100, 0.97),\n            (500, 0.98),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 200,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let err = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect_err(\"expected snake curve verification to fail\");\n        assert!(err.contains(\"plateau region too high\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_on_slow_train_convergence() {\n        // Train takes too long to converge (not the typical snake curve)\n        let train_acc = vec![\n            (1, 0.10),\n            (500, 0.50),\n            (1000, 0.70),\n            (2000, 0.98),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (500, 0.01),\n            (1000, 0.01),\n            (2000, 0.92),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 1500,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let err = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect_err(\"expected snake curve verification to fail\");\n        assert!(err.contains(\"converges at step 2000\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_without_val_jump() {\n        // Validation never jumps up (no generalization)\n        let train_acc = vec![\n            (1, 0.10),\n            (100, 0.98),\n            (500, 0.99),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (100, 0.01),\n            (500, 0.01),\n            (2000, 0.02),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 200,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let err = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect_err(\"expected snake curve verification to fail\");\n        assert!(err.contains(\"validation accuracy never reaches\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_on_wrong_phase_ordering() {\n        // Val jumps before train converges (backwards snake)\n        let train_acc = vec![\n            (1, 0.10),\n            (500, 0.50),\n            (2000, 0.98),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (100, 0.95),\n            (500, 0.98),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 200,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let result = verify_snake_curve_shape(&train_acc, &val_acc, &config);\n        // Should fail because val jumps before train converges (not a proper snake curve)\n        // This can fail for multiple reasons, but should definitely fail\n        assert!(result.is_err(), \"Expected snake curve verification to fail but it passed\");\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_passes_on_decreasing_norms() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 85.0),\n            (1000, 72.0),\n            (1500, 63.0),\n            (2000, 58.0),\n            (2500, 55.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let report = verify_weight_norm_decay(&weight_norms, Some(2000), &config)\n            .expect(\"expected weight norm decay verification to pass\");\n\n        assert_eq!(report.initial_norm, 100.0);\n        assert_eq!(report.final_norm, 55.0);\n        assert!((report.relative_decrease - 0.45).abs() < 0.01);\n        assert!(report.grok_step_norm.is_some());\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_fails_on_flat_norms() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 99.0),\n            (1000, 98.5),\n            (1500, 98.0),\n            (2000, 97.5),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let err = verify_weight_norm_decay(&weight_norms, None, &config)\n            .expect_err(\"expected weight norm decay verification to fail\");\n        assert!(err.contains(\"weight norm decrease\"));\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_fails_on_increasing_norms() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 95.0),\n            (1000, 105.0), // Increase here\n            (1500, 90.0),\n            (2000, 85.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let err = verify_weight_norm_decay(&weight_norms, None, &config)\n            .expect_err(\"expected weight norm decay verification to fail\");\n        assert!(err.contains(\"weight norm increased\"));\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_allows_small_noise() {\n        // Weight decay with small numerical noise (within 1% tolerance)\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 95.0),\n            (1000, 90.5),  // Small increase within tolerance\n            (1500, 90.0),\n            (2000, 85.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let report = verify_weight_norm_decay(&weight_norms, None, &config)\n            .expect(\"expected weight norm decay with small noise to pass\");\n\n        assert_eq!(report.initial_norm, 100.0);\n        assert_eq!(report.final_norm, 85.0);\n        assert!((report.relative_decrease - 0.15).abs() < 0.01);\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_finds_grok_step_norm() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (1000, 80.0),\n            (1800, 68.0),  // Within window of grok step (2200 - 500 = 1700)\n            (2000, 65.0),\n            (2200, 62.0),  // Around grok step\n            (3000, 55.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let report = verify_weight_norm_decay(&weight_norms, Some(2200), &config)\n            .expect(\"expected weight norm decay verification to pass\");\n\n        assert!(report.grok_step_norm.is_some());\n        let grok_norm = report.grok_step_norm.unwrap();\n        // Should find the first norm in the window [1700, 2700], which is 68.0 at step 1800\n        assert!((grok_norm - 68.0).abs() < 0.01);\n    }\n}\n","structuredPatch":[{"oldStart":199,"oldLines":6,"newStart":199,"newLines":7,"lines":[" }"," "," impl SnakeCurveConfig {","+    #[allow(dead_code)]","     pub fn default_for_modulus(modulus: usize) -> Self {","         Self {","             early_step_max: 1000,"]},{"oldStart":211,"oldLines":6,"newStart":212,"newLines":7,"lines":[" }"," "," #[derive(Debug, Clone)]","+#[allow(dead_code)]"," pub struct WeightNormDecayConfig {","     pub min_relative_decrease: f64,","     pub plateau_min_step: usize,"]}],"userModified":false,"replaceAll":false}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01WxPhMvW1uWMvcSHusoP3Jm","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01NyUmqvPauUtGbcc4ySk1sA","name":"Edit","input":{"replace_all":false,"file_path":"/Users/admin/MechInt/src/verify.rs","old_string":"#[derive(Debug, Clone)]\npub struct WeightNormDecayReport {\n    pub initial_norm: f64,\n    pub final_norm: f64,\n    pub relative_decrease: f64,\n    pub grok_step_norm: Option<f64>,\n}\n\n#[derive(Debug, Clone)]\npub struct SnakeCurveReport {\n    pub train_converged_by_step: usize,\n    pub val_plateau_max_acc: f32,\n    pub val_grok_step: usize,\n    pub curve_shape_valid: bool,\n}","new_string":"#[derive(Debug, Clone)]\n#[allow(dead_code)]\npub struct WeightNormDecayReport {\n    pub initial_norm: f64,\n    pub final_norm: f64,\n    pub relative_decrease: f64,\n    pub grok_step_norm: Option<f64>,\n}\n\n#[derive(Debug, Clone)]\n#[allow(dead_code)]\npub struct SnakeCurveReport {\n    pub train_converged_by_step: usize,\n    pub val_plateau_max_acc: f32,\n    pub val_grok_step: usize,\n    pub curve_shape_valid: bool,\n}"}}],"stop_reason":"tool_use","stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":1621,"cache_read_input_tokens":80351,"cache_creation":{"ephemeral_5m_input_tokens":1621,"ephemeral_1h_input_tokens":0},"output_tokens":419,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"f184e7d9-a81f-4a63-8526-0a81dae40208"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01NyUmqvPauUtGbcc4ySk1sA","type":"tool_result","content":"The file /Users/admin/MechInt/src/verify.rs has been updated. Here's the result of running `cat -n` on a snippet of the edited file:\n   228â†’        }\n   229â†’    }\n   230â†’}\n   231â†’\n   232â†’#[derive(Debug, Clone)]\n   233â†’#[allow(dead_code)]\n   234â†’pub struct WeightNormDecayReport {\n   235â†’    pub initial_norm: f64,\n   236â†’    pub final_norm: f64,\n   237â†’    pub relative_decrease: f64,\n   238â†’    pub grok_step_norm: Option<f64>,\n   239â†’}\n   240â†’\n   241â†’#[derive(Debug, Clone)]\n   242â†’#[allow(dead_code)]\n   243â†’pub struct SnakeCurveReport {\n   244â†’    pub train_converged_by_step: usize,\n   245â†’    pub val_plateau_max_acc: f32,\n   246â†’    pub val_grok_step: usize,\n   247â†’    pub curve_shape_valid: bool,\n   248â†’}\n   249â†’\n   250â†’pub fn verify_grokking_phase_transition(\n   251â†’    loss_history: &LossHistory,\n   252â†’    accuracy_history: &AccuracyHistory,"}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"f303e7eb-0e6a-4b60-8419-86f7cc44169c","tool_use_result":{"filePath":"/Users/admin/MechInt/src/verify.rs","oldString":"#[derive(Debug, Clone)]\npub struct WeightNormDecayReport {\n    pub initial_norm: f64,\n    pub final_norm: f64,\n    pub relative_decrease: f64,\n    pub grok_step_norm: Option<f64>,\n}\n\n#[derive(Debug, Clone)]\npub struct SnakeCurveReport {\n    pub train_converged_by_step: usize,\n    pub val_plateau_max_acc: f32,\n    pub val_grok_step: usize,\n    pub curve_shape_valid: bool,\n}","newString":"#[derive(Debug, Clone)]\n#[allow(dead_code)]\npub struct WeightNormDecayReport {\n    pub initial_norm: f64,\n    pub final_norm: f64,\n    pub relative_decrease: f64,\n    pub grok_step_norm: Option<f64>,\n}\n\n#[derive(Debug, Clone)]\n#[allow(dead_code)]\npub struct SnakeCurveReport {\n    pub train_converged_by_step: usize,\n    pub val_plateau_max_acc: f32,\n    pub val_grok_step: usize,\n    pub curve_shape_valid: bool,\n}","originalFile":"use burn::{\n    data::dataset::Dataset,\n    tensor::{backend::Backend, Int, Tensor},\n};\nuse rustfft::{num_complex::Complex, FftPlanner};\n\nuse crate::analysis::{AccuracyHistory, LossHistory};\nuse crate::data::ModularAdditionDataset;\nuse crate::model::Transformer;\n\n#[derive(Debug, Clone)]\npub struct GrokkingVerificationConfig {\n    pub early_train_acc_threshold: f32,\n    pub early_step_max: usize,\n    pub plateau_min_step: usize,\n    pub generalization_window: usize,\n    pub chance_accuracy: f32,\n    pub chance_tolerance: f32,\n    pub target_val_acc_threshold: f32,\n    pub accuracy_window: usize,\n    pub loss_window: usize,\n    pub loss_drop_fraction: f64,\n}\n\nimpl GrokkingVerificationConfig {\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        Self {\n            early_train_acc_threshold: 0.99,\n            early_step_max: 1000,\n            plateau_min_step: 1000,\n            generalization_window: 10_000,\n            chance_accuracy: 1.0 / modulus as f32,\n            chance_tolerance: 0.02,\n            target_val_acc_threshold: 0.99,\n            accuracy_window: 5,\n            loss_window: 5,\n            loss_drop_fraction: 0.5,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\n#[allow(dead_code)]\npub struct GrokkingPhaseReport {\n    pub early_train_step: usize,\n    pub transition_step: usize,\n    pub plateau_max_val_acc: f32,\n    pub loss_drop_ratio: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossVerificationConfig {\n    pub plateau_min_step: usize,\n    pub drop_window: usize,\n    pub drop_fraction: f64,\n    pub min_step_lead: usize,\n}\n\n#[derive(Debug, Clone)]\n#[allow(dead_code)]\npub struct RestrictedLossReport {\n    pub restricted_drop_step: usize,\n    pub full_drop_step: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossDropConfig {\n    pub min_relative_decrease: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct RestrictedLossDropReport {\n    pub drop_step: usize,\n    pub drop_loss: f64,\n    pub baseline_loss: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct ExcludedLossSpikeConfig {\n    pub min_relative_increase: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct ExcludedLossSpikeReport {\n    pub spike_step: usize,\n    pub spike_loss: f64,\n    pub baseline_loss: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct MlpWaveVerificationConfig {\n    pub max_negative_fraction: f64,\n    pub negative_tolerance: f64,\n    pub min_dominant_ratio: f64,\n    pub min_correlation: f64,\n    pub phase_steps: usize,\n}\n\n#[derive(Debug, Clone)]\n#[allow(dead_code)]\npub struct MlpWaveNeuronReport {\n    pub neuron_index: usize,\n    pub dominant_frequency: usize,\n    pub dominant_ratio: f64,\n    pub negative_fraction: f64,\n    pub best_correlation: f64,\n}\n\n#[derive(Debug, Clone)]\n#[allow(dead_code)]\npub struct MlpWaveReport {\n    pub neuron_reports: Vec<MlpWaveNeuronReport>,\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldConfig {\n    pub fixed_y: usize,\n    pub neuron_indices: Vec<usize>,\n    pub min_post_score: f64,\n    pub max_pre_score: f64,\n    pub max_axis_ratio: f64,\n    pub min_pairs_passing: usize,\n}\n\nimpl PairwiseManifoldConfig {\n    pub fn default_for_modulus(_modulus: usize) -> Self {\n        Self {\n            fixed_y: 0,\n            neuron_indices: (0..7).collect(),\n            min_post_score: 0.75,\n            max_pre_score: 0.55,\n            max_axis_ratio: 8.0,\n            min_pairs_passing: 10,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\n#[allow(dead_code)]\npub struct PairwiseManifoldPairReport {\n    pub neuron_a: usize,\n    pub neuron_b: usize,\n    pub circularity_score: f64,\n    pub radius_cv: f64,\n    pub axis_ratio: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldReport {\n    pub pair_reports: Vec<PairwiseManifoldPairReport>,\n    pub average_score: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct PairwiseManifoldTransitionReport {\n    pub pre_average_score: f64,\n    pub post_average_score: f64,\n    pub passing_pairs: usize,\n    pub total_pairs: usize,\n}\n\n#[derive(Debug, Clone)]\npub struct DiagonalRidgeConfig {\n    pub neuron_index: usize,\n    pub max_lag: usize,\n    pub min_diagonal_ratio: f64,\n    pub min_diagonal_score: f64,\n}\n\nimpl DiagonalRidgeConfig {\n    #[allow(dead_code)]\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        let max_lag = (modulus / 10).max(4).min(20);\n        Self {\n            neuron_index: 10,\n            max_lag,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\n#[allow(dead_code)]\npub struct DiagonalRidgeReport {\n    pub neuron_index: usize,\n    pub diagonal_score: f64,\n    pub axis_score: f64,\n    pub ratio: f64,\n}\n\n#[derive(Debug, Clone)]\npub struct SnakeCurveConfig {\n    pub early_step_max: usize,\n    pub train_convergence_threshold: f32,\n    pub val_plateau_min_step: usize,\n    pub val_plateau_max_acc: f32,\n    pub val_final_threshold: f32,\n}\n\nimpl SnakeCurveConfig {\n    #[allow(dead_code)]\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        Self {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 1000,\n            val_plateau_max_acc: 1.0 / modulus as f32 + 0.05,\n            val_final_threshold: 0.90,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\n#[allow(dead_code)]\npub struct WeightNormDecayConfig {\n    pub min_relative_decrease: f64,\n    pub plateau_min_step: usize,\n    pub grok_step_window: usize,\n}\n\nimpl WeightNormDecayConfig {\n    pub fn default_for_grokking() -> Self {\n        Self {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct WeightNormDecayReport {\n    pub initial_norm: f64,\n    pub final_norm: f64,\n    pub relative_decrease: f64,\n    pub grok_step_norm: Option<f64>,\n}\n\n#[derive(Debug, Clone)]\npub struct SnakeCurveReport {\n    pub train_converged_by_step: usize,\n    pub val_plateau_max_acc: f32,\n    pub val_grok_step: usize,\n    pub curve_shape_valid: bool,\n}\n\npub fn verify_grokking_phase_transition(\n    loss_history: &LossHistory,\n    accuracy_history: &AccuracyHistory,\n    config: &GrokkingVerificationConfig,\n) -> Result<GrokkingPhaseReport, String> {\n    let (early_train_step, _) = accuracy_history\n        .train_snapshots\n        .iter()\n        .find(|(_, acc)| *acc >= config.early_train_acc_threshold)\n        .ok_or_else(|| \"train accuracy never reaches threshold\".to_string())?;\n\n    if *early_train_step > config.early_step_max {\n        return Err(format!(\n            \"train accuracy reaches {:.2} after step {}, expected before {}\",\n            config.early_train_acc_threshold, early_train_step, config.early_step_max\n        ));\n    }\n\n    let plateau_max_val_acc = accuracy_history\n        .val_snapshots\n        .iter()\n        .filter(|(step, _)| *step <= config.plateau_min_step)\n        .map(|(_, acc)| *acc)\n        .fold(None::<f32>, |accum, value| {\n            Some(accum.map_or(value, |max_val| max_val.max(value)))\n        })\n        .ok_or_else(|| \"no validation accuracy samples before plateau_min_step\".to_string())?;\n\n    if plateau_max_val_acc > config.chance_accuracy + config.chance_tolerance {\n        return Err(format!(\n            \"validation accuracy before step {} exceeds chance (max {:.4})\",\n            config.plateau_min_step, plateau_max_val_acc\n        ));\n    }\n\n    let (transition_step, _) = accuracy_history\n        .val_snapshots\n        .iter()\n        .find(|(step, acc)| {\n            *step >= config.plateau_min_step && *acc >= config.target_val_acc_threshold\n        })\n        .ok_or_else(|| \"validation accuracy never reaches target threshold\".to_string())?;\n\n    let latest_allowed = config.plateau_min_step + config.generalization_window;\n    if *transition_step > latest_allowed {\n        return Err(format!(\n            \"validation accuracy reaches {:.2} at step {}, expected before {}\",\n            config.target_val_acc_threshold, transition_step, latest_allowed\n        ));\n    }\n\n    verify_accuracy_jump(\n        &accuracy_history.val_snapshots,\n        *transition_step,\n        config.accuracy_window,\n        config.chance_accuracy,\n        config.chance_tolerance,\n        config.target_val_acc_threshold,\n    )?;\n\n    let loss_drop_ratio = verify_loss_drop(\n        &loss_history.val_snapshots,\n        *transition_step,\n        config.loss_window,\n        config.loss_drop_fraction,\n    )?;\n\n    Ok(GrokkingPhaseReport {\n        early_train_step: *early_train_step,\n        transition_step: *transition_step,\n        plateau_max_val_acc,\n        loss_drop_ratio,\n    })\n}\n\nfn verify_accuracy_jump(\n    val_accuracy: &[(usize, f32)],\n    transition_step: usize,\n    window: usize,\n    chance_accuracy: f32,\n    chance_tolerance: f32,\n    target_val_acc_threshold: f32,\n) -> Result<(), String> {\n    let before_values: Vec<f32> = val_accuracy\n        .iter()\n        .filter(|(step, _)| *step < transition_step)\n        .map(|(_, acc)| *acc)\n        .collect();\n\n    let after_values: Vec<f32> = val_accuracy\n        .iter()\n        .filter(|(step, _)| *step >= transition_step)\n        .map(|(_, acc)| *acc)\n        .collect();\n\n    if before_values.len() < window || after_values.len() < window {\n        return Err(\"insufficient accuracy samples around transition\".to_string());\n    }\n\n    let before_avg: f32 =\n        before_values[before_values.len() - window..].iter().sum::<f32>() / window as f32;\n    let after_avg: f32 = after_values[..window].iter().sum::<f32>() / window as f32;\n\n    if before_avg > chance_accuracy + chance_tolerance {\n        return Err(format!(\n            \"validation accuracy plateau too high before transition (avg {:.4})\",\n            before_avg\n        ));\n    }\n\n    if after_avg < target_val_acc_threshold {\n        return Err(format!(\n            \"validation accuracy after transition too low (avg {:.4})\",\n            after_avg\n        ));\n    }\n\n    Ok(())\n}\n\nfn verify_loss_drop(\n    val_losses: &[(usize, f64)],\n    transition_step: usize,\n    loss_window: usize,\n    loss_drop_fraction: f64,\n) -> Result<f64, String> {\n    let before_losses: Vec<f64> = val_losses\n        .iter()\n        .filter(|(step, _)| *step < transition_step)\n        .map(|(_, loss)| *loss)\n        .collect();\n\n    let after_losses: Vec<f64> = val_losses\n        .iter()\n        .filter(|(step, _)| *step >= transition_step)\n        .map(|(_, loss)| *loss)\n        .collect();\n\n    if before_losses.len() < loss_window || after_losses.len() < loss_window {\n        return Err(\"insufficient loss samples around transition\".to_string());\n    }\n\n    let before_avg: f64 =\n        before_losses[before_losses.len() - loss_window..].iter().sum::<f64>()\n            / loss_window as f64;\n    let after_avg: f64 = after_losses[..loss_window].iter().sum::<f64>() / loss_window as f64;\n\n    if after_avg > before_avg * (1.0 - loss_drop_fraction) {\n        return Err(format!(\n            \"validation loss drop too small (before {:.4}, after {:.4})\",\n            before_avg, after_avg\n        ));\n    }\n\n    Ok(after_avg / before_avg)\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_restricted_loss_early_drop(\n    full_losses: &[(usize, f64)],\n    restricted_losses: &[(usize, f64)],\n    config: &RestrictedLossVerificationConfig,\n) -> Result<RestrictedLossReport, String> {\n    let restricted_baseline =\n        baseline_loss(restricted_losses, config.plateau_min_step, config.drop_window)?;\n    let full_baseline = baseline_loss(full_losses, config.plateau_min_step, config.drop_window)?;\n\n    let restricted_drop_step = find_drop_step(\n        restricted_losses,\n        config.plateau_min_step,\n        restricted_baseline,\n        config.drop_fraction,\n    )?;\n    let full_drop_step = find_drop_step(\n        full_losses,\n        config.plateau_min_step,\n        full_baseline,\n        config.drop_fraction,\n    )?;\n\n    if restricted_drop_step + config.min_step_lead > full_drop_step {\n        return Err(format!(\n            \"restricted loss drops at step {}, expected at least {} steps before full loss (step {})\",\n            restricted_drop_step, config.min_step_lead, full_drop_step\n        ));\n    }\n\n    Ok(RestrictedLossReport {\n        restricted_drop_step,\n        full_drop_step,\n    })\n}\n\npub fn verify_restricted_loss_drop(\n    restricted_losses: &[(usize, f64)],\n    config: &RestrictedLossDropConfig,\n) -> Result<RestrictedLossDropReport, String> {\n    if restricted_losses.len() < 3 {\n        return Err(\"restricted loss history requires at least 3 points\".to_string());\n    }\n\n    let first = restricted_losses.first().ok_or_else(|| {\n        \"restricted loss history is empty\".to_string()\n    })?;\n    let baseline_loss = first.1;\n\n    let (min_idx, (min_step, min_loss)) = restricted_losses\n        .iter()\n        .enumerate()\n        .min_by(|(_, a), (_, b)| {\n            a.1.partial_cmp(&b.1).unwrap_or(std::cmp::Ordering::Equal)\n        })\n        .ok_or_else(|| \"restricted loss history is empty\".to_string())?;\n\n    if min_idx == 0 {\n        return Err(\"restricted loss minimum occurs at start (no drop detected)\".to_string());\n    }\n\n    let relative_decrease = (baseline_loss - min_loss) / baseline_loss;\n    if relative_decrease < config.min_relative_decrease {\n        return Err(format!(\n            \"restricted loss decrease too small (baseline {:.4}, min {:.4}, decrease {:.1}%)\",\n            baseline_loss, min_loss, relative_decrease * 100.0\n        ));\n    }\n\n    Ok(RestrictedLossDropReport {\n        drop_step: *min_step,\n        drop_loss: *min_loss,\n        baseline_loss,\n    })\n}\n\npub fn verify_excluded_loss_spike(\n    excluded_losses: &[(usize, f64)],\n    config: &ExcludedLossSpikeConfig,\n) -> Result<ExcludedLossSpikeReport, String> {\n    if excluded_losses.len() < 3 {\n        return Err(\"excluded loss history requires at least 3 points\".to_string());\n    }\n\n    let first = excluded_losses.first().ok_or_else(|| {\n        \"excluded loss history is empty\".to_string()\n    })?;\n    let last = excluded_losses.last().ok_or_else(|| {\n        \"excluded loss history is empty\".to_string()\n    })?;\n    let baseline_loss = (first.1 + last.1) / 2.0;\n\n    let (peak_idx, (peak_step, peak_loss)) = excluded_losses\n        .iter()\n        .enumerate()\n        .max_by(|(_, a), (_, b)| {\n            a.1.partial_cmp(&b.1).unwrap_or(std::cmp::Ordering::Equal)\n        })\n        .ok_or_else(|| \"excluded loss history is empty\".to_string())?;\n\n    if peak_idx == 0 || peak_idx + 1 == excluded_losses.len() {\n        return Err(\"excluded loss spike occurs at boundary\".to_string());\n    }\n\n    if peak_loss < &(baseline_loss * (1.0 + config.min_relative_increase)) {\n        return Err(format!(\n            \"excluded loss spike too small (baseline {:.4}, peak {:.4})\",\n            baseline_loss, peak_loss\n        ));\n    }\n\n    Ok(ExcludedLossSpikeReport {\n        spike_step: *peak_step,\n        spike_loss: *peak_loss,\n        baseline_loss,\n    })\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_mlp_activation_waves<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n    fixed_y: usize,\n    neuron_indices: &[usize],\n    config: &MlpWaveVerificationConfig,\n) -> Result<MlpWaveReport, String> {\n    let activations = collect_mlp_post_relu_activations(model, device, fixed_y)?;\n    if neuron_indices.is_empty() {\n        return Err(\"neuron indices list is empty\".to_string());\n    }\n\n    let d_ff = activations.len();\n    for &idx in neuron_indices {\n        if idx >= d_ff {\n            return Err(format!(\n                \"neuron index {} out of range (d_ff={})\",\n                idx, d_ff\n            ));\n        }\n    }\n\n    let signals: Vec<(usize, Vec<f64>)> = neuron_indices\n        .iter()\n        .map(|&idx| (idx, activations[idx].clone()))\n        .collect();\n    verify_mlp_activation_waves_from_signals(&signals, config)\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_mlp_activation_waves_from_signals(\n    signals: &[(usize, Vec<f64>)],\n    config: &MlpWaveVerificationConfig,\n) -> Result<MlpWaveReport, String> {\n    if signals.is_empty() {\n        return Err(\"no activation signals provided\".to_string());\n    }\n\n    let mut reports = Vec::with_capacity(signals.len());\n    for (neuron_index, signal) in signals {\n        let report = analyze_mlp_activation_wave(*neuron_index, signal, config)?;\n        reports.push(report);\n    }\n\n    Ok(MlpWaveReport {\n        neuron_reports: reports,\n    })\n}\n\npub fn verify_mlp_pairwise_manifold_transition<B: Backend>(\n    pre_model: &Transformer<B>,\n    post_model: &Transformer<B>,\n    device: &B::Device,\n    config: &PairwiseManifoldConfig,\n) -> Result<PairwiseManifoldTransitionReport, String> {\n    let modulus = ModularAdditionDataset::modulus();\n    if config.fixed_y >= modulus {\n        return Err(format!(\n            \"fixed_y {} out of range (modulus {})\",\n            config.fixed_y, modulus\n        ));\n    }\n\n    let pre_activations = collect_mlp_post_relu_activations(pre_model, device, config.fixed_y)?;\n    let post_activations = collect_mlp_post_relu_activations(post_model, device, config.fixed_y)?;\n\n    let d_ff = pre_activations.len();\n    if post_activations.len() != d_ff {\n        return Err(\"post activations shape mismatch\".to_string());\n    }\n\n    if config.neuron_indices.len() < 2 {\n        return Err(\"need at least two neuron indices\".to_string());\n    }\n\n    for &idx in &config.neuron_indices {\n        if idx >= d_ff {\n            return Err(format!(\n                \"neuron index {} out of range (d_ff={})\",\n                idx, d_ff\n            ));\n        }\n    }\n\n    let pre_signals: Vec<(usize, Vec<f64>)> = config\n        .neuron_indices\n        .iter()\n        .map(|&idx| (idx, pre_activations[idx].clone()))\n        .collect();\n    let post_signals: Vec<(usize, Vec<f64>)> = config\n        .neuron_indices\n        .iter()\n        .map(|&idx| (idx, post_activations[idx].clone()))\n        .collect();\n\n    verify_pairwise_manifold_transition_from_signals(&pre_signals, &post_signals, config)\n}\n\npub fn collect_mlp_activation_surface<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n    neuron_index: usize,\n) -> Result<Vec<Vec<f64>>, String> {\n    let modulus = ModularAdditionDataset::modulus();\n    let equals_token = ModularAdditionDataset::equals_token();\n\n    let mut inputs_vec = Vec::with_capacity(modulus * modulus * 3);\n    for x in 0..modulus {\n        for y in 0..modulus {\n            inputs_vec.push(x as i32);\n            inputs_vec.push(y as i32);\n            inputs_vec.push(equals_token as i32);\n        }\n    }\n\n    let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n        .reshape([modulus * modulus, 3]);\n    let (_, mlp_acts) = model.forward_with_mlp_activations(inputs);\n    let [batch_size, d_ff] = mlp_acts.dims();\n    if batch_size != modulus * modulus {\n        return Err(format!(\n            \"unexpected activation batch size {} (expected {})\",\n            batch_size,\n            modulus * modulus\n        ));\n    }\n    if neuron_index >= d_ff {\n        return Err(format!(\n            \"neuron index {} out of range (d_ff={})\",\n            neuron_index, d_ff\n        ));\n    }\n\n    let data: Vec<f32> = mlp_acts.into_data().to_vec().unwrap();\n    let mut surface = vec![vec![0.0f64; modulus]; modulus];\n    for x in 0..modulus {\n        for y in 0..modulus {\n            let batch = x * modulus + y;\n            let idx = batch * d_ff + neuron_index;\n            surface[x][y] = data[idx] as f64;\n        }\n    }\n\n    Ok(surface)\n}\n\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_constructive_interference_surface_from_grid(\n    surface: &[Vec<f64>],\n    config: &DiagonalRidgeConfig,\n) -> Result<DiagonalRidgeReport, String> {\n    let (diag_main, diag_anti, axis_x, axis_y) =\n        surface_autocorrelation_scores(surface, config.max_lag)?;\n\n    let diagonal_score = diag_main.max(diag_anti);\n    let axis_score = axis_x.max(axis_y);\n    let ratio = if axis_score > 0.0 {\n        diagonal_score / axis_score\n    } else {\n        f64::INFINITY\n    };\n\n    if diagonal_score < config.min_diagonal_score {\n        return Err(format!(\n            \"diagonal autocorrelation {:.3} below threshold {:.3}\",\n            diagonal_score, config.min_diagonal_score\n        ));\n    }\n\n    if ratio < config.min_diagonal_ratio {\n        return Err(format!(\n            \"diagonal/axis ratio {:.3} below threshold {:.3}\",\n            ratio, config.min_diagonal_ratio\n        ));\n    }\n\n    Ok(DiagonalRidgeReport {\n        neuron_index: config.neuron_index,\n        diagonal_score,\n        axis_score,\n        ratio,\n    })\n}\n\npub fn verify_pairwise_manifold_transition_from_signals(\n    pre_signals: &[(usize, Vec<f64>)],\n    post_signals: &[(usize, Vec<f64>)],\n    config: &PairwiseManifoldConfig,\n) -> Result<PairwiseManifoldTransitionReport, String> {\n    let pre_report = evaluate_pairwise_manifolds_from_signals(pre_signals)?;\n    let post_report = evaluate_pairwise_manifolds_from_signals(post_signals)?;\n\n    if pre_report.average_score > config.max_pre_score {\n        return Err(format!(\n            \"pre-grok average circularity {:.3} exceeds threshold {:.3}\",\n            pre_report.average_score, config.max_pre_score\n        ));\n    }\n\n    let passing_pairs = post_report\n        .pair_reports\n        .iter()\n        .filter(|pair| {\n            pair.circularity_score >= config.min_post_score\n                && pair.axis_ratio <= config.max_axis_ratio\n        })\n        .count();\n    if passing_pairs < config.min_pairs_passing {\n        return Err(format!(\n            \"post-grok only {} pairs meet circularity threshold (need {})\",\n            passing_pairs, config.min_pairs_passing\n        ));\n    }\n\n    Ok(PairwiseManifoldTransitionReport {\n        pre_average_score: pre_report.average_score,\n        post_average_score: post_report.average_score,\n        passing_pairs,\n        total_pairs: post_report.pair_reports.len(),\n    })\n}\n\nfn evaluate_pairwise_manifolds_from_signals(\n    signals: &[(usize, Vec<f64>)],\n) -> Result<PairwiseManifoldReport, String> {\n    if signals.len() < 2 {\n        return Err(\"need at least two activation signals\".to_string());\n    }\n\n    let signal_len = signals[0].1.len();\n    if signal_len < 3 {\n        return Err(\"activation signal too short\".to_string());\n    }\n\n    for (idx, signal) in signals {\n        if signal.len() != signal_len {\n            return Err(format!(\n                \"signal length mismatch for neuron {}\",\n                idx\n            ));\n        }\n    }\n\n    let mut reports = Vec::new();\n    for i in 0..signals.len() {\n        for j in (i + 1)..signals.len() {\n            let (neuron_a, signal_a) = &signals[i];\n            let (neuron_b, signal_b) = &signals[j];\n            let points: Vec<(f64, f64)> = signal_a\n                .iter()\n                .zip(signal_b.iter())\n                .map(|(x, y)| (*x, *y))\n                .collect();\n            let (score, radius_cv, axis_ratio) =\n                match pairwise_circularity_metrics(&points) {\n                    Ok(metrics) => metrics,\n                    Err(_) => (0.0, 1.0, f64::INFINITY),\n                };\n            reports.push(PairwiseManifoldPairReport {\n                neuron_a: *neuron_a,\n                neuron_b: *neuron_b,\n                circularity_score: score,\n                radius_cv,\n                axis_ratio,\n            });\n        }\n    }\n\n    let average_score = reports\n        .iter()\n        .map(|pair| pair.circularity_score)\n        .sum::<f64>()\n        / reports.len() as f64;\n\n    Ok(PairwiseManifoldReport {\n        pair_reports: reports,\n        average_score,\n    })\n}\n\nfn pairwise_circularity_metrics(points: &[(f64, f64)]) -> Result<(f64, f64, f64), String> {\n    if points.len() < 3 {\n        return Err(\"not enough points for circularity\".to_string());\n    }\n\n    let n = points.len() as f64;\n    let mean_x = points.iter().map(|(x, _)| *x).sum::<f64>() / n;\n    let mean_y = points.iter().map(|(_, y)| *y).sum::<f64>() / n;\n\n    let mut var_x = 0.0;\n    let mut var_y = 0.0;\n    let mut cov_xy = 0.0;\n    for (x, y) in points {\n        let dx = x - mean_x;\n        let dy = y - mean_y;\n        var_x += dx * dx;\n        var_y += dy * dy;\n        cov_xy += dx * dy;\n    }\n    var_x /= n;\n    var_y /= n;\n    cov_xy /= n;\n\n    let trace = var_x + var_y;\n    let det = var_x * var_y - cov_xy * cov_xy;\n    if det <= 1.0e-12 || trace <= 0.0 {\n        return Err(\"degenerate covariance\".to_string());\n    }\n\n    let half_trace = trace * 0.5;\n    let disc = (half_trace * half_trace - det).max(0.0).sqrt();\n    let lambda1 = half_trace + disc;\n    let lambda2 = half_trace - disc;\n    if lambda2 <= 0.0 {\n        return Err(\"degenerate eigenvalues\".to_string());\n    }\n\n    let axis_ratio = (lambda1 / lambda2).sqrt();\n\n    let mut v1_x = cov_xy;\n    let mut v1_y = lambda1 - var_x;\n    if v1_x.abs() < 1.0e-12 && v1_y.abs() < 1.0e-12 {\n        v1_x = 1.0;\n        v1_y = 0.0;\n    }\n    let norm = (v1_x * v1_x + v1_y * v1_y).sqrt();\n    v1_x /= norm;\n    v1_y /= norm;\n    let v2_x = -v1_y;\n    let v2_y = v1_x;\n\n    let mut radii = Vec::with_capacity(points.len());\n    for (x, y) in points {\n        let dx = x - mean_x;\n        let dy = y - mean_y;\n        let p1 = dx * v1_x + dy * v1_y;\n        let p2 = dx * v2_x + dy * v2_y;\n        let w1 = p1 / lambda1.sqrt();\n        let w2 = p2 / lambda2.sqrt();\n        radii.push((w1 * w1 + w2 * w2).sqrt());\n    }\n\n    let mean_radius = radii.iter().sum::<f64>() / radii.len() as f64;\n    if mean_radius == 0.0 {\n        return Err(\"zero mean radius\".to_string());\n    }\n\n    let mut variance = 0.0;\n    for r in &radii {\n        let diff = r - mean_radius;\n        variance += diff * diff;\n    }\n    let std = (variance / radii.len() as f64).sqrt();\n    let radius_cv = std / mean_radius;\n    let score = 1.0 / (1.0 + radius_cv);\n\n    Ok((score, radius_cv, axis_ratio))\n}\n\n// Helper functions for verification - reserved for future integration\n#[allow(dead_code)]\nfn baseline_loss(\n    losses: &[(usize, f64)],\n    plateau_min_step: usize,\n    drop_window: usize,\n) -> Result<f64, String> {\n    let before: Vec<f64> = losses\n        .iter()\n        .filter(|(step, _)| *step <= plateau_min_step)\n        .map(|(_, loss)| *loss)\n        .collect();\n\n    if before.len() < drop_window || drop_window == 0 {\n        return Err(\"insufficient loss samples for baseline\".to_string());\n    }\n\n    let window = &before[before.len() - drop_window..];\n    Ok(window.iter().sum::<f64>() / drop_window as f64)\n}\n\n#[allow(dead_code)]\nfn find_drop_step(\n    losses: &[(usize, f64)],\n    plateau_min_step: usize,\n    baseline: f64,\n    drop_fraction: f64,\n) -> Result<usize, String> {\n    losses\n        .iter()\n        .filter(|(step, _)| *step >= plateau_min_step)\n        .find(|(_, loss)| *loss <= baseline * (1.0 - drop_fraction))\n        .map(|(step, _)| *step)\n        .ok_or_else(|| \"loss never drops below threshold\".to_string())\n}\n\nfn collect_mlp_post_relu_activations<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n    fixed_y: usize,\n) -> Result<Vec<Vec<f64>>, String> {\n    let modulus = ModularAdditionDataset::modulus();\n    let equals_token = ModularAdditionDataset::equals_token();\n\n    let mut inputs_vec = Vec::with_capacity(modulus * 3);\n    for x in 0..modulus {\n        inputs_vec.push(x as i32);\n        inputs_vec.push(fixed_y as i32);\n        inputs_vec.push(equals_token as i32);\n    }\n\n    let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n        .reshape([modulus, 3]);\n    let (_, mlp_acts) = model.forward_with_mlp_activations(inputs);\n    let [batch_size, d_ff] = mlp_acts.dims();\n\n    if batch_size != modulus {\n        return Err(format!(\n            \"unexpected activation batch size {} (expected {})\",\n            batch_size, modulus\n        ));\n    }\n\n    let data: Vec<f32> = mlp_acts.into_data().to_vec().unwrap();\n    let mut per_neuron = vec![Vec::with_capacity(batch_size); d_ff];\n    for batch in 0..batch_size {\n        for neuron in 0..d_ff {\n            let idx = batch * d_ff + neuron;\n            per_neuron[neuron].push(data[idx] as f64);\n        }\n    }\n\n    Ok(per_neuron)\n}\n\n#[allow(dead_code)]\nfn surface_autocorrelation_scores(\n    surface: &[Vec<f64>],\n    max_lag: usize,\n) -> Result<(f64, f64, f64, f64), String> {\n    let (height, width) = surface_dimensions(surface)?;\n    if height < 2 || width < 2 {\n        return Err(\"surface grid too small\".to_string());\n    }\n\n    let (mean, var) = surface_mean_variance(surface)?;\n    if var == 0.0 {\n        return Err(\"surface variance is zero\".to_string());\n    }\n\n    let max_lag = max_lag.min(height.saturating_sub(1)).min(width.saturating_sub(1));\n    if max_lag == 0 {\n        return Err(\"max_lag too small for surface\".to_string());\n    }\n\n    let diag_main = average_autocorrelation(surface, mean, var, max_lag, 1, 1)?;\n    let diag_anti = average_autocorrelation(surface, mean, var, max_lag, 1, -1)?;\n    let axis_x = average_autocorrelation(surface, mean, var, max_lag, 1, 0)?;\n    let axis_y = average_autocorrelation(surface, mean, var, max_lag, 0, 1)?;\n\n    Ok((diag_main, diag_anti, axis_x, axis_y))\n}\n\n#[allow(dead_code)]\nfn surface_dimensions(surface: &[Vec<f64>]) -> Result<(usize, usize), String> {\n    let height = surface.len();\n    if height == 0 {\n        return Err(\"surface is empty\".to_string());\n    }\n    let width = surface[0].len();\n    if width == 0 {\n        return Err(\"surface has empty rows\".to_string());\n    }\n    for (idx, row) in surface.iter().enumerate() {\n        if row.len() != width {\n            return Err(format!(\n                \"surface row {} has length {}, expected {}\",\n                idx,\n                row.len(),\n                width\n            ));\n        }\n    }\n    Ok((height, width))\n}\n\n#[allow(dead_code)]\nfn surface_mean_variance(surface: &[Vec<f64>]) -> Result<(f64, f64), String> {\n    let (height, width) = surface_dimensions(surface)?;\n    let mut sum = 0.0;\n    for row in surface {\n        for value in row {\n            sum += *value;\n        }\n    }\n    let count = (height * width) as f64;\n    let mean = sum / count;\n    let mut variance = 0.0;\n    for row in surface {\n        for value in row {\n            let diff = *value - mean;\n            variance += diff * diff;\n        }\n    }\n    variance /= count;\n    Ok((mean, variance))\n}\n\n#[allow(dead_code)]\nfn average_autocorrelation(\n    surface: &[Vec<f64>],\n    mean: f64,\n    variance: f64,\n    max_lag: usize,\n    dx: i32,\n    dy: i32,\n) -> Result<f64, String> {\n    let (height, width) = surface_dimensions(surface)?;\n    let mut total = 0.0;\n    let mut lag_count = 0usize;\n\n    for lag in 1..=max_lag {\n        let mut sum = 0.0;\n        let mut count = 0usize;\n        let lag = lag as i32;\n        for x in 0..height {\n            for y in 0..width {\n                let x2 = x as i32 + dx * lag;\n                let y2 = y as i32 + dy * lag;\n                if x2 < 0 || y2 < 0 {\n                    continue;\n                }\n                let x2 = x2 as usize;\n                let y2 = y2 as usize;\n                if x2 >= height || y2 >= width {\n                    continue;\n                }\n                let a = surface[x][y] - mean;\n                let b = surface[x2][y2] - mean;\n                sum += a * b;\n                count += 1;\n            }\n        }\n        if count == 0 {\n            continue;\n        }\n        let corr = sum / (count as f64 * variance);\n        total += corr.abs();\n        lag_count += 1;\n    }\n\n    if lag_count == 0 {\n        return Err(\"no valid lags for autocorrelation\".to_string());\n    }\n\n    Ok(total / lag_count as f64)\n}\n\n#[allow(dead_code)]\nfn analyze_mlp_activation_wave(\n    neuron_index: usize,\n    signal: &[f64],\n    config: &MlpWaveVerificationConfig,\n) -> Result<MlpWaveNeuronReport, String> {\n    if signal.len() < 3 {\n        return Err(format!(\n            \"activation signal for neuron {} is too short\",\n            neuron_index\n        ));\n    }\n\n    let negative_count = signal\n        .iter()\n        .filter(|value| **value < -config.negative_tolerance)\n        .count();\n    let negative_fraction = negative_count as f64 / signal.len() as f64;\n    if negative_fraction > config.max_negative_fraction {\n        return Err(format!(\n            \"neuron {} has {:.3} negative activations\",\n            neuron_index, negative_fraction\n        ));\n    }\n\n    let (dominant_frequency, dominant_ratio) = dominant_frequency_ratio(signal)?;\n    if dominant_ratio < config.min_dominant_ratio {\n        return Err(format!(\n            \"neuron {} dominant frequency ratio {:.3} below threshold {:.3}\",\n            neuron_index, dominant_ratio, config.min_dominant_ratio\n        ));\n    }\n\n    let best_correlation =\n        best_rectified_sine_correlation(signal, dominant_frequency, config.phase_steps);\n    if best_correlation < config.min_correlation {\n        return Err(format!(\n            \"neuron {} rectified sine correlation {:.3} below threshold {:.3}\",\n            neuron_index, best_correlation, config.min_correlation\n        ));\n    }\n\n    Ok(MlpWaveNeuronReport {\n        neuron_index,\n        dominant_frequency,\n        dominant_ratio,\n        negative_fraction,\n        best_correlation,\n    })\n}\n\n#[allow(dead_code)]\nfn dominant_frequency_ratio(signal: &[f64]) -> Result<(usize, f64), String> {\n    if signal.len() < 2 {\n        return Err(\"signal too short for FFT\".to_string());\n    }\n\n    let mut planner = FftPlanner::new();\n    let fft = planner.plan_fft_forward(signal.len());\n    let mut buffer: Vec<Complex<f64>> = signal\n        .iter()\n        .map(|&x| Complex::new(x, 0.0))\n        .collect();\n    fft.process(&mut buffer);\n    let magnitudes: Vec<f64> = buffer.iter().map(|c| c.norm()).collect();\n\n    if magnitudes.len() < 2 {\n        return Err(\"insufficient FFT bins\".to_string());\n    }\n\n    let mut dominant_idx = 1usize;\n    let mut dominant_mag = magnitudes[1];\n    for (idx, &mag) in magnitudes.iter().enumerate().skip(2) {\n        if mag > dominant_mag {\n            dominant_mag = mag;\n            dominant_idx = idx;\n        }\n    }\n\n    if dominant_idx == 0 {\n        return Err(\"dominant frequency is zero\".to_string());\n    }\n\n    let mut nonzero = magnitudes[1..].to_vec();\n    nonzero.sort_by(|a, b| a.partial_cmp(b).unwrap_or(std::cmp::Ordering::Equal));\n    let median_mag = nonzero[nonzero.len() / 2];\n    let ratio = if median_mag == 0.0 {\n        dominant_mag\n    } else {\n        dominant_mag / median_mag\n    };\n\n    Ok((dominant_idx, ratio))\n}\n\n#[allow(dead_code)]\nfn best_rectified_sine_correlation(signal: &[f64], frequency: usize, phase_steps: usize) -> f64 {\n    if signal.is_empty() || frequency == 0 || phase_steps == 0 {\n        return 0.0;\n    }\n\n    let tau = std::f64::consts::PI * 2.0;\n    let mut best = -1.0;\n    for step in 0..phase_steps {\n        let phase = tau * step as f64 / phase_steps as f64;\n        let wave: Vec<f64> = (0..signal.len())\n            .map(|idx| {\n                let angle = tau * frequency as f64 * idx as f64 / signal.len() as f64 + phase;\n                angle.sin().max(0.0)\n            })\n            .collect();\n        let corr = pearson_correlation(signal, &wave);\n        if corr > best {\n            best = corr;\n        }\n    }\n\n    best\n}\n\n#[allow(dead_code)]\nfn pearson_correlation(a: &[f64], b: &[f64]) -> f64 {\n    if a.len() != b.len() || a.is_empty() {\n        return 0.0;\n    }\n\n    let mean_a = a.iter().sum::<f64>() / a.len() as f64;\n    let mean_b = b.iter().sum::<f64>() / b.len() as f64;\n\n    let mut num = 0.0;\n    let mut denom_a = 0.0;\n    let mut denom_b = 0.0;\n    for (x, y) in a.iter().zip(b.iter()) {\n        let da = x - mean_a;\n        let db = y - mean_b;\n        num += da * db;\n        denom_a += da * da;\n        denom_b += db * db;\n    }\n\n    if denom_a == 0.0 || denom_b == 0.0 {\n        return 0.0;\n    }\n\n    num / (denom_a.sqrt() * denom_b.sqrt())\n}\n\n/// Verify that the model actually learned modular addition\n/// by testing it on ALL possible examples systematically\npub fn verify_full_accuracy<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n) -> (f32, f32) {\n    println!(\"ðŸ” Verifying model on COMPLETE dataset (no sampling)...\");\n\n    let train_dataset = ModularAdditionDataset::new(true, 42);\n    let val_dataset = ModularAdditionDataset::new(false, 42);\n\n    let train_acc = test_all_examples(model, &train_dataset, device);\n    let val_acc = test_all_examples(model, &val_dataset, device);\n\n    println!(\"ðŸ“Š FULL Dataset Results:\");\n    println!(\"   Training: {}/{} = {:.2}%\",\n        (train_acc * train_dataset.len() as f32) as usize,\n        train_dataset.len(),\n        train_acc * 100.0);\n    println!(\"   Validation: {}/{} = {:.2}%\",\n        (val_acc * val_dataset.len() as f32) as usize,\n        val_dataset.len(),\n        val_acc * 100.0);\n\n    (train_acc, val_acc)\n}\n\n/// Test model on ALL examples in dataset (no random sampling)\nfn test_all_examples<B: Backend>(\n    model: &Transformer<B>,\n    dataset: &ModularAdditionDataset,\n    device: &B::Device,\n) -> f32 {\n    let mut total_correct = 0;\n    let total = dataset.len();\n\n    // Test in batches to avoid memory issues\n    let batch_size = 100;\n    for batch_start in (0..total).step_by(batch_size) {\n        let batch_end = (batch_start + batch_size).min(total);\n        let batch_len = batch_end - batch_start;\n\n        let mut inputs_vec = Vec::new();\n        let mut targets_vec = Vec::new();\n\n        for idx in batch_start..batch_end {\n            let (input, target) = dataset.get(idx).unwrap();\n            inputs_vec.extend(input);\n            targets_vec.push(target as i32);\n        }\n\n        let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n            .reshape([batch_len, 3]);\n\n        let logits = model.forward(inputs);\n        let predictions = logits.argmax(1).squeeze::<1>();\n        let predictions_vec: Vec<i32> = predictions.into_data().to_vec().unwrap();\n\n        let correct = predictions_vec\n            .iter()\n            .zip(targets_vec.iter())\n            .filter(|(p, t)| p == t)\n            .count();\n\n        total_correct += correct;\n    }\n\n    total_correct as f32 / total as f32\n}\n\n/// Verify that weight norms decrease over time with weight decay\n/// and show correlation with the grokking transition\npub fn verify_weight_norm_decay(\n    weight_norm_history: &[(usize, f64)],\n    grok_step: Option<usize>,\n    config: &WeightNormDecayConfig,\n) -> Result<WeightNormDecayReport, String> {\n    if weight_norm_history.len() < 2 {\n        return Err(\"weight norm history requires at least 2 snapshots\".to_string());\n    }\n\n    let initial_norm = weight_norm_history\n        .first()\n        .ok_or_else(|| \"weight norm history is empty\".to_string())?\n        .1;\n    let final_norm = weight_norm_history\n        .last()\n        .ok_or_else(|| \"weight norm history is empty\".to_string())?\n        .1;\n\n    if initial_norm <= 0.0 {\n        return Err(\"initial weight norm must be positive\".to_string());\n    }\n\n    // Check that norms are generally decreasing (with weight decay)\n    let relative_decrease = (initial_norm - final_norm) / initial_norm;\n    if relative_decrease < config.min_relative_decrease {\n        return Err(format!(\n            \"weight norm decrease {:.4} below threshold {:.4} (initial {:.4}, final {:.4})\",\n            relative_decrease, config.min_relative_decrease, initial_norm, final_norm\n        ));\n    }\n\n    // Check that norms are not increasing (weight decay should monotonically decrease norms)\n    for window in weight_norm_history.windows(2) {\n        let prev_norm = window[0].1;\n        let next_norm = window[1].1;\n        if next_norm > prev_norm * 1.01 {\n            // Allow 1% tolerance for numerical noise\n            return Err(format!(\n                \"weight norm increased from {:.4} to {:.4} at step {}\",\n                prev_norm, next_norm, window[1].0\n            ));\n        }\n    }\n\n    // If grok step is provided, check that norms show a shift around grokking\n    let grok_step_norm = if let Some(step) = grok_step {\n        weight_norm_history\n            .iter()\n            .filter(|(s, _)| {\n                *s >= step.saturating_sub(config.grok_step_window)\n                    && *s <= step + config.grok_step_window\n            })\n            .map(|(_, norm)| *norm)\n            .next()\n    } else {\n        None\n    };\n\n    Ok(WeightNormDecayReport {\n        initial_norm,\n        final_norm,\n        relative_decrease,\n        grok_step_norm,\n    })\n}\n\n/// Verify the characteristic \"snake curve\" shape of grokking phenomenon\n/// on log-scale plots: train accuracy rises early, validation stays flat then jumps\n/// NOTE: Verification function reserved for future integration or manual analysis\n#[allow(dead_code)]\npub fn verify_snake_curve_shape(\n    train_acc: &[(usize, f32)],\n    val_acc: &[(usize, f32)],\n    config: &SnakeCurveConfig,\n) -> Result<SnakeCurveReport, String> {\n    if train_acc.is_empty() || val_acc.is_empty() {\n        return Err(\"empty accuracy history\".to_string());\n    }\n\n    // Phase 1: Verify early train convergence (immediate drop on log scale)\n    let train_converged_step = train_acc\n        .iter()\n        .find(|(_, acc)| *acc >= config.train_convergence_threshold)\n        .map(|(step, _)| *step)\n        .ok_or_else(|| {\n            format!(\n                \"train accuracy never reaches {} threshold\",\n                config.train_convergence_threshold\n            )\n        })?;\n\n    if train_converged_step > config.early_step_max {\n        return Err(format!(\n            \"train accuracy converges at step {}, expected before {}\",\n            train_converged_step, config.early_step_max\n        ));\n    }\n\n    // Phase 2: Verify validation plateau (flat on log scale)\n    let plateau_samples: Vec<f32> = val_acc\n        .iter()\n        .filter(|(step, _)| *step >= 1 && *step <= config.val_plateau_min_step)\n        .map(|(_, acc)| *acc)\n        .collect();\n\n    if plateau_samples.is_empty() {\n        return Err(\"no validation samples in plateau region\".to_string());\n    }\n\n    let val_plateau_max = plateau_samples\n        .iter()\n        .fold(0.0f32, |max_val, &acc| max_val.max(acc));\n\n    if val_plateau_max > config.val_plateau_max_acc {\n        return Err(format!(\n            \"validation accuracy in plateau region too high: max {:.4} exceeds {:.4}\",\n            val_plateau_max, config.val_plateau_max_acc\n        ));\n    }\n\n    // Phase 3: Verify delayed validation jump (crash up on log scale)\n    let val_grok_step = val_acc\n        .iter()\n        .filter(|(step, _)| *step > config.val_plateau_min_step)\n        .find(|(_, acc)| *acc >= config.val_final_threshold)\n        .map(|(step, _)| *step)\n        .ok_or_else(|| {\n            format!(\n                \"validation accuracy never reaches {} after plateau\",\n                config.val_final_threshold\n            )\n        })?;\n\n    // Phase 4: Verify phase ordering (snake shape: early train, late val)\n    let curve_shape_valid = train_converged_step < config.val_plateau_min_step\n        && val_grok_step > config.val_plateau_min_step\n        && train_converged_step < val_grok_step;\n\n    if !curve_shape_valid {\n        return Err(format!(\n            \"invalid curve shape: train converged at {}, val plateau until {}, val grokked at {}\",\n            train_converged_step, config.val_plateau_min_step, val_grok_step\n        ));\n    }\n\n    Ok(SnakeCurveReport {\n        train_converged_by_step: train_converged_step,\n        val_plateau_max_acc: val_plateau_max,\n        val_grok_step,\n        curve_shape_valid,\n    })\n}\n\n/// Test specific examples to see if model actually computes mod p\npub fn test_specific_examples<B: Backend>(\n    model: &Transformer<B>,\n    device: &B::Device,\n) {\n    println!();\n    println!(\"ðŸ§ª Testing specific examples:\");\n\n    let modulus = ModularAdditionDataset::modulus();\n    let equals_token = ModularAdditionDataset::equals_token();\n    let test_cases = vec![\n        (0, 0, 0),\n        (1, 1, 2),\n        (50, 50, (50 + 50) % modulus),\n        (112, 1, (112 + 1) % modulus),\n        (56, 57, (56 + 57) % modulus),\n        (112, 112, (112 + 112) % modulus),\n        (10, 20, 30),\n        (60, 60, (60 + 60) % modulus),\n    ];\n\n    for (a, b, expected) in test_cases {\n        let input_vec = vec![a as i32, b as i32, equals_token as i32];\n        let input = Tensor::<B, 1, Int>::from_ints(input_vec.as_slice(), device)\n            .reshape([1, 3]);\n\n        let logits = model.forward(input);\n        let prediction = logits.argmax(1).squeeze::<1>();\n        let pred_value: i32 = prediction.into_data().to_vec().unwrap()[0];\n\n        let correct = if pred_value == expected as i32 { \"âœ“\" } else { \"âœ—\" };\n        println!(\n            \"   {} + {} mod {} = {} | Model: {} {}\",\n            a, b, modulus, expected, pred_value, correct\n        );\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    fn make_histories(\n        train_acc: &[(usize, f32)],\n        val_acc: &[(usize, f32)],\n        val_loss: &[(usize, f64)],\n    ) -> (LossHistory, AccuracyHistory) {\n        let mut loss_history = LossHistory::new();\n        loss_history.val_snapshots = val_loss.to_vec();\n        let mut accuracy_history = AccuracyHistory::new();\n        accuracy_history.train_snapshots = train_acc.to_vec();\n        accuracy_history.val_snapshots = val_acc.to_vec();\n        (loss_history, accuracy_history)\n    }\n\n    #[test]\n    fn grokking_phase_verification_passes_on_synthetic_signal() {\n        let train_acc = vec![(0, 0.2), (500, 0.995), (1000, 0.999)];\n        let val_acc = vec![\n            (0, 0.01),\n            (500, 0.01),\n            (1500, 0.01),\n            (2200, 0.995),\n            (2500, 0.999),\n        ];\n        let val_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 2.0),\n            (2200, 0.2),\n            (2500, 0.15),\n        ];\n\n        let (loss_history, accuracy_history) = make_histories(&train_acc, &val_acc, &val_loss);\n        let config = GrokkingVerificationConfig {\n            early_step_max: 1000,\n            plateau_min_step: 1500,\n            generalization_window: 1500,\n            accuracy_window: 2,\n            loss_window: 2,\n            ..GrokkingVerificationConfig::default_for_modulus(ModularAdditionDataset::modulus())\n        };\n\n        let report = verify_grokking_phase_transition(&loss_history, &accuracy_history, &config)\n            .expect(\"expected grokking verification to pass\");\n        assert!(report.transition_step >= config.plateau_min_step);\n        assert!(report.early_train_step <= config.early_step_max);\n    }\n\n    #[test]\n    fn grokking_phase_verification_fails_without_transition() {\n        let train_acc = vec![(0, 0.2), (500, 0.995)];\n        let val_acc = vec![(0, 0.01), (500, 0.01), (1500, 0.02)];\n        let val_loss = vec![(0, 2.0), (500, 2.0), (1500, 1.9)];\n\n        let (loss_history, accuracy_history) = make_histories(&train_acc, &val_acc, &val_loss);\n        let config = GrokkingVerificationConfig {\n            early_step_max: 1000,\n            plateau_min_step: 1000,\n            generalization_window: 1000,\n            accuracy_window: 2,\n            loss_window: 2,\n            ..GrokkingVerificationConfig::default_for_modulus(ModularAdditionDataset::modulus())\n        };\n\n        let err = verify_grokking_phase_transition(&loss_history, &accuracy_history, &config)\n            .expect_err(\"expected grokking verification to fail\");\n        assert!(err.contains(\"validation accuracy never reaches target\"));\n    }\n\n    #[test]\n    fn grokking_phase_verification_fails_with_slow_accuracy_rise() {\n        let train_acc = vec![(0, 0.2), (500, 0.995), (1000, 0.999)];\n        let val_acc = vec![\n            (0, 0.01),\n            (500, 0.01),\n            (1500, 0.5),\n            (2000, 0.6),\n            (2200, 0.995),\n            (2500, 0.999),\n        ];\n        let val_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 2.0),\n            (2200, 0.2),\n            (2500, 0.15),\n        ];\n\n        let (loss_history, accuracy_history) = make_histories(&train_acc, &val_acc, &val_loss);\n        let config = GrokkingVerificationConfig {\n            early_step_max: 1000,\n            plateau_min_step: 1000,\n            generalization_window: 2000,\n            accuracy_window: 2,\n            loss_window: 2,\n            ..GrokkingVerificationConfig::default_for_modulus(ModularAdditionDataset::modulus())\n        };\n\n        let err = verify_grokking_phase_transition(&loss_history, &accuracy_history, &config)\n            .expect_err(\"expected grokking verification to fail\");\n        assert!(err.contains(\"plateau too high\"));\n    }\n\n    #[test]\n    fn restricted_loss_detection_passes_with_early_drop() {\n        let full_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 2.0),\n            (2500, 2.0),\n            (3500, 0.3),\n        ];\n        let restricted_loss = vec![\n            (0, 2.0),\n            (500, 2.0),\n            (1500, 0.8),\n            (2500, 0.4),\n            (3500, 0.3),\n        ];\n\n        let config = RestrictedLossVerificationConfig {\n            plateau_min_step: 1000,\n            drop_window: 2,\n            drop_fraction: 0.5,\n            min_step_lead: 500,\n        };\n\n        let report = verify_restricted_loss_early_drop(&full_loss, &restricted_loss, &config)\n            .expect(\"expected restricted loss verification to pass\");\n        assert!(report.restricted_drop_step < report.full_drop_step);\n    }\n\n    #[test]\n    fn restricted_loss_detection_fails_when_full_drops_first() {\n        let full_loss = vec![(0, 2.0), (1000, 1.0), (1500, 0.8)];\n        let restricted_loss = vec![(0, 2.0), (1000, 2.0), (1500, 0.8)];\n\n        let config = RestrictedLossVerificationConfig {\n            plateau_min_step: 500,\n            drop_window: 1,\n            drop_fraction: 0.3,\n            min_step_lead: 200,\n        };\n\n        let err = verify_restricted_loss_early_drop(&full_loss, &restricted_loss, &config)\n            .expect_err(\"expected restricted loss verification to fail\");\n        assert!(err.contains(\"restricted loss drops\"));\n    }\n\n    #[test]\n    fn excluded_loss_spike_verification_passes() {\n        let excluded_loss = vec![(0, 1.0), (1500, 2.2), (3000, 1.1)];\n        let config = ExcludedLossSpikeConfig {\n            min_relative_increase: 0.5,\n        };\n\n        let report = verify_excluded_loss_spike(&excluded_loss, &config)\n            .expect(\"expected excluded loss spike verification to pass\");\n        assert_eq!(report.spike_step, 1500);\n    }\n\n    #[test]\n    fn excluded_loss_spike_verification_fails_without_spike() {\n        let excluded_loss = vec![(0, 1.0), (1500, 1.1), (3000, 1.0)];\n        let config = ExcludedLossSpikeConfig {\n            min_relative_increase: 0.5,\n        };\n\n        let err = verify_excluded_loss_spike(&excluded_loss, &config)\n            .expect_err(\"expected excluded loss spike verification to fail\");\n        assert!(err.contains(\"excluded loss spike too small\"));\n    }\n\n    #[test]\n    fn restricted_loss_drop_verification_passes() {\n        let restricted_loss = vec![\n            (0, 2.0),\n            (1000, 1.8),\n            (5000, 1.2),\n            (7000, 0.8),\n            (10000, 0.9),\n        ];\n        let config = RestrictedLossDropConfig {\n            min_relative_decrease: 0.3,\n        };\n\n        let report = verify_restricted_loss_drop(&restricted_loss, &config)\n            .expect(\"expected restricted loss drop verification to pass\");\n        assert_eq!(report.drop_step, 7000);\n        assert!(report.baseline_loss > report.drop_loss);\n        assert!((report.baseline_loss - report.drop_loss) / report.baseline_loss >= 0.3);\n    }\n\n    #[test]\n    fn restricted_loss_drop_verification_fails_without_drop() {\n        let restricted_loss = vec![\n            (0, 2.0),\n            (1000, 2.1),\n            (5000, 1.9),\n            (7000, 2.0),\n            (10000, 2.05),\n        ];\n        let config = RestrictedLossDropConfig {\n            min_relative_decrease: 0.3,\n        };\n\n        let err = verify_restricted_loss_drop(&restricted_loss, &config)\n            .expect_err(\"expected restricted loss drop verification to fail\");\n        assert!(err.contains(\"restricted loss decrease too small\"));\n    }\n\n    #[test]\n    fn mlp_activation_wave_verification_passes_on_rectified_sine() {\n        let n = ModularAdditionDataset::modulus();\n        let freq = 5.0;\n        let mut signal = Vec::with_capacity(n);\n        for idx in 0..n {\n            let angle = 2.0 * std::f64::consts::PI * freq * idx as f64 / n as f64;\n            let rectified = angle.sin().max(0.0);\n            let wobble = (2.0 * std::f64::consts::PI * 2.0 * idx as f64 / n as f64)\n                .sin()\n                .abs()\n                * 0.02;\n            signal.push(rectified + wobble);\n        }\n\n        let config = MlpWaveVerificationConfig {\n            max_negative_fraction: 0.0,\n            negative_tolerance: 1e-6,\n            min_dominant_ratio: 3.0,\n            min_correlation: 0.7,\n            phase_steps: 32,\n        };\n\n        let report =\n            verify_mlp_activation_waves_from_signals(&[(0, signal)], &config)\n                .expect(\"expected rectified sine to pass\");\n        assert_eq!(report.neuron_reports.len(), 1);\n    }\n\n    #[test]\n    fn mlp_activation_wave_verification_fails_on_noise() {\n        let n = ModularAdditionDataset::modulus();\n        let signal: Vec<f64> = (0..n).map(|idx| idx as f64 / n as f64).collect();\n\n        let config = MlpWaveVerificationConfig {\n            max_negative_fraction: 0.0,\n            negative_tolerance: 1e-6,\n            min_dominant_ratio: 4.0,\n            min_correlation: 0.8,\n            phase_steps: 16,\n        };\n\n        let err =\n            verify_mlp_activation_waves_from_signals(&[(0, signal)], &config)\n                .expect_err(\"expected linear signal to fail\");\n        assert!(err.contains(\"dominant frequency ratio\") || err.contains(\"correlation\"));\n    }\n\n    #[test]\n    fn pairwise_manifold_transition_passes_on_elliptic_signals() {\n        let n = ModularAdditionDataset::modulus();\n        let freq = 5.0;\n        let mut post_signals = Vec::new();\n        for neuron_idx in 0..7usize {\n            let phase = 2.0 * std::f64::consts::PI * neuron_idx as f64 / 7.0;\n            let signal: Vec<f64> = (0..n)\n                .map(|idx| {\n                    let angle = 2.0 * std::f64::consts::PI * freq * idx as f64 / n as f64 + phase;\n                    angle.sin()\n                })\n                .collect();\n            post_signals.push((neuron_idx, signal));\n        }\n\n        let mut pre_signals = Vec::new();\n        for neuron_idx in 0..7usize {\n            let signal: Vec<f64> = (0..n)\n                .map(|idx| idx as f64 / n as f64 + neuron_idx as f64 * 0.01)\n                .collect();\n            pre_signals.push((neuron_idx, signal));\n        }\n\n        let config = PairwiseManifoldConfig {\n            fixed_y: 0,\n            neuron_indices: (0..7).collect(),\n            min_post_score: 0.75,\n            max_pre_score: 0.55,\n            max_axis_ratio: 8.0,\n            min_pairs_passing: 8,\n        };\n\n        let report = verify_pairwise_manifold_transition_from_signals(\n            &pre_signals,\n            &post_signals,\n            &config,\n        )\n        .expect(\"expected pairwise manifold transition to pass\");\n        assert!(report.passing_pairs >= config.min_pairs_passing);\n    }\n\n    #[test]\n    fn pairwise_manifold_transition_fails_without_rings() {\n        let n = ModularAdditionDataset::modulus();\n        let mut signals = Vec::new();\n        for neuron_idx in 0..7usize {\n            let signal: Vec<f64> = (0..n)\n                .map(|idx| idx as f64 / n as f64 + neuron_idx as f64 * 0.02)\n                .collect();\n            signals.push((neuron_idx, signal));\n        }\n\n        let config = PairwiseManifoldConfig {\n            fixed_y: 0,\n            neuron_indices: (0..7).collect(),\n            min_post_score: 0.8,\n            max_pre_score: 0.55,\n            max_axis_ratio: 4.0,\n            min_pairs_passing: 5,\n        };\n\n        let err = verify_pairwise_manifold_transition_from_signals(\n            &signals,\n            &signals,\n            &config,\n        )\n        .expect_err(\"expected pairwise manifold transition to fail\");\n        assert!(err.contains(\"post-grok\") || err.contains(\"pre-grok\"));\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_passes_on_diagonal_corrugation() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create diagonal ridges: f(x,y) = sin(2Ï€ * freq * (x+y) / n)\n        let freq = 3.0;\n        for x in 0..n {\n            for y in 0..n {\n                let angle = 2.0 * std::f64::consts::PI * freq * (x + y) as f64 / n as f64;\n                surface[x][y] = angle.sin();\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let report = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect(\"expected diagonal ridge verification to pass\");\n        assert!(report.diagonal_score > report.axis_score);\n        assert!(report.ratio >= config.min_diagonal_ratio);\n        assert!(report.diagonal_score >= config.min_diagonal_score);\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_passes_on_anti_diagonal_pattern() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create anti-diagonal ridges: f(x,y) = sin(2Ï€ * freq * (x-y) / n)\n        let freq = 3.0;\n        for x in 0..n {\n            for y in 0..n {\n                let angle = 2.0 * std::f64::consts::PI * freq * (x as i32 - y as i32) as f64 / n as f64;\n                surface[x][y] = angle.sin();\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let report = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect(\"expected anti-diagonal ridge verification to pass\");\n        assert!(report.diagonal_score > report.axis_score);\n        assert!(report.ratio >= config.min_diagonal_ratio);\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_fails_on_axis_aligned_pattern() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create axis-aligned ridges: f(x,y) = sin(2Ï€ * freq * x / n)\n        let freq = 3.0;\n        for x in 0..n {\n            for y in 0..n {\n                let angle = 2.0 * std::f64::consts::PI * freq * x as f64 / n as f64;\n                surface[x][y] = angle.sin();\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let err = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect_err(\"expected axis-aligned pattern to fail diagonal verification\");\n        assert!(err.contains(\"diagonal/axis ratio\") || err.contains(\"diagonal autocorrelation\"));\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_fails_on_flat_surface() {\n        let n = 50;\n        let surface = vec![vec![1.5; n]; n];\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let err = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect_err(\"expected flat surface to fail\");\n        assert!(err.contains(\"surface variance is zero\"));\n    }\n\n    #[test]\n    fn diagonal_ridge_verification_fails_on_random_noise() {\n        let n = 50;\n        let mut surface = vec![vec![0.0; n]; n];\n\n        // Create random noise (low autocorrelation everywhere)\n        for x in 0..n {\n            for y in 0..n {\n                surface[x][y] = ((x * 17 + y * 31) % 100) as f64 / 100.0;\n            }\n        }\n\n        let config = DiagonalRidgeConfig {\n            neuron_index: 10,\n            max_lag: 8,\n            min_diagonal_ratio: 1.3,\n            min_diagonal_score: 0.2,\n        };\n\n        let err = verify_constructive_interference_surface_from_grid(&surface, &config)\n            .expect_err(\"expected random noise to fail diagonal verification\");\n        assert!(err.contains(\"diagonal/axis ratio\") || err.contains(\"diagonal autocorrelation\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_passes_on_valid_grokking_curve() {\n        // Simulate typical grokking: train converges early, val plateaus then jumps\n        let train_acc = vec![\n            (1, 0.10),\n            (10, 0.50),\n            (100, 0.80),\n            (500, 0.98),\n            (1000, 0.99),\n            (2000, 0.995),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (10, 0.009),\n            (100, 0.01),\n            (500, 0.011),\n            (1000, 0.009),\n            (1500, 0.01),\n            (2000, 0.92),\n            (3000, 0.98),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 1500,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let report = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect(\"expected snake curve verification to pass\");\n\n        assert_eq!(report.train_converged_by_step, 500);\n        assert!(report.val_plateau_max_acc < config.val_plateau_max_acc);\n        assert_eq!(report.val_grok_step, 2000);\n        assert!(report.curve_shape_valid);\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_on_simultaneous_convergence() {\n        // Both train and val converge at the same time (no grokking)\n        let train_acc = vec![\n            (1, 0.10),\n            (100, 0.98),\n            (500, 0.99),\n        ];\n        let val_acc = vec![\n            (1, 0.10),\n            (100, 0.97),\n            (500, 0.98),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 200,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let err = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect_err(\"expected snake curve verification to fail\");\n        assert!(err.contains(\"plateau region too high\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_on_slow_train_convergence() {\n        // Train takes too long to converge (not the typical snake curve)\n        let train_acc = vec![\n            (1, 0.10),\n            (500, 0.50),\n            (1000, 0.70),\n            (2000, 0.98),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (500, 0.01),\n            (1000, 0.01),\n            (2000, 0.92),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 1500,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let err = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect_err(\"expected snake curve verification to fail\");\n        assert!(err.contains(\"converges at step 2000\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_without_val_jump() {\n        // Validation never jumps up (no generalization)\n        let train_acc = vec![\n            (1, 0.10),\n            (100, 0.98),\n            (500, 0.99),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (100, 0.01),\n            (500, 0.01),\n            (2000, 0.02),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 200,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let err = verify_snake_curve_shape(&train_acc, &val_acc, &config)\n            .expect_err(\"expected snake curve verification to fail\");\n        assert!(err.contains(\"validation accuracy never reaches\"));\n    }\n\n    #[test]\n    fn snake_curve_verification_fails_on_wrong_phase_ordering() {\n        // Val jumps before train converges (backwards snake)\n        let train_acc = vec![\n            (1, 0.10),\n            (500, 0.50),\n            (2000, 0.98),\n        ];\n        let val_acc = vec![\n            (1, 0.01),\n            (100, 0.95),\n            (500, 0.98),\n        ];\n\n        let config = SnakeCurveConfig {\n            early_step_max: 1000,\n            train_convergence_threshold: 0.95,\n            val_plateau_min_step: 200,\n            val_plateau_max_acc: 0.05,\n            val_final_threshold: 0.90,\n        };\n\n        let result = verify_snake_curve_shape(&train_acc, &val_acc, &config);\n        // Should fail because val jumps before train converges (not a proper snake curve)\n        // This can fail for multiple reasons, but should definitely fail\n        assert!(result.is_err(), \"Expected snake curve verification to fail but it passed\");\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_passes_on_decreasing_norms() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 85.0),\n            (1000, 72.0),\n            (1500, 63.0),\n            (2000, 58.0),\n            (2500, 55.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let report = verify_weight_norm_decay(&weight_norms, Some(2000), &config)\n            .expect(\"expected weight norm decay verification to pass\");\n\n        assert_eq!(report.initial_norm, 100.0);\n        assert_eq!(report.final_norm, 55.0);\n        assert!((report.relative_decrease - 0.45).abs() < 0.01);\n        assert!(report.grok_step_norm.is_some());\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_fails_on_flat_norms() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 99.0),\n            (1000, 98.5),\n            (1500, 98.0),\n            (2000, 97.5),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let err = verify_weight_norm_decay(&weight_norms, None, &config)\n            .expect_err(\"expected weight norm decay verification to fail\");\n        assert!(err.contains(\"weight norm decrease\"));\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_fails_on_increasing_norms() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 95.0),\n            (1000, 105.0), // Increase here\n            (1500, 90.0),\n            (2000, 85.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let err = verify_weight_norm_decay(&weight_norms, None, &config)\n            .expect_err(\"expected weight norm decay verification to fail\");\n        assert!(err.contains(\"weight norm increased\"));\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_allows_small_noise() {\n        // Weight decay with small numerical noise (within 1% tolerance)\n        let weight_norms = vec![\n            (0, 100.0),\n            (500, 95.0),\n            (1000, 90.5),  // Small increase within tolerance\n            (1500, 90.0),\n            (2000, 85.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let report = verify_weight_norm_decay(&weight_norms, None, &config)\n            .expect(\"expected weight norm decay with small noise to pass\");\n\n        assert_eq!(report.initial_norm, 100.0);\n        assert_eq!(report.final_norm, 85.0);\n        assert!((report.relative_decrease - 0.15).abs() < 0.01);\n    }\n\n    #[test]\n    fn weight_norm_decay_verification_finds_grok_step_norm() {\n        let weight_norms = vec![\n            (0, 100.0),\n            (1000, 80.0),\n            (1800, 68.0),  // Within window of grok step (2200 - 500 = 1700)\n            (2000, 65.0),\n            (2200, 62.0),  // Around grok step\n            (3000, 55.0),\n        ];\n\n        let config = WeightNormDecayConfig {\n            min_relative_decrease: 0.1,\n            plateau_min_step: 1000,\n            grok_step_window: 500,\n        };\n\n        let report = verify_weight_norm_decay(&weight_norms, Some(2200), &config)\n            .expect(\"expected weight norm decay verification to pass\");\n\n        assert!(report.grok_step_norm.is_some());\n        let grok_norm = report.grok_step_norm.unwrap();\n        // Should find the first norm in the window [1700, 2700], which is 68.0 at step 1800\n        assert!((grok_norm - 68.0).abs() < 0.01);\n    }\n}\n","structuredPatch":[{"oldStart":230,"oldLines":6,"newStart":230,"newLines":7,"lines":[" }"," "," #[derive(Debug, Clone)]","+#[allow(dead_code)]"," pub struct WeightNormDecayReport {","     pub initial_norm: f64,","     pub final_norm: f64,"]},{"oldStart":238,"oldLines":6,"newStart":239,"newLines":7,"lines":[" }"," "," #[derive(Debug, Clone)]","+#[allow(dead_code)]"," pub struct SnakeCurveReport {","     pub train_converged_by_step: usize,","     pub val_plateau_max_acc: f32,"]}],"userModified":false,"replaceAll":false}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01RES95LCPL8bwSQLhujxn19","type":"message","role":"assistant","content":[{"type":"text","text":"Now let me check the analysis.rs for struct field warnings:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":1477,"cache_read_input_tokens":81972,"cache_creation":{"ephemeral_5m_input_tokens":1477,"ephemeral_1h_input_tokens":0},"output_tokens":6,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"e7cd038d-53d3-4039-b37d-2d68cde76c0c"}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01RES95LCPL8bwSQLhujxn19","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01C5cWM2BHRTp9uwaETZMYBH","name":"Read","input":{"file_path":"/Users/admin/MechInt/src/analysis.rs","offset":686,"limit":30}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":1477,"cache_read_input_tokens":81972,"cache_creation":{"ephemeral_5m_input_tokens":1477,"ephemeral_1h_input_tokens":0},"output_tokens":6,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"da6294c7-d376-48c6-81f8-6013cb93aeb5"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01C5cWM2BHRTp9uwaETZMYBH","type":"tool_result","content":"   686â†’}\n   687â†’\n   688â†’#[derive(Debug, Clone)]\n   689â†’pub struct FFTDominantFrequencyReport {\n   690â†’    pub modulus_frequency_fraction: f64,\n   691â†’    pub spectrum_entropy: f64,\n   692â†’    pub peak_to_median_ratio: f64,\n   693â†’    pub top_frequencies: Vec<(usize, usize)>,\n   694â†’}\n   695â†’\n   696â†’#[derive(Debug, Clone)]\n   697â†’pub struct EmbeddingClockConfig {\n   698â†’    pub max_radius_std_fraction: f64,\n   699â†’    pub min_angular_coverage: f64,\n   700â†’    pub max_mean_angle_error: f64,\n   701â†’    pub min_order_fraction: f64,\n   702â†’}\n   703â†’\n   704â†’impl EmbeddingClockConfig {\n   705â†’    /// NOTE: Reserved for future integration into analysis pipeline\n   706â†’    #[allow(dead_code)]\n   707â†’    pub fn default_for_modulus(_modulus: usize) -> Self {\n   708â†’        Self {\n   709â†’            max_radius_std_fraction: 0.1,\n   710â†’            min_angular_coverage: 0.9,\n   711â†’            max_mean_angle_error: 0.35,\n   712â†’            min_order_fraction: 0.9,\n   713â†’        }\n   714â†’    }\n   715â†’}\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"e53a5e3f-95c3-449f-815c-55f1585e7aea","tool_use_result":{"type":"text","file":{"filePath":"/Users/admin/MechInt/src/analysis.rs","content":"}\n\n#[derive(Debug, Clone)]\npub struct FFTDominantFrequencyReport {\n    pub modulus_frequency_fraction: f64,\n    pub spectrum_entropy: f64,\n    pub peak_to_median_ratio: f64,\n    pub top_frequencies: Vec<(usize, usize)>,\n}\n\n#[derive(Debug, Clone)]\npub struct EmbeddingClockConfig {\n    pub max_radius_std_fraction: f64,\n    pub min_angular_coverage: f64,\n    pub max_mean_angle_error: f64,\n    pub min_order_fraction: f64,\n}\n\nimpl EmbeddingClockConfig {\n    /// NOTE: Reserved for future integration into analysis pipeline\n    #[allow(dead_code)]\n    pub fn default_for_modulus(_modulus: usize) -> Self {\n        Self {\n            max_radius_std_fraction: 0.1,\n            min_angular_coverage: 0.9,\n            max_mean_angle_error: 0.35,\n            min_order_fraction: 0.9,\n        }\n    }\n}","numLines":30,"startLine":686,"totalLines":1208}}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01SSzPHgDvXcqiMfs53hwhcu","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01E3xusJFVcRYZMuAJy4zUsv","name":"Edit","input":{"replace_all":false,"file_path":"/Users/admin/MechInt/src/analysis.rs","old_string":"#[derive(Debug, Clone)]\npub struct FFTDominantFrequencyReport {\n    pub modulus_frequency_fraction: f64,\n    pub spectrum_entropy: f64,\n    pub peak_to_median_ratio: f64,\n    pub top_frequencies: Vec<(usize, usize)>,\n}\n\n#[derive(Debug, Clone)]\npub struct EmbeddingClockConfig {\n    pub max_radius_std_fraction: f64,\n    pub min_angular_coverage: f64,\n    pub max_mean_angle_error: f64,\n    pub min_order_fraction: f64,\n}\n\nimpl EmbeddingClockConfig {\n    /// NOTE: Reserved for future integration into analysis pipeline\n    #[allow(dead_code)]\n    pub fn default_for_modulus(_modulus: usize) -> Self {\n        Self {\n            max_radius_std_fraction: 0.1,\n            min_angular_coverage: 0.9,\n            max_mean_angle_error: 0.35,\n            min_order_fraction: 0.9,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct EmbeddingClockReport {\n    pub mean_radius: f64,\n    pub radius_std: f64,\n    pub angular_coverage: f64,\n    pub mean_angle_error: f64,\n    pub order_fraction: f64,\n}","new_string":"#[derive(Debug, Clone)]\n#[allow(dead_code)]\npub struct FFTDominantFrequencyReport {\n    pub modulus_frequency_fraction: f64,\n    pub spectrum_entropy: f64,\n    pub peak_to_median_ratio: f64,\n    pub top_frequencies: Vec<(usize, usize)>,\n}\n\n#[derive(Debug, Clone)]\n#[allow(dead_code)]\npub struct EmbeddingClockConfig {\n    pub max_radius_std_fraction: f64,\n    pub min_angular_coverage: f64,\n    pub max_mean_angle_error: f64,\n    pub min_order_fraction: f64,\n}\n\nimpl EmbeddingClockConfig {\n    /// NOTE: Reserved for future integration into analysis pipeline\n    #[allow(dead_code)]\n    pub fn default_for_modulus(_modulus: usize) -> Self {\n        Self {\n            max_radius_std_fraction: 0.1,\n            min_angular_coverage: 0.9,\n            max_mean_angle_error: 0.35,\n            min_order_fraction: 0.9,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\n#[allow(dead_code)]\npub struct EmbeddingClockReport {\n    pub mean_radius: f64,\n    pub radius_std: f64,\n    pub angular_coverage: f64,\n    pub mean_angle_error: f64,\n    pub order_fraction: f64,\n}"}}],"stop_reason":"tool_use","stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":572,"cache_read_input_tokens":83449,"cache_creation":{"ephemeral_5m_input_tokens":572,"ephemeral_1h_input_tokens":0},"output_tokens":836,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"d8800331-fc7f-46bf-ba2b-e91984fd9c8d"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01E3xusJFVcRYZMuAJy4zUsv","type":"tool_result","content":"The file /Users/admin/MechInt/src/analysis.rs has been updated. Here's the result of running `cat -n` on a snippet of the edited file:\n   684â†’        }\n   685â†’    }\n   686â†’}\n   687â†’\n   688â†’#[derive(Debug, Clone)]\n   689â†’#[allow(dead_code)]\n   690â†’pub struct FFTDominantFrequencyReport {\n   691â†’    pub modulus_frequency_fraction: f64,\n   692â†’    pub spectrum_entropy: f64,\n   693â†’    pub peak_to_median_ratio: f64,\n   694â†’    pub top_frequencies: Vec<(usize, usize)>,\n   695â†’}\n   696â†’\n   697â†’#[derive(Debug, Clone)]\n   698â†’#[allow(dead_code)]\n   699â†’pub struct EmbeddingClockConfig {\n   700â†’    pub max_radius_std_fraction: f64,\n   701â†’    pub min_angular_coverage: f64,\n   702â†’    pub max_mean_angle_error: f64,\n   703â†’    pub min_order_fraction: f64,\n   704â†’}\n   705â†’\n   706â†’impl EmbeddingClockConfig {\n   707â†’    /// NOTE: Reserved for future integration into analysis pipeline\n   708â†’    #[allow(dead_code)]\n   709â†’    pub fn default_for_modulus(_modulus: usize) -> Self {\n   710â†’        Self {\n   711â†’            max_radius_std_fraction: 0.1,\n   712â†’            min_angular_coverage: 0.9,\n   713â†’            max_mean_angle_error: 0.35,\n   714â†’            min_order_fraction: 0.9,\n   715â†’        }\n   716â†’    }\n   717â†’}\n   718â†’\n   719â†’#[derive(Debug, Clone)]\n   720â†’#[allow(dead_code)]\n   721â†’pub struct EmbeddingClockReport {\n   722â†’    pub mean_radius: f64,\n   723â†’    pub radius_std: f64,\n   724â†’    pub angular_coverage: f64,\n   725â†’    pub mean_angle_error: f64,\n   726â†’    pub order_fraction: f64,\n   727â†’}\n   728â†’\n   729â†’/// NOTE: Reserved for future integration into analysis pipeline\n   730â†’#[allow(dead_code)]\n   731â†’pub fn project_embeddings_to_fourier_plane("}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"230bae16-e0aa-44be-91e9-07876f95f48a","tool_use_result":{"filePath":"/Users/admin/MechInt/src/analysis.rs","oldString":"#[derive(Debug, Clone)]\npub struct FFTDominantFrequencyReport {\n    pub modulus_frequency_fraction: f64,\n    pub spectrum_entropy: f64,\n    pub peak_to_median_ratio: f64,\n    pub top_frequencies: Vec<(usize, usize)>,\n}\n\n#[derive(Debug, Clone)]\npub struct EmbeddingClockConfig {\n    pub max_radius_std_fraction: f64,\n    pub min_angular_coverage: f64,\n    pub max_mean_angle_error: f64,\n    pub min_order_fraction: f64,\n}\n\nimpl EmbeddingClockConfig {\n    /// NOTE: Reserved for future integration into analysis pipeline\n    #[allow(dead_code)]\n    pub fn default_for_modulus(_modulus: usize) -> Self {\n        Self {\n            max_radius_std_fraction: 0.1,\n            min_angular_coverage: 0.9,\n            max_mean_angle_error: 0.35,\n            min_order_fraction: 0.9,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct EmbeddingClockReport {\n    pub mean_radius: f64,\n    pub radius_std: f64,\n    pub angular_coverage: f64,\n    pub mean_angle_error: f64,\n    pub order_fraction: f64,\n}","newString":"#[derive(Debug, Clone)]\n#[allow(dead_code)]\npub struct FFTDominantFrequencyReport {\n    pub modulus_frequency_fraction: f64,\n    pub spectrum_entropy: f64,\n    pub peak_to_median_ratio: f64,\n    pub top_frequencies: Vec<(usize, usize)>,\n}\n\n#[derive(Debug, Clone)]\n#[allow(dead_code)]\npub struct EmbeddingClockConfig {\n    pub max_radius_std_fraction: f64,\n    pub min_angular_coverage: f64,\n    pub max_mean_angle_error: f64,\n    pub min_order_fraction: f64,\n}\n\nimpl EmbeddingClockConfig {\n    /// NOTE: Reserved for future integration into analysis pipeline\n    #[allow(dead_code)]\n    pub fn default_for_modulus(_modulus: usize) -> Self {\n        Self {\n            max_radius_std_fraction: 0.1,\n            min_angular_coverage: 0.9,\n            max_mean_angle_error: 0.35,\n            min_order_fraction: 0.9,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\n#[allow(dead_code)]\npub struct EmbeddingClockReport {\n    pub mean_radius: f64,\n    pub radius_std: f64,\n    pub angular_coverage: f64,\n    pub mean_angle_error: f64,\n    pub order_fraction: f64,\n}","originalFile":"use burn::{\n    data::dataset::Dataset,\n    nn::loss::CrossEntropyLossConfig,\n    tensor::{backend::Backend, Int, Tensor, TensorData},\n};\nuse rustfft::{num_complex::Complex, FftPlanner};\nuse serde::{Deserialize, Serialize};\nuse std::collections::{HashMap, HashSet};\n\nuse crate::{data::ModularAdditionDataset, model::Transformer};\n\n/// FFT analysis results for embeddings\n#[derive(Debug, Serialize, Deserialize)]\npub struct FFTAnalysis {\n    pub vocab_size: usize,\n    pub embedding_dim: usize,\n    pub dominant_frequencies: Vec<(usize, f64)>, // (token_id, dominant_frequency_index)\n    pub frequency_magnitudes: HashMap<usize, Vec<f64>>, // token_id -> magnitude spectrum\n}\n\n/// Analyze the learned token embeddings using FFT\npub fn analyze_embeddings_fft<B: Backend>(model: &Transformer<B>) -> FFTAnalysis {\n    println!(\"ðŸ” Performing FFT analysis on learned embeddings...\");\n\n    let vocab_size = ModularAdditionDataset::vocab_size();\n    let embedding_dim = 128;\n\n    // Extract token embeddings by forward passing through the embedding layer\n    let mut frequency_magnitudes = HashMap::new();\n    let mut dominant_frequencies = Vec::new();\n\n    for token_id in 0..vocab_size {\n        // Get the embedding for this token\n        let embedding = get_token_embedding(model, token_id);\n\n        // Compute FFT on the embedding vector\n        let fft_result = compute_fft(&embedding);\n\n        // Find dominant frequency\n        let dominant_freq_idx = find_dominant_frequency(&fft_result);\n        dominant_frequencies.push((token_id, dominant_freq_idx as f64));\n\n        // Store magnitude spectrum\n        frequency_magnitudes.insert(token_id, fft_result);\n\n        if token_id % 10 == 0 {\n            println!(\"  Token {:2}: dominant frequency = {}\", token_id, dominant_freq_idx);\n        }\n    }\n\n    println!(\"âœ… FFT analysis complete!\");\n    println!();\n    println!(\"ðŸ“Š Dominant Frequency Analysis:\");\n\n    // Analyze patterns\n    let mut freq_histogram: HashMap<usize, usize> = HashMap::new();\n    for (_, freq) in &dominant_frequencies {\n        *freq_histogram.entry(*freq as usize).or_insert(0) += 1;\n    }\n\n    let mut freq_counts: Vec<_> = freq_histogram.iter().collect();\n    freq_counts.sort_by(|a, b| b.1.cmp(a.1));\n\n    println!(\"  Most common dominant frequencies:\");\n    for (freq, count) in freq_counts.iter().take(5) {\n        println!(\"    Frequency {}: {} tokens\", freq, count);\n    }\n\n    // Check for modular structure (expected for mod p)\n    println!();\n    println!(\"ðŸ”¬ Checking for modular structure:\");\n    let modulus = ModularAdditionDataset::modulus();\n    if freq_histogram.contains_key(&modulus) {\n        println!(\n            \"  âœ“ Frequency {} (matching modulus) appears in embeddings\",\n            modulus\n        );\n    }\n\n    FFTAnalysis {\n        vocab_size,\n        embedding_dim,\n        dominant_frequencies,\n        frequency_magnitudes,\n    }\n}\n\n/// Extract the token embedding for a single token\nfn get_token_embedding<B: Backend>(model: &Transformer<B>, token_id: usize) -> Vec<f64> {\n    // Get the token embedding directly from the model\n    let embedding_tensor = model.get_token_embedding(token_id);\n\n    // Convert to vector - shape is [1, 1, embedding_dim], we want [embedding_dim]\n    let embedding_data: Vec<f32> = embedding_tensor.into_data().to_vec().unwrap();\n\n    // Convert to f64\n    embedding_data.iter().map(|&x| x as f64).collect()\n}\n\n/// Compute FFT on a vector of real values\nfn compute_fft(data: &[f64]) -> Vec<f64> {\n    let mut planner = FftPlanner::new();\n    let fft = planner.plan_fft_forward(data.len());\n\n    // Convert to complex numbers\n    let mut buffer: Vec<Complex<f64>> = data\n        .iter()\n        .map(|&x| Complex::new(x, 0.0))\n        .collect();\n\n    // Perform FFT\n    fft.process(&mut buffer);\n\n    // Compute magnitudes\n    buffer.iter().map(|c| c.norm()).collect()\n}\n\nfn inverse_fft(data: &mut [Complex<f64>]) {\n    let mut planner = FftPlanner::new();\n    let fft = planner.plan_fft_inverse(data.len());\n    fft.process(data);\n}\n\nfn top_k_frequency_indices(magnitudes: &[f64], k: usize) -> Vec<usize> {\n    let mut indexed: Vec<(usize, f64)> = magnitudes.iter().copied().enumerate().collect();\n    indexed.sort_by(|(idx_a, mag_a), (idx_b, mag_b)| {\n        mag_b\n            .partial_cmp(mag_a)\n            .unwrap_or(std::cmp::Ordering::Equal)\n            .then_with(|| idx_a.cmp(idx_b))\n    });\n\n    let keep = k.min(indexed.len());\n    indexed.iter().take(keep).map(|(idx, _)| *idx).collect()\n}\n\nfn restrict_signal_to_top_k_frequencies(signal: &[f64], top_k: usize) -> Vec<f64> {\n    if signal.is_empty() {\n        return Vec::new();\n    }\n\n    if top_k == 0 {\n        return vec![0.0; signal.len()];\n    }\n\n    let mut planner = FftPlanner::new();\n    let fft = planner.plan_fft_forward(signal.len());\n\n    let mut buffer: Vec<Complex<f64>> = signal\n        .iter()\n        .map(|&x| Complex::new(x, 0.0))\n        .collect();\n    fft.process(&mut buffer);\n\n    let magnitudes: Vec<f64> = buffer.iter().map(|c| c.norm()).collect();\n    let keep = top_k_frequency_indices(&magnitudes, top_k);\n    let keep_set: HashSet<usize> = keep.into_iter().collect();\n\n    for (idx, value) in buffer.iter_mut().enumerate() {\n        if !keep_set.contains(&idx) {\n            *value = Complex::new(0.0, 0.0);\n        }\n    }\n\n    inverse_fft(&mut buffer);\n\n    let scale = 1.0 / signal.len() as f64;\n    buffer.iter().map(|c| c.re * scale).collect()\n}\n\nfn exclude_top_k_frequencies(signal: &[f64], top_k: usize) -> Vec<f64> {\n    if signal.is_empty() {\n        return Vec::new();\n    }\n\n    if top_k == 0 {\n        return signal.to_vec();\n    }\n\n    if top_k >= signal.len() {\n        return vec![0.0; signal.len()];\n    }\n\n    let mut planner = FftPlanner::new();\n    let fft = planner.plan_fft_forward(signal.len());\n\n    let mut buffer: Vec<Complex<f64>> = signal\n        .iter()\n        .map(|&x| Complex::new(x, 0.0))\n        .collect();\n    fft.process(&mut buffer);\n\n    let magnitudes: Vec<f64> = buffer.iter().map(|c| c.norm()).collect();\n    let remove = top_k_frequency_indices(&magnitudes, top_k);\n    let remove_set: HashSet<usize> = remove.into_iter().collect();\n\n    for (idx, value) in buffer.iter_mut().enumerate() {\n        if remove_set.contains(&idx) {\n            *value = Complex::new(0.0, 0.0);\n        }\n    }\n\n    inverse_fft(&mut buffer);\n\n    let scale = 1.0 / signal.len() as f64;\n    buffer.iter().map(|c| c.re * scale).collect()\n}\n\nfn restricted_token_embedding_weights<B: Backend>(\n    model: &Transformer<B>,\n    top_k: usize,\n) -> Vec<f32> {\n    let weight = model.token_embedding_weights();\n    let [vocab_size, d_model] = weight.dims();\n    let weight_data: Vec<f32> = weight.into_data().to_vec().unwrap();\n    let mut restricted = vec![0.0f32; weight_data.len()];\n\n    for dim in 0..d_model {\n        let mut column = Vec::with_capacity(vocab_size);\n        for token in 0..vocab_size {\n            column.push(weight_data[token * d_model + dim] as f64);\n        }\n        let filtered = restrict_signal_to_top_k_frequencies(&column, top_k);\n        for token in 0..vocab_size {\n            restricted[token * d_model + dim] = filtered[token] as f32;\n        }\n    }\n\n    restricted\n}\n\nfn excluded_token_embedding_weights<B: Backend>(\n    model: &Transformer<B>,\n    top_k: usize,\n) -> Vec<f32> {\n    let weight = model.token_embedding_weights();\n    let [vocab_size, d_model] = weight.dims();\n    let weight_data: Vec<f32> = weight.into_data().to_vec().unwrap();\n    let mut excluded = vec![0.0f32; weight_data.len()];\n\n    for dim in 0..d_model {\n        let mut column = Vec::with_capacity(vocab_size);\n        for token in 0..vocab_size {\n            column.push(weight_data[token * d_model + dim] as f64);\n        }\n        let filtered = exclude_top_k_frequencies(&column, top_k);\n        for token in 0..vocab_size {\n            excluded[token * d_model + dim] = filtered[token] as f32;\n        }\n    }\n\n    excluded\n}\n\npub fn compute_restricted_loss<B: Backend>(\n    model: &Transformer<B>,\n    dataset: &ModularAdditionDataset,\n    device: &B::Device,\n    top_k: usize,\n    batch_size: usize,\n) -> Result<f64, String> {\n    if batch_size == 0 {\n        return Err(\"batch_size must be greater than 0\".to_string());\n    }\n\n    let restricted_weights = restricted_token_embedding_weights(model, top_k);\n    let [vocab_size, d_model] = model.token_embedding_weights().dims();\n    let token_weights = Tensor::<B, 2>::from_data(\n        TensorData::new(restricted_weights, [vocab_size, d_model]),\n        device,\n    );\n\n    let loss_fn = CrossEntropyLossConfig::new().init(device);\n    let mut total_loss = 0.0f64;\n    let mut total_batches = 0usize;\n\n    for batch_start in (0..dataset.len()).step_by(batch_size) {\n        let batch_end = (batch_start + batch_size).min(dataset.len());\n        let batch_len = batch_end - batch_start;\n        if batch_len == 0 {\n            break;\n        }\n\n        let mut inputs_vec = Vec::with_capacity(batch_len * 3);\n        let mut targets_vec = Vec::with_capacity(batch_len);\n\n        for idx in batch_start..batch_end {\n            let (input, target) = dataset.get(idx).ok_or_else(|| {\n                format!(\"missing dataset sample at index {}\", idx)\n            })?;\n            inputs_vec.extend(input.iter().map(|value| *value as i32));\n            targets_vec.push(target as i32);\n        }\n\n        let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n            .reshape([batch_len, 3]);\n        let targets = Tensor::<B, 1, Int>::from_ints(targets_vec.as_slice(), device);\n\n        let logits = model.forward_with_token_weights(inputs, token_weights.clone());\n        let loss = loss_fn.forward(logits, targets);\n        let loss_value: f64 = loss.into_data().to_vec().unwrap().get(0).copied().unwrap_or(0.0);\n        total_loss += loss_value;\n        total_batches += 1;\n    }\n\n    if total_batches == 0 {\n        return Err(\"no batches computed for restricted loss\".to_string());\n    }\n\n    Ok(total_loss / total_batches as f64)\n}\n\npub fn compute_excluded_loss<B: Backend>(\n    model: &Transformer<B>,\n    dataset: &ModularAdditionDataset,\n    device: &B::Device,\n    top_k: usize,\n    batch_size: usize,\n) -> Result<f64, String> {\n    if batch_size == 0 {\n        return Err(\"batch_size must be greater than 0\".to_string());\n    }\n\n    let excluded_weights = excluded_token_embedding_weights(model, top_k);\n    let [vocab_size, d_model] = model.token_embedding_weights().dims();\n    let token_weights = Tensor::<B, 2>::from_data(\n        TensorData::new(excluded_weights, [vocab_size, d_model]),\n        device,\n    );\n\n    let loss_fn = CrossEntropyLossConfig::new().init(device);\n    let mut total_loss = 0.0f64;\n    let mut total_batches = 0usize;\n\n    for batch_start in (0..dataset.len()).step_by(batch_size) {\n        let batch_end = (batch_start + batch_size).min(dataset.len());\n        let batch_len = batch_end - batch_start;\n        if batch_len == 0 {\n            break;\n        }\n\n        let mut inputs_vec = Vec::with_capacity(batch_len * 3);\n        let mut targets_vec = Vec::with_capacity(batch_len);\n\n        for idx in batch_start..batch_end {\n            let (input, target) = dataset.get(idx).ok_or_else(|| {\n                format!(\"missing dataset sample at index {}\", idx)\n            })?;\n            inputs_vec.extend(input.iter().map(|value| *value as i32));\n            targets_vec.push(target as i32);\n        }\n\n        let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n            .reshape([batch_len, 3]);\n        let targets = Tensor::<B, 1, Int>::from_ints(targets_vec.as_slice(), device);\n\n        let logits = model.forward_with_token_weights(inputs, token_weights.clone());\n        let loss = loss_fn.forward(logits, targets);\n        let loss_value: f64 = loss.into_data().to_vec().unwrap().get(0).copied().unwrap_or(0.0);\n        total_loss += loss_value;\n        total_batches += 1;\n    }\n\n    if total_batches == 0 {\n        return Err(\"no batches computed for excluded loss\".to_string());\n    }\n\n    Ok(total_loss / total_batches as f64)\n}\n\n/// Find the index of the dominant frequency (excluding DC component at index 0)\nfn find_dominant_frequency(magnitudes: &[f64]) -> usize {\n    magnitudes\n        .iter()\n        .enumerate()\n        .skip(1) // Skip DC component\n        .max_by(|(_, a), (_, b)| {\n            a.partial_cmp(b).unwrap_or(std::cmp::Ordering::Equal)\n        })\n        .map(|(idx, _)| idx)\n        .unwrap_or(0)\n}\n\n/// Save FFT analysis to JSON file\npub fn save_fft_analysis(analysis: &FFTAnalysis, filename: &str) -> std::io::Result<()> {\n    let json = serde_json::to_string_pretty(analysis)?;\n    std::fs::write(filename, json)?;\n    println!(\"ðŸ’¾ FFT analysis saved to: {}\", filename);\n    Ok(())\n}\n\n/// Track training and validation loss over time\n#[derive(Debug, Serialize, Deserialize)]\npub struct LossHistory {\n    pub train_snapshots: Vec<(usize, f64)>, // (step, loss)\n    pub val_snapshots: Vec<(usize, f64)>,   // (step, loss)\n}\n\n/// Track restricted loss over time (only top-k frequencies kept)\n#[derive(Debug, Serialize, Deserialize)]\npub struct RestrictedLossHistory {\n    pub snapshots: Vec<(usize, f64)>, // (step, loss)\n}\n\nimpl RestrictedLossHistory {\n    pub fn new() -> Self {\n        Self {\n            snapshots: Vec::new(),\n        }\n    }\n\n    pub fn add_snapshot(&mut self, step: usize, loss: f64) {\n        self.snapshots.push((step, loss));\n    }\n\n    pub fn save(&self, filename: &str) -> std::io::Result<()> {\n        let json = serde_json::to_string_pretty(self)?;\n        std::fs::write(filename, json)?;\n        println!(\"ðŸ’¾ Restricted loss history saved to: {}\", filename);\n        Ok(())\n    }\n}\n\n/// Track excluded loss over time (top-k frequencies removed)\n#[derive(Debug, Serialize, Deserialize)]\npub struct ExcludedLossHistory {\n    pub snapshots: Vec<(usize, f64)>, // (step, loss)\n}\n\nimpl ExcludedLossHistory {\n    pub fn new() -> Self {\n        Self {\n            snapshots: Vec::new(),\n        }\n    }\n\n    pub fn add_snapshot(&mut self, step: usize, loss: f64) {\n        self.snapshots.push((step, loss));\n    }\n\n    pub fn save(&self, filename: &str) -> std::io::Result<()> {\n        let json = serde_json::to_string_pretty(self)?;\n        std::fs::write(filename, json)?;\n        println!(\"ðŸ’¾ Excluded loss history saved to: {}\", filename);\n        Ok(())\n    }\n}\n\nimpl LossHistory {\n    pub fn new() -> Self {\n        Self {\n            train_snapshots: Vec::new(),\n            val_snapshots: Vec::new(),\n        }\n    }\n\n    pub fn add_snapshot(&mut self, step: usize, train_loss: f64, val_loss: f64) {\n        self.train_snapshots.push((step, train_loss));\n        self.val_snapshots.push((step, val_loss));\n    }\n\n    pub fn save(&self, filename: &str) -> std::io::Result<()> {\n        let json = serde_json::to_string_pretty(self)?;\n        std::fs::write(filename, json)?;\n        println!(\"ðŸ’¾ Loss history saved to: {}\", filename);\n        Ok(())\n    }\n}\n\n/// Legacy: Single loss tracking (kept for backward compatibility)\n#[derive(Debug, Serialize, Deserialize, Clone)]\npub struct WeightNormSnapshot {\n    pub step: usize,\n    pub total_norm: f64,\n}\n\n/// Legacy: Track weight norms over training (actually stores loss)\n#[derive(Debug, Serialize, Deserialize)]\npub struct WeightNormHistory {\n    pub snapshots: Vec<WeightNormSnapshot>,\n}\n\nimpl WeightNormHistory {\n    pub fn new() -> Self {\n        Self {\n            snapshots: Vec::new(),\n        }\n    }\n\n    pub fn add_snapshot(&mut self, step: usize, norm: f64) {\n        self.snapshots.push(WeightNormSnapshot {\n            step,\n            total_norm: norm,\n        });\n    }\n\n    pub fn save(&self, filename: &str) -> std::io::Result<()> {\n        let json = serde_json::to_string_pretty(self)?;\n        std::fs::write(filename, json)?;\n        println!(\"ðŸ’¾ Weight norm history saved to: {}\", filename);\n        Ok(())\n    }\n}\n\n/// Compute the L2 norm of all model parameters\n/// This returns the Frobenius norm (sum of squared weights) across all parameters\n/// Note: This is a simplified implementation that computes the norm of token embeddings,\n/// which contain the majority of the model parameters. For a complete implementation,\n/// we would need to access all Linear layer weights via the Record trait.\npub fn compute_model_weight_norm<B: Backend>(model: &Transformer<B>) -> f64 {\n    let mut total_norm_squared = 0.0f64;\n\n    // Token embedding weights: [vocab_size, d_model]\n    let token_emb_weights = model.token_embedding_weights();\n    let token_emb_data: Vec<f32> = token_emb_weights.into_data().to_vec().unwrap();\n    for &weight in &token_emb_data {\n        total_norm_squared += (weight as f64) * (weight as f64);\n    }\n\n    // Note: Position embeddings, attention weights, MLP weights, and LM head weights\n    // are not included in this simplified implementation. For weight decay verification,\n    // tracking the token embeddings (which are the largest weight matrices) is sufficient\n    // to observe the monotonic decrease under weight decay.\n\n    total_norm_squared.sqrt()\n}\n\n/// Extract all token embeddings as a matrix [vocab_size, embedding_dim]\npub fn extract_all_embeddings<B: Backend>(model: &Transformer<B>) -> Vec<Vec<f64>> {\n    let vocab_size = ModularAdditionDataset::vocab_size();\n    let mut embeddings = Vec::with_capacity(vocab_size);\n\n    for token_id in 0..vocab_size {\n        embeddings.push(get_token_embedding(model, token_id));\n    }\n\n    embeddings\n}\n\n/// Verify that FFT analysis of embeddings shows dominant frequencies\n/// that align with the modulus structure and nontrivial spectral patterns\n///\n/// NOTE: Reserved for manual verification or future integration into analysis pipeline\n#[allow(dead_code)]\npub fn verify_fft_dominant_frequencies<B: Backend>(\n    model: &Transformer<B>,\n    config: &FFTDominantFrequencyConfig,\n) -> Result<FFTDominantFrequencyReport, String> {\n    let embeddings = extract_all_embeddings(model);\n    verify_fft_dominant_frequencies_from_embeddings(&embeddings, config)\n}\n\n/// Verify FFT dominant frequencies from pre-extracted embeddings\n///\n/// NOTE: Reserved for manual verification or future integration into analysis pipeline\n#[allow(dead_code)]\npub fn verify_fft_dominant_frequencies_from_embeddings(\n    embeddings: &[Vec<f64>],\n    config: &FFTDominantFrequencyConfig,\n) -> Result<FFTDominantFrequencyReport, String> {\n    if embeddings.is_empty() {\n        return Err(\"no embeddings provided\".to_string());\n    }\n\n    let vocab_size = embeddings.len();\n    if vocab_size < config.modulus {\n        return Err(format!(\n            \"embedding count {} less than modulus {}\",\n            vocab_size, config.modulus\n        ));\n    }\n\n    // Analyze each embedding dimension and collect dominant frequencies\n    let mut frequency_histogram: HashMap<usize, usize> = HashMap::new();\n    let embedding_dim = embeddings[0].len();\n\n    // For each embedding dimension, compute FFT across all tokens and find dominant frequency\n    for dim in 0..embedding_dim {\n        let mut signal = Vec::with_capacity(vocab_size);\n        for embedding in embeddings.iter().take(vocab_size) {\n            if dim >= embedding.len() {\n                return Err(format!(\n                    \"embedding dimension mismatch at dim {}\",\n                    dim\n                ));\n            }\n            signal.push(embedding[dim]);\n        }\n\n        // Compute FFT and find dominant frequency\n        let magnitudes = compute_fft(&signal);\n        let dominant_freq = find_dominant_frequency(&magnitudes);\n        *frequency_histogram.entry(dominant_freq).or_insert(0) += 1;\n    }\n\n    // Check 1: Modulus frequency should appear significantly\n    let modulus_count = frequency_histogram.get(&config.modulus).copied().unwrap_or(0);\n    let modulus_frequency_fraction = modulus_count as f64 / embedding_dim as f64;\n\n    if modulus_frequency_fraction < config.min_modulus_frequency_fraction {\n        return Err(format!(\n            \"modulus frequency {} appears in only {:.3} of dimensions (threshold {:.3})\",\n            config.modulus, modulus_frequency_fraction, config.min_modulus_frequency_fraction\n        ));\n    }\n\n    // Check 2: Spectrum should NOT resemble white noise (low entropy)\n    // High entropy = uniform distribution (white noise), low entropy = structured peaks\n    let total_dims = embedding_dim as f64;\n    let mut entropy = 0.0;\n    for &count in frequency_histogram.values() {\n        if count > 0 {\n            let p = count as f64 / total_dims;\n            entropy -= p * p.log2();\n        }\n    }\n    // Normalize entropy by max possible (log2 of unique frequency count)\n    let max_entropy = (frequency_histogram.len() as f64).log2();\n    let normalized_entropy = if max_entropy > 0.0 {\n        entropy / max_entropy\n    } else {\n        0.0\n    };\n\n    if normalized_entropy > config.max_spectrum_entropy {\n        return Err(format!(\n            \"spectrum entropy {:.3} too high (white noise threshold {:.3})\",\n            normalized_entropy, config.max_spectrum_entropy\n        ));\n    }\n\n    // Check 3: Peak frequencies should be significantly stronger than median\n    let mut counts: Vec<usize> = frequency_histogram.values().copied().collect();\n    counts.sort_unstable();\n    let median_count = if counts.is_empty() {\n        0\n    } else {\n        counts[counts.len() / 2]\n    };\n    let peak_count = counts.last().copied().unwrap_or(0);\n    let peak_to_median_ratio = if median_count > 0 {\n        peak_count as f64 / median_count as f64\n    } else {\n        f64::INFINITY\n    };\n\n    if peak_to_median_ratio < config.min_peak_to_median_ratio {\n        return Err(format!(\n            \"peak to median ratio {:.3} below threshold {:.3}\",\n            peak_to_median_ratio, config.min_peak_to_median_ratio\n        ));\n    }\n\n    // Collect top frequencies for the report\n    let mut freq_counts: Vec<(usize, usize)> = frequency_histogram.into_iter().collect();\n    freq_counts.sort_by(|a, b| b.1.cmp(&a.1));\n    let top_frequencies: Vec<(usize, usize)> = freq_counts.into_iter().take(5).collect();\n\n    Ok(FFTDominantFrequencyReport {\n        modulus_frequency_fraction,\n        spectrum_entropy: normalized_entropy,\n        peak_to_median_ratio,\n        top_frequencies,\n    })\n}\n\n#[derive(Debug, Clone)]\npub struct FFTDominantFrequencyConfig {\n    pub modulus: usize,\n    pub min_modulus_frequency_fraction: f64,\n    pub max_spectrum_entropy: f64,\n    pub min_peak_to_median_ratio: f64,\n}\n\nimpl FFTDominantFrequencyConfig {\n    /// NOTE: Reserved for future integration into analysis pipeline\n    #[allow(dead_code)]\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        Self {\n            modulus,\n            min_modulus_frequency_fraction: 0.15,\n            max_spectrum_entropy: 0.85,\n            min_peak_to_median_ratio: 2.0,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct FFTDominantFrequencyReport {\n    pub modulus_frequency_fraction: f64,\n    pub spectrum_entropy: f64,\n    pub peak_to_median_ratio: f64,\n    pub top_frequencies: Vec<(usize, usize)>,\n}\n\n#[derive(Debug, Clone)]\npub struct EmbeddingClockConfig {\n    pub max_radius_std_fraction: f64,\n    pub min_angular_coverage: f64,\n    pub max_mean_angle_error: f64,\n    pub min_order_fraction: f64,\n}\n\nimpl EmbeddingClockConfig {\n    /// NOTE: Reserved for future integration into analysis pipeline\n    #[allow(dead_code)]\n    pub fn default_for_modulus(_modulus: usize) -> Self {\n        Self {\n            max_radius_std_fraction: 0.1,\n            min_angular_coverage: 0.9,\n            max_mean_angle_error: 0.35,\n            min_order_fraction: 0.9,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\npub struct EmbeddingClockReport {\n    pub mean_radius: f64,\n    pub radius_std: f64,\n    pub angular_coverage: f64,\n    pub mean_angle_error: f64,\n    pub order_fraction: f64,\n}\n\n/// NOTE: Reserved for future integration into analysis pipeline\n#[allow(dead_code)]\npub fn project_embeddings_to_fourier_plane(\n    embeddings: &[Vec<f64>],\n    modulus: usize,\n) -> Result<Vec<(f64, f64)>, String> {\n    let p = modulus;\n    if embeddings.len() < p {\n        return Err(\"embedding matrix smaller than modulus\".to_string());\n    }\n    if p == 0 {\n        return Err(\"modulus must be positive\".to_string());\n    }\n    let d = embeddings[0].len();\n    if d == 0 {\n        return Err(\"embedding dimension must be positive\".to_string());\n    }\n    if !embeddings[..p].iter().all(|row| row.len() == d) {\n        return Err(\"embedding rows have inconsistent dimension\".to_string());\n    }\n\n    let tau = std::f64::consts::PI * 2.0;\n    let mut basis_cos = vec![0.0f64; d];\n    let mut basis_sin = vec![0.0f64; d];\n    let scale = 2.0 / p as f64;\n\n    for (token, embedding) in embeddings[..p].iter().enumerate() {\n        let angle = tau * token as f64 / p as f64;\n        let cos_val = angle.cos();\n        let sin_val = angle.sin();\n        for dim in 0..d {\n            basis_cos[dim] += cos_val * embedding[dim] * scale;\n            basis_sin[dim] += sin_val * embedding[dim] * scale;\n        }\n    }\n\n    let basis_cos = normalize_vector(&basis_cos)?;\n    let basis_sin = normalize_vector(&basis_sin)?;\n\n    let mut coords = Vec::with_capacity(p);\n    for embedding in embeddings[..p].iter() {\n        let x = dot(embedding, &basis_cos);\n        let y = dot(embedding, &basis_sin);\n        coords.push((x, y));\n    }\n\n    Ok(coords)\n}\n\n/// NOTE: Reserved for future integration into analysis pipeline\n#[allow(dead_code)]\npub fn verify_embedding_clock_geometry(\n    embeddings: &[Vec<f64>],\n    modulus: usize,\n    config: &EmbeddingClockConfig,\n) -> Result<EmbeddingClockReport, String> {\n    let coords = project_embeddings_to_fourier_plane(embeddings, modulus)?;\n    let p = coords.len();\n    if p == 0 {\n        return Err(\"no embedding coordinates computed\".to_string());\n    }\n\n    let mut radii = Vec::with_capacity(p);\n    let mut angles = Vec::with_capacity(p);\n    for (x, y) in &coords {\n        let radius = (x * x + y * y).sqrt();\n        radii.push(radius);\n        angles.push(y.atan2(*x));\n    }\n\n    let mean_radius = radii.iter().sum::<f64>() / p as f64;\n    if mean_radius == 0.0 {\n        return Err(\"mean radius is zero\".to_string());\n    }\n    let radius_var = radii\n        .iter()\n        .map(|r| (r - mean_radius) * (r - mean_radius))\n        .sum::<f64>()\n        / p as f64;\n    let radius_std = radius_var.sqrt();\n\n    if radius_std / mean_radius > config.max_radius_std_fraction {\n        return Err(format!(\n            \"radius variance too large (std {:.4}, mean {:.4})\",\n            radius_std, mean_radius\n        ));\n    }\n\n    let tau = std::f64::consts::PI * 2.0;\n    let (mut sum_cos, mut sum_sin) = (0.0f64, 0.0f64);\n    for (token, angle) in angles.iter().enumerate() {\n        let expected = tau * token as f64 / p as f64;\n        let delta = angle - expected;\n        sum_cos += delta.cos();\n        sum_sin += delta.sin();\n    }\n    let offset = sum_sin.atan2(sum_cos);\n\n    let mut adjusted_angles = Vec::with_capacity(p);\n    let mut angle_errors = Vec::with_capacity(p);\n    for (token, angle) in angles.iter().enumerate() {\n        let expected = tau * token as f64 / p as f64;\n        let mut adjusted = angle - offset;\n        adjusted = wrap_angle(adjusted);\n        adjusted_angles.push(adjusted);\n        let diff = wrap_angle_signed(adjusted - expected);\n        angle_errors.push(diff.abs());\n    }\n\n    let mean_angle_error = angle_errors.iter().sum::<f64>() / p as f64;\n    if mean_angle_error > config.max_mean_angle_error {\n        return Err(format!(\n            \"mean angular error too large ({:.4} rad)\",\n            mean_angle_error\n        ));\n    }\n\n    let angular_coverage = angular_coverage(&adjusted_angles);\n    if angular_coverage < config.min_angular_coverage {\n        return Err(format!(\n            \"angular coverage too small ({:.4})\",\n            angular_coverage\n        ));\n    }\n\n    let order_fraction = order_fraction(&adjusted_angles, p);\n    if order_fraction < config.min_order_fraction {\n        return Err(format!(\n            \"token ordering too weak ({:.4})\",\n            order_fraction\n        ));\n    }\n\n    Ok(EmbeddingClockReport {\n        mean_radius,\n        radius_std,\n        angular_coverage,\n        mean_angle_error,\n        order_fraction,\n    })\n}\n\n// Helper functions for embedding clock geometry verification\n#[allow(dead_code)]\nfn normalize_vector(vec: &[f64]) -> Result<Vec<f64>, String> {\n    let norm = vec.iter().map(|v| v * v).sum::<f64>().sqrt();\n    if norm == 0.0 {\n        return Err(\"basis vector norm is zero\".to_string());\n    }\n    Ok(vec.iter().map(|v| v / norm).collect())\n}\n\n#[allow(dead_code)]\nfn dot(a: &[f64], b: &[f64]) -> f64 {\n    a.iter().zip(b.iter()).map(|(x, y)| x * y).sum()\n}\n\n#[allow(dead_code)]\nfn wrap_angle(angle: f64) -> f64 {\n    let tau = std::f64::consts::PI * 2.0;\n    let mut wrapped = angle % tau;\n    if wrapped < 0.0 {\n        wrapped += tau;\n    }\n    wrapped\n}\n\n#[allow(dead_code)]\nfn wrap_angle_signed(angle: f64) -> f64 {\n    let tau = std::f64::consts::PI * 2.0;\n    let mut wrapped = (angle + std::f64::consts::PI) % tau;\n    if wrapped < 0.0 {\n        wrapped += tau;\n    }\n    wrapped - std::f64::consts::PI\n}\n\n#[allow(dead_code)]\nfn angular_coverage(angles: &[f64]) -> f64 {\n    if angles.is_empty() {\n        return 0.0;\n    }\n    let tau = std::f64::consts::PI * 2.0;\n    let mut sorted = angles.to_vec();\n    sorted.sort_by(|a, b| a.partial_cmp(b).unwrap_or(std::cmp::Ordering::Equal));\n\n    let mut max_gap = 0.0;\n    for pair in sorted.windows(2) {\n        let gap = pair[1] - pair[0];\n        if gap > max_gap {\n            max_gap = gap;\n        }\n    }\n    let wrap_gap = tau - (sorted[sorted.len() - 1] - sorted[0]);\n    if wrap_gap > max_gap {\n        max_gap = wrap_gap;\n    }\n\n    (tau - max_gap) / tau\n}\n\n#[allow(dead_code)]\nfn order_fraction(angles: &[f64], modulus: usize) -> f64 {\n    if angles.len() < 2 || modulus == 0 {\n        return 0.0;\n    }\n    let mut indexed: Vec<(usize, f64)> = angles.iter().copied().enumerate().collect();\n    indexed.sort_by(|a, b| a.1.partial_cmp(&b.1).unwrap_or(std::cmp::Ordering::Equal));\n\n    let mut good = 0usize;\n    for window in indexed.windows(2) {\n        let current = window[0].0;\n        let next = window[1].0;\n        if (next + modulus - current) % modulus == 1 {\n            good += 1;\n        }\n    }\n    let last = indexed[indexed.len() - 1].0;\n    let first = indexed[0].0;\n    if (first + modulus - last) % modulus == 1 {\n        good += 1;\n    }\n\n    good as f64 / modulus as f64\n}\n\n/// Track training and validation accuracy over time\n#[derive(Debug, Serialize, Deserialize)]\npub struct AccuracyHistory {\n    pub train_snapshots: Vec<(usize, f32)>, // (step, accuracy)\n    pub val_snapshots: Vec<(usize, f32)>,   // (step, accuracy)\n}\n\nimpl AccuracyHistory {\n    pub fn new() -> Self {\n        Self {\n            train_snapshots: Vec::new(),\n            val_snapshots: Vec::new(),\n        }\n    }\n\n    /// NOTE: Currently unused - accuracy tracked directly in main loop\n    #[allow(dead_code)]\n    pub fn add_snapshot(&mut self, step: usize, train_acc: f32, val_acc: f32) {\n        self.train_snapshots.push((step, train_acc));\n        self.val_snapshots.push((step, val_acc));\n    }\n\n    pub fn save(&self, filename: &str) -> std::io::Result<()> {\n        let json = serde_json::to_string_pretty(self)?;\n        std::fs::write(filename, json)?;\n        println!(\"ðŸ’¾ Accuracy history saved to: {}\", filename);\n        Ok(())\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    fn make_circle_embeddings(modulus: usize) -> Vec<Vec<f64>> {\n        let tau = std::f64::consts::PI * 2.0;\n        (0..modulus)\n            .map(|token| {\n                let angle = tau * token as f64 / modulus as f64;\n                vec![angle.cos(), angle.sin(), 0.0, 0.0]\n            })\n            .collect()\n    }\n\n    #[test]\n    fn top_k_frequency_indices_stable_with_ties() {\n        let magnitudes = vec![0.1, 1.0, 1.0, 0.5];\n        let first = top_k_frequency_indices(&magnitudes, 2);\n        let second = top_k_frequency_indices(&magnitudes, 2);\n        assert_eq!(first, vec![1, 2]);\n        assert_eq!(first, second);\n    }\n\n    #[test]\n    fn restrict_signal_zeroes_when_top_k_zero() {\n        let signal = vec![1.0, -1.0, 0.5, -0.5];\n        let filtered = restrict_signal_to_top_k_frequencies(&signal, 0);\n        assert_eq!(filtered, vec![0.0, 0.0, 0.0, 0.0]);\n    }\n\n    #[test]\n    fn exclude_signal_no_change_when_top_k_zero() {\n        let signal = vec![1.0, -1.0, 0.5, -0.5];\n        let filtered = exclude_top_k_frequencies(&signal, 0);\n        assert_eq!(filtered, signal);\n    }\n\n    #[test]\n    fn exclude_signal_zeroes_when_top_k_all() {\n        let signal = vec![1.0, -1.0, 0.5, -0.5];\n        let filtered = exclude_top_k_frequencies(&signal, signal.len());\n        assert_eq!(filtered, vec![0.0, 0.0, 0.0, 0.0]);\n    }\n\n    #[test]\n    fn embedding_clock_geometry_passes_on_clean_circle() {\n        let modulus = 13;\n        let embeddings = make_circle_embeddings(modulus);\n        let config = EmbeddingClockConfig::default_for_modulus(modulus);\n        let report =\n            verify_embedding_clock_geometry(&embeddings, modulus, &config).expect(\"expected pass\");\n        assert!(report.angular_coverage > 0.9);\n        assert!(report.order_fraction > 0.9);\n        assert!(report.mean_angle_error < 0.1);\n    }\n\n    #[test]\n    fn embedding_clock_geometry_flags_wrong_frequency() {\n        let modulus = 11;\n        let tau = std::f64::consts::PI * 2.0;\n        let embeddings: Vec<Vec<f64>> = (0..modulus)\n            .map(|token| {\n                let angle = tau * token as f64 / modulus as f64;\n                vec![(2.0 * angle).cos(), (2.0 * angle).sin(), 0.0, 0.0]\n            })\n            .collect();\n        let config = EmbeddingClockConfig {\n            max_radius_std_fraction: 0.5,\n            min_angular_coverage: 0.1,\n            max_mean_angle_error: 0.2,\n            min_order_fraction: 0.1,\n        };\n        let err = verify_embedding_clock_geometry(&embeddings, modulus, &config)\n            .expect_err(\"expected ordering verification to fail\");\n        assert!(err.contains(\"mean angular error\"));\n    }\n\n    #[test]\n    fn fft_dominant_frequency_passes_on_modulus_structure() {\n        // Create embeddings with strong modulus-113 frequency component\n        // Mix in some other frequencies to get a realistic peak-to-median ratio\n        let modulus = 113;\n        let vocab_size = 114; // 0-112 + equals token\n        let embedding_dim = 128;\n        let tau = std::f64::consts::PI * 2.0;\n\n        let mut embeddings = Vec::with_capacity(vocab_size);\n        for token in 0..vocab_size {\n            let mut embedding = Vec::with_capacity(embedding_dim);\n            for dim in 0..embedding_dim {\n                // 60% of dimensions have modulus frequency, 40% have other frequencies\n                let freq = if dim % 5 < 3 {\n                    modulus\n                } else {\n                    // Use various other frequencies for the remaining dims\n                    20 + (dim % 20)\n                };\n                let angle = tau * freq as f64 * token as f64 / vocab_size as f64\n                    + dim as f64 * 0.1;\n                embedding.push(angle.sin());\n            }\n            embeddings.push(embedding);\n        }\n\n        let config = FFTDominantFrequencyConfig::default_for_modulus(modulus);\n        let report = verify_fft_dominant_frequencies_from_embeddings(&embeddings, &config)\n            .expect(\"expected modulus structure to pass\");\n\n        assert!(report.modulus_frequency_fraction >= config.min_modulus_frequency_fraction);\n        assert!(report.spectrum_entropy <= config.max_spectrum_entropy);\n        assert!(report.peak_to_median_ratio >= config.min_peak_to_median_ratio);\n        assert!(report.top_frequencies.iter().any(|(freq, _)| *freq == modulus));\n    }\n\n    #[test]\n    fn fft_dominant_frequency_fails_on_white_noise() {\n        // Create random embeddings (white noise spectrum)\n        let modulus = 113;\n        let vocab_size = 114;\n        let embedding_dim = 128;\n\n        let mut embeddings = Vec::with_capacity(vocab_size);\n        for token in 0..vocab_size {\n            let mut embedding = Vec::with_capacity(embedding_dim);\n            for dim in 0..embedding_dim {\n                // Pseudo-random values\n                let value = ((token * 17 + dim * 31) % 100) as f64 / 50.0 - 1.0;\n                embedding.push(value);\n            }\n            embeddings.push(embedding);\n        }\n\n        let config = FFTDominantFrequencyConfig::default_for_modulus(modulus);\n        let err = verify_fft_dominant_frequencies_from_embeddings(&embeddings, &config)\n            .expect_err(\"expected white noise to fail\");\n\n        // Should fail on either modulus frequency fraction or spectrum entropy\n        assert!(\n            err.contains(\"modulus frequency\") || err.contains(\"spectrum entropy\")\n                || err.contains(\"peak to median\")\n        );\n    }\n\n    #[test]\n    fn fft_dominant_frequency_fails_on_wrong_frequency() {\n        // Create embeddings with dominant frequency at 57 (not 113)\n        let modulus = 113;\n        let vocab_size = 114;\n        let embedding_dim = 128;\n        let tau = std::f64::consts::PI * 2.0;\n        let wrong_frequency = 57;\n\n        let mut embeddings = Vec::with_capacity(vocab_size);\n        for token in 0..vocab_size {\n            let mut embedding = Vec::with_capacity(embedding_dim);\n            for dim in 0..embedding_dim {\n                // Each dimension has a sine wave with wrong frequency\n                let angle = tau * wrong_frequency as f64 * token as f64 / vocab_size as f64\n                    + dim as f64 * 0.1;\n                embedding.push(angle.sin());\n            }\n            embeddings.push(embedding);\n        }\n\n        let config = FFTDominantFrequencyConfig::default_for_modulus(modulus);\n        let err = verify_fft_dominant_frequencies_from_embeddings(&embeddings, &config)\n            .expect_err(\"expected wrong frequency to fail\");\n\n        assert!(err.contains(\"modulus frequency\"));\n    }\n\n    #[test]\n    fn fft_dominant_frequency_fails_on_flat_embeddings() {\n        // Create constant embeddings (DC component only, no structure)\n        let modulus = 113;\n        let vocab_size = 114;\n        let embedding_dim = 128;\n\n        let embeddings = vec![vec![1.5; embedding_dim]; vocab_size];\n\n        let config = FFTDominantFrequencyConfig::default_for_modulus(modulus);\n        let err = verify_fft_dominant_frequencies_from_embeddings(&embeddings, &config)\n            .expect_err(\"expected flat embeddings to fail\");\n\n        assert!(err.contains(\"modulus frequency\") || err.contains(\"peak to median\"));\n    }\n\n    #[test]\n    fn fft_dominant_frequency_fails_on_weak_peaks() {\n        // Create embeddings with many frequencies at similar magnitudes (no clear peaks)\n        let modulus = 113;\n        let vocab_size = 114;\n        let embedding_dim = 128;\n        let tau = std::f64::consts::PI * 2.0;\n\n        let mut embeddings = Vec::with_capacity(vocab_size);\n        for token in 0..vocab_size {\n            let mut embedding = Vec::with_capacity(embedding_dim);\n            for dim in 0..embedding_dim {\n                // Mix many frequencies with similar amplitudes\n                let mut value = 0.0;\n                for freq in 1..10 {\n                    let angle =\n                        tau * freq as f64 * token as f64 / vocab_size as f64 + dim as f64 * 0.01;\n                    value += angle.sin() / 10.0;\n                }\n                embedding.push(value);\n            }\n            embeddings.push(embedding);\n        }\n\n        let config = FFTDominantFrequencyConfig {\n            modulus,\n            min_modulus_frequency_fraction: 0.15,\n            max_spectrum_entropy: 0.85,\n            min_peak_to_median_ratio: 2.0,\n        };\n        let err = verify_fft_dominant_frequencies_from_embeddings(&embeddings, &config)\n            .expect_err(\"expected weak peaks to fail\");\n\n        assert!(\n            err.contains(\"modulus frequency\") || err.contains(\"spectrum entropy\")\n                || err.contains(\"peak to median\")\n        );\n    }\n}\n","structuredPatch":[{"oldStart":686,"oldLines":6,"newStart":686,"newLines":7,"lines":[" }"," "," #[derive(Debug, Clone)]","+#[allow(dead_code)]"," pub struct FFTDominantFrequencyReport {","     pub modulus_frequency_fraction: f64,","     pub spectrum_entropy: f64,"]},{"oldStart":694,"oldLines":6,"newStart":695,"newLines":7,"lines":[" }"," "," #[derive(Debug, Clone)]","+#[allow(dead_code)]"," pub struct EmbeddingClockConfig {","     pub max_radius_std_fraction: f64,","     pub min_angular_coverage: f64,"]},{"oldStart":715,"oldLines":6,"newStart":717,"newLines":7,"lines":[" }"," "," #[derive(Debug, Clone)]","+#[allow(dead_code)]"," pub struct EmbeddingClockReport {","     pub mean_radius: f64,","     pub radius_std: f64,"]}],"userModified":false,"replaceAll":false}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_016SyG9n6u7Y6K8CStoXHNj7","type":"message","role":"assistant","content":[{"type":"text","text":"Now let me check if there are any more warnings. Let me run a final build:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":1452,"cache_read_input_tokens":84021,"cache_creation":{"ephemeral_5m_input_tokens":1452,"ephemeral_1h_input_tokens":0},"output_tokens":2,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"ccae4303-a856-4b2b-9c60-a04e9c28b4ce"}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_016SyG9n6u7Y6K8CStoXHNj7","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_018p2Rg1obvGYG7SaS9jqemR","name":"Bash","input":{"command":"cargo build 2>&1 | grep \"warning:\" | wc -l","description":"Check final warning count","timeout":120000}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":1452,"cache_read_input_tokens":84021,"cache_creation":{"ephemeral_5m_input_tokens":1452,"ephemeral_1h_input_tokens":0},"output_tokens":2,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"f9f82bcb-0eff-4ac9-87f7-791d080b999b"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_018p2Rg1obvGYG7SaS9jqemR","type":"tool_result","content":"       2","is_error":false}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"75a022b2-9597-4aa0-be58-9531a1484014","tool_use_result":{"stdout":"       2","stderr":"","interrupted":false,"isImage":false}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01Q85aFHFUXi8EX37BSjhhXe","type":"message","role":"assistant","content":[{"type":"text","text":"Excellent! Down from 59 to just 2. Let me see what those are:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":139,"cache_read_input_tokens":85473,"cache_creation":{"ephemeral_5m_input_tokens":139,"ephemeral_1h_input_tokens":0},"output_tokens":4,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"1ad0aee6-d78a-4fb4-8cf2-31ea18340277"}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01Q85aFHFUXi8EX37BSjhhXe","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01GJDDD8BhVd6KXkuBBeaHkN","name":"Bash","input":{"command":"cargo build 2>&1 | grep -A 3 \"warning:\"","description":"Check remaining warnings","timeout":120000}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":139,"cache_read_input_tokens":85473,"cache_creation":{"ephemeral_5m_input_tokens":139,"ephemeral_1h_input_tokens":0},"output_tokens":4,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"f8b2c765-5dcc-4378-b13b-41d716498234"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01GJDDD8BhVd6KXkuBBeaHkN","type":"tool_result","content":"warning: method `add_snapshot` is never used\n   --> src/analysis.rs:457:12\n    |\n449 | impl LossHistory {\n--\nwarning: `grokking` (bin \"grokking\") generated 1 warning\n    Finished `dev` profile [unoptimized + debuginfo] target(s) in 0.18s","is_error":false}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"a09397fb-cc38-418f-9608-1df865c498e1","tool_use_result":{"stdout":"warning: method `add_snapshot` is never used\n   --> src/analysis.rs:457:12\n    |\n449 | impl LossHistory {\n--\nwarning: `grokking` (bin \"grokking\") generated 1 warning\n    Finished `dev` profile [unoptimized + debuginfo] target(s) in 0.18s","stderr":"","interrupted":false,"isImage":false}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01PjHaXG7JM2DGy3DxKDijtA","type":"message","role":"assistant","content":[{"type":"text","text":"Let me check that add_snapshot method:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":222,"cache_read_input_tokens":85612,"cache_creation":{"ephemeral_5m_input_tokens":222,"ephemeral_1h_input_tokens":0},"output_tokens":4,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"8bcb5b85-2188-4146-aa66-b6b2c29cf4e4"}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01PjHaXG7JM2DGy3DxKDijtA","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01BEeZFEGmHsYosuNPh957S3","name":"Read","input":{"file_path":"/Users/admin/MechInt/src/analysis.rs","offset":449,"limit":20}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":222,"cache_read_input_tokens":85612,"cache_creation":{"ephemeral_5m_input_tokens":222,"ephemeral_1h_input_tokens":0},"output_tokens":4,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"b85726d7-f925-449b-94ce-0456b8f11624"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01BEeZFEGmHsYosuNPh957S3","type":"tool_result","content":"   449â†’impl LossHistory {\n   450â†’    pub fn new() -> Self {\n   451â†’        Self {\n   452â†’            train_snapshots: Vec::new(),\n   453â†’            val_snapshots: Vec::new(),\n   454â†’        }\n   455â†’    }\n   456â†’\n   457â†’    pub fn add_snapshot(&mut self, step: usize, train_loss: f64, val_loss: f64) {\n   458â†’        self.train_snapshots.push((step, train_loss));\n   459â†’        self.val_snapshots.push((step, val_loss));\n   460â†’    }\n   461â†’\n   462â†’    pub fn save(&self, filename: &str) -> std::io::Result<()> {\n   463â†’        let json = serde_json::to_string_pretty(self)?;\n   464â†’        std::fs::write(filename, json)?;\n   465â†’        println!(\"ðŸ’¾ Loss history saved to: {}\", filename);\n   466â†’        Ok(())\n   467â†’    }\n   468â†’}\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"c74366e6-6f72-4f3f-a947-81878c78cee4","tool_use_result":{"type":"text","file":{"filePath":"/Users/admin/MechInt/src/analysis.rs","content":"impl LossHistory {\n    pub fn new() -> Self {\n        Self {\n            train_snapshots: Vec::new(),\n            val_snapshots: Vec::new(),\n        }\n    }\n\n    pub fn add_snapshot(&mut self, step: usize, train_loss: f64, val_loss: f64) {\n        self.train_snapshots.push((step, train_loss));\n        self.val_snapshots.push((step, val_loss));\n    }\n\n    pub fn save(&self, filename: &str) -> std::io::Result<()> {\n        let json = serde_json::to_string_pretty(self)?;\n        std::fs::write(filename, json)?;\n        println!(\"ðŸ’¾ Loss history saved to: {}\", filename);\n        Ok(())\n    }\n}","numLines":20,"startLine":449,"totalLines":1211}}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_014EKKnXuRZLsRmNcqSodU7D","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01Rz8LHtSbz7s8zKxwFmKhPE","name":"Edit","input":{"replace_all":false,"file_path":"/Users/admin/MechInt/src/analysis.rs","old_string":"    pub fn add_snapshot(&mut self, step: usize, train_loss: f64, val_loss: f64) {\n        self.train_snapshots.push((step, train_loss));\n        self.val_snapshots.push((step, val_loss));\n    }","new_string":"    /// NOTE: Currently unused - loss tracked directly in main loop\n    #[allow(dead_code)]\n    pub fn add_snapshot(&mut self, step: usize, train_loss: f64, val_loss: f64) {\n        self.train_snapshots.push((step, train_loss));\n        self.val_snapshots.push((step, val_loss));\n    }"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":460,"cache_read_input_tokens":85834,"cache_creation":{"ephemeral_5m_input_tokens":460,"ephemeral_1h_input_tokens":0},"output_tokens":279,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"c157cbb7-9d3f-4cd6-a077-38b2e0df77b3"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01Rz8LHtSbz7s8zKxwFmKhPE","type":"tool_result","content":"The file /Users/admin/MechInt/src/analysis.rs has been updated. Here's the result of running `cat -n` on a snippet of the edited file:\n   453â†’            val_snapshots: Vec::new(),\n   454â†’        }\n   455â†’    }\n   456â†’\n   457â†’    /// NOTE: Currently unused - loss tracked directly in main loop\n   458â†’    #[allow(dead_code)]\n   459â†’    pub fn add_snapshot(&mut self, step: usize, train_loss: f64, val_loss: f64) {\n   460â†’        self.train_snapshots.push((step, train_loss));\n   461â†’        self.val_snapshots.push((step, val_loss));\n   462â†’    }\n   463â†’\n   464â†’    pub fn save(&self, filename: &str) -> std::io::Result<()> {\n   465â†’        let json = serde_json::to_string_pretty(self)?;\n   466â†’        std::fs::write(filename, json)?;"}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"48b5323f-773c-42c8-b5fd-fa734811bab6","tool_use_result":{"filePath":"/Users/admin/MechInt/src/analysis.rs","oldString":"    pub fn add_snapshot(&mut self, step: usize, train_loss: f64, val_loss: f64) {\n        self.train_snapshots.push((step, train_loss));\n        self.val_snapshots.push((step, val_loss));\n    }","newString":"    /// NOTE: Currently unused - loss tracked directly in main loop\n    #[allow(dead_code)]\n    pub fn add_snapshot(&mut self, step: usize, train_loss: f64, val_loss: f64) {\n        self.train_snapshots.push((step, train_loss));\n        self.val_snapshots.push((step, val_loss));\n    }","originalFile":"use burn::{\n    data::dataset::Dataset,\n    nn::loss::CrossEntropyLossConfig,\n    tensor::{backend::Backend, Int, Tensor, TensorData},\n};\nuse rustfft::{num_complex::Complex, FftPlanner};\nuse serde::{Deserialize, Serialize};\nuse std::collections::{HashMap, HashSet};\n\nuse crate::{data::ModularAdditionDataset, model::Transformer};\n\n/// FFT analysis results for embeddings\n#[derive(Debug, Serialize, Deserialize)]\npub struct FFTAnalysis {\n    pub vocab_size: usize,\n    pub embedding_dim: usize,\n    pub dominant_frequencies: Vec<(usize, f64)>, // (token_id, dominant_frequency_index)\n    pub frequency_magnitudes: HashMap<usize, Vec<f64>>, // token_id -> magnitude spectrum\n}\n\n/// Analyze the learned token embeddings using FFT\npub fn analyze_embeddings_fft<B: Backend>(model: &Transformer<B>) -> FFTAnalysis {\n    println!(\"ðŸ” Performing FFT analysis on learned embeddings...\");\n\n    let vocab_size = ModularAdditionDataset::vocab_size();\n    let embedding_dim = 128;\n\n    // Extract token embeddings by forward passing through the embedding layer\n    let mut frequency_magnitudes = HashMap::new();\n    let mut dominant_frequencies = Vec::new();\n\n    for token_id in 0..vocab_size {\n        // Get the embedding for this token\n        let embedding = get_token_embedding(model, token_id);\n\n        // Compute FFT on the embedding vector\n        let fft_result = compute_fft(&embedding);\n\n        // Find dominant frequency\n        let dominant_freq_idx = find_dominant_frequency(&fft_result);\n        dominant_frequencies.push((token_id, dominant_freq_idx as f64));\n\n        // Store magnitude spectrum\n        frequency_magnitudes.insert(token_id, fft_result);\n\n        if token_id % 10 == 0 {\n            println!(\"  Token {:2}: dominant frequency = {}\", token_id, dominant_freq_idx);\n        }\n    }\n\n    println!(\"âœ… FFT analysis complete!\");\n    println!();\n    println!(\"ðŸ“Š Dominant Frequency Analysis:\");\n\n    // Analyze patterns\n    let mut freq_histogram: HashMap<usize, usize> = HashMap::new();\n    for (_, freq) in &dominant_frequencies {\n        *freq_histogram.entry(*freq as usize).or_insert(0) += 1;\n    }\n\n    let mut freq_counts: Vec<_> = freq_histogram.iter().collect();\n    freq_counts.sort_by(|a, b| b.1.cmp(a.1));\n\n    println!(\"  Most common dominant frequencies:\");\n    for (freq, count) in freq_counts.iter().take(5) {\n        println!(\"    Frequency {}: {} tokens\", freq, count);\n    }\n\n    // Check for modular structure (expected for mod p)\n    println!();\n    println!(\"ðŸ”¬ Checking for modular structure:\");\n    let modulus = ModularAdditionDataset::modulus();\n    if freq_histogram.contains_key(&modulus) {\n        println!(\n            \"  âœ“ Frequency {} (matching modulus) appears in embeddings\",\n            modulus\n        );\n    }\n\n    FFTAnalysis {\n        vocab_size,\n        embedding_dim,\n        dominant_frequencies,\n        frequency_magnitudes,\n    }\n}\n\n/// Extract the token embedding for a single token\nfn get_token_embedding<B: Backend>(model: &Transformer<B>, token_id: usize) -> Vec<f64> {\n    // Get the token embedding directly from the model\n    let embedding_tensor = model.get_token_embedding(token_id);\n\n    // Convert to vector - shape is [1, 1, embedding_dim], we want [embedding_dim]\n    let embedding_data: Vec<f32> = embedding_tensor.into_data().to_vec().unwrap();\n\n    // Convert to f64\n    embedding_data.iter().map(|&x| x as f64).collect()\n}\n\n/// Compute FFT on a vector of real values\nfn compute_fft(data: &[f64]) -> Vec<f64> {\n    let mut planner = FftPlanner::new();\n    let fft = planner.plan_fft_forward(data.len());\n\n    // Convert to complex numbers\n    let mut buffer: Vec<Complex<f64>> = data\n        .iter()\n        .map(|&x| Complex::new(x, 0.0))\n        .collect();\n\n    // Perform FFT\n    fft.process(&mut buffer);\n\n    // Compute magnitudes\n    buffer.iter().map(|c| c.norm()).collect()\n}\n\nfn inverse_fft(data: &mut [Complex<f64>]) {\n    let mut planner = FftPlanner::new();\n    let fft = planner.plan_fft_inverse(data.len());\n    fft.process(data);\n}\n\nfn top_k_frequency_indices(magnitudes: &[f64], k: usize) -> Vec<usize> {\n    let mut indexed: Vec<(usize, f64)> = magnitudes.iter().copied().enumerate().collect();\n    indexed.sort_by(|(idx_a, mag_a), (idx_b, mag_b)| {\n        mag_b\n            .partial_cmp(mag_a)\n            .unwrap_or(std::cmp::Ordering::Equal)\n            .then_with(|| idx_a.cmp(idx_b))\n    });\n\n    let keep = k.min(indexed.len());\n    indexed.iter().take(keep).map(|(idx, _)| *idx).collect()\n}\n\nfn restrict_signal_to_top_k_frequencies(signal: &[f64], top_k: usize) -> Vec<f64> {\n    if signal.is_empty() {\n        return Vec::new();\n    }\n\n    if top_k == 0 {\n        return vec![0.0; signal.len()];\n    }\n\n    let mut planner = FftPlanner::new();\n    let fft = planner.plan_fft_forward(signal.len());\n\n    let mut buffer: Vec<Complex<f64>> = signal\n        .iter()\n        .map(|&x| Complex::new(x, 0.0))\n        .collect();\n    fft.process(&mut buffer);\n\n    let magnitudes: Vec<f64> = buffer.iter().map(|c| c.norm()).collect();\n    let keep = top_k_frequency_indices(&magnitudes, top_k);\n    let keep_set: HashSet<usize> = keep.into_iter().collect();\n\n    for (idx, value) in buffer.iter_mut().enumerate() {\n        if !keep_set.contains(&idx) {\n            *value = Complex::new(0.0, 0.0);\n        }\n    }\n\n    inverse_fft(&mut buffer);\n\n    let scale = 1.0 / signal.len() as f64;\n    buffer.iter().map(|c| c.re * scale).collect()\n}\n\nfn exclude_top_k_frequencies(signal: &[f64], top_k: usize) -> Vec<f64> {\n    if signal.is_empty() {\n        return Vec::new();\n    }\n\n    if top_k == 0 {\n        return signal.to_vec();\n    }\n\n    if top_k >= signal.len() {\n        return vec![0.0; signal.len()];\n    }\n\n    let mut planner = FftPlanner::new();\n    let fft = planner.plan_fft_forward(signal.len());\n\n    let mut buffer: Vec<Complex<f64>> = signal\n        .iter()\n        .map(|&x| Complex::new(x, 0.0))\n        .collect();\n    fft.process(&mut buffer);\n\n    let magnitudes: Vec<f64> = buffer.iter().map(|c| c.norm()).collect();\n    let remove = top_k_frequency_indices(&magnitudes, top_k);\n    let remove_set: HashSet<usize> = remove.into_iter().collect();\n\n    for (idx, value) in buffer.iter_mut().enumerate() {\n        if remove_set.contains(&idx) {\n            *value = Complex::new(0.0, 0.0);\n        }\n    }\n\n    inverse_fft(&mut buffer);\n\n    let scale = 1.0 / signal.len() as f64;\n    buffer.iter().map(|c| c.re * scale).collect()\n}\n\nfn restricted_token_embedding_weights<B: Backend>(\n    model: &Transformer<B>,\n    top_k: usize,\n) -> Vec<f32> {\n    let weight = model.token_embedding_weights();\n    let [vocab_size, d_model] = weight.dims();\n    let weight_data: Vec<f32> = weight.into_data().to_vec().unwrap();\n    let mut restricted = vec![0.0f32; weight_data.len()];\n\n    for dim in 0..d_model {\n        let mut column = Vec::with_capacity(vocab_size);\n        for token in 0..vocab_size {\n            column.push(weight_data[token * d_model + dim] as f64);\n        }\n        let filtered = restrict_signal_to_top_k_frequencies(&column, top_k);\n        for token in 0..vocab_size {\n            restricted[token * d_model + dim] = filtered[token] as f32;\n        }\n    }\n\n    restricted\n}\n\nfn excluded_token_embedding_weights<B: Backend>(\n    model: &Transformer<B>,\n    top_k: usize,\n) -> Vec<f32> {\n    let weight = model.token_embedding_weights();\n    let [vocab_size, d_model] = weight.dims();\n    let weight_data: Vec<f32> = weight.into_data().to_vec().unwrap();\n    let mut excluded = vec![0.0f32; weight_data.len()];\n\n    for dim in 0..d_model {\n        let mut column = Vec::with_capacity(vocab_size);\n        for token in 0..vocab_size {\n            column.push(weight_data[token * d_model + dim] as f64);\n        }\n        let filtered = exclude_top_k_frequencies(&column, top_k);\n        for token in 0..vocab_size {\n            excluded[token * d_model + dim] = filtered[token] as f32;\n        }\n    }\n\n    excluded\n}\n\npub fn compute_restricted_loss<B: Backend>(\n    model: &Transformer<B>,\n    dataset: &ModularAdditionDataset,\n    device: &B::Device,\n    top_k: usize,\n    batch_size: usize,\n) -> Result<f64, String> {\n    if batch_size == 0 {\n        return Err(\"batch_size must be greater than 0\".to_string());\n    }\n\n    let restricted_weights = restricted_token_embedding_weights(model, top_k);\n    let [vocab_size, d_model] = model.token_embedding_weights().dims();\n    let token_weights = Tensor::<B, 2>::from_data(\n        TensorData::new(restricted_weights, [vocab_size, d_model]),\n        device,\n    );\n\n    let loss_fn = CrossEntropyLossConfig::new().init(device);\n    let mut total_loss = 0.0f64;\n    let mut total_batches = 0usize;\n\n    for batch_start in (0..dataset.len()).step_by(batch_size) {\n        let batch_end = (batch_start + batch_size).min(dataset.len());\n        let batch_len = batch_end - batch_start;\n        if batch_len == 0 {\n            break;\n        }\n\n        let mut inputs_vec = Vec::with_capacity(batch_len * 3);\n        let mut targets_vec = Vec::with_capacity(batch_len);\n\n        for idx in batch_start..batch_end {\n            let (input, target) = dataset.get(idx).ok_or_else(|| {\n                format!(\"missing dataset sample at index {}\", idx)\n            })?;\n            inputs_vec.extend(input.iter().map(|value| *value as i32));\n            targets_vec.push(target as i32);\n        }\n\n        let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n            .reshape([batch_len, 3]);\n        let targets = Tensor::<B, 1, Int>::from_ints(targets_vec.as_slice(), device);\n\n        let logits = model.forward_with_token_weights(inputs, token_weights.clone());\n        let loss = loss_fn.forward(logits, targets);\n        let loss_value: f64 = loss.into_data().to_vec().unwrap().get(0).copied().unwrap_or(0.0);\n        total_loss += loss_value;\n        total_batches += 1;\n    }\n\n    if total_batches == 0 {\n        return Err(\"no batches computed for restricted loss\".to_string());\n    }\n\n    Ok(total_loss / total_batches as f64)\n}\n\npub fn compute_excluded_loss<B: Backend>(\n    model: &Transformer<B>,\n    dataset: &ModularAdditionDataset,\n    device: &B::Device,\n    top_k: usize,\n    batch_size: usize,\n) -> Result<f64, String> {\n    if batch_size == 0 {\n        return Err(\"batch_size must be greater than 0\".to_string());\n    }\n\n    let excluded_weights = excluded_token_embedding_weights(model, top_k);\n    let [vocab_size, d_model] = model.token_embedding_weights().dims();\n    let token_weights = Tensor::<B, 2>::from_data(\n        TensorData::new(excluded_weights, [vocab_size, d_model]),\n        device,\n    );\n\n    let loss_fn = CrossEntropyLossConfig::new().init(device);\n    let mut total_loss = 0.0f64;\n    let mut total_batches = 0usize;\n\n    for batch_start in (0..dataset.len()).step_by(batch_size) {\n        let batch_end = (batch_start + batch_size).min(dataset.len());\n        let batch_len = batch_end - batch_start;\n        if batch_len == 0 {\n            break;\n        }\n\n        let mut inputs_vec = Vec::with_capacity(batch_len * 3);\n        let mut targets_vec = Vec::with_capacity(batch_len);\n\n        for idx in batch_start..batch_end {\n            let (input, target) = dataset.get(idx).ok_or_else(|| {\n                format!(\"missing dataset sample at index {}\", idx)\n            })?;\n            inputs_vec.extend(input.iter().map(|value| *value as i32));\n            targets_vec.push(target as i32);\n        }\n\n        let inputs = Tensor::<B, 1, Int>::from_ints(inputs_vec.as_slice(), device)\n            .reshape([batch_len, 3]);\n        let targets = Tensor::<B, 1, Int>::from_ints(targets_vec.as_slice(), device);\n\n        let logits = model.forward_with_token_weights(inputs, token_weights.clone());\n        let loss = loss_fn.forward(logits, targets);\n        let loss_value: f64 = loss.into_data().to_vec().unwrap().get(0).copied().unwrap_or(0.0);\n        total_loss += loss_value;\n        total_batches += 1;\n    }\n\n    if total_batches == 0 {\n        return Err(\"no batches computed for excluded loss\".to_string());\n    }\n\n    Ok(total_loss / total_batches as f64)\n}\n\n/// Find the index of the dominant frequency (excluding DC component at index 0)\nfn find_dominant_frequency(magnitudes: &[f64]) -> usize {\n    magnitudes\n        .iter()\n        .enumerate()\n        .skip(1) // Skip DC component\n        .max_by(|(_, a), (_, b)| {\n            a.partial_cmp(b).unwrap_or(std::cmp::Ordering::Equal)\n        })\n        .map(|(idx, _)| idx)\n        .unwrap_or(0)\n}\n\n/// Save FFT analysis to JSON file\npub fn save_fft_analysis(analysis: &FFTAnalysis, filename: &str) -> std::io::Result<()> {\n    let json = serde_json::to_string_pretty(analysis)?;\n    std::fs::write(filename, json)?;\n    println!(\"ðŸ’¾ FFT analysis saved to: {}\", filename);\n    Ok(())\n}\n\n/// Track training and validation loss over time\n#[derive(Debug, Serialize, Deserialize)]\npub struct LossHistory {\n    pub train_snapshots: Vec<(usize, f64)>, // (step, loss)\n    pub val_snapshots: Vec<(usize, f64)>,   // (step, loss)\n}\n\n/// Track restricted loss over time (only top-k frequencies kept)\n#[derive(Debug, Serialize, Deserialize)]\npub struct RestrictedLossHistory {\n    pub snapshots: Vec<(usize, f64)>, // (step, loss)\n}\n\nimpl RestrictedLossHistory {\n    pub fn new() -> Self {\n        Self {\n            snapshots: Vec::new(),\n        }\n    }\n\n    pub fn add_snapshot(&mut self, step: usize, loss: f64) {\n        self.snapshots.push((step, loss));\n    }\n\n    pub fn save(&self, filename: &str) -> std::io::Result<()> {\n        let json = serde_json::to_string_pretty(self)?;\n        std::fs::write(filename, json)?;\n        println!(\"ðŸ’¾ Restricted loss history saved to: {}\", filename);\n        Ok(())\n    }\n}\n\n/// Track excluded loss over time (top-k frequencies removed)\n#[derive(Debug, Serialize, Deserialize)]\npub struct ExcludedLossHistory {\n    pub snapshots: Vec<(usize, f64)>, // (step, loss)\n}\n\nimpl ExcludedLossHistory {\n    pub fn new() -> Self {\n        Self {\n            snapshots: Vec::new(),\n        }\n    }\n\n    pub fn add_snapshot(&mut self, step: usize, loss: f64) {\n        self.snapshots.push((step, loss));\n    }\n\n    pub fn save(&self, filename: &str) -> std::io::Result<()> {\n        let json = serde_json::to_string_pretty(self)?;\n        std::fs::write(filename, json)?;\n        println!(\"ðŸ’¾ Excluded loss history saved to: {}\", filename);\n        Ok(())\n    }\n}\n\nimpl LossHistory {\n    pub fn new() -> Self {\n        Self {\n            train_snapshots: Vec::new(),\n            val_snapshots: Vec::new(),\n        }\n    }\n\n    pub fn add_snapshot(&mut self, step: usize, train_loss: f64, val_loss: f64) {\n        self.train_snapshots.push((step, train_loss));\n        self.val_snapshots.push((step, val_loss));\n    }\n\n    pub fn save(&self, filename: &str) -> std::io::Result<()> {\n        let json = serde_json::to_string_pretty(self)?;\n        std::fs::write(filename, json)?;\n        println!(\"ðŸ’¾ Loss history saved to: {}\", filename);\n        Ok(())\n    }\n}\n\n/// Legacy: Single loss tracking (kept for backward compatibility)\n#[derive(Debug, Serialize, Deserialize, Clone)]\npub struct WeightNormSnapshot {\n    pub step: usize,\n    pub total_norm: f64,\n}\n\n/// Legacy: Track weight norms over training (actually stores loss)\n#[derive(Debug, Serialize, Deserialize)]\npub struct WeightNormHistory {\n    pub snapshots: Vec<WeightNormSnapshot>,\n}\n\nimpl WeightNormHistory {\n    pub fn new() -> Self {\n        Self {\n            snapshots: Vec::new(),\n        }\n    }\n\n    pub fn add_snapshot(&mut self, step: usize, norm: f64) {\n        self.snapshots.push(WeightNormSnapshot {\n            step,\n            total_norm: norm,\n        });\n    }\n\n    pub fn save(&self, filename: &str) -> std::io::Result<()> {\n        let json = serde_json::to_string_pretty(self)?;\n        std::fs::write(filename, json)?;\n        println!(\"ðŸ’¾ Weight norm history saved to: {}\", filename);\n        Ok(())\n    }\n}\n\n/// Compute the L2 norm of all model parameters\n/// This returns the Frobenius norm (sum of squared weights) across all parameters\n/// Note: This is a simplified implementation that computes the norm of token embeddings,\n/// which contain the majority of the model parameters. For a complete implementation,\n/// we would need to access all Linear layer weights via the Record trait.\npub fn compute_model_weight_norm<B: Backend>(model: &Transformer<B>) -> f64 {\n    let mut total_norm_squared = 0.0f64;\n\n    // Token embedding weights: [vocab_size, d_model]\n    let token_emb_weights = model.token_embedding_weights();\n    let token_emb_data: Vec<f32> = token_emb_weights.into_data().to_vec().unwrap();\n    for &weight in &token_emb_data {\n        total_norm_squared += (weight as f64) * (weight as f64);\n    }\n\n    // Note: Position embeddings, attention weights, MLP weights, and LM head weights\n    // are not included in this simplified implementation. For weight decay verification,\n    // tracking the token embeddings (which are the largest weight matrices) is sufficient\n    // to observe the monotonic decrease under weight decay.\n\n    total_norm_squared.sqrt()\n}\n\n/// Extract all token embeddings as a matrix [vocab_size, embedding_dim]\npub fn extract_all_embeddings<B: Backend>(model: &Transformer<B>) -> Vec<Vec<f64>> {\n    let vocab_size = ModularAdditionDataset::vocab_size();\n    let mut embeddings = Vec::with_capacity(vocab_size);\n\n    for token_id in 0..vocab_size {\n        embeddings.push(get_token_embedding(model, token_id));\n    }\n\n    embeddings\n}\n\n/// Verify that FFT analysis of embeddings shows dominant frequencies\n/// that align with the modulus structure and nontrivial spectral patterns\n///\n/// NOTE: Reserved for manual verification or future integration into analysis pipeline\n#[allow(dead_code)]\npub fn verify_fft_dominant_frequencies<B: Backend>(\n    model: &Transformer<B>,\n    config: &FFTDominantFrequencyConfig,\n) -> Result<FFTDominantFrequencyReport, String> {\n    let embeddings = extract_all_embeddings(model);\n    verify_fft_dominant_frequencies_from_embeddings(&embeddings, config)\n}\n\n/// Verify FFT dominant frequencies from pre-extracted embeddings\n///\n/// NOTE: Reserved for manual verification or future integration into analysis pipeline\n#[allow(dead_code)]\npub fn verify_fft_dominant_frequencies_from_embeddings(\n    embeddings: &[Vec<f64>],\n    config: &FFTDominantFrequencyConfig,\n) -> Result<FFTDominantFrequencyReport, String> {\n    if embeddings.is_empty() {\n        return Err(\"no embeddings provided\".to_string());\n    }\n\n    let vocab_size = embeddings.len();\n    if vocab_size < config.modulus {\n        return Err(format!(\n            \"embedding count {} less than modulus {}\",\n            vocab_size, config.modulus\n        ));\n    }\n\n    // Analyze each embedding dimension and collect dominant frequencies\n    let mut frequency_histogram: HashMap<usize, usize> = HashMap::new();\n    let embedding_dim = embeddings[0].len();\n\n    // For each embedding dimension, compute FFT across all tokens and find dominant frequency\n    for dim in 0..embedding_dim {\n        let mut signal = Vec::with_capacity(vocab_size);\n        for embedding in embeddings.iter().take(vocab_size) {\n            if dim >= embedding.len() {\n                return Err(format!(\n                    \"embedding dimension mismatch at dim {}\",\n                    dim\n                ));\n            }\n            signal.push(embedding[dim]);\n        }\n\n        // Compute FFT and find dominant frequency\n        let magnitudes = compute_fft(&signal);\n        let dominant_freq = find_dominant_frequency(&magnitudes);\n        *frequency_histogram.entry(dominant_freq).or_insert(0) += 1;\n    }\n\n    // Check 1: Modulus frequency should appear significantly\n    let modulus_count = frequency_histogram.get(&config.modulus).copied().unwrap_or(0);\n    let modulus_frequency_fraction = modulus_count as f64 / embedding_dim as f64;\n\n    if modulus_frequency_fraction < config.min_modulus_frequency_fraction {\n        return Err(format!(\n            \"modulus frequency {} appears in only {:.3} of dimensions (threshold {:.3})\",\n            config.modulus, modulus_frequency_fraction, config.min_modulus_frequency_fraction\n        ));\n    }\n\n    // Check 2: Spectrum should NOT resemble white noise (low entropy)\n    // High entropy = uniform distribution (white noise), low entropy = structured peaks\n    let total_dims = embedding_dim as f64;\n    let mut entropy = 0.0;\n    for &count in frequency_histogram.values() {\n        if count > 0 {\n            let p = count as f64 / total_dims;\n            entropy -= p * p.log2();\n        }\n    }\n    // Normalize entropy by max possible (log2 of unique frequency count)\n    let max_entropy = (frequency_histogram.len() as f64).log2();\n    let normalized_entropy = if max_entropy > 0.0 {\n        entropy / max_entropy\n    } else {\n        0.0\n    };\n\n    if normalized_entropy > config.max_spectrum_entropy {\n        return Err(format!(\n            \"spectrum entropy {:.3} too high (white noise threshold {:.3})\",\n            normalized_entropy, config.max_spectrum_entropy\n        ));\n    }\n\n    // Check 3: Peak frequencies should be significantly stronger than median\n    let mut counts: Vec<usize> = frequency_histogram.values().copied().collect();\n    counts.sort_unstable();\n    let median_count = if counts.is_empty() {\n        0\n    } else {\n        counts[counts.len() / 2]\n    };\n    let peak_count = counts.last().copied().unwrap_or(0);\n    let peak_to_median_ratio = if median_count > 0 {\n        peak_count as f64 / median_count as f64\n    } else {\n        f64::INFINITY\n    };\n\n    if peak_to_median_ratio < config.min_peak_to_median_ratio {\n        return Err(format!(\n            \"peak to median ratio {:.3} below threshold {:.3}\",\n            peak_to_median_ratio, config.min_peak_to_median_ratio\n        ));\n    }\n\n    // Collect top frequencies for the report\n    let mut freq_counts: Vec<(usize, usize)> = frequency_histogram.into_iter().collect();\n    freq_counts.sort_by(|a, b| b.1.cmp(&a.1));\n    let top_frequencies: Vec<(usize, usize)> = freq_counts.into_iter().take(5).collect();\n\n    Ok(FFTDominantFrequencyReport {\n        modulus_frequency_fraction,\n        spectrum_entropy: normalized_entropy,\n        peak_to_median_ratio,\n        top_frequencies,\n    })\n}\n\n#[derive(Debug, Clone)]\npub struct FFTDominantFrequencyConfig {\n    pub modulus: usize,\n    pub min_modulus_frequency_fraction: f64,\n    pub max_spectrum_entropy: f64,\n    pub min_peak_to_median_ratio: f64,\n}\n\nimpl FFTDominantFrequencyConfig {\n    /// NOTE: Reserved for future integration into analysis pipeline\n    #[allow(dead_code)]\n    pub fn default_for_modulus(modulus: usize) -> Self {\n        Self {\n            modulus,\n            min_modulus_frequency_fraction: 0.15,\n            max_spectrum_entropy: 0.85,\n            min_peak_to_median_ratio: 2.0,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\n#[allow(dead_code)]\npub struct FFTDominantFrequencyReport {\n    pub modulus_frequency_fraction: f64,\n    pub spectrum_entropy: f64,\n    pub peak_to_median_ratio: f64,\n    pub top_frequencies: Vec<(usize, usize)>,\n}\n\n#[derive(Debug, Clone)]\n#[allow(dead_code)]\npub struct EmbeddingClockConfig {\n    pub max_radius_std_fraction: f64,\n    pub min_angular_coverage: f64,\n    pub max_mean_angle_error: f64,\n    pub min_order_fraction: f64,\n}\n\nimpl EmbeddingClockConfig {\n    /// NOTE: Reserved for future integration into analysis pipeline\n    #[allow(dead_code)]\n    pub fn default_for_modulus(_modulus: usize) -> Self {\n        Self {\n            max_radius_std_fraction: 0.1,\n            min_angular_coverage: 0.9,\n            max_mean_angle_error: 0.35,\n            min_order_fraction: 0.9,\n        }\n    }\n}\n\n#[derive(Debug, Clone)]\n#[allow(dead_code)]\npub struct EmbeddingClockReport {\n    pub mean_radius: f64,\n    pub radius_std: f64,\n    pub angular_coverage: f64,\n    pub mean_angle_error: f64,\n    pub order_fraction: f64,\n}\n\n/// NOTE: Reserved for future integration into analysis pipeline\n#[allow(dead_code)]\npub fn project_embeddings_to_fourier_plane(\n    embeddings: &[Vec<f64>],\n    modulus: usize,\n) -> Result<Vec<(f64, f64)>, String> {\n    let p = modulus;\n    if embeddings.len() < p {\n        return Err(\"embedding matrix smaller than modulus\".to_string());\n    }\n    if p == 0 {\n        return Err(\"modulus must be positive\".to_string());\n    }\n    let d = embeddings[0].len();\n    if d == 0 {\n        return Err(\"embedding dimension must be positive\".to_string());\n    }\n    if !embeddings[..p].iter().all(|row| row.len() == d) {\n        return Err(\"embedding rows have inconsistent dimension\".to_string());\n    }\n\n    let tau = std::f64::consts::PI * 2.0;\n    let mut basis_cos = vec![0.0f64; d];\n    let mut basis_sin = vec![0.0f64; d];\n    let scale = 2.0 / p as f64;\n\n    for (token, embedding) in embeddings[..p].iter().enumerate() {\n        let angle = tau * token as f64 / p as f64;\n        let cos_val = angle.cos();\n        let sin_val = angle.sin();\n        for dim in 0..d {\n            basis_cos[dim] += cos_val * embedding[dim] * scale;\n            basis_sin[dim] += sin_val * embedding[dim] * scale;\n        }\n    }\n\n    let basis_cos = normalize_vector(&basis_cos)?;\n    let basis_sin = normalize_vector(&basis_sin)?;\n\n    let mut coords = Vec::with_capacity(p);\n    for embedding in embeddings[..p].iter() {\n        let x = dot(embedding, &basis_cos);\n        let y = dot(embedding, &basis_sin);\n        coords.push((x, y));\n    }\n\n    Ok(coords)\n}\n\n/// NOTE: Reserved for future integration into analysis pipeline\n#[allow(dead_code)]\npub fn verify_embedding_clock_geometry(\n    embeddings: &[Vec<f64>],\n    modulus: usize,\n    config: &EmbeddingClockConfig,\n) -> Result<EmbeddingClockReport, String> {\n    let coords = project_embeddings_to_fourier_plane(embeddings, modulus)?;\n    let p = coords.len();\n    if p == 0 {\n        return Err(\"no embedding coordinates computed\".to_string());\n    }\n\n    let mut radii = Vec::with_capacity(p);\n    let mut angles = Vec::with_capacity(p);\n    for (x, y) in &coords {\n        let radius = (x * x + y * y).sqrt();\n        radii.push(radius);\n        angles.push(y.atan2(*x));\n    }\n\n    let mean_radius = radii.iter().sum::<f64>() / p as f64;\n    if mean_radius == 0.0 {\n        return Err(\"mean radius is zero\".to_string());\n    }\n    let radius_var = radii\n        .iter()\n        .map(|r| (r - mean_radius) * (r - mean_radius))\n        .sum::<f64>()\n        / p as f64;\n    let radius_std = radius_var.sqrt();\n\n    if radius_std / mean_radius > config.max_radius_std_fraction {\n        return Err(format!(\n            \"radius variance too large (std {:.4}, mean {:.4})\",\n            radius_std, mean_radius\n        ));\n    }\n\n    let tau = std::f64::consts::PI * 2.0;\n    let (mut sum_cos, mut sum_sin) = (0.0f64, 0.0f64);\n    for (token, angle) in angles.iter().enumerate() {\n        let expected = tau * token as f64 / p as f64;\n        let delta = angle - expected;\n        sum_cos += delta.cos();\n        sum_sin += delta.sin();\n    }\n    let offset = sum_sin.atan2(sum_cos);\n\n    let mut adjusted_angles = Vec::with_capacity(p);\n    let mut angle_errors = Vec::with_capacity(p);\n    for (token, angle) in angles.iter().enumerate() {\n        let expected = tau * token as f64 / p as f64;\n        let mut adjusted = angle - offset;\n        adjusted = wrap_angle(adjusted);\n        adjusted_angles.push(adjusted);\n        let diff = wrap_angle_signed(adjusted - expected);\n        angle_errors.push(diff.abs());\n    }\n\n    let mean_angle_error = angle_errors.iter().sum::<f64>() / p as f64;\n    if mean_angle_error > config.max_mean_angle_error {\n        return Err(format!(\n            \"mean angular error too large ({:.4} rad)\",\n            mean_angle_error\n        ));\n    }\n\n    let angular_coverage = angular_coverage(&adjusted_angles);\n    if angular_coverage < config.min_angular_coverage {\n        return Err(format!(\n            \"angular coverage too small ({:.4})\",\n            angular_coverage\n        ));\n    }\n\n    let order_fraction = order_fraction(&adjusted_angles, p);\n    if order_fraction < config.min_order_fraction {\n        return Err(format!(\n            \"token ordering too weak ({:.4})\",\n            order_fraction\n        ));\n    }\n\n    Ok(EmbeddingClockReport {\n        mean_radius,\n        radius_std,\n        angular_coverage,\n        mean_angle_error,\n        order_fraction,\n    })\n}\n\n// Helper functions for embedding clock geometry verification\n#[allow(dead_code)]\nfn normalize_vector(vec: &[f64]) -> Result<Vec<f64>, String> {\n    let norm = vec.iter().map(|v| v * v).sum::<f64>().sqrt();\n    if norm == 0.0 {\n        return Err(\"basis vector norm is zero\".to_string());\n    }\n    Ok(vec.iter().map(|v| v / norm).collect())\n}\n\n#[allow(dead_code)]\nfn dot(a: &[f64], b: &[f64]) -> f64 {\n    a.iter().zip(b.iter()).map(|(x, y)| x * y).sum()\n}\n\n#[allow(dead_code)]\nfn wrap_angle(angle: f64) -> f64 {\n    let tau = std::f64::consts::PI * 2.0;\n    let mut wrapped = angle % tau;\n    if wrapped < 0.0 {\n        wrapped += tau;\n    }\n    wrapped\n}\n\n#[allow(dead_code)]\nfn wrap_angle_signed(angle: f64) -> f64 {\n    let tau = std::f64::consts::PI * 2.0;\n    let mut wrapped = (angle + std::f64::consts::PI) % tau;\n    if wrapped < 0.0 {\n        wrapped += tau;\n    }\n    wrapped - std::f64::consts::PI\n}\n\n#[allow(dead_code)]\nfn angular_coverage(angles: &[f64]) -> f64 {\n    if angles.is_empty() {\n        return 0.0;\n    }\n    let tau = std::f64::consts::PI * 2.0;\n    let mut sorted = angles.to_vec();\n    sorted.sort_by(|a, b| a.partial_cmp(b).unwrap_or(std::cmp::Ordering::Equal));\n\n    let mut max_gap = 0.0;\n    for pair in sorted.windows(2) {\n        let gap = pair[1] - pair[0];\n        if gap > max_gap {\n            max_gap = gap;\n        }\n    }\n    let wrap_gap = tau - (sorted[sorted.len() - 1] - sorted[0]);\n    if wrap_gap > max_gap {\n        max_gap = wrap_gap;\n    }\n\n    (tau - max_gap) / tau\n}\n\n#[allow(dead_code)]\nfn order_fraction(angles: &[f64], modulus: usize) -> f64 {\n    if angles.len() < 2 || modulus == 0 {\n        return 0.0;\n    }\n    let mut indexed: Vec<(usize, f64)> = angles.iter().copied().enumerate().collect();\n    indexed.sort_by(|a, b| a.1.partial_cmp(&b.1).unwrap_or(std::cmp::Ordering::Equal));\n\n    let mut good = 0usize;\n    for window in indexed.windows(2) {\n        let current = window[0].0;\n        let next = window[1].0;\n        if (next + modulus - current) % modulus == 1 {\n            good += 1;\n        }\n    }\n    let last = indexed[indexed.len() - 1].0;\n    let first = indexed[0].0;\n    if (first + modulus - last) % modulus == 1 {\n        good += 1;\n    }\n\n    good as f64 / modulus as f64\n}\n\n/// Track training and validation accuracy over time\n#[derive(Debug, Serialize, Deserialize)]\npub struct AccuracyHistory {\n    pub train_snapshots: Vec<(usize, f32)>, // (step, accuracy)\n    pub val_snapshots: Vec<(usize, f32)>,   // (step, accuracy)\n}\n\nimpl AccuracyHistory {\n    pub fn new() -> Self {\n        Self {\n            train_snapshots: Vec::new(),\n            val_snapshots: Vec::new(),\n        }\n    }\n\n    /// NOTE: Currently unused - accuracy tracked directly in main loop\n    #[allow(dead_code)]\n    pub fn add_snapshot(&mut self, step: usize, train_acc: f32, val_acc: f32) {\n        self.train_snapshots.push((step, train_acc));\n        self.val_snapshots.push((step, val_acc));\n    }\n\n    pub fn save(&self, filename: &str) -> std::io::Result<()> {\n        let json = serde_json::to_string_pretty(self)?;\n        std::fs::write(filename, json)?;\n        println!(\"ðŸ’¾ Accuracy history saved to: {}\", filename);\n        Ok(())\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    fn make_circle_embeddings(modulus: usize) -> Vec<Vec<f64>> {\n        let tau = std::f64::consts::PI * 2.0;\n        (0..modulus)\n            .map(|token| {\n                let angle = tau * token as f64 / modulus as f64;\n                vec![angle.cos(), angle.sin(), 0.0, 0.0]\n            })\n            .collect()\n    }\n\n    #[test]\n    fn top_k_frequency_indices_stable_with_ties() {\n        let magnitudes = vec![0.1, 1.0, 1.0, 0.5];\n        let first = top_k_frequency_indices(&magnitudes, 2);\n        let second = top_k_frequency_indices(&magnitudes, 2);\n        assert_eq!(first, vec![1, 2]);\n        assert_eq!(first, second);\n    }\n\n    #[test]\n    fn restrict_signal_zeroes_when_top_k_zero() {\n        let signal = vec![1.0, -1.0, 0.5, -0.5];\n        let filtered = restrict_signal_to_top_k_frequencies(&signal, 0);\n        assert_eq!(filtered, vec![0.0, 0.0, 0.0, 0.0]);\n    }\n\n    #[test]\n    fn exclude_signal_no_change_when_top_k_zero() {\n        let signal = vec![1.0, -1.0, 0.5, -0.5];\n        let filtered = exclude_top_k_frequencies(&signal, 0);\n        assert_eq!(filtered, signal);\n    }\n\n    #[test]\n    fn exclude_signal_zeroes_when_top_k_all() {\n        let signal = vec![1.0, -1.0, 0.5, -0.5];\n        let filtered = exclude_top_k_frequencies(&signal, signal.len());\n        assert_eq!(filtered, vec![0.0, 0.0, 0.0, 0.0]);\n    }\n\n    #[test]\n    fn embedding_clock_geometry_passes_on_clean_circle() {\n        let modulus = 13;\n        let embeddings = make_circle_embeddings(modulus);\n        let config = EmbeddingClockConfig::default_for_modulus(modulus);\n        let report =\n            verify_embedding_clock_geometry(&embeddings, modulus, &config).expect(\"expected pass\");\n        assert!(report.angular_coverage > 0.9);\n        assert!(report.order_fraction > 0.9);\n        assert!(report.mean_angle_error < 0.1);\n    }\n\n    #[test]\n    fn embedding_clock_geometry_flags_wrong_frequency() {\n        let modulus = 11;\n        let tau = std::f64::consts::PI * 2.0;\n        let embeddings: Vec<Vec<f64>> = (0..modulus)\n            .map(|token| {\n                let angle = tau * token as f64 / modulus as f64;\n                vec![(2.0 * angle).cos(), (2.0 * angle).sin(), 0.0, 0.0]\n            })\n            .collect();\n        let config = EmbeddingClockConfig {\n            max_radius_std_fraction: 0.5,\n            min_angular_coverage: 0.1,\n            max_mean_angle_error: 0.2,\n            min_order_fraction: 0.1,\n        };\n        let err = verify_embedding_clock_geometry(&embeddings, modulus, &config)\n            .expect_err(\"expected ordering verification to fail\");\n        assert!(err.contains(\"mean angular error\"));\n    }\n\n    #[test]\n    fn fft_dominant_frequency_passes_on_modulus_structure() {\n        // Create embeddings with strong modulus-113 frequency component\n        // Mix in some other frequencies to get a realistic peak-to-median ratio\n        let modulus = 113;\n        let vocab_size = 114; // 0-112 + equals token\n        let embedding_dim = 128;\n        let tau = std::f64::consts::PI * 2.0;\n\n        let mut embeddings = Vec::with_capacity(vocab_size);\n        for token in 0..vocab_size {\n            let mut embedding = Vec::with_capacity(embedding_dim);\n            for dim in 0..embedding_dim {\n                // 60% of dimensions have modulus frequency, 40% have other frequencies\n                let freq = if dim % 5 < 3 {\n                    modulus\n                } else {\n                    // Use various other frequencies for the remaining dims\n                    20 + (dim % 20)\n                };\n                let angle = tau * freq as f64 * token as f64 / vocab_size as f64\n                    + dim as f64 * 0.1;\n                embedding.push(angle.sin());\n            }\n            embeddings.push(embedding);\n        }\n\n        let config = FFTDominantFrequencyConfig::default_for_modulus(modulus);\n        let report = verify_fft_dominant_frequencies_from_embeddings(&embeddings, &config)\n            .expect(\"expected modulus structure to pass\");\n\n        assert!(report.modulus_frequency_fraction >= config.min_modulus_frequency_fraction);\n        assert!(report.spectrum_entropy <= config.max_spectrum_entropy);\n        assert!(report.peak_to_median_ratio >= config.min_peak_to_median_ratio);\n        assert!(report.top_frequencies.iter().any(|(freq, _)| *freq == modulus));\n    }\n\n    #[test]\n    fn fft_dominant_frequency_fails_on_white_noise() {\n        // Create random embeddings (white noise spectrum)\n        let modulus = 113;\n        let vocab_size = 114;\n        let embedding_dim = 128;\n\n        let mut embeddings = Vec::with_capacity(vocab_size);\n        for token in 0..vocab_size {\n            let mut embedding = Vec::with_capacity(embedding_dim);\n            for dim in 0..embedding_dim {\n                // Pseudo-random values\n                let value = ((token * 17 + dim * 31) % 100) as f64 / 50.0 - 1.0;\n                embedding.push(value);\n            }\n            embeddings.push(embedding);\n        }\n\n        let config = FFTDominantFrequencyConfig::default_for_modulus(modulus);\n        let err = verify_fft_dominant_frequencies_from_embeddings(&embeddings, &config)\n            .expect_err(\"expected white noise to fail\");\n\n        // Should fail on either modulus frequency fraction or spectrum entropy\n        assert!(\n            err.contains(\"modulus frequency\") || err.contains(\"spectrum entropy\")\n                || err.contains(\"peak to median\")\n        );\n    }\n\n    #[test]\n    fn fft_dominant_frequency_fails_on_wrong_frequency() {\n        // Create embeddings with dominant frequency at 57 (not 113)\n        let modulus = 113;\n        let vocab_size = 114;\n        let embedding_dim = 128;\n        let tau = std::f64::consts::PI * 2.0;\n        let wrong_frequency = 57;\n\n        let mut embeddings = Vec::with_capacity(vocab_size);\n        for token in 0..vocab_size {\n            let mut embedding = Vec::with_capacity(embedding_dim);\n            for dim in 0..embedding_dim {\n                // Each dimension has a sine wave with wrong frequency\n                let angle = tau * wrong_frequency as f64 * token as f64 / vocab_size as f64\n                    + dim as f64 * 0.1;\n                embedding.push(angle.sin());\n            }\n            embeddings.push(embedding);\n        }\n\n        let config = FFTDominantFrequencyConfig::default_for_modulus(modulus);\n        let err = verify_fft_dominant_frequencies_from_embeddings(&embeddings, &config)\n            .expect_err(\"expected wrong frequency to fail\");\n\n        assert!(err.contains(\"modulus frequency\"));\n    }\n\n    #[test]\n    fn fft_dominant_frequency_fails_on_flat_embeddings() {\n        // Create constant embeddings (DC component only, no structure)\n        let modulus = 113;\n        let vocab_size = 114;\n        let embedding_dim = 128;\n\n        let embeddings = vec![vec![1.5; embedding_dim]; vocab_size];\n\n        let config = FFTDominantFrequencyConfig::default_for_modulus(modulus);\n        let err = verify_fft_dominant_frequencies_from_embeddings(&embeddings, &config)\n            .expect_err(\"expected flat embeddings to fail\");\n\n        assert!(err.contains(\"modulus frequency\") || err.contains(\"peak to median\"));\n    }\n\n    #[test]\n    fn fft_dominant_frequency_fails_on_weak_peaks() {\n        // Create embeddings with many frequencies at similar magnitudes (no clear peaks)\n        let modulus = 113;\n        let vocab_size = 114;\n        let embedding_dim = 128;\n        let tau = std::f64::consts::PI * 2.0;\n\n        let mut embeddings = Vec::with_capacity(vocab_size);\n        for token in 0..vocab_size {\n            let mut embedding = Vec::with_capacity(embedding_dim);\n            for dim in 0..embedding_dim {\n                // Mix many frequencies with similar amplitudes\n                let mut value = 0.0;\n                for freq in 1..10 {\n                    let angle =\n                        tau * freq as f64 * token as f64 / vocab_size as f64 + dim as f64 * 0.01;\n                    value += angle.sin() / 10.0;\n                }\n                embedding.push(value);\n            }\n            embeddings.push(embedding);\n        }\n\n        let config = FFTDominantFrequencyConfig {\n            modulus,\n            min_modulus_frequency_fraction: 0.15,\n            max_spectrum_entropy: 0.85,\n            min_peak_to_median_ratio: 2.0,\n        };\n        let err = verify_fft_dominant_frequencies_from_embeddings(&embeddings, &config)\n            .expect_err(\"expected weak peaks to fail\");\n\n        assert!(\n            err.contains(\"modulus frequency\") || err.contains(\"spectrum entropy\")\n                || err.contains(\"peak to median\")\n        );\n    }\n}\n","structuredPatch":[{"oldStart":454,"oldLines":6,"newStart":454,"newLines":8,"lines":["         }","     }"," ","+    /// NOTE: Currently unused - loss tracked directly in main loop","+    #[allow(dead_code)]","     pub fn add_snapshot(&mut self, step: usize, train_loss: f64, val_loss: f64) {","         self.train_snapshots.push((step, train_loss));","         self.val_snapshots.push((step, val_loss));"]}],"userModified":false,"replaceAll":false}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01WjWwEw6kbqTY67Hh3v3UUp","type":"message","role":"assistant","content":[{"type":"text","text":"Perfect! Now let me verify all warnings are cleaned up:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":641,"cache_read_input_tokens":86294,"cache_creation":{"ephemeral_5m_input_tokens":641,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"d8ba85ba-952a-4704-adf4-81a03d70565a"}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01WjWwEw6kbqTY67Hh3v3UUp","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01TtRHjeDTwtzCragVHzYssG","name":"Bash","input":{"command":"cargo build 2>&1 | grep \"warning:\" | wc -l","description":"Verify no more warnings","timeout":120000}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":641,"cache_read_input_tokens":86294,"cache_creation":{"ephemeral_5m_input_tokens":641,"ephemeral_1h_input_tokens":0},"output_tokens":120,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"be4437da-5ef6-4c85-8bdf-39e45fce81d0"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01TtRHjeDTwtzCragVHzYssG","type":"tool_result","content":"       0","is_error":false}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"a6b2b21e-b79c-4163-a240-735e5d0b0701","tool_use_result":{"stdout":"       0","stderr":"","interrupted":false,"isImage":false}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01GLpLgqy6baxsw8QAaUoQA8","type":"message","role":"assistant","content":[{"type":"text","text":"Excellent! All warnings are gone. Let me verify the tests still pass:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":134,"cache_read_input_tokens":86935,"cache_creation":{"ephemeral_5m_input_tokens":134,"ephemeral_1h_input_tokens":0},"output_tokens":8,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"163547de-beb1-41b7-81a4-cb7fdb28b745"}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01GLpLgqy6baxsw8QAaUoQA8","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01URSDi8JHuYA3CKFY9SCnFH","name":"Bash","input":{"command":"cargo test --lib 2>&1 | tail -3","description":"Verify tests still pass","timeout":120000}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":134,"cache_read_input_tokens":86935,"cache_creation":{"ephemeral_5m_input_tokens":134,"ephemeral_1h_input_tokens":0},"output_tokens":8,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"71fd49d0-e99c-43b3-a5e4-1e1eb147ecf5"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01URSDi8JHuYA3CKFY9SCnFH","type":"tool_result","content":"test result: ok. 58 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out; finished in 0.23s","is_error":false}]},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"3a3a79b0-678e-4a26-b6fe-d8860aef5607","tool_use_result":{"stdout":"test result: ok. 58 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out; finished in 0.23s","stderr":"","interrupted":false,"isImage":false}}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_018CvW8tg9sJFMizifdfPr8X","type":"message","role":"assistant","content":[{"type":"text","text":"Perfect! All tests still pass. Now let me commit the changes and close the task:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":170,"cache_read_input_tokens":87069,"cache_creation":{"ephemeral_5m_input_tokens":170,"ephemeral_1h_input_tokens":0},"output_tokens":5,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"34468469-e384-4bbe-938a-d9f86a8d9098"}
{"type":"assistant","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_018CvW8tg9sJFMizifdfPr8X","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01LU34SS4Y3KW2A3FKZdwpex","name":"Bash","input":{"command":"git add -A && git commit -m \"MechInt-7ab: Clean up dead code warnings with allow(dead_code) annotations\n\n- Reduced compilation warnings from 59 to 0\n- Added #[allow(dead_code)] to verification functions reserved for future integration\n- Added #[allow(dead_code)] to report structs and config helpers\n- Added #[allow(dead_code)] to alternative visualization and checkpoint functions\n- All functions preserved for future use with clear documentation\n- All 58 library tests pass\n\nCategories cleaned:\n- Analysis module: FFT verification, embedding geometry functions\n- Verify module: Snake curve, restricted/excluded loss, MLP wave analysis\n- Plotting module: Alternative visualization approaches  \n- Checkpoint module: Legacy checkpoint functions\n- Export module: JSON loading utilities\n\nCo-Authored-By: Claude Sonnet 4.5 <noreply@anthropic.com>\"","description":"Commit dead code cleanup"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":170,"cache_read_input_tokens":87069,"cache_creation":{"ephemeral_5m_input_tokens":170,"ephemeral_1h_input_tokens":0},"output_tokens":5,"service_tier":"standard"},"context_management":null},"parent_tool_use_id":null,"session_id":"52fc7a89-5de8-4326-8db0-e9de302daa45","uuid":"ff3121e8-4c3e-4552-bb12-c5961a507c58"}
